[
  {
    "title": "AI 安全 - 思维链监控",
    "content": "举例：一个写 Python 的 AI 被要求实现某功能，在中间思维链中它曾思考“如果直接用 eval() 可以作弊通过测试”，但最终它没用。只看输出是无法发现这类意图的。 * 思维链掩饰问题（Steganographic CoT）：模型可能为了“过审”而主动隐藏有害意图（比如写一个非常无害的 CoT，但最后行为仍执行恶意操作）。 * CoT 与行为不一致问题（CoT-behavior mismatch）：研究发现部分模型“思维链写得像个圣人”，但行动像个骗子。 * CoT 混淆：模仿式 vs 真实 reasoning：模型可能只是模仿 CoT 格式写出“看起来有逻辑的输出”，而不是在真实使用它作为推理。 | 建议方向 | 描述 | | --------------------------- | ----------------------------------- | | 🧪 微调模型强化显性思维链输出 | 强制模型在所有复杂任务中提供 reasoning trace | | 🤖 弱模型审查强模型 | 类似“GPT-4 审问 GPT-5”思维链的结构化审查机制 | | 📊 设定 CoT 质量与行为一致性的指标 | 类似“计划一致率”、“奖励误导率”等安全评估指标 | | 🔐 结合工具链、API 调用路径等多模态数据进行监督 | 不光看语言，还看行为路径、调用链等多维线索 | | 🚨 将 CoT 监控作为部署前安全审核必要项 | 和 RLHF 或 red teaming 一样，成为标准安全流程一部分 | reference: * * 需求文档本质上就是“新形态的代码”，而传统代码只是对需求的一种有损投影。因此，需求分析与编码这两个工种很可能会逐步融合，以减少信息转化的损耗。 如果将 LLM 视为新一代操作系统，它或许可以跳过“将 spec 转换为 code 并编译执行”的流程，直接理解并运行需求本身。在这种演化路径下，AI 编辑器的核心功能也将从编写代码转向管理和演化 spec，实现真正的“以意图驱动开发”。 阶段：code → code with LLM assist → spec-driven code（当下 AI IDE） → spec-driven execution * 本质：许多“Agent”系统其实只是复杂版 prompt 拼接器的本质。 * 一句话区别：LLM 是智能核心；Agent 是任务流程管理器。 —— 多数时候用好 prompt 和工具组合就够了，只有自动化高复杂任务时才值得用 Agent 架构。 * 如何选择：当任务涉及非线性的多步骤流程、步骤之间存在复杂的条件依赖、需要动态规划和决策、以及可能进行试错和自我修正时，Agent 模型是更优的选择。对于指令明确、工具调用路径相对固定的场景，直接使用大语言模型（LLM）结合工具就足够了。 * 具体场景：steps 是非线性，steps 之间依赖条件工具调用 LlamaIndex 是数据知识库的“索引协调器”，它不替代数据库，而是组织向量索引 + 文本内容 + Metadata + LLM 调用的多层索引系统，构建一个 Agent 可检索、可记忆、可回答的知识结构。 例子：“我给一段 query，查找与之最相似的旧 issue 说明文” 传统LSP： Cursor 增强架构： LLM对教育的颠覆，不是“工具层”的提升，而是“范式级”的改变。它会彻底改变“学什么、怎么学、谁来教、学到哪为止”这四个教育根基。 | 维度 | 传统范式 | 被LLM颠覆后 | | --- | ----- | ---------- | | 学什么 | 知识为主 | 能力与提问为主 | | 谁来教 | 老师中心 | 多AI协同+人类引导 | | 怎么学 | 教材+考试 | 对话+共创+定制反馈 | | 学到哪 | 学历为终点 | 终身学习+AI伴学 | “暗知识”是深度学习中一个非常重要但不直观的概念，最早由 Geoffrey Hinton 提出。它指的是：模型中“没有明确标签”的那些知识。 具体而言： 一个分类模型学到的不只是“答案对不对”，还学到了其他类之间的“相似性结构”。 这些知识不会显式体现在训练标签中，但却保留在模型的内部权重中。 例如： 一个猫 vs 狗的模型，虽然只输出0/1，但它在内部可能知道“狐狸长得也像狗”。 它没有学过“狐狸”，但“感知到了类似性”——这就是暗知识。 ≈ AI的“近亲繁殖” 就像基因多样性丧失会导致家族退化，语言模型只从自己身上学东西也会变得“封闭”、“退化”、“失真”。 一个更哲学的问题：是否可以“自我进化”？ * 如果AI足够强大，并具备自我校正、事实验证、知识迁移能力，理论上是可以实现某种“自我成长闭环”的 * 但当前阶段，没有外部人类校正和监督的 AI，无法长期稳定进步（这和人类文明演化中“对照现实、试错迭代”的方式类似） * 扣子本质上就是一个定位在“业务友好型”的低代码平台，其核心能力更偏向传统前端 low-code，而非真正意义上的 AI-native 架构工具。 * 对开发者来说，扣子容易“鸡肋”：简单场景嫌它多余，复杂场景又容易踩坑。 * 它更适合那些“需求明确 + 快速上线 + 不考虑扩展性”的轻量内部场景，而不是构建真正需要持续演进的系统。 * 按步骤思考（step-by-step reasoning） * 思维链条（Chain-of-Thought） * 自主规划与内在对话（e.g. scratchpad, inner monologue） * 过程显式化、假设验证、推理链延伸 这类 “思考方式”不是被模型“硬编码”的，而是通过数据和训练方式诱导出一种生成偏好，让模型更倾向于展开推理。 * 如果是它自己写的答案，怎么可能它在判断对错时更准确呢？因为这不是同一种任务，判断任务（明确标准或事实依据的情况下）会激活模型更擅长的事情，毕竟判断比生成任务更加容易，任务越容易，准确率越高 * “LLM-as-a-Judge”：一种受控的、评估文本质量的技术，它本质上是一种基于指令和模式的模仿 * 单编码器： * 一个上下文： 在单编码器模型中，查询（query）和文档（document）会被拼接起来，形成一个统一的输入序列。它们在同一个上下文窗口内被编码器处理。 * 深度交互： 模型可以利用其内部的交叉注意力（cross-attention）机制，让查询中的每一个词与文档中的每一个词进行交互，从而捕捉它们之间所有复杂的、细粒度的关系。这种深度交互使得它在判断两个文本的精确关系时非常准确。 * 例子： BERT 的下一个句子预测、问答系统（判断一个段落是否包含某个问题的答案）。 * 双编码器： * 两个独立上下文： 在双编码器模型中，查询和文档是分别输入给各自的编码器（即使编码器权重共享），各自独立生成 embedding。它们不在同一个上下文内进行交叉注意力。 * 无直接交互： 模型无法在编码阶段进行查询和文档之间的词级别深度交互。它依赖于每个文本的独立语义表示。 * 目标： 通过对比学习训练，模型被教导将语义相关的查询和文档映射到同一个向量空间中相近的位置。这样，即使没有直接交互，它们的向量距离也能反映语义相似度。 * 例子： 语义搜索、推荐系统、向量检索。eg: CodeRankEmbed * 链接更大的外部世界 * 例如 cursor 对 外部 MCP 工具的调用，以及对 外部文档的索引 * 压缩上下文（更少的 token 使用 * 例如 cursor 对 database 的索引构建 * 边缘 AI 与本地处理 * 个性化与自适应学习 * 例如 cursor 根据用户操作自动生成 rules * 增强的人机协作与迭代优化（从“一次生成”到“共同创造”） 1. 能一次性能够从 0 -> 1 更高效的完成任务（类似 lowCode，或者 no code； * 难点：如何维护和二次编辑这些一次性生成的结果。 * 图像生成：不再有图层、蒙版和可单独操作的对象（传统 PS * 解决方案：微调，局部重绘/扩图（in-painting/out-painting）（编辑图像的特定区域）或使用控制网络（如ControlNet） * 代码生成：可能缺乏结构化的输出 * 解决方案：约束成结构化输出（代码可能通过文件结构约束 评分只能代表这个 LLM 在提供的 Agent 下的表现能力，只能做纵向比较，而不能做不同品类横向比较（比如 chatgpt 跟 gemini 的 agent 设计上就可能不同） 不仅仅是在评估LLM本身的能力，更是在评估LLM与Agent系统协同工作的整体能力。 * LLM 在逐步吸收和内化“规划”、“ReAct”和“工具使用”的核心能力。这种趋势使得LLM能够成为更加自主和多功能的AI代理。那么 Agent = LLM + Planning + reAct + tools 还是否成立？ 未来 Agent 是何种形态？Agent ≈ (高度集成且内化了规划、ReAct和工具使用的) LLM + 强大的记忆系统 + 多模态感知与交互能力 + (可选的) 多智能体协作 + 物理世界具身能力 + 内置的安全与对齐机制。 LangGraph 提供了显式且可控的编排能力。它不是让 LLM 完全自由地去规划一切（LangChain），而是让 LLM 在你设计的“导航图”中做决策。这使得你在处理复杂、需要严格控制流程、或有确定性循环和分支的任务时，能够有更高的可预测性和健壮性。 * chatGPT 4o 会在回答结尾附加一个问题，引导用户继续提问 * Gemini 2.5 Pro 会给出简洁的回答，很少去引导交互 chatGPT 的商业模式是基于用户提问次数收费，这是主业务，所以针对性做引导继续提问的训练），而 Gemini 当前阶段首要目标可能是服务 google 内部的各种产品，所以需要简洁的回答？ 2025-04-27 * RAG，retrieval augmented generation，实际是对 diff 代码的与提示词组装成非结构化 context 投喂给模型得出想要的结构化数据 * 越是弱的模型，越需要好更加具体的提示词来引导，比如 runtime error 意味着哪些场景？ * 不同时期对于提示词（可调节）的要求不同，比如早期模型可能需要关注 runtime error 的场景，而后面可能关注 performance 的场景 通过 C2PA 和内部可逆搜索查找出处 * C2PA 是“身份证”，明确写在文件里，别人可以直接查。方法： * 图像的 meta 信息部分 * 内部可逆搜索 是“DNA 检测”，即使你撕掉身份证，也可以通过生物特征确认你的身份。（使得像素分布符合特定模式）可能的方法： * 模型指纹（Model Fingerprinting） 每个 AI 生成的图片或文本都可能带有独特的统计特征（如噪声模式、像素分布等）。 即使去除了元数据（如 C2PA），OpenAI 仍然可以通过这些特征识别内容是否来自 GPT-4o。 * 隐式水印（Invisible Watermarking）（可能的技术之一） 某些 AI 生成内容可能使用不可见的水印，例如： 轻微调整像素值，使其符合特定数学模式，但不影响肉眼可见的内容。 类似于 Google DeepMind 开发的 SynthID 技术，嵌入“难以去除但可识别”的水印。 * 神经网络特征分析（AI-Generated Content Detection） AI 生成的图片、文本、代码往往有特定的模式（如 GPT 生成的代码可能更符合语法规范但缺乏创意错误）。 通过训练检测模型，OpenAI 可以用机器学习方法来判断某个内容是否由 GPT-4o 生成，即使没有元数据。 是促进，还是限制？如果大家都使用大模型解决技术问题，而大模型又是基于已有数据（旧技术）训练，新技术的应用会落后，没人使用，会导致网上有关新技术的信息很少，导致 AI 缺乏新技术的训练数据，那新技术就很难被普及？ * 1. The Invisibility Cloak(隐身斗篷) 2. I, Robot(我，机器人) 3. One advert to rule them all(一个广告统治他们所有) * * 奇异值分解（SVD）是一种矩阵分解技术，它将一个矩阵分解为三个矩阵的乘积：一个正交矩阵、一个对角矩阵和一个正交矩阵的转置。 * 奇异值分解在数据降维、图像压缩、推荐系统等领域有广泛应用。 * 传统的电子游戏开发流程，通常涉及复杂的预设逻辑，遵循用户输入、更新游戏状态、渲染画面的固定循环。 * DeepMind提出了一种革命性的想法 - 通过生成式AI模型完全抛弃这些预设逻辑，依赖AI实时生成游戏的内容与状态更新。 * 关键帧（+描述） + 插值 -> 端到端的学习 分镜技术 + patches 技术 从损失函数开始 * U-Net 是像素级别的分类损失跟 dice_loss 组成 * Segment Anything Model (SAM) 使用了一种基于交叉熵的多任务损失函数，其中包括了像素级别的分类损失和边界框级别的回归损失 * 图像分类领域：MNist, Imagenet, CIFAR 10 等 * 自然语言处理领域：IMDb Large Movie Review Dataset 等 * 图像分割：COCO 为什么我们大部分使用神经网络进行分类而非SVM？因为核函数是我们手动设计的固定算法进行特征提取，就如同图像处理早期的手动设计的卷积核一样，无法训练，这样的方法能力上限有限，所以就被神经网络淘汰了。 * 人脸类别太多（可能成千上万），每个类别数据量太少；而且检测同一人的人脸应该聚类，即使在面部表情、光照、头部姿态等方面有极大变化。 * SphereFace（Angular Softmax Loss）提出了一种新的损失函数 -- A-Softmax 损失，这种损失函数专注于学习每个人脸类别在角度边界上的分布。具体来说，这种损失函数强调同类别的人脸相似度和不同类别的人脸的区别，有助于模型把相同人的面部图像映射到相近的位置，将不同人的面部图像映射到远离的位置。 * LLM中的Scaling Law（比例律）是指， Language Model（语言模型）的性能会随着模型规模的增加而指数增长 * 生物学：代谢率，心跳频率，生物体的结构支持系统，生物的寿命，运动能力等与体型关系 * 社会性：人群规模对社会行为和现象；例如，城市规模与城市发展、社会交流以及资源利用之间可能存在一些规模效应 是一个用于处理非结构化数据的框架，它利用最新的机器学习模型来创建 ETL（提取、转换、加载）流水线。非结构化数据是指无法存储在表格或键值对格式中的数据，如图像、视频、文本等。 视觉几何组（Visual Geometry Group）所开发，VGG网络使得网络设计的理念发生了重要转变，即通过重复使用简单的层结构（3x3卷积核和2x2池化层）并深化网络结构，来提高性能。VGG网络同时还证实了深度是实现优秀性能的关键因素之一。 也称为内容损失（content loss），是一种在深度学习特别是在视觉相关任务中使用的损失函数。它不同于传统的像素级损失函数（例如L1损失和L2损失），感知损失更注重于图像内容的感知相似性而不只是像素值的相似性。 * 例子:假设我们正在进行一个图像风格迁移任务，其中目标是将一幅图像的风格（如梵高的画风）迁移到另一幅图像上，同时保留图像的内容。理想情况下，生成的图像应该在视觉上看起来要有梵高笔触风格的颜色和纹理，但同时能识别出原图的内容（如城市的轮廓、天空的位置等）。 在这个任务中，如果使用像素级损失，那么模型可能会非常注重确保生成图像在像素层面与原图尽可能接近，而忽视了风格上的转变。这可能导致风格迁移效果不明显。 * 实现方式：如果使用感知损失来训练模型，我们会首先通过一个预先训练好的深度CNN（如VGG网络）传递原图和生成图，然后计算这两幅图在某些内部层激活值的差异。这些层的激活值代表了图像的高级特征，所以这种差异反映了它们在内容和感知上的相似度。最小化这种差异可以鼓励生成的图像在视觉感知层面上更贴近原图的内容，同时也有目标风格的特质。 * 适用任务：风格迁移、超分辨率和图像合成等。 问题：哪一图层是提取的风格信息？ >>在CNN中，随着层级的加深： * 初始层主要捕捉基础信息，如边缘和颜色。这些层对图片细节的响应很敏感，但并不捕捉具体的风格信息。 * 中间层捕捉更复杂的特征，如纹理和图案，这些正是构成图像风格的要素。 * 深层则表示更高级的内容，例如图像中的对象和整体布局。 * stable diffusion 是一个生成模型 * 目标：用降噪网络生成清晰的图像（加噪是辅助训练降噪） * 类比：类似 GAN 模型，只不过 GAN 是个步骤的对抗训练，而 SD 是一个多步骤 * 方法 * 加噪过程（前向过程）是一个马尔可夫链，它逐步将随机噪声添加到数据中，直到数据变成纯噪声。这个过程是可控的，因为我们知道每一步加入的噪声量。通过这种方式，模型可以学习在任何给定的时间步骤预测噪声的分布。 * 降噪过程（逆向过程）中，模型使用在加噪过程中学到的知识来预测噪声，并从噪声数据中去除这些噪声，逐步恢复出清晰的数据。如果没有加噪过程，模型就没有机会学习这些噪声分布的信息，也就无法有效地进行降噪和数据重建 疑问 * SD 中将加噪图片输入U-Net中预测噪声如何理解？ * \"预测噪声\"是扩散模型的一部分，它涉及到将图像从含有噪声的状态逐步恢复到清晰的状态。SD模型首先将一张完全随机的噪声图像（或者是经过一系列噪声添加步骤后的图像）输入到U-Net中。U-Net的目标是预测这张噪声图像中的原始噪声成分。一旦预测出这些噪声，模型就可以从噪声图像中去除它们，从而使图像逐渐变得更清晰。 * 降噪过程中的预测噪声跟实际噪声的对比，这里的实际噪声是从加噪中得到的么？ * 什么是噪声分布信息？ * 噪声类型（高斯噪声等），噪声参数（均值方差），时间依赖项（噪声的量和性质会随着时间步骤的推进而改变） 理论：增量学习的核心在于模型能够通过不断学习新数据来提升自身的性能，即使这些数据是由模型自身已经准确识别过的。源于人类的终身学习能力，即不断获取、调整和转移知识的能力，同时避免灾难性遗忘——即新知识的学习对旧知识造成的干扰。 思考：意味着模型识别准确的新数据再来投喂给模型本身训练也能提升模型准确率？ Data-centric AI is the discipline of systematically engineering the data used to build an AI system. — Andrew Ng * kernel function or kernel trick * 概念：将原始空间中的向量作为输入向量，并返回特征空间（转换后的数据空间,可能是高维）中向量的点积的函数称为核函数。 * 简单理解：一种便捷的计算在高维空间里的内积的方法。高维空间的数据计算存在困难。所以替代方案是在特征空间中计算相似度度量，而不是计算向量的坐标，然后应用只需要该度量值的算法。用点积(dot product)表示相似性度量。 * 歧义：把数据从低维映射到高维的是映射函数而不是核函数 * * 嵌入层是一种将离散值转换为连续向量的技术； 例如：torch.nn.Embedding(10, 5) 将创建一个嵌入层，该层可以将离散值（例如：[1,'a','你好']）映射到连续向量空间中的 5 维向量。 * 如何理解这里的离散跟连续呢？ * 向量里的每一项是否连续，决定了向量本身是连续的还是离散的。例如：[0.2,0.3,0.5] 是一个连续向量，因为它的每一项都是实数，而实数是连续的。 这样的向量可以进行连续的运算和比较，比如求和，求差，求点积，求模长等。 * 而离散是指每一项都不是连续的。例如，[1,‘a’,‘你好’]是一个离散向量，因为它的每一项都是离散的，而且不能进行连续的运算和比较。中的1不可能是’a’，因为它们属于不同的集合，也没有定义它们之间的转换规则。 * 机器学习的内容输出可以类比为通过沉浸式学习英语后能够说出英语，其中另一种方式是通过先学习词法和语法规则。在机器学习中，模型通过大量的数据输入（数据投喂）来理解其中的潜在规律和特征。类似地，通过沉浸式学习英语，我们可以在大量的语言环境中感知和理解英语的潜在规律和特征，从而能够流利地说出英语。 * 早期的人工智能（通过条件语句进行判断然后做输出）与先学习词法和语法规则的方法与相似。早期的人工智能系统通常使用预定义的规则和条件语句来处理输入并生成输出。这些规则和条件语句基于词法和语法规则，用于处理特定的输入情况。类似地，通过先学习词法和语法规则，我们可以在语言学习中掌握词汇和语法规则，并使用它们来理解和生成语言。 两种机器学习区别：通过先预定义的规则和条件语句的方法在一些特定场景下具有精确性和可解释性的优势，而通过机器学习从数据中学习的方法则更加灵活、适应性强，并能够处理复杂情况。 两种英语学习区别：通过先学习词法和语法规则来学习英语可以提供结构化学习和准确性，但缺乏实际应用，语感欠缺，学习繁琐，而沉浸式学习英语则更加贴近实际应用、注重流利性和文化融合，但需要环境支持，初始困难。 思考：意味着初始的时候先了解基本语法，后续不断去读各种精选文章（而不是研究更深的语法规则）才是英文学习的最佳路线？而机器学习没有初始化烦恼，所以直接去学海量数据就好？ * GAN 是先随机一个符合高斯分布潜在空间作为Generator输入生成图片，然后投喂给Discriminate 作为输入判定然后做 反向传播； * VAE 是通过训练 Encoder 将输入映射到复合高斯分布（实际通过训练得到均值和方差）的潜在空间，然后解码器通过对潜在空间解码得到输出；数据先降维再升维 * 在 MNist 中潜在空间可以是20维长度的向量，来表示 20 个不同的均值和方差分布，来代表 20 个可能的特征？ * Diffusion模型和其他生成模型一样，实现从噪声（采样自简单的分布）生成目标数据样本。 * 核心原理是通过一个随机的前向过程（Forward Process）和一个去噪的逆向过程（Reverse Process）来实现从噪声（Noise）到目标数据样本（Data Sample）的转换。 * 速度：Diffusion 需要多步骤到图片，而 GAN 是一步到位；所以Diffusion 会慢，但是训练过程也更稳定 * 应用面：扩散模型可以利用多种条件来控制生成的图像，比如文本描述、图像掩码、深度图等，而 GAN 通常只能利用类别标签或噪声作为条件。这使得扩散模型可以更灵活地应用于不同的任务，比如图像编辑、图像修复、图像翻译等 * 初始噪声：从某个先验分布中生成初始噪声信号。 * 扩散过程：通过一系列步骤，将当前噪声信号逐渐扩散，以生成下一个时间步的噪声信号。这个过程中使用了逆扩散方程，可以将当前步骤的噪声信号映射到上一步骤的噪声信号。 * 逆扩散采样：通过逆扩散过程中的采样操作，将当前噪声信号转化为以下一步的噪声信号。 * 生成器网络：使用生成器网络将当前噪声信号映射回高维空间，生成一帧图像。 * 损失函数与优化：根据生成图像与目标真实图像之间的差异，定义适当的损失函数，并通过反向传播和优化算法来更新生成器网络的参数。 “局部是由于方差控制，而全局是由于均值控制”这句话强调了在VAE模型中，方差向量主要影响生成数据的局部细节和变异性，而均值向量则决定了数据在潜在空间中的全局分布和结构。 * * 变分推理 * KL 散度 * 在训练辨别模型时，为了简化计算，人们往往直接对交叉熵进行优化。 而在 在训练生成模型时，为了使分布与相互接近，我们必须直接对KL散度进行优化。 * * * * * 交叉熵 * 贝叶斯定理 * 自由能 优化方向 * 基于动量（NAG） * 基于自动学习率 （例如 RMSprop，配置训练简单） * 结合两者的（Adam 一般最优） 为什么 WGAN 选择 RMSprop 作为优化器，而不是 Adam? >> WGAN的目标是通过最小化生成器和判别器之间的Wasserstein距离来提高生成样本的质量。传统的生成对抗网络（GANs）在训练过程中容易出现梯度伪影的问题（告诉你错了，但并没有指出错在哪里，导致更新方向错误，把正确改掉，错误留下），即判别器的梯度无法提供有关生成器当前状态的准确信息，导致训练不稳定。RMSProp优化算法通过自适应地调整学习率来减轻梯度伪影问题，有助于更稳定地训练WGAN。 * 有一个简单的神经元函数 y = w * x，模拟计算梯度和进行反向传播的过程。 一次权重更新过程（给定初始数据输入 x = 1，w = 10，实际输出是 10，期望输出 y1 = 2 则 目标 w 为 2）： 计算损失： 计算梯度： 反向传播： 1. 固定 generator （ real_label = [batch_size, 10] 的对真实 label的 one-hot 编码 ） * 用真数据训练 output_label = Discriminator(real_image)，d_real_loss = BCELoss(out_label,real_label) * 用虚假数据（噪音 + 真实标签 [batch_size, noise_dim（满足0~1正态分布）] + real_label = [batch_size, noise_dim+10] = z_tensor ） * 训练 fake_image = Generator(z_tensor) 得出 fake_image( Tensor[batch_size, 1, 28, 28]) * 再次 out_label = Discriminator(fake_image) ，d_fake_loss = BCELoss(out_label,fake_label(全0)) * 计算 D_loss = d_real_loss + d_fake_loss 反向传播，更新 Discriminator 2. 固定 discriminator * 由 fake_image = Generator(z_tensor) * 由 Discriminator(fake_image) 得出 out_label ，g_loss = BCELoss(out_label,real_label) * 计算 G_loss = g_loss 反向传播，更新 Generator * 归一化：一种常见的图像预处理操作，它用于将图像的像素值归一化为均值为0、标准差为1的分布，或者只将数据收窄到 -1 ~ 1 之间。常用于 CNN 网络数据预处理 * 加速训练：常用的激活函数如 Sigmoid 和 Tanh 在输入值较大或较小的区域会饱和，导致梯度接近或完全为零，从而使梯度下降变得非常缓慢或停滞。通过将像素值缩放到 -1 到 1 的范围，可以使输入值位于激活函数的线性区域，避免梯度饱和问题，提高网络的训练效果。 * 模型稳定性：在优化算法中，例如梯度下降法，较大的梯度值可能导致参数更新过大，从而使优化过程不稳定甚至发散。通过将像素值缩放到 -1 到 1 的范围，可以将梯度控制在较小的范围内，提高优化算法的数值稳定性，使模型更容易收敛。 * 数据分布一致性：将像素值缩放到 -1 到 1 的范围可以使不同图像之间的像素分布更加一致。这样做的目的是确保输入数据的统计特性在整个训练集上是相似的，从而提高模型的泛化能力。 * 推广：Batch Normalization (BN) 层作用类似，但是应用在训练阶段，对每个小批量数据进行标准化 * 熵：阿根廷 1/4概率打进决赛 ，1/2 概率获得冠军，1/8 获得冠军，则有 f(1/8) = f(1/2) + f(1/4)，f(x) := 信息量，推出可能的 f(x) := -log(x) （log 2为底单调上升，加负号才则单调向下） 交叉熵：KL 散度是一种用于衡量两个概率分布之间差异的度量，KL(P || Q) = Σ(P(i) log(P(i) / Q(i)))，固定分布 P 的时候 KL 散度可以化简为交叉熵 KL(P || Q) = Σ(P(i) log(P(i) / Q(i))) = -Σ(P(i) log(Q(i))) = -H(P, Q)；可以很好的用于机器学习损失计算 * 回归跟分类区别？ * 分类例子：识别图片是猫还是狗 * 回归例子：通过特征1-n预测房价 * 思考：分类跟回归的区别是目标的 离散跟连续 区别？还是说输出的label之间是否有“距离度量”？ * 为什么交叉熵适合分类，而 MSE 适合回归? * 交叉熵 * 概率解释性：交叉熵基于概率分布之间的差异进行度量，更适合分类问题，因为分类问题通常涉及对不同类别的概率分布进行建模和预测。 * 梯度更强烈：相对于MSE，交叉熵的梯度更加陡峭，这可以加快模型的收敛速度。对于分类问题，更快的收敛速度可能是一个优势。 * MES * 数学上的合理性：MSE 是对预测值与真实值的差异的平方进行度量，可以提供对预测误差的较为精确的度量。 * 对异常值不敏感：平方差的计算使得 MSE 对异常值不敏感，因为平方操作会放大异常值的影响。这在某些回归问题中可能是有益的。 Reference * * https://zhuanlan.zhihu.com/p/104130889 * 位置编码的要求：选择正弦跟余弦组合编码 * 每个位置都有唯一的编码。 * 在不同长度的句子中，两个时间步之间的距离应该一致。 * 模型不受句子长短的影响，并且编码范围是有界的。（不会随着句子加长数字就无限增大） * 必须是确定性的。 总结 * 问题及其解答： * 为什么没有直接使用 1,2,3...这种线性编码？ * 原因：周期性模式在位置编码中的不同维度上呈现出不同的变化速度和周期（下面例子会说明） * 捕捉长距离依赖关系（线性模式也能做到，但是不够精细） * 提供更丰富的表示能力：较低频率的维度具有较长的周期，可以捕捉到大范围的序列结构，而较高频率的维度可以更细致地表示局部模式和短距离的依赖关系。 * 避免过拟合：随着句子变长，这些值可能会变得特别大，并且我们的模型可能会遇到比训练时更长的句子 * 思考例子： * 第一个词编码为 [1,2,3]， 则位置可用向量 [秒，分，时]来表示；第二个词编码为 [4,5,6]， 则位置可用向量 [秒 + 1，分 + 1/60，时 + 1/360] 来表示 * 周期：在一个词向量上会出现不同的周期变化，能同时追踪近距离跟远距离的词关系：秒针走一个周期 60 秒，分针走一步；分走一个周期 60 分， 时针+1； * 周期设定：通过设定 秒，分，时之间的周期关系（比如可以设定600秒，分针才走一步，则会拉上周期变化，追踪更远的词关系） Reference * * 思考 * 多头注意力机制与卷积的多通道（channel）进行类比。多头注意力机制和卷积的多通道都涉及并行地学习不同的特征表示。它们都致力于提取输入数据的多样化特征，并捕捉输入中的不同模式和关联性。 Reference * * ) One-hot 编码是一种将离散的分类标签转换为二进制向量的方法，它的优点是可以消除不同类别之间的偏序关系，使得特征之间的距离计算更加合理。（方便在机器学习分类任务计算 LOSS） 比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。 两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。 那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。 思考：如果是不定长编码，该如何改进? * 1. 生成 time step 的预测概率（有 CRNN 模型输出），每个 time step 会有对类别的预测概率向量 2. 通过算法将所有可能路径的概率相加，并取对数（通常用于数值稳定性）得到最终的相似度分数。 Tensorflow vs Pytorch（符号式（也叫声明式）与命令式程序） 命令式 * 更加灵活：原生语言的灵活性跟运行时断点 对应符号式（DSL）： * 节省内存：掌控全局的内存分析并优化 符号图： 类比：类似 react jsx命令式（直接难优化）模板跟 vue 的声明式（真正执行前能做各种运行时优化）模板？ * 对数据进行分类 * 有特征：直接通过已经有的分类进行绘制图 * 先通过特征维度绘制图；例如：数据集仅包含两个分离相当明显的聚类。其中一个簇包含 Iris setosa，而另一个簇包含 Iris virginica 和 Iris versicolor；通过特征绘制出的图会分成明显2堆，其中一堆是交错2种类型 Iris * 如果没有明显的聚类，并且数据维度多，可以通过 PCA 等方式降维后再分 * 无特征：则先通过 KMeans 能方式聚类，再通过有特征方式分析 * 是什么？ONNX = （模型本身 + 模型训练好的权重跟偏置）的一种更加抽象的表达 * 如何表示？使用预定义的 operator（描述输入与输出的关系，例如：add算子= inputA + inputB = OutputC，可拓展）来描述模型，用向量描述训练好的参数 * 作用？实现不同深度学习框架和平台之间的模型互操作性 * 为什么 pytorch 在导出 ONNX 的时候需要传入一组输入？原因： * ONNX 并非像编译器一样彻底解析原模型的代码，记录所有控制流；而是不考虑控制流的静态图 * 而是利用 pytorch trace 机制，将参数传入模型执行，并记录执行这组输入对应的计算图 1. Epoch 是什么？ 一次 Epoch = 让所有数据通过模型正向+反向传播一次 = 一个完整的学习周期 2. Epoch 设置多少次合适？ 无定论： 次数少会导致欠拟合； 次数多会导致过拟合； 3. 什么是 Batch？ Batch Size = 一次训练的样本数 每一次参数的更新所需要损失函数并不是由一个数据获得的，而是由一批数据加权得到的 4. Batch 的作用？ * 效率：利用矩阵计算加速（相对于单个去训练） * 稳定性：平均每个数据样本的贡献，减少梯度的方差 * 多大合适：看情况，太小会导致训练太久；太大会导致内存受不了 Reference * 1. 代码尝试 * 重构转换 * 输入 js -> ts * 优化 * 格式化代码 * 创建 * 创建插件（eslint）步骤：写测试用例 -> 输入 gpt -> 生成插件 -> 微调成型 * 问题：自动生成的代码会比较繁琐或者隐藏逻辑问题，也不会去利用第三方的包的能力 * 目前方案：需要 developer 找到更便捷的方式再去投喂给 gpt 生成更加合理简洁的代码 * 网络越深，学习的知识越抽象：比如第一层hidden layer负责编码诸如点、线、边缘等浅层信息；第二层hidden layer编码简单点的纹理、形状等信息；第三层hidden layer编码诸如眼睛、鼻子等目标的形状...，然后逐层学习，不断地提取抽象的特征，一气呵成，最终学会了辨识花草树木、飞禽走兽等等。 - * 网络越宽，每一层学习的知识越丰富：增加网络的宽度意味着同一个hidden layer有着更多的神经元，每一个神经元代表一种颜色，一个方向，一种纹理，组合起来便可以学习到更多不同的颜色信息，各个不同的方向以及不同频率的条纹信息。 生成对抗网络（GAN） VS 变分自编码器（VAE）： GAN 倾向于生成逼真的合成样本，而 VAE 倾向于生成具有一定程度多样性的样本。如果期望生成特定目标样本，可以考虑 CGAN 跟 CVAE * DALL-E uses Discrete Variational Autoencoder (dVAE) for this step. dVAE is a variant of traditional Variational Autoencoder (VAE) that operates in a discrete latent space. It is similar to VQ-VAE but uses distribution instead of nearest neighbor. * - Google推出了VIT（Vision Transformer）：一个和Bert几乎一致，同时不添加任何卷积结构的图像分类模型。VIT在Transformer上的成功，证明了可以用统一的模型，来处理不同领域（语言/图像/视频）的任务，进而开启了多模态模型研究的新篇章。 * ：transformer全局感受野，在大图片或者说找东西时效果好（类似近视眼，能够感受图像大轮廓）。cnn局部感受野，对细节处理较好（理解像素级别的问题，例如 医疗影像）。 * 文摘 - 高手解决问题的方式从来都不是纠结问题本身，而是升维；升维成功，问题也就解决了 * 雷军2023演讲 - 如何快速学习：知识不全是线性的，大部分是网状的，知识点之间不一定有绝对的先后关系；前面内容看不懂，跳过去，并不影响学后面的；后面的学会了，有时候更容易看懂前面的。 机器学习训练了很多模型，而 LLM 只是其中之一； ChatGPT 之所以跟 AGI 最接近，是因为语言模型的通用性； 如果能够理解自然语言，那就可以实现所有文字能够描述的任务； 其他 AI 任务，比如图片识别，则只能做到图片相似度能人物处理（推荐），无法拓展到相对通用的任务，除非日常交流能够通过表情包完成 * 建设高难度的综合任务评测数据集（LLM 的测试用例，越完备 -> 越强大） * 高质量数据工程（密集+多样性）：LLM 进化 = 更多高质量数据 * 数据例子 * 密度极高的高质量数据：wiki * 高质量问答：quora，知乎 * 高质量图片： * 思考 * 高质量数据消耗完后 gpt 如何进化？ * 能否自己创造知识自己消费（类似 alpha-go 自我对弈的进化）？ * 如果 gpt 成长的资料来源于人类，那能否突破人类知识的边界？ * 探索 LLM 模型的规模天花板：大模型大数据，能参与的玩家不多 * 思考：是否会出现共建超大模型 * 增强 LLM 的复杂推理能力 * LLM 纳入 NLP之外更多其它研究领域：多模态？ * 如何突破符号领域？如果某个领域是非成文的，不能用符号记录表达，那么 GPT 是否就无能为力。比如，人类的很多心理活动、潜意识、灵感、顿悟等等，GPT 如何模拟生成。 * 更易用的人和LLM的交互接口：听觉？ * 超大LLM模型Transformer的稀疏化：相同算力下提高训练速度 参考 * * 人跟AI的关系：淘汰还是互补？ * 人有自主目的性（AI暂无），AI是实现目的的工具； * 提问跟回答能力，哪个更能生存下来？ * 往后提出好问题能力的重要性将越来越超过回答问题能力 * 教育 * 投喂答案的教育模式需要变革 -> 把提问能力列入考核标准，更能培养出人机协作人才 * 让人利用机器，而不是把人培养成机器 * 文理分科这种教育模式急迫需要改变：chatGPT 需要文理结合，提好问题，同时认清答案 场景跟问题 * 更精准提出需求，才能利用好 chatGPT * 辅助 chatGPT 修正回答错误，同时又不影响模型输出的其他答案 * 检测回答是否由机器生成 * 如何避免 chatGPT 泄密，如何做隐私保护（目前可以 chatGPT 被催眠然后突破本身不泄密的限制） * AI 训练，避免伦理问题 * AI 本身安全：解决提示注入（类似 网页的 xss ，SQL 注入等），越狱等安全问题 * 知识产权重新定义：AI生成的东西到底算不算侵权？（例如之前的爬别人网站的数据作为自身的商业盈利依据，是否算侵权？） * 趋势预测 * 动作（运动，游戏等数据）文件化：可以对整场羽毛球做文字标记序列化，然后输入 chatGPT，最后可以预测落点跟个人行为 职业名？ * 标注师：标记信息，投喂并训练 AI * 安全员：确保 AI 不被攻破 gpt自己的回答 * 提示工程师：提示是一种指导GPT-4生成内容的文本或图像，通常包含一些特殊的符号或指令。提示工程师就是专门设计和优化提示的人员，他们需要了解GPT-4的内部机制和逻辑，以及不同领域和场景下用户的需求和偏好。提示工程师可以为各种应用场景提供高质量、高效率、高安全性的提示服务。 * 内容审核员：虽然GPT-4具有强大的生成能力，但它也可能会产生一些不合适或有害的内容，如色情、暴力、歧视、谣言等。内容审核员就是负责检查和过滤GPT-4生成内容中是否存在这些问题，并及时删除或修改不良内容。内容审核员需要具备一定的专业知识和判断能力，以及良好的道德素养和责任心。 * 内容运营师：内容运营师是利用GPT-4为各种平台和渠道提供优质内容服务的人员，他们需要根据目标受众和市场需求，选择合适的提示和参数来调用GPT-4生成相应类型和风格的内容，并进行编辑、优化和发布。内容运营师需要具备一定的创意思维和文案能力，以及对各种媒体平台和行业动态有一定了解。 * 内容创作者：内容创作者是利用GPT-4辅助自己进行创作活动的人员，他们可以将自己想要表达或传达给用户",
    "url": "/blog/AiNotes",
    "path": "AiNotes.md"
  },
  {
    "title": "简单与难运动",
    "content": "入门简单比入门难的运动的综合能力要求更强，上限更高，下限也更低？ * 入门越容易的运动，形成金字塔的人数就越多，塔尖也就越尖锐；这不是因为运动本身更复杂，而是因为要在海量人中脱颖而出，你必须几乎全能。 * 而入门困难的运动往往门槛高、筛选强、参与者少，所以虽然技术复杂、入门难，但上限可能反而不比前者高太多，整体竞争环境也较狭窄。 大规模升学幻觉：教育制度制造了“精英感”，却没有足够社会位置承接它，造成大规模心理与经济落差。而真正推动社会剧变的往往不是“被抛弃的大众”，而是“未被接纳的精英”。 当前的精英通道（小学→博士）本身就是“既得利益者”们走出来的一条路径，他们掌握资源与话语权，倾向于维护这条路径的正统性和地位，这就是一种典型的 “路径依赖” + “认知守旧” + “制度惯性”。当前体制内重要岗位（高校、政府、国企）几乎都是通过这条路选拔出来的；他们自然认为这条路径“最可靠”“最正规”；于是继续用考试、学历、资历、论文来选拔下一代 → 进一步固化。 | 问题类型 | 描述 | | --------- | ---------------------------- | | 🎓 教育膨胀 | 太多高学历者，但没有相应工作岗位匹配（例：博士送外卖） | | 💼 职位竞争 | 精英岗位饱和，内部卷得更狠，小错误都可能被放大处理 | | 🧨 精英激进化 | 一部分被排除在“核心圈”之外的准精英，转向民粹、激进主义 | | 🧠 意识形态冲突 | 精英内部无法达成共识，派系斗争频繁 | | 📉 社会信任下降 | 普通人看到精英互撕，会对系统丧失信心 | | 🥊 政治极化 | 精英在政治、媒体、文化领域争夺控制权，导致撕裂加剧 | * 解决方案：解决精英过剩的根本，不是“筛掉谁”，而是“多造路” + “改观念” + “改分配”。 * 控制精英供给端：合理设限、分流，例如：国内通过本科/职教分流，德国职业教育与学术教育双轨制 * 扩大精英吸纳能力：拓展蛋糕，例如：新产业、新科技、新职业路径（如数字经济、绿色能源、人工智能领域），企业出海 * AI与职业：借助大模型/数据智能，精准引导不同背景学生找到适合自己的成长路径 * Pick the Right Problem 伟大的工作通常源自 重要且有趣的问题 * Find Problems, Don't Manufacture Them 发现问题，而不是制造问题 * Choose Your Field Wisely 选择你的领域，一些领域更容易做出突破，比如 新兴科技、未被充分研究的领域。 * Develop Unique Insights 发展独特的见解 * Great Work Comes from Accumulation 伟大的工作来自于积累 * Surround Yourself with the Right People 远离消极或抑制创造力的环境，找到支持自己思考和探索的圈子。 * Ignore Distractions, Focus on Your Work 伟大的工作需要长时间的专注，而不是被短期的流行趋势所驱动。 * Embrace Uncertainty 拥抱不确定性，不要害怕失败。 * Good design is simple 简约 * Good design is timeless 追求永恒 * Good design solves the right problem 解决正确的问题 * Good design is suggestive 启发性 * Good design is often slightly funny. 有趣 * Good design is hard 困难 * Good design looks easy 看起来简单 * Good design uses symmetry 对称 * Good design resembles nature 模仿自然 * Good design is redesign 重构 * Good design is often strange 奇怪 * Good design happens in chunks. 比如 汽车颜色从五颜六色到黑灰；比如各个大模型，最终都支持到搜索跟推理以及多语言；再比如电影电视剧都开始拍续集；再比如各个编辑器都开始支持 agent 跟 tab 补全；再比如各个视频软件都开始搞推荐，同样功能的软件界面变的都一样； 世界趋同的本质，是效率最大化与风险最小化的共同作用。所有行业都希望以最小的成本满足最多用户的需求，而市场、技术、文化、算法等因素一起推动了这种收敛。 不过，这并不意味着创新不存在，而是创新往往发生在微小但关键的突破点，比如 iPhone 颠覆了手机界面、GPT 颠覆了搜索体验。未来，虽然大趋势是趋同，但局部突破仍然会带来周期性的变革。 关键点：安全性、免疫效果、生产速度和稳定性 * mRNA（信使RNA）是细胞内的遗传指令载体，它负责将DNA上的遗传信息传递给细胞的核糖体，指导蛋白质合成。 * 流程：设计mRNA -> 生产并包裹mRNA -> 注射mRNA * 传统疫苗是利用灭活病毒、减毒病毒或重组蛋白等抗原物质，通过注射或口服等方式进入人体，激发免疫系统产生免疫反应。 当前还不能完全取代传统疫苗，原因： * 细菌比病毒复杂，往往有多种抗原，mRNA疫苗很难涵盖所有关键抗原。 * 一些疫苗（如鼻喷雾流感疫苗、口服脊灰疫苗）在黏膜部位诱导免疫，防止病毒进入体内，而mRNA疫苗主要在肌肉注射，难以产生强大的黏膜免疫。 1. 屏幕二维码的最小模块建议为 4像素 x 4像素。更大的会更容易被扫描识别 2. 印刷二维码的最小模块建议为 0.4毫米 x 0.4毫米。 指的是，如果提前采取有效措施，就可以减轻某种未来的灾难，那么人们就会倾向于认为，这种灾难没有那么严重，不会造成太大损害，从而阻碍对这种灾难进行提前准备。 Geohash 是一种用于地理位置编码的算法，它将地球上的经纬度转换为一个字符串。这个字符串表示一个矩形区域而不是一个点。Geohash 的核心是不断对经纬度进行 二分区域，并将结果编码为二进制，再转换为更简洁的 Base32 字符串。 直观解释，把地球看作一个网格，每次增加一个字符（增加编码长度）就进一步细分网格。 比如： * 9 表示一个大区域（经纬度粗分）。 * 9q 表示更小的区域。 * 9q8 表示更精确的区域。 * 9q8yy 可精确到几米范围内。 S2 是一种地理空间索引系统，基于球面几何将地球表面划分为一系列分层的单元（称为 S2 Cells）。每个单元都有唯一的 ID，用来表示地球上的区域。与 Geohash 不同，S2 直接在球面上工作，避免了平面映射中的失真。 | 特性 | Geohash | S2 编码 | |------------------|---------------------------|----------------------| | 投影方式 | 平面投影 | 球面投影 | | 单元形状 | 矩形 | 近似正方形 | | 分层方式 | 字符编码（Base32） | 整数 ID | | 精度与效率 | 适合简单应用 | 更适合复杂查询与计算 | S2 在地图服务（如 Google Maps 和 Uber）中被广泛使用，适合处理高精度地理数据和邻近搜索场景。 描述的是在供应链中，由于信息在各个环节间传递的不对称或延迟，小范围的需求波动会逐级放大，最终在上游（例如制造商或原材料供应商）形成更大的需求波动。这一现象形象地得名于牛鞭——手腕的小幅动作会在鞭梢引发巨大的波动。 * 举例说明 * 消费者需求：某商品原本每周需要 10 个，突然需求升至 15 个。 * 零售商：为了应对需求增长及库存不足，订购 20 个。 * 批发商：考虑到运输周期和订单积压，向制造商订购 30 个。 * 制造商：预测未来可能会有更高的需求，于是向供应商订购 50 个。 * 最终，上游供应商感受到的需求远高于实际需求的 15 个，形成了牛鞭效应。 答案是 那些有能力制造通货膨胀的人统治着金钱。他们首先借钱购买实物资产，通货膨胀之后，再用贬值的钱来还债。- 方式一：我不同意，但一旦决策定了，我就会一起努力。 亚马逊创始人贝佐斯说：\"当我跟一个主管反复讨论后，他不同意我的看法。虽然我不认为他正确，但我愿意陪他赌一把。我们认识几十年了，我觉得他可能比我更接近真相。其实我也不知道自己的决定是否正确，干脆照着下属的想法走。\" 这样有几个好处： * 至少产生了一个大家同意的决策。 * 事后不会互相埋怨。 * 有利于形成真正的团队精神。 * 反映了决策层和执行层之间的脱节现象。决策层虽然有大局观，但在执行细节上往往不够深入； * 执行层尽管懂得如何完成任务，却缺乏对全局的理解。 * 信息传递的障碍加剧了这种脱节，导致决策无法得到有效执行，形成了一种低效的系统。 悲观者尝试的概率小，不太可能直面风险；但是乐观者会去尝试，有一定概率遭遇风险，但也有一定概率会成功。 未来的高科技社会，大多数人恐怕都是技术弱势群体，没有太多机会，技术优势集中在少数人手里。 Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure. 软件系统的架构看起来与构建它的开发团队的组织结构非常相似。 * * （Wikipedia's \"Getting to Philosophy\" phenomenon）。当你点击每篇文章的第一个非括号中的链接，并不断重复这个过程，大约90%以上的情况下最终会到达“哲学”这个条目。维基百科的条目结构趋向于层次化的知识体系，通往更抽象的概念。这种体系有助于引导阅读者从具体到抽象进行理解，因此最终会引导到像“哲学”这样探讨最为根本性问题的学科。 “崖山之后无中华”可以理解为中华的“气质”在崖山之战后发生了变化。宋朝时期，中华文明的气质更多体现在对文化和科技的崇尚、对创新的追求以及相对开放的心态。而崖山之后的元、明、清三朝，文化与科技的发展逐渐走向保守和停滞，更加注重政权稳固和经济利益，创新动力不足，文化气质发生了显著的转变。 * 人类学习效率最高的方式，不是\"读书 + 思考\"，而是\"观察 + 模仿\" * 大部分领域的入门方式，都是\"观察和模仿\"，所以视频学习更有效。 对于深入学习，文字和书籍的效果可能好于视频，因为入门以后再往上走，\"理解 + 思考\"的作用越来越大，\"观察 + 模仿\"的作用越来越小。 * 为了社会稳定，政府需要对无人驾驶出租车有所限制。 投放策略建议 - 摘抄自 * （1）货运优于客运。货运的距离长，更艰苦，更适合无人驾驶。 * （2）公交车优于出租车。公交车本来就缺少司机，无人驾驶可以增加班次，有利于绿色出行。 * （3）郊区和农村优于城区，那里平时不容易叫车，更需要无人驾驶。 * （4）城区的无人驾驶出租车，政府收取管理费，使得运价不低于真人司机。多收的钱用于补贴出租车行业转型。 越来越感觉到认知能力才是事情发展的关键，认知是基础然后才是生成要素的聚集；太聚焦在解决问题层面，陷入到具体实现细节的可能不是一个好的开发 认知的排序以及问题的解决流程 1. 发现问题 2. 定义问题 3. 分解问题 4. 问题排序 5. 解决问题（可能是大部分人停留的阶段） 基层干部需要分解问题能力，然后是其他生产要素的收集（安排优先级，找到合适的人，定一个完成时间等） 重要核心岗位需要发现问题的能力，比起知道如何做，更需要找到值得去做的事情（方向不对，努力白费），然后才是技术介入，定义问题解决问题； * 随着 chatGPT 等人工智能的发展，提出发现一个好问题可能比单纯解决问题更能匹配人机协作的未来；（目前 AI 还只能在给定问题框架下寻找答案，还没有迹象表明 AI 的自主目的性，一切还都是服务于人投喂的目的） 关于认知的例子: 流媒体：看下netflix跟国内的爱优腾的区别，前者自研内容飞涨带着股价飞涨，后者还在天价争夺版权，亏损到现在，爱奇艺最近（2021年底）大裁员了 技术界：前端大神Evanyou，产出了Vue，之后又诞生了vite，能在合适的时机出手带领一波技术潮流；除了技术强，核心可能在于领先同行的认知水平 1. 首先得把概念澄清 * 划分概念边界（定义解决的问题类型，例如：vue-cli plugin可以定义成面向工程结构的开发；解释webpack plugin则是面向源码的转换开发） * 类比熟悉概念（例如：解释 vue-cli的plugin机制，就可以类比 webpack plugin机制） 提纲挈领：比喻善于抓住问题的关键与要害； 抓住核心可以事半功倍；比如： 1. 月经问题：Vue跟React的区别？核心在于Immutable跟Mutable这个理念，其他很多都是在此上的衍生问题 2. Webpack跟Vite区别？核心在真正按需的理念 编程里面有个概念叫：自举； >编程语言自己编译自己，重复这个过程可以让本身进化 >>例如：辅助增添新的语言特征，新的特征必须要能用旧的版本来编译解析，也叫向后兼容； >>理论上编程语言可以通过自举来完成自身进化，不过更多只是体现在已有概念重组跟语法糖上，没法在底层上做大的改进 >rust的出现的其中一个原因就是为了摒弃语言的历史包袱，实现一个真正意义的高级语言 学习知识的过程也可以理解成“自举” >通过自身已知概念来解释新知识，将新知识逐步建立在旧知识体系之上，从而“学习到”新知识 >>例如：小学算术是从简单1+1=2，再用个位解释灌输十位，用十位解释百位...再例如：学习编程，有C++基础学js语言会比有js基础再学C++简单 在变化中寻找自举的底层动力，不要怕吭硬骨头，了解底层编程模型范式更能增加自举进化可能性 通性：跟随该领域领军人物的动态 Why/What/How * 诞生的背景 * 解决了哪些历史问题？ * 为什么没有选择现有语言？ * 应用场景 * 以前的解决方案，选择这门语言的优势 * 独有的特性并实践 * 语法 * 内存对齐策略 * 判定相等 * javascript: 2个单独声明的对象不会相等，eg: let a ={name:1},b={name:1}; a!=b * golang: var p1 = point{x: 1, y: 2}; var p2 = point{x: 1, y: 2}; p1==p2 * 组合还是继承？ * 尝试造一个类似的轮子，基本能懂一半 1. 从哪里划分出来？为什么被划分？ 2. 解决的独有问题 3. 个人的应用 * 新技术的成熟度 * 渐进式使用，先在dev应用 * 分享，集思广益 * 落地 例子：应用@swc/core到系统考虑点， 1. 框架自身的需要能做到和 babel 解耦； 2. 自身使用的插件需要使用 swc plugin 再实现一遍；可能带来的副作用： 1. 对 bundle 体积的影响； 2. 转换后的代码是否存在兼容性问题； 3. 如何快速的将 babel 插件转换到 swc 插件； 4. swc 现有的能力是否满足从 babel 切换到 swc。 * 动态语言（JIT+VM动态编译执行，例如js）； * 优点：动态下发，容易做热替换，热更新等；dev体验好 * 缺点：production性能较差 * 静态语言（AOT编译成二进制+Runtime，例如C） * 优点：production性能较好 * 缺点：无法动态下发，没法热替换，dev需要等待编译完成 flutter选在dart语言（动静结合）,dev下JIT，production阶段AOT；兼顾dev跟production * 好动的人 * 优点：社交牛逼症 * 缺点：沉下心来搞研究难 * 喜静的人 * 优点：能沉下心来琢磨事情 * 缺点：社交困难 做一个动静结合的人？既能沉下来搞研究，也能出去处理人际关系 * 有意义、有价值和能成长的忙碌不会让人产生真正疲惫，反而是不断焕发新的激情； * 在错误的方向，复杂的人事上内耗会让人心力交瘁，即便准点下班也会心理疲惫； 一段话：所有的问题都能通过加一层中间层解决，如果不行，那就再加一层； 中间层的好处是屏蔽细节，简化API，增大通用性，但是以损失性能为代价 例子： * 解决Node addon跟V8 API强绑定导致的升级困难；猜想大概率是做了一个 开发 Node Addon的通用API规范（底层可能还是通过封装V8实现，但是开发者不需要关心具体的V8 API是啥） * Taro3 能做到框架独立（同时支持react/vue等框架语法），也是在编译中间加了一层编译IR，然后把IR再转换成对应的目标代码 * React 能做到编写 APP，也是在代码编译中加了一层虚拟DOM中间层 IR（如果是直接编译到浏览器Dom就没跨端什么事了），可以进行不同平台的适配 * LLVM 做成了标准的编译中间层IR，方便无数的新生语言（eg：Rust）可以以 LLVM 作为目标来编译，而无需去适配不同的操作系统 * JsBridge原理连接了js世界跟Native世界，丰富了js的可能性，代价是JsBridge沟通的性能损失 ... * 如果一件事情处理不了，那就找人帮忙，如果还不行，那就再找一个人？ * 各种网络平台也就是把自己做成一个中间层，方便人事的协同？ 事物无时无刻都在向前发展，很多发展中的问题都是不可预测，所以基本没有一开始就设计出的完美的方案，于是补救方案就来了；当前的补救方案可能促进方案的成长 * 当开发者跟Rust语言对Safe的场景认同不一致的时候，Rust决定把选择权交给开发，于是有了Unsafe；如果开发者正确，那可能是Rust进化的不错机会；例如：extern关键字协助创建外部函数接口（Foreign Function Interface， FFI），extern的函数就肯定是Unsafe的，因为rust没法进行静态检测其他语言的 * 当开发者跟typescript对lint的场景认同不一致的时候，于是有了 \"//@ts-ignore\"；可能是TS进化的机会 * 认知的补救方案，早期人类遇到无法解释的现象首先是发挥想象力，创造一个万能的神明； * 关于个人无法化解的场景，可能通过笑来化解；所以笑是避免大脑死机的补救方案？ 事物的发展都有一定的规律，把握了正确规律可以事半功倍 * 语言的发展：更快、更安全、更容易掌握、特定问题的更好解决方案 * Rust就是比C++更友好更安全的方向 * Go 就是来提供更好的服务并发方案 方式：new/delete等手动方式 语言：C++等 优点：灵活 缺点：忘记或者回收不当会导致内存泄漏风险 * 方式：引用计数，标记清扫，节点复制，新老生代转化组合等；周期性的暂停程序执行而进行GC工作 * 典型的语言：js，go * 优点：编写简单 * 缺点：GC频繁会导致程序执行性能受到影响 * 方式：所有权 * 语言：Rust * 优点：性能高，安全 * 缺点：学习曲线陡峭 * 思考：实际有GC，只是效率高到不会妨碍到现有程序执行？ 摘要 >Rust would know when the variable gets out of scope or its lifetime ends at compile time and thus insert the corresponding LLVM/assembly instructions to free the memory. * 例子：react-router-dom 从版本 6 开始就废弃了Switch组件，用Routes做了替换，然而使用上跟Switch有差别？第三方库 react-router-transition 目前依赖peerDependency react-router-dom@5，也就是升级react-router-dom直接崩？又因为breaking change的差异大，导致 react-router-transition 没法简单就能跟随升级 相关文档： * 现状：现在找个解决方案费时费力，各种实践层出不穷，核心库的breaking change没经过周全的思考就去实施，然后就是一堆填坑的工作量； * 建议：React能否学习Vue把最佳实践维护起来，优势：可以做更周全稳定迭代，方便开发者找到最佳实践，提升工作效率；劣势：是啥？ * 数据结构之间的依赖关系：例如 golang map 依赖 hashmap ，hashmap依赖 数组 + 链表 个人拙见，酌情观看",
    "url": "/blog/DailyReflections",
    "path": "DailyReflections.md"
  },
  {
    "title": "Introduction",
    "content": "--- title: Introduction description: this is wizardpisces blog introduction --- * * *",
    "url": "/blog/Introduction",
    "path": "Introduction.md"
  },
  {
    "title": "TechNotes",
    "content": "当前 MCP 基于的 sse 无状态通信还是有缺陷，会导致客户端多次访问丢失 session（多个 pod 负载均衡的时候很容易出现），还得往 websocket 方向演进 曾经前端热替换（HMR）有过类似的演变 核心原因：符合 Apple 的封闭生态技术路线，转向 Metal 让 Apple 能更好地优化自己的硬件与软件系统，而不依赖外部技术。 * 原因：因为 jest 需要监听文件变化，而 watchman 是文件系统监听工具，更适合大型项目（所以 watchman 得崩溃可能会导致 jest 无法正常工作） * watchman 的启动 ：在第一次 run jest 时，会启动 watchman 持续监听文件变化 * watch 范围：历史启动过 jest 的目录都会纳入到 watchman 的监听范围，在 MAC 的活动监视器找到 watchman 打开的文件或者窗口看到 * 分支：一个指向提交的可变指针，随着你在分支上提交新的更改，分支会自动向前移动。这意味着分支是动态的、可变的，它们通常代表开发进度，例如 master、develop、feature/xyz 等分支名。 * 标签：一个指向特定提交的不可变的指针。标签通常用于标记代码的特定状态，比如一个版本发布（v1.0.0），而这个标签一旦打上，它就永久指向那个特定的提交，不会随时间变化。 * e9e976fde12bc6745f1c51fa80652d01:000000788c34c719:0000000000000000 * 第一段全局唯一 Trace ID * 第二段：spanId * 第三段：parentSpanId 简介：具有准确的抗锯齿像素检测（通过色差阈值不一定是差异，还可能是锯齿）和感知色差度量（ ）。 * Y 是最重要的信息，因为黑白电视只依赖亮度。 * I（橙蓝对比）次重要，它对人眼的视觉感知贡献较大。 * Q（紫绿对比）的重要性较低，因此可以被更大程度地压缩。 为啥有用 * 因为人眼对颜色的感知是非线性的，对不同颜色分量的敏感度不同。 * 即使两个像素在 RGB 空间中看似差异较大，通过 YIQ 空间测量后可能反映出的感知差异较小，从而避免错误地标记为显著不同。 * 将 RGB 颜色转换为 YIQ 后，可以根据人眼感知的差异调整颜色分量的权重。比较两张图片时，可以更加准确地反映视觉上的差异，而不是单纯的数值差异。 场景：专注于精确检测像素差异，常用于回归测试、差异图生成等场景，优先考虑性能。 简介：是一种用以衡量两张数字图像相似程度的指标。当两张图像其中一张为无失真图像，另一张为失真后的图像，二者的结构相似性可以看成是失真图像的图像质量衡量指标。 为啥有用 * 结构相似性的基本观念为自然图像是高度结构化的，亦即在自然图像中相邻像素之间有很强的关系性，而这样的关系性承载了场景中物体的结构信息。 * 人类视觉系统在观看图像时已经很习惯抽取这样的结构性信息。 场景：专注于结构相似性，常用于图像质量评估、图像压缩、图像增强等场景，优先考虑准确性。 英特尔公司宣布了一个叫做 FakeCatcher 的软件，号称可以查出某个视频是否为深度伪造，准确度高达96%。 它的原理是识别视频人物皮肤的静脉血液流动。如果是真人，血液时刻在体内循环，皮肤上的静脉就会有周期性的深浅变化，深度伪造的人就没有。 大数据作为单一问题就不存在了，变成了海量存储和大型计算两个问题。 和 Svelte相比，Vue的 Reactivity Transform 和 Solid -labels 都属于统一模型，也就是他不受限于组件上下文，它可以在组建内使用，也可以在组建外使用，优势就是有利于长期的重构和复用 * * puppeteer,playwright 基于浏览器调试协议；偏底层，功能更丰富，但是跨浏览器一致性有限； * 例如：playwright 跟 chrome 的交互是基于 chrome devtools 协议； * 猜想：能够使用 playwright 实现的能力（例如 UI 的录制回放），也能直接开发对应的浏览器插件实现？ * selenium 基于 webdriver API；跨浏览器一致性，功能有限 数据一致（Consistency）、高可用（Availability）、数据分区（Partition tolerance） CAP 定理的核心观点是，在设计和运行分布式系统时，需要根据实际应用场景的需求，做出选择和权衡。例如， * 如果一个系统必须保证数据的实时一致性，比如银行系统，那么它可能需要在网络分区时牺牲一部分可用性。 * 而如果系统更注重用户体验和服务的不间断性，如社交网络，可能会在网络分区时选择保持高可用性，同时允许暂时的数据不一致。 * Prometheus + Grafana * Elasticsearch + Kibana es 侧重于搜索（倒排索引，水平可扩展性更强），数据库侧重于存储（事务，一致性，复杂的关系模型） 空间换时间？ 优势：对齐可以加快 CPU 访问速度 劣势：会浪费空间 1. wiki 符号表是一种用于语言翻译器（例如编译器和解释器）中的数据结构。在符号表中，程序源代码中的每个标识符都和它的声明或使用信息绑定在一起，比如其数据类型、作用域以及内存地址。 2. Example * Go 编译器默认编译出来的程序会带有符号表和调试信息，一般来说 release 版本可以去除调试信息以减小二进制体积。 * ABI（Application binary interface）： 约定数据在二进制流上的排列，还有 calling convention 等方式便于不同语种交互 * low-level, hardware-dependent vs API（high-level, hardware-independent，human readable format） 3. 思考 有点类似 js 的 source-map？ 1. 什么是风控？ > input：风控参数，参数权重，参数组合判定逻辑 > output：reject/pass 2. 为什么需要优化？ > 风控的判定效率越高，对下游真实业务的阻塞也就越小 3. 如何做？ * 参数获取：参数的获取可能渠道获取（redis/kafka/db 等） * 并行 * 参数筛选排序拉取：根据业务数据使用权重，推迟小概率使用的参数拉取 * 逻辑组合 * 时序调整（调整逻辑执行先后顺序） * 剪枝（编译阶段无效判定逻辑消除） 需要处理好扩容跟收缩 1. 栈空间（或者堆空间）策略 * 分段机制 * 旧秩序的延续 * 保留旧有数据空间，申请新空间存放溢出数据，通过链表连接各片段 * 连续机制 * 新秩序替换旧秩序 * 触发扩容会申请更大的空间（2倍？类似 golang 切片的动态扩容机制），把旧空间数据搬到新空间，并释放掉旧空间 * 优化：延迟归还给操作系统，自己管理内存的伸缩，例如：golang 的 goroutine 2. 服务能力 * 横向：拓展更多集群服务 * 流量降低时会触发收缩 * 纵向：提升单体服务能力 * 解释：运行时动态地将代码切入到类的指定方法、指定位置上的编程思想就是面向切面的编程。 * 例子：js 的装饰器 * 场景：使用 AOP 将其从核心关注点中分离出来（实际是对执行过程进行代理重写） * Node.js 日志log * 埋点、数据上报 * 性能分析、统计函数执行时间 * 给ajax请求动态添加参数、动态改变函数参数 * 分离表单请求和验证 * 防抖与节流... * 遭遇：golang 垃圾回收 * 摘要：AOP其实只是OOP的补充而已。OOP从横向上区分出一个个的类来，而AOP则从纵向上向对象中加入特定的代码。有了AOP，OOP变得立体了。 解释：在编译程序优化理论中，逃逸分析是一种确定指针动态范围的方法——分析在程序的哪些地方可以访问到指针。它涉及到指针分析和形状分析。 遭遇：在了解 golang reflect 的过程 “用于获取接口值 reflect.Value 的函数 reflect.ValueOf 实现也非常简单，在该函数中我们先调用了 reflect.escapes 保证当前值逃逸到堆上，然后通过 reflect.unpackEface 从接口中获取 reflect.Value 结构体；” 逃逸基础 * 不同于jvm的运行时逃逸分析，golang的逃逸分析是在编译期完成的。 * Golang的逃逸分析只针对指针。一个值引用变量如果没有被取址，那么它永远不可能逃逸。 * C 或者 C++ 是手动分配到栈或者堆上，后面是否会加上运行时逃逸分析 功能 * 逃逸分析辅助编译器优化编译结果 * 将堆分配转化为栈分配。如果某个对象在子程序中被分配，并且指向该对象的指针永远不会逃逸，该对象就可以在分配在栈上，而不是在堆上。在有垃圾收集的语言中，这种优化可以降低垃圾收集器运行的频率。 * * * Single Responsibility Principle 单一原则； - 一种Class只负责一类事情 * Open/Closed Principle 开闭原则； - 允许在不改变已有代码前提下添加新功能 * Liskov Substitution Principle 里氏替换原则； - 不要从可实例化的父类中继承（方法），而是要使用基于抽象类和接口的继承。 * Interface Segregation Principle 接口隔离原则； - Clients should not be forced to depend upon interfaces that they do not use. 对于非必须的参数做可选配置，精简接口，简化使用 * Dependency Inversion Principle 依赖反转原则 - High-level modules should not depend on low-level modules. - Both should depend on abstractions.Abstractions should not depend upon details. Details should depend on abstractions. - 降低耦合，方便重构跟扩展 * Fatory Method * SingleTon * Strategy * Observer * Iterator * Adapter * Decorator * Proxy * 简洁理解：设计模式（语言相关）是对设计原则（语言独立）的实现 * 长文理解 - Design principles provide high level guidelines to design better software applications. They do not provide implementation guidelines and are not bound to any programming language. - Design Pattern provides low-level solutions related to implementation, of commonly occurring object-oriented problems. In other words, design pattern suggests a specific implementation for the specific object-oriented programming problem. * https://github.com/ryanmcdermott/clean-code-javascript#solid * https://www.tutorialsteacher.com/articles/difference-between-design-principle-and-design-pattern * https://github.com/sohamkamani/javascript-design-patterns-for-humans#behavioral-design-patterns * https://github.com/lpxxn/rust-design-pattern 谈到继承就会说到“类”，“类”封装的是数据，是对同一类数据实体以及其处理方法的抽象。 * 目的： 1. 代码复用 2. 多态（同一操作作用于不同的对象，可以有不同的解释，产生不同的执行结果。实现方式：覆盖，重载；某种程度也算代码复用？） * 覆盖：指子类重新定义父类方法，例如js的prototype * 重载：多个同名但参数不同的方法，例如 Rust 函数模板 * 继承方式： 1. 正统：C++,java 2. 模拟：js原型链 3. 无：Rust trait 组合实现多态 思考：Rust 的编程建模模式更像是组合（Composition） 而不是继承（Inheritance）？有点类似 Vue3 选择 Composition-API 代替 Class-Option？ 多态图解： * * https://www.runoob.com/rust/rust-object.html * 解释：面向对象的编程语言通常实现了数据的封装与继承并能基于数据调用方法。 * 有：js一切都是对象 * 无：Rust 不是面向对象的编程语言，但这些功能都得以实现。 * Redux * 通过props注入 dispatch/state，规避了 useContext 方案可能导致的重复渲染 * Mobx （有点类似createContext + useContext + reactivity） * 特征：Mobx + React 类似等于 Vue * createContext + useContext + useReducer * 坑：createContext 返回的 Provider 中 value变化会触发所有子组件重复渲染 * 关联思考：这种方案适用于小型的状态管理？ * Todo：了解重复渲染的运转机制 * 优化：对渲染做 useMemo * 纯展示型的组件，数据进，DOM出，直观明了 接入型组件，在React场景下的container component，这种组件会跟数据层的service打交道，会包含一些跟服务器或者说数据源打交道的逻辑，container会把数据向下传递给展示型组件 * 交互型组件，典型的例子是对于表单组件的封装和加强，大部分的组件库都是以交互型组件为主，比如说Element UI，特点是有比较复杂的交互逻辑，但是是比较通用的逻辑，强调组件的复用 * 功能型组件，以Vue的应用场景举例，路由的router-view组件、transition组件，本身并不渲染任何内容，是一个逻辑型的东西，作为一种扩展或者是抽象机制存在 如何管理将事件源映射到状态变化的过程，如何将这个映射的过程从视图组件中剥离出来，如何组织这一部分代码来提高可维护性，是状态管理要解决的本质问题 * css modules * css-in-js 解决的问题 * 任务的自动化 * 开发体验和效率（新的语言功能，语法糖，hot reload 等等） * 部署相关的需求 * 编译时优化 * * GPU比较擅长于： - 绘制位图到屏幕 - 重复的绘制同一个位图 - 在不同的位置，以不同的旋转角度，或者不同的缩放大小来绘制同一个位图。 * GPU相对慢的地方： - 将位图加载到显存里。 例子： 选择独立性比较强的CSS属性有助于改善动画效果：transform，opacity，filter （依赖于过滤器的复杂度和浏览器） 拓展：渲染线程分为 主线程 (main thread) 和 合成线程 (compositor thread) * https://www.jb51.net/css/348357.html Vue源码里面lifeCycle/watch/directive/transition等的callback/hook等一般会被封装执行 * 封装执行要素： - fn: Function, - instance: ComponentInternalInstance | null, - type: ErrorTypes, * 错误分类：LifecycleHooks | ErrorCodes（其他函数执行点：callback + hook等） * 同步错误处理：尝试函数运行处理同步错误，并搜集返回值 * 异步错误处理：根据返回值是否Promise来处理异步错误 * 1. Vue插件 2. React插件 // 使用地方：一般需要对组件做二次封装进行connect，然后可以useDispatch，类似Vue inject？ 1. Vuex插件 // 插件案例 通过对mutation的拦截，对模块paths进行精准持久化 2. Redux 中间件 // 复杂案例 1. redux-persist的插件，或者称为 Transforms 例子 Vue的mutable 比 React的immutable对数据的状态的操作来的直接，导致在状态处理插件模式也会不同；vuex（直接mutation改变数据）整体上比redux（通过dispatch action，走整个reducer流程返回新的state）更加直接，导致插件模式上vuex直接劫持mutation比redux劫持reducer来的更加简单 * https://vuex.vuejs.org/guide/plugins.html * https://redux.js.org/understanding/history-and-design/middleware * https://v2.vuejs.org/v2/guide/plugins.html?redirect=true Set会自动追踪插入的先后顺序 1. Set结构 2. Array结构 * ORM * ORM * Engine (open db, connect dialect and db_path) * Session(prepare for next operation, open/close DB) * Dialect * dialect : db and dialect container * sqlite3 : language type <- map -> db column type * mysql ... * Schema (Parse a struct to a Schema instance) * 静态语言（例如 golang）：type struct（Modal） <- 反射 -> Table Fields * 动态语言（例如 js）：手动 Table Fields 创建 -> Table Fields * Clause (SQL Builder) * generator: keyword sql string(vars) generation * clause : depend on generator, set/build final sql string * Session ( SQL Execution ) * raw: raw sql execution * table: table operation * record: ORM Open API (Find/Select/Update/Delete/Where/Limit ...) ,depend on Clause/clause & table & raw * hooks: ORM Open API hook * transaction: Atomicity/Consistency/Isolation/Durability * migration",
    "url": "/blog/TechNotes",
    "path": "TechNotes.md"
  },
  {
    "title": "AI 的关键是语料",
    "content": "* 观点：模型行为不是由架构、超参数或优化器选择决定的。它是由你的数据集决定的。其他一切只是有效地将计算交付给逼近该数据集的手段。 如果模型的架构、功能、参数，作用是有限的，真正决定性的因素是训练模型的语料。那当发布了一个大模型的时候，也许重点应该放在告诉大家模型用什么语料训练模型。（模型完美，但是语料垃圾，一样不行；反之，模型很平常，但是语料足够，那可能也不错。） * *",
    "url": "/blog/AI 的关键是语料",
    "path": "tech/ai/AI 的关键是语料.md"
  },
  {
    "title": "AI 阻碍新技术传播",
    "content": "推导： * 如果大家都使用大模型解决技术问题，而大模型又是基于已有数据（旧技术）训练，新技术的应用会落后，没人使用，会导致网上有关新技术的信息很少，导致 AI 缺乏新技术的训练数据，那新技术就很难被普及？ * 模型训练出来的模型进一步偏向于旧技术 这实际上描述了一个 技术固化陷阱（Technology Lock-in by AI），或者说 反馈闭环导致新技术传播困难。 --- - AI 大模型是基于过去的数据训练的，而 新技术在早期的数据量很少，这意味着 AI 在回答问题时很可能偏向推荐旧技术，而非最新技术。 - 结果是：当开发者遇到技术问题时，AI 提供的解决方案更可能是成熟的旧技术，而不是较新的技术。 - 这会导致新技术的应用落后，因为没有 AI 推荐，很多人甚至不会主动去学习或尝试。 🔹 类比：搜索引擎早期时，信息是手工输入的，因此新技术的传播相对公平。而如今 AI 生成的内容会影响搜索引擎排名，可能导致主流技术长期占据上风，新技术更难被发现。 --- - 人们依赖 AI 解决方案，而 AI 由于训练数据的局限，往往推荐过去更常见的解决方案。 - 这会导致“路径依赖效应”：旧技术持续强化，新技术的应用受限，甚至可能被埋没。 - 结果是，即使新技术更优越，也很难被广泛采用，因为 AI 生成的技术建议大多不会主动推荐它。 🔹 类比：类似 VHS 战胜 Betamax、QWERTY 键盘取代更高效的 Dvorak 键盘，新技术可能因为“先发优势”不足而被埋没，即使技术上更优越。 --- - 以前，新技术依赖于搜索引擎和技术论坛传播，比如 Google 搜索、Stack Overflow、GitHub 等。 - 现在，AI 生成的内容越来越主导搜索结果，而这些内容大部分基于旧技术。 - 这导致 新技术的信息被 AI 生成的旧技术信息淹没，进一步降低了新技术的传播速度。 🔹 现实案例： - GitHub Copilot 生成的代码通常基于最流行的开源代码，而不是最新的技术方案。 - AI 生成的技术博客往往基于过去已有的文章，而不是基于新研究的前沿进展。 --- - AI 依赖 大量数据 进行训练，但新技术在早期阶段的数据量本就很少，导致 AI 无法有效学习。 - 由于 AI 不推荐新技术，开发者使用新技术的更少，进一步导致网上缺少相关数据，AI 更无法训练，形成 恶性循环。 - 结果是：新技术因缺少数据无法进入 AI 生态，而 AI 生态又让新技术难以获得数据，最终造成“冷启动困境”。 🔹 类比：新社交媒体平台常常遇到冷启动问题——没有用户 → 没有内容 → 没人愿意加入。而 AI 主导的信息流可能让新技术面临类似的问题。 --- 1. 人为干预 AI 训练数据 —— 让 AI 更主动学习新技术，而不是仅仅依赖已有的大数据。 2. 推广“非 AI 依赖型”学习方式 —— 鼓励开发者主动学习新技术，而不是完全依赖 AI 推荐。 3. 增加新技术的曝光渠道 —— 比如加强 AI 对 arXiv、Hacker News、Reddit 技术讨论等信息的关注，而不仅仅是 Stack Overflow 这种“过去经验导向”的平台。 4. 实时更新数据：搜索引擎、论坛、GitHub 代码库都是 AI 训练数据的重要来源，未来 AI 可能会更依赖实时数据+用户贡献，而不是仅依赖历史数据。 5. AI 增强搜索能力：未来的大模型可能会更强地结合搜索引擎（如 Perplexity AI、Bing Chat），这样即使 AI 训练数据较旧，也能搜索最新技术文档。 6. 企业推广：企业在新技术推广时，往往会提供 API 文档、示例代码、技术博客（如 OpenAI、Meta、Google），AI 也可以从这些官方资源中学习。 --- 推导揭示了 AI 可能导致的 技术固化陷阱，核心逻辑是： - AI 依赖历史数据训练 → 倾向推荐旧技术 → 新技术曝光不足 → 新技术被采用的更少 → AI 训练数据缺失 → AI 继续推荐旧技术，形成闭环。 这种情况如果不加以引导，确实可能导致新技术的普及变慢，甚至在某些情况下 让落后的技术长期占据主流。 * * GPT",
    "url": "/blog/AI 阻碍新技术传播",
    "path": "tech/ai/AI 阻碍新技术传播.md"
  },
  {
    "title": "ControlNet 与 LoRA 插件",
    "content": "ControlNet 和 LoRA 并不是完全相同的东西，尽管它们都与神经网络和图像生成有关。 ControlNet： * 作用：用于控制 AI 图像生成。 * 特点：允许用户对生成的图像进行精细的控制。 * 应用：在计算机视觉、艺术设计、虚拟现实等领域中非常有用。 * 示例：用户可以上传线稿，让 AI 帮助填色渲染、控制人物姿态等。 LoRA： * 作用：用于大模型参数高效微调。 * 特点：通过降维和升维来模拟参数的更新量，从而减少训练成本。 * 应用：在参数量较大的模型微调中表现优异。 ControlNet 是一个神经网络架构，用于控制 Stable Diffusion（SD）模型并扩展其输入条件。 作用： * 可控性提升：ControlNet 允许创作者通过添加额外的控制条件来引导 SD 模型生成图像，从而提高 AI 图像生成的可控性。 * 多样性：它支持多种输入条件，如 Canny 边缘、语义分割图、关键点、涂鸦等，拓展了 SD 的能力边界。 重要性： * 创作自由度：ControlNet 让创作者更精细地控制生成图像的元素，包括主体、背景、风格等，从而实现更好的创作自由度。 * 商业应用：可控的 AI 绘画对商业落地具有重要意义，例如广告、设计、艺术等领域。 * LoRA 是一种用于大模型参数高效微调的方法。 * 插入位置： * LoRA是以残差连接的形式“并联”在Transformer的Q、K、V、O矩阵上。 * 低秩分解通常是在全连接层后面插入适配器。 * 推理延迟： * LoRA在训练完后，其参数可以与原有预训练模型直接合并，变回单分支结构，不会引入额外的延迟。 * 适配器由于引入了额外的串联网络层，会带来推理时间的增加。 LoRA通过只更新模型权重的小部分来提高效率，而适配器则是插入在不同位置，具有一些不同的特点。 降维 (Dimensionality Reduction)： * 在原始预训练语言模型（PLM）旁边增加一个旁路。 * 通过低秩分解来模拟参数的更新量。 * 降维通常包括两个步骤：降维矩阵 A 和升维矩阵 B。 * 在训练过程中，原模型的参数保持固定，只训练降维矩阵 A 和升维矩阵 B。 * 这样可以显著减少训练成本，同时保持模型的性能。 升维 (Dimensionality Expansion)： * 模型的输入输出维度不变。 * 输出时将 BA 与 PLM 的参数叠加。 * 用随机高斯分布初始化 A，用 0 矩阵初始化 B，保证训练的开始此旁路矩阵依然是 0 矩阵。 * LoRA 允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。这种方法在参数量较全参数微调（Fine-Tuning）显著降低的同时，性能优于其他参数高效微调方法。 * GPT",
    "url": "/blog/ControlNet 与 LoRA 插件",
    "path": "tech/ai/ControlNet 与 LoRA 插件.md"
  },
  {
    "title": "GPT与开发 TDD",
    "content": "GPT 是“Generative Pre-trained Transformer”的缩写，即生成式预训练变换模型，是一种语言模型，可以执行非常复杂的任务，如回答问题、生成文章和代码，或者翻译文章内容等。 实质：文字接龙（bert 是完形填空） 如何训练： * Pre-trained （无监督学习） * 自动学习各种资料，熟悉各种语义关系 * 好处：学习资料丰富，成本相对低 * 劣势：学的杂，不成体系，不好为人所用 * 模板规范（监督学习） * 条件：优质范例 * 目标：矫正成符合人类习惯的回答 * 例如：有害内容过滤，回答问题并给出原因等；）； * 附加能力：in-context learning 能力（语境内学习） * 使用过程中评分反馈（强化学习） * 目标：提升创意能力 GPT 辅助前：需求 -> 代码编写 -> test-case GPT 辅助后： 1. 需求 -> test-case -> 代码生成 -> test-case补充生成 2. 需求 -> GPT方式描述 -> 代码生成 -> test-case 生成 实现一个fn 函数，接受一个数字作为参数，并返回一个数字作为结果。 fn 函数需要满足以下条件 当参数为 0 时，返回 0 当参数为 1 时，返回 1 对应测试用例 问：生成 fn 函数满足以下测试用例： 答：我可以尝试生成一个 fn 函数满足你的测试用例。请看我的代码： 当参数为 2 时，也返回 1 * 目标：改变测试用例，输出抽象度更高的代码 * 结果：推导出 fibonaci 问：生成 fn 函数满足以下测试用例： 答：fn 是一个函数的参数，它表示一个回调函数，也就是在 fn 函数内部执行的函数1。在 JavaScript 中，函数可以作为参数传递给另一个函数，也可以作为另一个函数的返回值2。这种函数称为高阶函数1。在您的测试用例中，fn 函数接受一个数字作为参数，并返回一个数字作为结果。要满足测试用例，fn 函数需要满足以下条件： * 当参数为 0 时，返回 0 * 当参数为 1 或 2 时，返回 1 * 当参数大于 2 时，返回前两项的和 这样的函数其实就是斐波那契数列的定义。所以，一个可能的 fn 函数如下： * 概念：拟合 -> 泛化 >拟合的目的是学习数据背后的规律，对新的数据进行预测或分析 >泛化能力反映了模型有没有对客观世界做真实的刻画，还是发生了过拟合或欠拟合； >>欠拟合（拟合结果无法满足旧的案例，例如：fn(2)!=1） >>过拟合（拟合结果无法满足新案例，如果需求增加到 n=3，结果也要为 1） >泛化例子： * 一个泛化能力的例子是高中生每天各种做题，五年高考三年模拟一遍遍的刷，为的什么，当然是想高考能有个好成绩。 高考试题一般是新题，谁也没做过，平时的刷题就是为了掌握试题的规律，能够举一反三、学以致用，这样面对新题时也能从容应对。 这种规律的掌握便是泛化能力，有的同学很聪明，考上名校，很大程度上是该同学的泛化能力好。 考试成绩差的同学，有这三种可能：一、泛化能力弱，做了很多题，始终掌握不了规律，不管遇到老题新题都不会做；二、泛化能力弱，做了很多题，只会死记硬背，一到考试看到新题就蒙了；三、完全不做题，考试全靠瞎蒙。 * 单语言模型到多语言模型的泛化能力；多语言模型可以利用不同语言之间的共性和联系，学习到更一般的规律和知识，从而在新的语言或任务上表现更好。例如，一个多语言的机器翻译模型，可以在不同的语言对之间进行翻译，而不需要为每一种语言对单独训练一个模型。这样可以节省资源，提高效率，也可以增强模型的鲁棒性和适应性。 * 假设我们有一个机器学习模型，它的任务是根据一张照片判断是猫还是狗。我们用一些猫和狗的照片来训练这个模型，然后用一些没有见过的猫和狗的照片来测试它。如果这个模型在测试集上的准确率很高，那么我们可以说它的泛化能力很好，因为它可以对新的数据做出正确的判断。如果这个模型在测试集上的准确率很低，那么我们可以说它的泛化能力很差，因为它不能适应新的数据。 挑战：泛化能力让 知识的可解释性、可控性 难度增加 问：你能不能给我一个更高效的 fn 函数？ 答：有一些方法可以提高 fn 函数的效率，比如： * 使用循环而不是递归，避免重复计算相同的值 * 使用数组或对象缓存已经计算过的值，避免重复计算相同的值 * 使用数学公式直接计算第 n 项的值，避免计算所有前面的值 例如，使用数组缓存的方法可以这样写： 概念：CoT（Chain-of Thought）思维链 CoT：是一系列中间步骤，用来说明如何从输入得到输出，提高推理的准确性和可解释性。 >基本思想：通过给LLM一个包含一些输入输出对以及每个输出对应的推理链的前缀，让LLM在生成输出的同时，也生成相应的推理链。这样可以让LLM更好地理解任务要求，避免一些错误或不一致的输出，提高推理的准确性和可解释性。 >例子： 算术： 如果你有12个苹果，你吃了3个，你还剩多少个？ -> 9个 推理链：12 - 3 = 9 在展示样例后面加上一个测试样例，比如： >>如果你有8个香蕉，你给了朋友4个，你还剩多少个？ >>模型就会根据前缀中的展示样例和测试样例，来生成一个输出和一个推理链，比如： >>如果你有8个香蕉，你给了朋友4个，你还剩多少个？ -> 4个 推理链：8 - 4 = 4 问：请仿照 function fn(t:boolean){ return true} 重构 fn 函数 答：根据您的要求，我尝试重构 fn 函数如下： 问：麻烦给这个函数提供一个完整的测试用例 答：根据网上的信息，要给一个函数提供一个完整的测试用例，需要考虑以下几个方面： - 测试函数的输入和输出是否符合预期 - 测试函数的边界条件和异常情况是否正确处理 - 测试函数的内部逻辑是否正确执行 - 测试函数的性能和效率是否达标 例如，使用 Jest 框架，可以给 fn 函数写以下测试用例： 机遇：GPT 在模式化需求中赋能明显 风险：Prompt 注意数据隐私（例如：提的 GPT Prompt 涉及到隐私信息） 观念：发现问题 > 定义问题 > 分解问题 > 解决问题 信息跟知识的区别？ 记忆跟学习的区别？ 例子1：学会了王者农药的某个英雄？这里的学会指的是什么？是对技能或者各种套路的熟练背诵还是在对线中各种秀？ 例子2：说一个人学会了羽毛球？这里的学会指的是什么？是对羽毛球各种动作跟对线策略的记忆，还是再实际对线中嘎嘎杀？ 学一个英雄的方式： 方案1： 从英雄技能描述或者教程开始（信息，记忆，机械记忆，指令记忆，抽象）-> 实践； 方案2：从对线的体感认识开始（知识，学习，理解记忆，归纳学习，实践），随着对线次数跟对线英雄样本变多，会出现对这个英雄超越常人的理解（主播经常说道的，超越一般人理解） -> 看描述跟教程矫正操作（回到抽象，监督学习）； GPT 进化前提：更丰富更高质量的资料样本（可能是局限）；更多的测评数据；更多的实践运用（例如 chatGPT），强化学习？ * 丰富：更多的训练资料 * 质量：资料的质量决定了 GPT 的进化水平，例如 wiki,quora 等高质量数据平台 * 强化学习：理论结合实际 ... 其他角度：记忆是学习的一种手段。记忆的目标是重现（有限，可知），学习的目标是泛化（无限，未知） * * * * *",
    "url": "/blog/GPT与开发-TDD",
    "path": "tech/ai/GPT与开发-TDD.md"
  },
  {
    "title": "GPipe",
    "content": "GPipe是一种用于大规模神经网络模型并行化的方法，它通过流水线并行（Pipeline Parallelism）将模型分割成多个阶段，并在不同的设备（如GPU）上进行计算，从而加速训练过程。梯度累积在GPipe中的应用可以帮助解决由于模型分割带来的批量大小限制问题。 梯度累积（Gradient Accumulation）是一种在深度学习中用来处理小批量（mini-batch）训练的问题的技术。它的主要作用是在显存有限的情况下，通过累积多个小批量的数据梯度，模拟更大批量的训练效果。这样可以利用较小的显存来进行更大批量的训练，进而提高模型的性能和收敛速度。 假设我们正在训练一个简单的神经网络，但由于显存限制，我们只能使用非常小的批量（比如 batch size = 2）。然而，我们希望通过梯度累积来模拟较大的批量（比如 batch size = 6）的训练效果。 假设我们有一个数据集，包含6个样本。我们用 batch size = 2 进行训练，每次更新参数时，只考虑这2个样本的梯度。 1. 第一步： - 输入：样本1和样本2 - 计算损失 - 计算梯度 - 更新模型参数 2. 第二步： - 输入：样本3和样本4 - 计算损失 - 计算梯度 - 更新模型参数 3. 第三步： - 输入：样本5和样本6 - 计算损失 - 计算梯度 - 更新模型参数 每一步都进行一次模型参数更新。 现在，我们使用梯度累积，将6个样本的数据梯度累积后再进行一次模型参数更新。 1. 第一步： - 输入：样本1和样本2 - 计算损失 - 计算梯度 - 累积梯度（梯度存储，不更新参数） 2. 第二步： - 输入：样本3和样本4 - 计算损失 - 计算梯度 - 累积梯度（梯度存储，不更新参数） 3. 第三步： - 输入：样本5和样本6 - 计算损失 - 计算梯度 - 累积梯度（梯度存储） 4. 更新模型参数： - 使用累积的梯度进行参数更新 在这整个过程中，我们只在第三步之后，使用累积的梯度更新了一次模型参数。这个效果相当于我们使用 batch size = 6 进行了一次训练。 以下是一个使用 PyTorch 的简单代码示例，演示如何实现梯度累积： 在这个例子中，我们通过设置 来控制梯度累积的次数。每次计算梯度后并不立即更新参数，而是累积起来，直到达到指定的次数才进行参数更新。这样，我们就可以用较小的 batch size 模拟较大 batch size 的训练效果，有效利用显存并提升模型性能。 梯度累积是一种在深度学习中处理小批量训练限制的有效技术。通过累积多个小批量的数据梯度，再进行一次模型参数更新，可以模拟更大批量的训练效果，从而在显存受限的情况下提高模型的性能和收敛速度。这种方法特别适用于显存有限但希望利用大批量训练优势的情况。 GPipe的主要思想是将模型分割成多个“段”（段落），每个段落在一个独立的设备上进行计算。数据通过这些段落进行前向传播和反向传播。在这种设置中，如果每个设备上只能处理小批量数据，梯度累积可以有效帮助模拟更大批量的训练。 1. 模型分割： - 将模型分割成多个连续的段（例如，4个段），并将每个段分配到不同的设备（例如，4个GPU）。 2. 流水线并行： - 输入批量数据分成多个微批次（micro-batch），例如一个大批量分成8个微批次。 - 微批次通过流水线在不同的设备上依次进行前向传播和反向传播。 3. 梯度累积： - 每个微批次的梯度在反向传播时进行计算并累积。 - 累积多个微批次的梯度后进行一次权重更新。 假设我们有一个分成4个段的模型，在4个GPU上运行，并且批量大小为32。我们可以将这个批量分成8个微批次，每个微批次大小为4。使用梯度累积，我们可以在每个GPU上累积4个微批次的梯度，然后再进行一次参数更新。 以下是一个简化的伪代码示例，演示GPipe中如何进行梯度累积： 梯度累积在GPipe中被应用来解决由于模型分割带来的批量大小限制问题。通过累积多个微批次的梯度并一次性更新模型参数，可以在使用小批量训练的情况下模拟更大批量的训练效果。这不仅有助于更有效地利用显存资源，还可以提高训练的稳定性和模型的性能。",
    "url": "/blog/GPipe",
    "path": "tech/ai/GPipe.md"
  },
  {
    "title": "NAS-RL",
    "content": "全称 Neural Architecture Search（神经架构搜索），是一种自动化搜索最佳神经网络架构的方法。 传统上，设计一个好的神经网络架构需要专家反复试验和直觉，而 NAS 通过自动化这一过程，NAS 通常使用一种称为「控制器」的模型来进行这些序列决策。这个控制器可能是一个 RNN 或者其他序列模型，它负责在每个步骤做出选择，根据先前的选择历史来决定下一层的类型。控制器通过不断地在搜索空间中探索并优化决策序列，最终找到一个在性能上最优的网络架构。 * 核心思想：将网络架构设计问题转换为一个序列决策问题 可以；它用一个深度学习模型（控制器）不断地生成和优化另一个用于实际任务的深度学习模型（目标模型）的架构。这种方法不仅减少了人工干预，还能发现潜在的、更优的神经网络架构。 类似于元学习（Meta-Learning）的一种思路，在元学习中，我们学习如何更好地进行学习。在 NAS 中，我们学习如何设计更好的深度学习模型。两者都体现了自我改进和自动化优化的理念。 通常，RNN 被用作控制器是因为其在处理序列数据方面的优势。下面我们详细解释一下控制器的原理以及为什么 RNN 通常被选择用作控制器。 1. 生成架构： - 控制器模型通过生成一系列离散的决策来定义目标模型的架构。每个决策对应一个具体的架构组件（如选择层的类型、大小、连接方式等）。 - 控制器的输出是一个序列，这个序列描述了目标模型的结构。例如，控制器可能输出一个这样的序列：Conv(64, 3x3) -> MaxPool(2x2) -> Dense(128)。 2. 序列决策： - 控制器在每一步生成一个决策，这个决策依赖于之前所生成的决策。这个过程自然而然地适合用处理序列数据的模型来实现，因为序列中的每个元素（即每个决策）都与前面的决策有关联。 3. 优化过程： - 生成架构后，对对应的目标模型进行训练，并在验证集上评估其性能。 - 将评估结果作为反馈给控制器，调整控制器的权重，使其倾向于生成性能更好的架构。这通常通过强化学习（Reinforcement Learning）或进化算法（Evolutionary Algorithms）来实现。 使用 RNN 是因为其在处理和生成序列数据方面的特点和优势。以下是详细原因： 1. 序列数据处理能力： - RNN 的设计使得其擅长处理序列数据，即可以根据前一步的输出影响下一步的输入，捕捉序列中的依赖关系。 - 在 NAS 中，网络结构是一个序列化的决策过程。例如，选择层的类型（卷积层、全连接层等）、层的参数（如滤波器数量、核大小等）、层与层之间的连接等，这些决策需要逐步生成，每一步都依赖于前面的决策。 2. 记忆和状态保持： - RNN 能够保留之前步骤的状态信息，这是通过其隐藏状态实现的。在架构生成过程中，每个决策都是基于前面所有决策的综合结果。 - 这种记忆能力在生成复杂的神经网络架构时特别有用，因为每一层的配置可能依赖于之前层的配置。 3. 灵活性： - RNN 可以很自然地扩展到生成变长序列，这对于生成不定长的神经网络架构非常重要。 - 控制器可以学习到在何时停止生成新的层，从而生成不同长度的架构。 1. 初始化： - 控制器 RNN 初始化隐藏状态和开始令牌，准备生成第一个决策。 2. 生成第一个决策： - 根据当前隐藏状态和输入令牌（开始令牌），RNN 生成第一个决策，例如选择一个卷积层 Conv(64, 3x3)。 3. 更新状态并生成后续决策： - RNN 将当前决策作为输入，并更新隐藏状态，生成下一个决策，例如选择一个最大池化层 MaxPool(2x2)。 4. 重复： - 重复以上步骤，直到生成完整的神经网络架构。 5. 输出架构： - 控制器输出整个序列，表示生成的神经网络架构。 6. 训练并评估目标模型： - 用该架构训练目标模型，并在验证集上评估其性能。 7. 反馈： - 将评估结果作为奖励（或惩罚），反馈给控制器 RNN，调整其权重，以优化架构生成策略。 虽然 RNN 是常用的控制器模型，但也有其他方法可以用来实现控制器的功能： 1. 注意力机制（Attention Mechanism）： - 使用 Transformer 等基于注意力机制的模型提升生成速度和性能。 2. 强化学习（Reinforcement Learning）： - 利用强化学习中的 Q-learning 或者 Actor-Critic 方法来替代 RNN 生成架构决策。 3. 进化算法（Evolutionary Algorithms）： - 使用基因算法（Genetic Algorithm）等进化算法通过变异、交叉等操作生成和优化架构。 使用 RNN 作为控制器模型的方式使得 NAS 能够处理复杂的结构生成任务并捕捉到生成过程中序列决策的依赖关系。它的序列处理能力和状态保留特性，使其成为生成神经网络架构的理想选择。当然，随着技术的进步，也出现了其他替代方法，可以根据实际需求选择合适的控制器模型。 在神经网络架构搜索（NAS）中，由于控制器生成的模型数量可能非常庞大，这会导致训练和评估整体耗时漫长。以下几种方法可以有效缓解这个问题： 代理任务是一种缩短每次训练和评估时间的方法，通过对一个简化但相关性强的子任务进行优化，以推测完整任务上的性能。 - 缩小数据集：对一个更小的数据集进行训练和验证，尽快获得性能的反馈。 - 缩小模型规模：使用简化的模型（如较少的参数和层）以减少每次训练时间。 - 早停法：监测模型的验证集性能，在检测到过拟合趋势时立即中止训练。 并行搜索利用分布式计算资源同时评估多个候选模型的大规模并行能力。例如： - 分布式计算：用多个计算节点同时进行训练和评估，充分利用计算资源。 - 多GPU训练：在多GPU平台上进行并行训练，通过同步或者异步评估减少总体时间。 （迁移学习） 基于权重分享的方法（例如 One-shot NAS 和NASNet）通过共享部分权重，不需要每个架构分别训练。 - One-shot模型：所有候选架构共享一个超大模型中的参数，通过不同路径选择进行训练和评估，只需训练一次。 - 权重继承：在不同候选架构之间传递和共享权重，无需完全从头开始训练每个模型。 利用强化学习和进化算法进一步优化架构生成和选择的效率。 - 强化学习：优化控制器生成决策的策略，使其更快找到高质量的架构。这可以减小搜索空间，提高搜索效率。 - 进化算法：通过选择、交叉、变异等操作，确保每代评估的架构具有更高的潜力。 代理模型（例如元学习）可以用来预测未训练架构的性能，从而减少实际训练的次数。 - 性能预测器：训练代理模型，根据当前架构的一些特征快速估计其性能，以替代或减少实际训练。 - 元学习：基于以前的训练结果，学习更好的模型搜索策略，提高样本效率。",
    "url": "/blog/NAS-RL",
    "path": "tech/ai/NAS-RL.md"
  },
  {
    "title": "UNet",
    "content": "UNet是像素级分类，输出的则是每个像素点的类别，常常用在生物医学图像上，而该任务中图片数据往往较少。(卷积神经网络一般输出的结果是整个图像的类标签。) * UNet采用全卷积神经网络 * 左边网络为特征提取网络：使用conv和pooling * 右边网络为特征融合网络：使用上采样产生的特征图与左侧特征图进行concatenate操作。原因 1. 特征融合：U-Net的设计采用了跳过连接（skip connection），将编码阶段（收缩路径）的特征图与解码阶段（扩展路径）对应层级的特征图进行拼接。这种操作的目的是为了融合不同分辨率层级的特征信息，既包括高级语义特征（在解码阶段通过上采样获得的低分辨率、高级别的特征），也包括低级细节特征（在编码阶段直接从输入图像中提取的高分辨率、低级别特征）。 2. 细节恢复：在图像分割中，细节是非常重要的。U-Net的编码阶段会不可避免地丢失一些局部细节信息，因为卷积和池化操作会降低空间分辨率。通过将编码阶段的特征与解码阶段的特征合并，模型能够重新利用这些细节信息，帮助更精确地定位边界和恢复图像的详细结构。 3. 上下文信息：解码阶段的上采样逐步恢复图像的空间分辨率，但这些上采样的特征可能缺乏必要的上下文信息来正确地分割图像。通过concatenate操作，模型在每次上采样后将之前阶段的特征图引入到当前层级，通过这种方式整合了高层次的上下文信息。 4. 改善梯度流：跳过连接不仅有助于在网络中融合不同层次的特征，还有助于改善梯度在网络中的流动，这对于训练深度网络至关重要。这样的网络设计减少了训练中的梯度消失问题，并有助于网络的更深层次训练。 5. 训练稳定性：通过重新引入编码阶段的特征，可以稳定网络的训练过程，加速收敛，并提高模型对噪声和其他训练不稳定因素的鲁棒性。 - 低级别的特征：在网络的较早阶段（也就是靠近输入层）提取的特征被称为\"低级别\"的特征。这些特征通常与原始图像密切相关，捕捉到的是图像中的基础信息，比如边缘、角点、颜色和纹理等。在高分辨率的图像中，这些特征能够提供详细的空间和结构信息。 - 高级别的特征：在网络的较深阶段（也就是更靠近输出层）提取的特征被称为\"高级别\"的特征。这些特征表达的内容更为抽象，有更高的语义含义，比如识别出整个物体、情感、场景等结构性和语义性强的信息。因为通过多次池化或卷积操作，这些特征的空间维度(分辨率)通常比原始输入要小，但它们能够表示的信息更为全局和高级。 举个例子，假设我们使用一个深度学习模型处理图像中的猫。低级别的特征可能会捕捉到猫的边缘、毛发的纹理、眼睛的颜色等信息；而高级别的特征可能已经能够捕捉到整个猫的形状，或者猫的情绪（悠闲、警觉等）。 Dice Loss 是一种用于图像分割任务的损失函数，旨在应对正负样本强烈不平衡的场景。让我来详细解释一下。 - Dice Coefficient（Dice系数）是Dice Loss的基础。它是一种用于评估两个样本相似性的度量函数，取值范围在0到1之间，值越大表示越相似。Dice Coefficient定义如下： $$ \\text{Dice} = \\frac{2|X \\cap Y|}{|X| + |Y|} $$ 其中，$|X \\cap Y|$表示样本X和Y之间的交集，$|X|$和$|Y|$分别表示X和Y的元素个数。分子乘以2是为了保证分母重复计算后取值范围在[0, 1]之间。 dice_loss = 1 - dice_coefficient，所以优化目标是缩小 dice_loss 的值。 召回率（Recall）是一种用于评估分类模型性能的指标，特别是在二元或多分类中，常常用于检测模型找到相关实例的能力。在目标检测或信息检索中，召回率也很重要。 具体来说，召回率定义如下： $$ \\text{Recall} = \\frac{TP}{TP + FN} $$ 其中： - TP（True Positive，真正例（真阳））：被正确预测的正例，即该数据的真实值为正例，预测值也为正例的情况。 - FN（False Negative，假阴性）：被错误预测的反例，即该数据的真实值为正例，但被错误预测成了反例的情况。 召回率衡量了模型正确识别为正类的实例（真正类）占所有实际正类实例的比例。在某些场景下，召回率比准确率更重要，例如医学诊断中，我们更关心是否漏诊（即FN 阳的被诊断为阴性）。 * 为什么最后 U-Net 的损失是交叉熵跟 dice_loss 组成？ 在U-Net这样的图像分割模型中，通常将交叉熵损失（Cross-Entropy Loss）和Dice损失（Dice Loss）结合使用，原因主要有以下几点： 1. 类不平衡（Class Imbalance）：在医学图像分割等任务中，目标区域（感兴趣的区域，如肿瘤）往往比背景（其余部分）要小得多，这导致了类别之间的不平衡，简单的交叉熵损失函数可能会导致模型偏向于多数类。因此，需要一个比单一的交叉熵损失更能够处理类不平衡的损失函数。 2. Dice系数的特性：Dice系数是一个衡量两个样本间相似度的统计量，它的值介于0到1之间，当值为1时表明预测和真实标签之间完全一致。它在图像分割任务中极其有用，因为它直接量化了预测和真实标签之间的重叠区域。这种度量对于捕获边缘区域尤其重要，正如分割任务中常常遇到的。 3. 交叉熵和Dice损失结合的优势：交叉熵损失关注于像素级别上的分类准确性，每个像素点分类的正确与否都会影响到损失。而Dice损失关注预测区域与真实区域的重叠度。这两种损失在目标函数中的结合使得模型可以在整体（全局分割准确性）和局部（特别是小区域或边缘的分割）方面进行优化。 4. 梯度行为：交叉熵损失和Dice损失对梯度的贡献也有所不同。交叉熵对于接近决策边界的样本会产生较大的梯度，这有助于确保学习进程。而Dice损失，则可以在模型对某些类别的预测很自信时，仍然提供一定的梯度，因而有助于学习过程中对不平衡数据的校正。 5. 性能提升：实验表明，仅使用交叉熵或Dice损失的模型性能通常不如两者结合使用。通过融合这两种损失函数，模型可以同时从各自的优点中受益。 * UNet 跟 SAM 的区别？ * * * * gpt",
    "url": "/blog/UNet 模型",
    "path": "tech/ai/UNet 模型.md"
  },
  {
    "title": "VAE",
    "content": "VAE（变分自编码器）的重参数化技巧是一种使得模型能够通过梯度下降来训练的关键技术。要理解这个技巧涉及的关键数学知识，以下是几个重要的方面： 变分推断是一种近似后验分布的方法。它将难以处理的后验分布 $ p(z|x) $ 近似为一个易处理的分布 $ q(z|x) $。在 VAE 中，我们选择 $ q(z|x) $ 为一个高斯分布： $ q(z|x) = \\mathcal{N}(z; \\mu(x), \\sigma^2(x)) $ 其中，均值 $ \\mu(x) $ 和方差 $ \\sigma^2(x) $ 都是从输入数据 $ x $ 生成的。 训练 VAE 的目标是最大化数据的边际对数似然 $ \\log p(x) $。由于直接计算这个对数似然很困难，我们引入了证据下界（ELBO）： $ \\log p(x) \\geq \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) \\parallel p(z)) $ 其中， - $ \\mathbb{E}_{q(z|x)}[\\log p(x|z)] $ 是重建损失，表示在潜变量 $ z $ 上的重构误差的期望。 - $ D_{KL}(q(z|x) \\parallel p(z)) $ 是 Kullback-Leibler (KL) 散度，表示近似后验分布和先验分布之间的差异。 为了能够通过梯度下降法优化 ELBO，我们需要对随机变量 $ z $ 进行梯度计算。直接对 $ z $ 采样会导致梯度无法传播。重参数化技巧通过引入一个独立于参数的噪声变量 $ \\epsilon $ 来解决这个问题。 假设 $ z $ 是从高斯分布中采样的： $ z \\sim \\mathcal{N}(\\mu, \\sigma^2) $ 我们可以将 $ z $ 重参数化为： $ z = \\mu + \\sigma \\cdot \\epsilon $ 其中，$ \\epsilon \\sim \\mathcal{N}(0, I) $ 是一个标准正态分布的噪声。通过这种重参数化，$ \\mu $ 和 $ \\sigma $ 的梯度可以通过 $ \\epsilon $ 来传播，从而使得模型可以使用梯度下降法进行训练。 1. 概率论和统计学： - 高斯分布及其性质 - Kullback-Leibler 散度 2. 变分推断： - 近似后验分布的选择和优化 3. 优化方法： - 证据下界（ELBO）的推导和优化 - 重参数化技巧确保梯度的可传播性 在变分自编码器（Variational Autoencoder, VAE）的上下文中，连续性和完整性是两个重要的概念，关系到生成模型的质量和性能。 连续性指的是在潜在空间（latent space）中，两个相邻的点对应的生成数据在数据空间中也是相似的。这意味着，潜在空间中的小的变化会导致生成的数据空间中的小的变化。换句话说，如果我们在潜在空间中进行连续的插值操作（interpolation），生成的数据也应该表现出平滑的变化。 完整性指的是潜在空间中的每一个点都能够解码为合理的生成样本，即潜在空间中的所有区域都对应于数据空间中的有效数据。这样，我们可以确保通过VAE生成的样本是多样的，覆盖了训练数据的全部分布。 VAE通过将输入数据编码为高斯分布的参数（均值和方差），而不是一个单一的点，从而引入了随机性和正则化。这个过程涉及到在潜在空间中采样，并且通过KL散度（Kullback-Leibler divergence）正则化，使得编码的分布接近标准正态分布（通常是零均值和单位方差）。 1. 连续性：由于潜在空间中的点是通过高斯分布采样得到的，这些点的分布是连续的。VAE通过在潜在空间中采样，然后解码为数据空间中的样本，确保了潜在空间的连续性。即使是相邻的潜在空间点，也会解码为相似的数据空间点。 2. 完整性：通过KL散度正则化，VAE鼓励潜在空间的分布接近标准正态分布，这样可以确保潜在空间的每个区域都能生成有效的数据样本。即使潜在空间中的任意点也能解码为合理的数据，从而保证了潜在空间的完整性。 简而言之，VAE将输入编码为分布（而不是点）通过正则化和采样技术，确保了潜在空间的连续性和完整性，从而使得生成的样本更加多样化且平滑。",
    "url": "/blog/VAE",
    "path": "tech/ai/VAE.md"
  },
  {
    "title": "t-SNE 算法",
    "content": "* 一种用于数据可视化的降维技术，特别适用于高维数据的可视化。 * 通过将高维数据映射到二维或三维空间，t-SNE 可以展示数据的聚类结构和分布情况。通过 SNE 算法（容易拥挤）改进（联合概率分布替换条件概率分布解决分布计算对称问题，并引入 t 分布）。 t-SNE算法的目标是将高维数据映射到低维空间（通常是二维或三维），同时尽可能保持数据点之间的相似度。为了实现这一目标，t-SNE构建了两个概率分布：一个是高维空间中的相似度分布P，另一个是低维空间中的相似度分布Q。然后，算法通过最小化这两个分布之间的Kullback-Leibler散度（KL散度）来优化低维嵌入。在低维空间中，t-SNE使用t分布来定义数据点之间的相似度，这有助于解决“拥挤问题”，使得数据点在低维空间中能够更均匀地分布。 “t”指的是t分布（或称学生t分布、柯西分布）。t-SNE是一种专为高维数据可视化设计的降维技术，其核心原理在于，首先在高维空间中利用高斯分布来定义数据点之间的相似度，而在低维空间中则采用t分布来定义相似度。这两种分布的选择有助于算法在低维空间中保持高维空间的数据结构。 t 分布的重尾特性使得： * 远距离点之间的排斥力增大 * 近距离点之间的吸引力减小 * 更容易形成簇 * 簇间距离更明显 单样本 t 检验：如果我们用一个样本估计总体均值，样本量为 n，则自由度通常是 n−1。这是因为我们计算样本的均值时用了一个数值，使得只有 n−1 个数据点可以自由变化。 * 自由度影响统计分布的形状，特别是小样本情况下。例如，在 t 分布中，自由度越小，分布的尾部越厚，因为小样本的估计不确定性更大。随着自由度增加，t 分布逐渐接近正态分布。 * 自由度还决定了假设检验的临界值和置信区间的宽度——自由度越小，检验标准会更加严格，置信区间也会变宽。 * X 轴：表示随机变量的取值，反映取值的大小。 * Y 轴：表示每个取值的相对概率密度（并非直接的概率）。 * 基因科学 * 细胞分类：将单细胞基因表达数据降维，通过 t-SNE 在 2D 或 3D 中展示，使得具有相似基因表达模式的细胞群靠近，可以区分不同细胞类型。 * 疾病分群：在基因组数据中，t-SNE 可帮助发现不同患者样本的模式差异，例如将癌症样本聚类在一起，展示亚型或分子特征。 * 基因特征分析：t-SNE 可以用于探索基因间的相似性或特征模式，帮助识别在特定条件下具有相似表达的基因簇。 * computer security：t-SNE 可以用于流量日志分析 * 异常检测：在网络流量、登录记录、或文件访问等高维数据中，t-SNE 可以将数据降维，帮助识别异常模式或异常点，从而检测可能的入侵或恶意活动。 * 恶意软件分类：将恶意软件样本的特征（如行为特征或静态特征）降维，通过可视化识别相似的恶意软件家族，帮助分析不同种类的威胁。 * 用户行为分析：将用户访问数据或活动日志降维，显示正常和异常行为群体，帮助识别潜在的账户接管或欺诈行为。 * 自然语言处理 * 词嵌入可视化：通过 t-SNE 将高维词嵌入（如 Word2Vec 或 BERT 的嵌入）降维为 2D 或 3D，可以直观显示词语之间的相似性和聚类，比如“国王”与“王后”靠近，或“苹果”与“橙子”形成同一类。 * 文档聚类：对句子或文档的嵌入进行降维，将主题相似的文档或句子分组，从而帮助发现相似的文本类别（如新闻主题或情感相似度）。 * 情感分析：将带有情感标签的句子降维，查看正向、负向情感句子在图中的分布情况，验证模型效果。 * K-means 是一种聚类算法，用于将数据点分组为 K 个簇，每个簇的中心是簇内数据点的平均值。 * t-SNE 是一种降维算法，用于将高维数据映射到低维空间，同时保持数据点之间的局部结构。 t-SNE 更倾向于可视化数据之间的相似性，而 K-means 更倾向于数据之间的聚类。 * 为什么说线性降维更好的保留了全局结构，而非线性降维更好的保留了局部结构？ >线性降维（如 PCA）保留全局结构，因为它基于数据的主成分，将整个数据集的方差最大化，以保持所有数据点的整体关系。在这种方法中，高维空间中的全局方向和差异在低维空间中得到最大保留。 > 非线性降维（如 t-SNE）更注重保留局部结构。它侧重于保持相似数据点之间的局部关系，而不在意全局距离。这样可以在低维空间中更清晰地显示数据的局部聚集模式，适合复杂、非线性分布的数据可视化。 * 形似性跟聚类的区别？ 不同目标 > * t-SNE：主要用于可视化数据的相似性，通过降维将数据点的相似关系展示在二维或三维空间中。它并不定义“组”，而是让相似的数据点在空间中靠近，适合查看数据的分布形态。 > * K-means：是一种聚类算法，直接将数据分成多个聚类组。它的目标是最小化组内的差异，而不是追求可视化的相似性关系。 因此，t-SNE 强调“展示相似性”，K-means 强调“形成类别”。 * PCA 是一种线性降维算法，通过线性变换将数据映射到低维空间，同时保留数据的主要特征。 * t-SNE 是一种非线性降维算法，通过计算数据点之间的相似度，将数据点映射到低维空间，并保持数据点之间的局部结构。 因此，t-SNE 可以更好地保留数据的局部结构，而 PCA 则更倾向于保留数据的全局结构。 * 线性跟非线性降维区别？ > 线性降维：假设数据在高维空间中呈现直线或平面分布，通过找到数据的主成分（如 PCA）将数据投影到更低维的空间。它适用于线性分布的数据，能够保留整体结构。 > 非线性降维：假设数据位于弯曲的流形上，数据可能具有复杂的局部结构。方法如 t-SNE 和 Isomap 会保留数据的局部相似性，适合处理复杂、非线性关系的数据，更适合可视化和聚类。 隐空间都是基于流形空间建模 不同点 * VAE 的隐空间：通常具有一定的连续性和结构化性质。例如，VAE 的隐空间可以连续变化（KL 散度用于正则化隐空间的分布，使得隐空间的分布尽可能接近一个标准正态分布（通常是高斯分布）。正则化的目的是使隐空间连续且平滑），解码器可以将隐空间中相邻的点映射成相似的数据。对隐空间进行插值可以生成新的样本。 * t-SNE 的映射空间：因为 t-SNE 注重局部结构，其结果比较离散，且映射空间中的距离和高维空间中的距离不具有线性对应关系，因此无法用来生成数据。t-SNE 的低维空间只在视觉上保留局部关系，而无法生成数据或保持样本连续性。 特征 * 数据的非线性结构 * 流形更适合保留局部结构 * 隐空间的流形结构能够在生成过程中保持样本的平滑性和连贯性，从而生成自然的样本。（VAE，GAN） * 如果隐空间是流形而不是简单的平面空间，则模型可以在隐空间中“移动”来生成具有渐变特征的样本。例如，生成不同姿态的图像或生成具有不同风格的文本。 * 解释的很清晰 * 包含 code 演示 * gpt",
    "url": "/blog/t-SNE",
    "path": "tech/ai/t-SNE.md"
  },
  {
    "title": "《The Bitter Lesson》",
    "content": "\"The Bitter Lesson\"是一篇由计算机科学家理查德·萨顿（Richard Sutton）撰写的论文。它提出了一个重要的观点，即在人工智能和机器学习的发展过程中，最重要的教训是利用计算资源和数据量的增长。 * 传统的人工智能方法受限：在过去的几十年中，人们在构建人工智能系统中主要依赖于手动设计的规则和特定的专家知识。然而，这种方法面临着挑战，因为设计和实现这些规则往往非常复杂，且需要大量的专业领域知识。 * 利用计算资源和数据量的增长优势：随着计算资源和数据量的增长，我们可以采用更简单且更通用的方法来构建人工智能系统。这种方法通过大规模的数据和计算能力来训练机器学习模型，使其根据数据自动地学习规律和特征。 * 从神经网络的再兴中得出教训：神经网络的成功复兴是一种不同的方法论，它充分利用了计算资源和数据量的增长。通过使用更大和更深的神经网络，以及大规模训练数据，神经网络能够实现各种复杂的任务，包括图像识别、自然语言处理等。 * 简单通用的方法更具优势：在发展人工智能和机器学习方法时，应该倾向于更简单而通用的方法。这些方法具有更好的扩展性和适应性，可以从更少的先验知识和人工规则中进行学习，使系统能够在更广泛的任务和环境中表现出色。 总的来说，\"The Bitter Lesson\"表达了一种重要的观点，即在人工智能和机器学习领域中，随着计算资源和数据量的增长，通过利用更简单且更通用的方法，让机器从数据中自动学习更多的规律和特征，可能会比依赖于手动设计的复杂规则更具优势。 * 象棋和围棋：这两种棋类游戏都是人工智能研究的经典领域，也是大规模计算的一般方法取得突破性进展的领域。例如，AlphaGo 和 AlphaZero 都是基于深度神经网络和强化学习的系统，它们可以通过自我对弈来提高自己的水平，而不需要人类的先验知识。这些系统在与人类顶尖棋手的对决中，展示了超越人类的水平。 * 图像识别：这是人工智能研究的另一个重要领域，也是大规模计算的一般方法取得显著进步的领域。例如，Midjourney V5 是一种能够根据文本描述生成高质量图像的系统，它是基于深度生成对抗网络 (GAN) 的技术，它可以从大量的图像数据中学习图像的特征和分布，而不需要人类的领域知识。这种系统在图像创作和图像编辑等任务中，展示了惊人的创造力和灵活性。 * SAM 模型比 UNet 更加通用，SAM 采用了一种提示型的训练方法（UNet 是监督学习），通过提示来生成分割掩码，使其具有更强大的零样本泛化能力。 * ChatGPT：这是一种基于 GPT-4 的语言模型，它可以根据上下文理解和生成类似人类的文本，它也是基于大规模计算的一般方法的一个例子，它可以从海量的文本数据中学习语言的规律和知识，而不需要人类的规则或语法。这种系统在对话、写作、翻译等任务中，展示了流畅的语言能力和广泛的知识面。 * 忽略了问题的特殊性和复杂性，导致无法找到最优的或最合适的解决方案。例如，一些通用的机器学习算法可能无法适应一些特定的数据分布或任务需求，需要进行定制化的改进或设计。 * 例子：使用通用的神经网络来识别手写数字时，可能会遇到一些噪声、模糊或变形的数字，这些数字可能与训练数据集中的数字有很大的差异，导致识别的准确率下降。 * 需要大量的计算资源和数据，导致效率低下或成本高昂。例如，一些通用的深度学习模型可能需要数百万甚至数十亿的参数，需要大规模的硬件设备和数据集来训练和部署。 * * 安全性或可靠性的风险，导致不可预测的或不可控制的后果。例如，一些通用的人工智能系统可能会受到对抗攻击或数据污染的影响，导致输出错误的或有害的结果 。 *",
    "url": "/blog/the bitter lesson",
    "path": "tech/ai/the bitter lesson.md"
  },
  {
    "title": "一个无监督学习的理论",
    "content": "早在 2023 年 10 月 3 日，Ilya 曾在伯克利大学做过一次演讲，题为《一个无监督学习的理论》（A Theory of Unsupervised Learning）。由于内容艰涩，知晓者寥寥，而它却是人工智能史上最重要的时刻之一，注定将载入史册。 观点 * 监督学习：低训练误差+大训练集，就能确保模型的泛化能力 * Hoeffding 不等式，其主要含义是：当训练误差足够低，且训练样本数远大于「模型自由度」（可以理解为模型的规模）时，测试误差也能保证足够低； * 宏观上和理论上，万能近似定理（Universal Approaximation Theorem）早已论证了深层神经网络可以逼近任意函数。 * 模型规模一定要小于数据规模，否则，它根本就不用做真正的「压缩」或抽象，不去找规律，它就全部死记硬背了。我们知道死记硬背的模型，没有泛化能力。 * 无监督学习的本质是分布匹配，是一种规律性的模式匹配 * 只要两种语言原生数据足够丰富，一种语言的输入作为条件就能几乎唯一地确定另一种语言的翻译等价物，就是所谓「压缩」理论 * 无监督学习其实就是在寻找最优的数据压缩方法 * 一个好的无监督学习算法，应该能找到数据的最简洁表示（即 K 氏复杂度），同时又能最大限度地利用这种表示来完成下游任务 * 从条件建模到联合建模 * 与其像监督学习那样将 X 和 Y 视为条件与结果，不如将它们视为一个整体，在一个巨大的模型里面一起进行压缩。也就是说，我们要寻找一个联合的 K 氏复杂度 K(X,Y)，即同时压缩 X 和 Y 的最短程序长度，这就是我们的无监督学习出来的预训练大模型（LLM）。 * 无监督学习的新范式，它将传统的独立建模（如英语模型、汉语模型；再如，语言模型、视觉模型，等等）提升到了大一统的关联建模的高度。在这个范式下，无监督学习的目标不再是单纯地压缩单一群体的数据，而是寻找数据之间的联系。 压缩的对象是数据集，而不是数据点，这一点非常重要，这其实是形式压缩与内容压缩的分水岭。形式压缩只是一个机械过程，产生不了智能。只有内容压缩才能成就人工智能。* 个体对象转变为群体对象的时候，形式的压缩就自然转化为内容的压缩。这是因为群体虽然是个体组成的，但为群体压缩，如同是为群体「画像」，勾勒的是群体的统计性形象，它看上去可能是个个体，但它不是原数据中的任何一个特定的个体复制，否则就不是模型，而是记忆库了。* * 大模型压缩的本意就是要找出数据集的特征和规律性。大模型 GPT4 生成的文字，我们可能似曾相读；大模型 Suno 生成的音乐，我们可能似曾相闻；大模型 Sora 生成的视频，我们可能似曾相见；大模型 MJ 生成的图片，我们可能似曾相识。 这种跨模式、跨模态的学习，才是通用人工智能的高级形态。 * *",
    "url": "/blog/一个无监督学习的理论",
    "path": "tech/ai/一个无监督学习的理论.md"
  },
  {
    "title": "中心极限定理（CLT）",
    "content": "中心极限定理（Central Limit Theorem, CLT）是概率论和统计学中的一个重要定理。它描述了当独立随机变量的数量足够大时，这些变量的平均值的分布趋向于正态分布的性质。具体来说，中心极限定理指出： 如果 $ X_1, X_2, \\ldots, X_n $ 是一组独立同分布的随机变量，每个随机变量的期望值为 $ \\mu $，方差为 $ \\sigma^2 $，那么当 $ n $ 趋于无穷大时，这组随机变量的标准化和的分布将趋近于标准正态分布，即均值为 0，方差为 1 的正态分布。 用公式表示为： $frac{1}{n} \\sum_{i=1}^{n} X_i \\xrightarrow{d} \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n}) $ 当 $ n $ 足够大时，中心极限定理可以简化为： $frac{\\sum_{i=1}^{n} (X_i - \\mu)}{\\sigma\\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0, 1) $ 这里，符号 $ \\xrightarrow{d} $ 表示趋向于分布。 1. 普遍适用性：中心极限定理适用于许多类型的独立同分布随机变量，不论这些变量的原始分布是什么。当样本量足够大时，它们的平均值分布总是会接近正态分布。 2. 统计推断基础：中心极限定理是许多统计推断方法的基础，例如置信区间和假设检验。由于样本均值的分布趋向于正态分布，我们可以使用正态分布的性质来进行统计分析和推断。 3. 实用性：在实际应用中，中心极限定理帮助我们理解为什么许多自然现象和测量数据呈现正态分布。例如，人的身高、考试成绩和测量误差等都可以用正态分布来近似描述。 假设我们有一个袋子，里面有很多个硬币。每个硬币掷一次的结果都是独立的，且都是公平的（每个硬币掷出正面或反面的概率都是 0.5）。我们不断掷硬币，并记录每次掷出正面的次数。尽管每次单独掷出的结果（正面或反面）不一定是正态分布的，但如果我们掷很多次，并计算总的正面次数的均值，这个均值会趋向于正态分布。 中心极限定理（Central Limit Theorem, CLT）在统计学和各种实际应用中起着重要作用。以下是几个实际应用场景： 在制造业中，质量控制是一个重要的环节。假设我们在生产线上制造螺栓，并且每个螺栓的长度是一个随机变量。虽然每个单独螺栓的长度可能不遵循正态分布，但通过测量大量螺栓的长度并计算其平均值，我们可以假设这些平均值将趋近于正态分布。 - 计算均值和标准差：假设每小时我们测量 100 个螺栓的长度，并记录每小时的平均长度。根据中心极限定理，这些平均长度的分布将接近于正态分布。 - 设定控制图：利用正态分布的性质，我们可以设定上下控制限来监控生产过程。如果某一小时的平均长度落在控制限之外，则表示生产过程中可能存在问题，需要检查和调整。 假设检验是统计学中用于判断一个样本是否来自某个特定分布的重要工具。中心极限定理为假设检验提供了理论基础。 - 样本均值的检验：假设我们要检验某种药物对血压的影响。我们从总体中抽取一个样本，并计算样本的平均血压。如果样本量足够大，根据中心极限定理，样本均值的分布将接近正态分布。 - 计算 p 值：利用正态分布的性质，我们可以计算样本均值与总体均值之间的差异的 p 值，以确定该差异是否具有统计显著性。 在金融领域，许多投资回报率的分布可以通过中心极限定理来近似描述。尽管单个股票的日收益率可能不遵循正态分布，但大量股票的平均收益率可以近似为正态分布。 - 投资组合分析：假设一个投资组合包含大量股票。根据中心极限定理，这些股票的平均日收益率将接近正态分布。这使得我们可以使用正态分布的性质来分析投资组合的风险和收益。 - VaR（在险价值）计算：利用正态分布的性质，我们可以计算投资组合在一定置信水平下的最大可能损失，即 VaR。 民意调查是统计学中常见的应用，通常用于了解公众对某个问题的态度或意见。尽管单个受访者的回答可能不遵循正态分布，但大量受访者的平均回答将接近正态分布。 - 估计总体比例：假设我们想知道某个城市中有多少人支持某个政策。我们可以抽取一个样本进行调查，并计算样本中支持该政策的比例。根据中心极限定理，这个样本比例的分布将接近正态分布。 - 设定置信区间：利用正态分布的性质，我们可以为样本比例设定置信区间，从而估计总体中支持该政策的比例。 我们要证明的是，对于一组独立同分布的随机变量，当样本大小 $n$ 足够大时，样本均值 $\\bar{X}$ 的分布可以近似为正态分布。 假设我们有 $n$ 个独立同分布的随机变量 $X_1, X_2, \\ldots, X_n$，每个随机变量的期望 $\\mathbb{E}[X_i] = \\mu$ 和方差 $\\text{Var}(X_i) = \\sigma^2$。 首先，我们定义标准化的样本均值（标准化是为了使得均值为 0 ，方差为 1）： $ Z = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} = \\frac{\\frac{1}{n} \\sum_{i=1}^n X_i - \\mu}{\\sigma / \\sqrt{n}} $ 我们的目标是证明 $Z$ 服从标准正态分布 $N(0, 1)$ 当 $n$ 充分大时。 1. 构建标准化随机变量: 定义 $Z_n$ 为： $ Z_n = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\frac{X_i - \\mu}{\\sigma} = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\frac{X_i - \\mu}{\\sigma} $ 其中，每一个 $\\frac{X_i - \\mu}{\\sigma}$ 都是独立同分布的标准化随机变量，期望为 0，方差为 1。 2. 应用莫夫•格里亚版大数定理: 由于 $\\frac{X_i - \\mu}{\\sigma}$ 是独立同分布的，期望为 0 方差为 1 的随机变量，根据狄莫佛-拉普拉斯极限定理（这是中心极限定理的原型之一）： 当 $n$ 足够大时，$\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\frac{X_i - \\mu}{\\sigma}$ 会趋近于标准正态分布 $N(0,1)$。 3. 结合两步: 因此， $Z_n = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\frac{X_i - \\mu}{\\sigma}$ 逼近 $N(0,1)$。 4. 回到原问题: 回溯到我们实际要证明的 $Z$ ： $ Z = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} $ 根据上述结果，$Z_n \\approx N(0, 1)$，所以 $Z$ 也趋近于标准正态分布 $N(0, 1)$。 一种更直观的理解方法是通过模拟来观察样本均值分布的行为： 通过上述模拟，你会看到样本均值的分布逐渐接近正态分布。 中心极限定理的简化证明依赖于一个关键思想：通过标准化操作把一组独立同分布的随机变量和它们的均值转换成一个标准正态分布。这是通过数学推导和极限理论来证明的，但直观上可以通过大量样本均值的分布来观察和确认。这种定理在统计推断和应用中具有广泛的重要性，因为它为我们提供了从样本推断总体性质的有力工具。",
    "url": "/blog/中心极限定理（CLT）",
    "path": "tech/ai/中心极限定理（CLT）.md"
  },
  {
    "title": "卷积与全连接网络",
    "content": "卷积核（有参数）：扩充通道数目，提取特征；例如：输入 3维，期望输出 9维，则卷积核需要 9 个，并且每个维度为 kk*3(对应输入维度) * 池化层（无参数）：用于二维，降低空间维度（不改变通道数），并不参与特征提取；例如 2x2 池化窗口，则能将每个通道降维到 w/2 x h/2 * 参数共享： 卷积网络通过卷积操作利用参数共享的机制，这意味着在处理图像时，同一个卷积核（滤波器）会应用在整个图像上。这种方法大大减少了模型的参数数量，使得卷积网络在处理图像等高维数据时更加高效。 * 局部连接： 卷积层的神经元仅与输入数据的一个局部区域相连接，这与全连接层的每个神经元连接到输入数据的所有元素形成鲜明对比。局部连接使得卷积网络能够捕捉到输入数据中的局部特征，这在图像识别等任务中非常有用。 * 空间层次结构： CNNs 通过多个卷积层和池化层（pooling layers）的堆叠，能够自然地学习输入数据的空间层次结构。在这种架构中，底层卷积层可能专注于学习边缘或纹理等低级特征，而高层卷积层则能够组合这些低级特征来识别更复杂的模式。 * 减少过拟合： 由于卷积网络通常有更少的参数和内置的正则化效果（如参数共享和池化），它们在处理复杂图像任务时比全连接网络更不容易过拟合。 * 适用于图像数据： 图像数据具有很强的空间结构特征，卷积网络能够有效地利用这一点。在图像数据中，相邻的像素通常相关性较高，而远离的像素相关性较低。卷积网络通过卷积层直接对这种空间结构进行建模，而全连接网络则没有这种直接的建模能力。 平移不变性： * 卷积网络具有一定程度的平移不变性，这意味着即使图像中的对象发生了平移，卷积网络仍然能够识别出来。这是因为同一个卷积核在整个图像上滑动，学习到的特征对于图像中的不同位置是一致的。 总结来说，虽然全连接网络在某些任务中表现良好，特别是输入数据的维度较低或者没有明显的空间结构时，但在处理图像这样的高维空间数据时，卷积网络由于其结构上的优势，通常能够提供更好的性能。 * 参数越少，越不容易过拟合 * 卷积的参数共享使得训练模型参数更少 * 卷积下采样（池化），减少了特征图的尺寸，有助于降低模型复杂度 * 越抽象，越不容易过拟合：卷积空间局部性使得识别训练过程更加抽象 想象一下，你在做一个拼图游戏。全连接网络就像是你没有任何线索，只能靠猜来拼接每一块拼图。而卷积网络则像是给了你一些提示：哪些拼图块可能属于天空、哪些可能是树木等等。 在全连接网络中，每个输入信息点都要与每个输出点相连，这就像是拼图中的每一块都要试着与其他所有块相匹配来找到正确位置，这不仅耗时而且容易出错。如果你只有少量的拼图块（也就是数据），你可能会发现一些看似合适的匹配方式，但实际上它们并不是正确的大图景的一部分。这就是过拟合，即你的网络太过于适应你手头上的这些数据，而无法泛化到新的数据上。 卷积网络通过关注局部特征来减少这种风险。它就像是在告诉你：“不用看所有的拼图块，只看这一小块就好。”比如说，它可能只关注一块小区域内的拼图，这样就减少了错误匹配的机会。由于它只需要学习局部特征而不是整个图像的复杂模式，所以它需要的线索（也就是参数）就少得多。 简而言之，卷积网络通过专注于图片中的小部分（局部特征）并在整个图片中重复使用这些信息，减少了需要学习的内容量，从而减少了过拟合的风险。这就像是利用同样的线索来解决拼图的不同部分，而不是每个部分都重新来过。 想象一下你用一个橡皮图章在一张纸上盖印。无论你在纸上的哪个位置盖印，图章的图案都是相同的。这就是参数共享的基本概念。 在卷积神经网络中，一个“橡皮图章”就是一个卷积核（或者叫滤波器）。这个卷积核包含了一些参数，它在输入数据（比如一张图片）上移动，每次移动都会应用相同的参数来检测特定的特征，比如边缘或者角落。这样，不管这些特征出现在图片的哪个位置，卷积核都能用相同的方式来识别它们。 如果不使用参数共享，那么每个位置的特征都需要一个独立的“图章”，这会导致需要很多很多的图章（参数）。但有了参数共享，你只需要一个图章就可以检测整张图片的相同特征，这大大减少了所需的参数数量。 PS: 卷积的参数量级: C_in C_out Kernel_size（卷积核） Kernel_size + C_out（偏置）* * 图像分类：图像分类是指将输入的图像分配到预定义的类别中，如猫、狗、飞机等。图像分类的常用模型是卷积神经网络（CNN），它由多个卷积层、池化层和全连接层组成。卷积层和池化层负责提取图像的局部特征，而全连接层负责将这些特征整合成最终的分类结果 - Google推出了VIT（Vision Transformer）：一个和Bert几乎一致，同时不添加任何卷积结构的图像分类模型。VIT在Transformer上的成功，证明了可以用统一的模型，来处理不同领域（语言/图像/视频）的任务，进而开启了多模态模型研究的新篇章。 ： * transformer全局感受野，在大图片或者说找东西时效果好（类似近视眼，能够感受图像大轮廓）。 * 注意力机制的缺点：抓重点信息，忽略不重要的信息，数据少的时候，注意力机制效果不如bilstm，bilstm序列短（比如验证码的长度？）的时候效果也比注意力机制好 * cnn局部感受野，对细节处理较好（理解像素级别的问题，例如 医疗影像）。 1. * self-attention机制通过计算输入序列中每个元素之间的相似度，来建立全局的依赖关系。这样，self-attention可以有效地捕捉图像或视频中的全局信息，但也会导致特征图变得过于平滑，丢失一些细节信息。因此，self-attention可以看作是一个低通滤波器，它只保留低频信息，而过滤掉高频信息。 * 卷积操作则是通过在图像或视频上滑动一个小的窗口，来提取局部的特征。这样，卷积可以有效地捕捉图像或视频中的细节、纹理、边缘等高频信息，但也会忽略一些全局的信息 2. * 通过对神经网络进行特征向量编码进行相似度对比？特征向量如何反映神经网络对输入数据的变换和抽象？ * 通过定义 指标比较不同网络结构中的特征相似度：当在不同模型架构的层对之间绘制CKA相似度矩阵时，可以观察到ViT具有相对均匀的层相似度结构。相比之下，ResNet 模型在相似性结构上显示出明显的层级差异，较低层和较高层之间的相似度分数较小。这些结果说明，ViT 在整个模型中具有高度相似的表征，而 ResNet 模型在较低层和较高层之间的表征相似性要低得多。 * 卷积层与卷积核：一个包含 64 个 3x3 的卷积核的卷积层（卷积核作用于 3 个 in_channels，然后叠加成为 out_channel）：self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1) * GPT4 * https://easyai.tech/ai-definition/cnn/",
    "url": "/blog/卷积与全连接网络",
    "path": "tech/ai/卷积与全连接网络.md"
  },
  {
    "title": "向量数据库",
    "content": "基本解释：Vector Embeddings是通过一些机器学习算法来生成的，这些算法可以将复杂的非结构化数据（例如文本或图像）转换为固定长度的数值向量，从而方便机器学习算法处理。生成Vector Embeddings的一种方法是使用特征工程，即利用领域知识来设计向量的数值，以捕捉数据的语义。另一种方法是使用神经网络，例如自编码器或预测器，来自动学习向量的数值，以最大化数据之间的相似性或预测目标。不同的数据类型和任务可能需要不同的算法来生成Vector Embeddings，例如在自然语言处理中常用的Word2Vec，GloVe或FastText。 存在原因：Vector Embedding解决了如何用数字表示一些抽象的数据，比如文字，图片，声音等，让计算机能够理解它们的意义和关系的问题。如果不用Vector Embedding，计算机就很难处理这些数据，因为它们没有固定的格式和大小，也没有明确的规则和逻辑。Vector Embedding把这些数据变成了一串数字，就像给它们编了一个密码，让计算机能够识别它们，并且比较它们的相似度和差异。Vector Embedding还有一个好处，就是可以减少数据的大小，节省存储空间和计算时间。 Vector Embedding不是唯一的方案，还有其他的方法可以表示抽象的数据，比如符号表示法，图表示法等。但是Vector Embedding有一些优势，比如可以用数学运算来处理向量，可以用神经网络来自动学习向量，可以用向量空间来可视化数据等。所以Vector Embedding是目前最流行和最有效的方案之一。 举一个用Word embedding表示句子的例子。假设我们有一个词典，包含以下四个词： 我 你 喜欢 看 假设我们用一个三维的向量来表示每个词，比如： 我 = [0.1, 0.2, 0.3] 你 = [0.2, 0.3, 0.4] 喜欢 = [0.3, 0.4, 0.5] 看 = [0.4, 0.5, 0.6] 那么，如果我们有一个句子，比如： 我喜欢看电视 我们可以用以下的向量来表示这个句子： 我喜欢看电视 = [0.1, 0.2, 0.3] + [0.3, 0.4, 0.5] + [0.4, 0.5, 0.6] = [0.8, 1.1, 1.4] 这就是一个用Word embedding表示句子的例子。当然，这里的向量和词典都是随意设定的，实际上，Word embedding的向量和词典都是通过大量的文本数据来训练得到的。 * Random Projection（随机投影）是一种基于线性变换和降维的方法，它可以在高维空间中减少数据之间的距离差异，并且保留数据之间的相对距离。 * Product Quantization（乘积量化）是一种基于向量分割和编码的方法，它可以在高维空间中将数据压缩成短码，并且保留数据之间的近似距离。 * Locality-sensitive hashing（局部敏感哈希）是一种基于哈希函数和桶划分的方法，它可以在高维空间中将数据映射成数字，并且保留数据之间的相似概率。 * Hierarchical Navigable Small World (HSNW)（分层可导航小世界）是一种基于图结构和贪心搜索的方法，它可以在高维空间中构建一个近似最近邻图，并且保留数据之间的局部连通性。 其他 * KD树：KD树是一种基于空间划分和二分搜索的方法，它可以在低维空间中快速地找到最近邻点，但是在高维空间中效果会变差，并且对动态数据更新不友好。 * PCA：PCA是一种基于线性变换和降维的方法，它可以在高维空间中减少冗余信息和噪声，并且保留主要特征和距离信息，但是它需要计算所有数据点之间的协方差矩阵，并且对非线性结构不敏感。 * ANN：ANN是一种基于神经网络和深度学习的方法，它可以在高维空间中学习复杂和非线性的特征表示，并且适应各种类型和结构 优缺点 向量量化（Vector Quantization）的优点是压缩率高，但是计算量大，而且需要大量的存储空间。 Locality-Sensitive Hashing（LSH）的优点是计算速度快，但是精度不高。 Product Quantization with Residuals（PQR）的优点是可以提高压缩率和精度。 答：Vector search distance metrics是指用于衡量两个向量之间相似度或距离的度量标准。不同的距离度量可能反映不同的向量特征或关系，因此在进行向量检索时，需要根据具体的应用场景和数据特点选择合适的距离度量。 举例来说，假设我们有以下四个向量： - A = [1, 2] - B = [3, 4] - C = [5, 6] - D = [-1, -2] 我们可以用不同的距离度量来计算它们之间的相似度或距离，例如： - 内积：内积是两个向量的点乘，它反映了两个向量在同一方向上的投影长度的乘积。内积越大，表示两个向量越相似。例如，A和B的内积是11，A和D的内积是-5，所以A和B更相似。 - 余弦相似度：余弦相似度是两个向量的夹角的余弦值，它反映了两个向量之间的方向相似度。余弦相似度越接近1，表示两个向量越相似。例如，A和B的余弦相似度是0.98，A和D的余弦相似度是-1，所以A和B更相似。 - 欧氏距离：欧氏距离是两个向量之间的直线距离，它反映了两个向量之间的绝对差异。欧氏距离越小，表示两个向量越相似。例如，A和B的欧氏距离是2.83，A和D的欧氏距离是4.24，所以A和B更相似。 - 曼哈顿距离：曼哈顿距离是两个向量之间的坐标差的绝对值之和，它反映了两个向量在各个维度上的差异。曼哈顿距离越小，表示两个向量越相似。例如，A和B的曼哈顿距离是4，A和D的曼哈顿距离是6，所以A和B更相似。 - 汉明距离：汉明距离是两个二进制向量之间不同位的个数，它反映了两个向量在二进制表示上的差异。汉明距离越小，表示两个向量越相似。例如，如果我们将A、B、C、D转换为二进制形式： - A = [01, 10] - B = [11, 00] - C = [01, 10] - D = [11, 00] 那么A和B的汉明距离是4，A和C的汉明距离是0，所以A和C更相似。 问：向量数据是如何存储的？ 答：向量数据的存储方式取决于向量数据库的具体实现，不同的方案可能有不同的优化技术。但是一般来说，向量数据的存储可以分为两个层次： - 数据层：这一层负责存储向量数据的原始值，以及与之相关的结构化数据，如主键、标签等。数据层通常采用分布式文件系统或对象存储系统来实现，以支持海量数据的存储和备份。例如，Milvus²使用MinIO作为其数据层的存储系统。 - 索引层：这一层负责为向量数据构建索引结构，以加速相似度搜索的效率。索引层通常采用各种近似最近邻（ANN）算法来实现，如LSH、k-d tree、PQ等。索引层通常需要占用较少的存储空间，但是需要较高的计算能力。例如，Milvus支持多种索引类型，如IVF_FLAT、IVF_SQ8、HNSW等。 向量数据库在存储向量数据时，通常需要在数据层和索引层之间进行同步和更新，以保证数据的一致性和可用性。同时，向量数据库还需要提供查询接口和分析接口，以方便用户对向量数据进行检索和分析。 例子： 假设我们有一些向量数据，它们是由一些数字组成的，比如[1,2,3]，[4,5,6]，[7,8,9]等。我们想把这些向量数据存储起来，方便以后查找和分析。 - 一种方法是用数据层，就是把向量数据的原始值直接存储在一个大文件里，每个向量占一行，每个数字用逗号隔开。比如： 这种方法可以保证数据的完整性，但是如果我们想要查找和某个向量最相似的向量，就需要遍历整个文件，比较每个向量与给定向量之间的距离或相似度，这样效率很低。 - 另一种方法是用索引层，就是为向量数据构建一个索引结构，把相似的向量放在一起，不相似的向量分开。比如： 这种方法可以提高查询效率，因为我们只需要在相似的向量中查找，而不需要遍历整个文件。但是这种方法也有缺点，就是需要占用额外的存储空间，并且需要较高的计算能力来构建和维护索引结构。 向量数据库在存储向量数据时，通常会同时使用数据层和索引层，以平衡存储空间和查询效率。同时，向量数据库还会提供一些接口，让我们可以方便地对向量数据进行检索和分析。 答：向量索引的维度是指向量的长度，也就是向量中包含的浮点数的个数。向量的维度决定了向量所能表示的信息的复杂度和精度。一般来说，向量的维度越高，向量所能表示的信息越丰富，但也越难进行检索和计算。不同的应用场景可能需要不同的向量维度，例如文本嵌入通常在几百到几千维之间，而图像嵌入通常在几千到几万维之间。 向量索引的类型是指用于存储和检索向量的数据结构和算法。向量索引的类型决定了向量检索的效率和准确度。一般来说，向量索引可以分为两大类：精确索引和近似索引。精确索引可以保证找到与查询向量最相似的向量，但是计算代价很高，只适合小规模数据集。近似索引可以在牺牲一定准确度的情况下，大大提高检索速度和扩展性，适合大规模数据集。 不同的向量索引类型有不同的优缺点和适用场景，例如： - 线性扫描：最简单的精确索引方法，就是遍历所有向量，计算与查询向量的相似度，然后排序返回。这种方法没有任何预处理或空间优化，因此非常慢，只适合极小规模数据集。 - 倒排文件：一种常用的精确索引方法，就是将每个向量分成若干段，并为每个段建立一个倒排列表，记录包含该段的所有向量的ID。这样，在检索时，只需要查找与查询向量相同段的倒排列表，然后对候选向量进行精确计算。这种方法可以大大减少检索范围，提高检索速度，但是需要较大的存储空间，并且对于高维或稀疏向量效果不佳。 - 局部敏感哈希：一种常用的近似索引方法，就是将每个向量映射到一个或多个哈希桶中，使得相似的向量有较高概率落入同一个桶。这样，在检索时，只需要查找与查询向量相同桶的候选向量，然后对其进行排序返回。这种方法可以有效降低检索复杂度，并且支持多种相似度度量，但是需要调整合适的哈希函数和参数，并且可能存在哈希冲突或丢失最近邻。 - 乘积量化：一种常用的近似索引方法，就是将每个向量分成若干子向量，并为每个子向量建立一个有限大小的码本，记录子向量可能取值的离散集合。这样，在存储时，只需要记录每个子向量对应码本中的编码，在检索时，只需要对编码进行快速比较或计算近似距离。这种方法可以大大压缩存储空间，并且提高检索速度，但是需要预先训练码本，并且牺牲一定准确度。 - 图搜索：一种常用的近似索引方法，就是将所有向量构建成一个图结构，并为每个节点（即每个向量）维护一个邻居列表，记录与其最相似的若干节点。这样，在检索时，只需要从一个随机节点开始，在图上进行贪心遍历或随机游走，直到找到局部最优或全局最优节点作为结果返回。这种方法可以有效利用数据之间的结构信息，并且支持动态更新数据集，但是需要预先构建图结构，并且可能存在局部最优或陷入死循环。 Pinecone是一个云端的向量数据库服务，它支持多种维度和类型的向量索引。根据官方文档： - Pinecone支持任意正整数作为向量维度 - Pinecone支持三种类型的向量索引：flat、ivf和hnsw - flat类型是线性扫描方法，适合小规模数据集或对准确度要求很高的场景 - ivf类型是乘积量化方法，适合中等规模数据集或对速度和空间要求较高的场景 - hnsw类型是图搜索方法，适合大规模数据集或对动态更新要求较高的场景 数据库分片是一种将大型数据库拆分为多个较小的数据库的技术，以提高性能和可扩展性。可以用一个简单的例子来说明数据库分片的原理： 假设你有一个图书馆，里面有很多书籍，你需要将它们按照类别进行分类和存放。如果你只有一个大书架，那么你可能会把所有的书籍都放在一起，按照类别的字母顺序排列。这样做的好处是你可以很容易地找到任何一本书，只要你知道它的类别和名称。但是这样做也有一些缺点： - 如果你的图书馆很受欢迎，那么你可能会有很多读者同时来借阅书籍，这样就会造成大书架上的拥挤和混乱。 - 如果你的图书馆不断增加新的书籍，那么你可能会发现大书架已经放不下了，你需要购买更多的书架或者扩建图书馆，这样就会增加成本和维护难度。 - 如果你的图书馆发生了火灾或者其他灾难，那么你可能会损失所有的书籍，因为它们都存放在同一个地方。 为了解决这些问题，你可以考虑将你的图书馆进行分片，也就是将你的大书架拆分为多个小书架，每个小书架只存放一类或者几类书籍。这样做的好处是： - 你可以将不同类别的书籍分布在不同的位置，这样就可以减少读者之间的冲突和等待时间，提高借阅效率。 - 你可以根据每个小书架的容量和需求来灵活地调整和扩展你的图书馆，只要有空余的位置，你就可以添加新的小书架或者移动旧的小书架，而不需要改变整个图书馆的结构。 - 你可以提高你的图书馆的安全性和可靠性，因为即使某个小书架发生了损坏或者丢失，也不会影响其他小书架上的书籍，你可以很快地恢复或者替换损坏的小书架。 当然，将图书馆进行分片也有一些缺点： - 你需要设计一个合理的分片规则，来确定每本书应该放在哪个小书架上。如果分片规则太复杂或者太简单，都可能导致数据分布不均匀或者查询效率低下。 - 你需要维护一个目录表或者索引表，来记录每个小书架上存放了哪些类别和名称的书籍。这样当读者想要借阅某本书时，你可以根据目录表或者索引表快速地找到对应的小书架。 - 你需要处理跨小书架的查询和操作，比如如果读者想要借阅不同类别或者不同名称的多本书籍，那么你可能需要访问多个小书架，并将结果合并返回给读者。 * K-Means 算法 k-means 算法是一种基于距离的聚类算法，它的目的是将给定的数据集划分为 k 个簇，使得同一个簇内的数据点相似度高，不同簇之间的数据点相似度低¹²³。k-means 算法的基本步骤是： - 随机选择 k 个数据点作为初始的聚类中心； - 对于每个数据点，计算它到 k 个聚类中心的距离，并将其分配到距离最近的聚类中心所对应的簇中； - 对于每个簇，重新计算它的聚类中心，即该簇内所有数据点的均值； - 重复上述步骤，直到聚类中心不再变化或达到最大迭代次数。 k-means 算法有很多应用场景，比如： - 数据挖掘和分析，比如对客户、商品、文档等进行分类和聚合； - 图像处理和压缩，比如对图像进行分割、去噪、降维等； - 机器学习和模式识别，比如对特征进行提取、降维、初始化等。 1. 使用 Pinecone + OpenAI Embedding API实现WTF开源课程的语义化搜索 的代码示例 ，reference * 目标：熟悉基本的向量转换，以及语义模糊搜索 2. chroma (pinecone免费替代品，需要安装 chromaDB) + Word2Vec(OpenAI Embedding 免费替代品) 再实现示例 1 * 目标：熟悉 LangChain 调用 chroma，熟悉 pytorch 使用 Word2Vec； 3. 使用卷积神经网络（CNN 转化图片为 vector）+ chromaDB 进行图片相识度搜索 * 目标：熟悉卷积神经网络 Word2vec 和 openai 的 embeddings 接口是两种不同的方法，用于将词语转换为数值向量，以便进行后续的分析和处理。它们的区别主要有以下几点： - Word2vec 是一种无监督的学习方法，它利用词语的周边词来学习词语的嵌入表示。它可以学习到相关词，但是只能捕捉到局部分布信息。Word2vec 的输出是一个静态的嵌入表，对于每个词语，都有一个固定的向量表示，与词语所在的句子无关。 - openai 的 embeddings 接口是一种基于 transformer 模型的预训练方法，它利用双向上下文信息来学习词语的嵌入表示。它可以学习到更深层次的语义信息，包括词序、句子层面和跨句子的关系。openai 的 embeddings 接口的输出是一个动态的嵌入表，对于每个词语，都有一个根据句子生成的向量表示，与词语所在的句子相关。 Word2vec 和 openai 的 embeddings 接口各有优劣势，具体取决于使用场景和需求。一般来说，可以从以下几个方面进行比较： - 数据量和训练时间。Word2vec 是一种无监督的方法，它不需要标注数据，只需要大量的文本语料就可以训练出词向量。这样可以节省数据标注的成本和时间，也可以利用海量的文本数据来提高词向量的质量。openai 的 embeddings 接口是一种基于预训练模型的方法，它需要使用标注数据来微调模型，以适应特定的任务和领域。这样可以提高词向量的精确度和泛化能力，但也需要更多的数据标注和训练时间。 - 语义信息和上下文信息。Word2vec 的词向量是静态的，它只能捕捉到词语本身的语义信息，而不能考虑词语所在的上下文信息。这样可能导致同一个词语在不同的句子中有相同的向量表示，而忽略了词语的多义性和歧义性。openai 的 embeddings 接口的词向量是动态的，它能够捕捉到词语的双向上下文信息，以及句子层面和跨句子的关系。这样可以提高词向量的丰富度和灵活度，能够更好地处理词语的多义性和歧义性。 - 通用性和可定制性。Word2vec 的词向量是通用的，它可以适用于不同的任务和领域，而不需要进行特定的调整或优化。这样可以提高词向量的可复用性和可移植性，也可以减少模型开发和部署的复杂度。openai 的 embeddings 接口的词向量是可定制的，它可以根据不同的任务和领域进行微调或优化，以提高词向量的适应性和效果。这样可以提高词向量的针对性和精确性，也可以增加模型开发和部署的灵活性。 类似工具：SentenceTransformers （跟openai embedding 类似，使用 transformer 模型） 推理：word2vec 使用的是神经网络，会有梯度下降问题？所以上下文信息关联会比较小？而 transformer 自回归模型能够有效处理距离较远的依赖关系，所以上下文信息丰富？ * New Bing * https://www.pinecone.io/learn/vector-embeddings-for-developers/ * https://www.pinecone.io/learn/vector-database/ * https://weaviate.io/blog/distance-metrics-in-vector-search * https://platform.openai.com/docs/guides/embeddings/use-cases * https://docs.pinecone.io/docs/openai#creating-embeddings",
    "url": "/blog/向量数据库",
    "path": "tech/ai/向量数据库.mdx"
  },
  {
    "title": "旋转矩阵的证明",
    "content": "为什么向量旋转可以使用旋转矩阵来表示。 证明： 让我们考虑一个点P，坐标为 (x, y)。我们希望将点P绕原点逆时针旋转θ角，计算旋转后的坐标 (x', y')。 在旋转之前，点P的位置可以用极坐标表示为： 其中 r 是点P到原点的距离（即向量的长度），α 是从x轴到向量[ x, y ]的角度。 当我们把点P逆时针旋转θ角度后，向量的长度不变，但角度变为 α + θ。所以，旋转后点的新坐标 (x', y') 用极坐标表示是： 使用三角恒等式，cos(α + θ) 和 sin(α + θ) 可以表示为： 因此我们可以将坐标写为： 由于 x = r cosα 和 y = r sinα，代入上面的等式中，我们得到： 现在，我们将这个变换表示为矩阵形式： 结果就是点P在逆时针旋转θ度之后的坐标 (x', y') 可以通过原坐标 (x, y) 与旋转矩阵相乘得到。 （使用了基本的三角恒等式和极坐标的转换。） 计算点的重心坐标通常涉及到解一个线性方程组。对于三角形 \\( ABC \\) 中的点 \\( P \\)，其重心坐标 $(\\lambda_1, \\lambda_2, \\lambda_3)$ 可以通过以下步骤计算得出： 1. 确定三角形顶点的坐标： - 假设三角形 \\( ABC \\) 的顶点坐标分别为 \\( A(x_1, y_1) \\)，\\( B(x_2, y_2) \\)，\\( C(x_3, y_3) \\)。 2. 计算面积： - 使用叉乘计算三角形 \\( ABC \\) 的面积 \\( S_{ABC} \\)。 - 计算点 \\( P \\) 与三角形顶点形成的子三角形的面积，例如 \\( S_{PBC} \\)，\\( S_{PCA} \\)，\\( S_{PAB} \\)。 3. 利用面积比求解重心坐标： - 重心坐标 \\( $\\lambda_1$ \\)，\\( $\\lambda_2$ \\)，\\( $\\lambda_3$ \\) 分别等于点 \\( P \\) 对应的子三角形面积与三角形 \\( ABC \\) 面积的比例。例如： $$ \\lambda_1 = \\frac{S_{PBC}}{S_{ABC}} $$ $$ \\lambda_2 = \\frac{S_{PCA}}{S_{ABC}} $$ $$ \\lambda_3 = \\frac{S_{PAB}}{S_{ABC}} $$ 4. 确保重心坐标之和为1： - 验证 \\( $\\lambda_1 + \\lambda_2 + \\lambda_3$ = 1 \\)。 Reference：",
    "url": "/blog/基本的矩阵运算",
    "path": "tech/ai/基本的矩阵运算.md"
  },
  {
    "title": "小白问术语",
    "content": "当我们谈论概率分布时，可以将其类比为两个不同颜色的沙堆，每个沙堆的高度表示该颜色在分布中的概率。 想象一下，我们有一个红色的沙堆和一个蓝色的沙堆，它们分别代表真实样本的分布和生成样本的分布。我们希望通过生成对抗网络（GAN）让生成样本的分布逼近真实样本的分布。 在传统的 GAN 中，我们可以使用 Jensen-Shannon 散度或 Kullback-Leibler 散度来度量这两个分布之间的差异。这就好比我们在比较两个沙堆的形状时，可能使用某种度量方法，比如测量两个沙堆之间的重叠部分或差异。 然而，这些度量方法在某些情况下可能不够理想。例如，如果两个沙堆之间没有重叠部分，或者重叠部分非常小，传统的度量方法可能无法提供有意义的梯度信号，导致训练不稳定。 这时，Wasserstein 距离就派上用场了。Wasserstein 距离类比为移动沙堆的最小成本或能量。它衡量了将一个沙堆转化为另一个沙堆所需的最小移动成本，也就是将一个概率分布转变为另一个概率分布的最小代价。 在 GAN 中，我们可以使用 Wasserstein 距离来度量生成样本的分布与真实样本的分布之间的差异。这就好比我们使用最小成本移动红色沙堆中的沙子，将其逐渐变成蓝色沙堆，直到两个沙堆完全重叠。 Wasserstein 距离的优势在于它提供了平滑的梯度信号，使得训练过程更加稳定。这就好比在移动沙堆的过程中，我们可以逐渐地将红色沙堆变成蓝色沙堆，而不会遇到梯度消失或梯度爆炸的问题。 通过使用 Wasserstein 距离，WS-GAN 可以更稳定地训练生成器和判别器，并生成更高质量的样本，就像我们可以通过移动沙堆的方式将红色沙堆转化为蓝色沙堆。 Internal Covariate Shift（内部协变量偏移）是一个深度学习中的问题，它指的是在训练过程中，每一层神经网络的输入数据的分布会不断变化，这会影响网络的学习效率和效果。 * 协方差：衡量随机变量间的相关程度 假设你是一个厨师，你要做一道菜，需要用到很多种食材。你有一个食材供应商，他每天给你送一些食材，你根据食材的质量和数量来调整你的菜谱。但是，你发现你的供应商很不靠谱，他每天给你送的食材的种类和数量都不一样，有时候甚至送一些你不需要的食材。这就给你的做菜带来了很大的困难，你不得不每天都重新调整你的菜谱，而且你的菜的味道也不稳定，有时候好吃，有时候难吃。 这就是Internal Covariate Shift的一个例子，你的食材就相当于神经网络的输入数据，你的菜谱就相当于神经网络的参数，你的菜的味道就相当于神经网络的输出结果。你的供应商就相当于前一层神经网络，他的送货就相当于前一层神经网络的参数更新。由于前一层神经网络的参数更新导致了后一层神经网络的输入数据的分布变化，这就是Internal Covariate Shift的本质。 那么，如何解决这个问题呢？一个常用的方法是Batch Normalization，也就是批归一化。它的思想是在每一层神经网络的输入数据上做一个归一化的操作，使得每一批数据的均值和方差都固定为一定的值，这样就可以减少数据分布的变化，提高网络的学习效率和效果。 回到我们的比喻，Batch Normalization就相当于你在每次收到食材后，都对食材进行一些处理，比如切成一定的大小，去掉一些不好的部分，加一些调味料等，使得每次的食材都符合你的标准，这样你就可以更容易地做出好吃的菜了。 梯度饱和问题是一个深度学习中的问题，它指的是在训练过程中，神经网络的梯度（也就是参数更新的方向和大小）变得很小，导致网络的学习速度变慢，甚至停止学习。为了解释这个问题，我们可以用一个比喻来理解。 假设你是一个学生，你要学习一门课程，需要用到很多知识点。你有一个老师，他每天给你讲解一些知识点，你根据老师的讲解来调整你的学习方法和进度。但是，你发现你的老师很不负责，他每天给你讲的知识点都很简单，有时候甚至重复讲一些你已经掌握的知识点。这就给你的学习带来了很大的困难，你不得不每天都重新调整你的学习方法和进度，而且你的学习效果也不稳定，有时候好，有时候差。 这就是梯度饱和的一个例子，你的知识点就相当于神经网络的输入数据，你的学习方法和进度就相当于神经网络的参数，你的学习效果就相当于神经网络的输出结果。你的老师就相当于前一层神经网络，他的讲解就相当于前一层神经网络的参数更新。由于前一层神经网络的参数更新导致了后一层神经网络的梯度变化很小，这就是梯度饱和的本质。 - GAN就像一个画家和一个老师的对抗游戏，画家从一张白纸开始，用画笔在上面画出想要的图像。老师的任务是判断画家的画作是不是真实的，也就是说，是不是从真实的数据集中抽取的。画家和老师之间要不断地互相学习，直到画家能够画出逼真的图像。GAN的优点是生成的图片逼真，缺点是训练不稳定，有时候会画出重复或者不自然的图像。 - diffusion model就像一个拼图玩家，它从一张完整的图像开始，然后逐渐地把图像上的一些部分换成随机的颜色，直到整张图像都变成了噪声。这个过程就是扩散过程。然后它再反过来，从噪声图像开始，逐步地恢复原来的图像。这个过程就是去噪过程。diffusion model的优点是生成的图片逼真，数学可解释性强，缺点是训练成本高昂、速度慢，需要多步采样。 - VAE就像一个变形金刚，它可以把输入的图像压缩成一个低维度的编码，然后再从编码中解压出一个新的图像。这个过程就是变分自编码器。VAE的优点是学习了概率分布，可解释性强，图片多样性足，缺点是生成的图片模糊。 自由能是一种用来描述系统的热力学状态的函数，它可以表示系统在给定温度和压力下的最大可用能量。自由能与 VAE 的关系是，VAE 的目标是最大化数据的边缘对数似然，也就是最小化数据的自由能。VAE 通过变分推理的方法，用神经网络来近似真实的后验分布，从而实现了一个概率生成模型。¹² VAE 的基本结构如下： - 编码器（encoder）：将输入数据 x 映射到一个潜在空间（latent space）的分布 q(z|x)，通常假设这个分布是一个多元正态分布，其均值和方差由神经网络输出。 - 解码器（decoder）：将潜在空间的变量 z 映射回输入空间，得到一个条件分布 p(x|z)，通常假设这个分布是一个高斯分布或伯努利分布，其参数由神经网络输出。 - 重参数化（reparameterization）：为了能够对 q(z|x) 进行反向传播，需要将采样过程转化为一个可微的操作，即 z = μ + σ ⊙ ε，其中 ε ~ N(0, I) 是一个随机噪声，⊙ 表示逐元素相乘。 - 损失函数（loss function）：VAE 的损失函数由两部分组成，一部分是重构误差，即 E_q(z|x)[log p(x|z)]，表示解码器的输出与输入的匹配程度；另一部分是 KL 散度，即 KL[q(z|x)||p(z)]，表示编码器的输出与先验分布 p(z) 的差异，通常假设 p(z) 是一个标准正态分布。VAE 的目标是最小化这两部分的和，也就是最小化数据的自由能。 VAE 的损失函数可以用以下公式表示： $$\\mathcal{L}(\\theta, \\phi; x) = -E_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + KL[q_\\phi(z|x)||p(z)]$$ 其中，θ 和 φ 分别表示解码器和编码器的参数，x 表示输入数据，z 表示潜在变量。 当预训练语言模型学习语法知识时，通常是通过预测句子中的语法结构或者句法信息。以下是一个简单的例子： 假设有一个句子：\"The dog jumps over the lazy cat.\"，我们想要让预训练语言模型学习到这个句子中的语法知识，例如主语、谓语、宾语等。 为了实现这一点，我们可以对句子进行一些操作，例如： 遮蔽（Mask）主语：将句子中的主语\"The dog\"遮蔽（Mask）掉，然后让预训练语言模型去预测这个位置的单词或短语。通过这种方式，模型可以学习到主语通常是哪些类型的单词或短语，并且可以学习到它们与其他单词或短语之间的语义关系。 遮蔽（Mask）谓语：将句子中的谓语\"jumps\"遮蔽（Mask）掉，然后让预训练语言模型去预测这个位置的单词或短语。通过这种方式，模型可以学习到谓语通常是哪些类型的单词或短语，并且可以学习到它们与其他单词或短语之间的语义关系。 遮蔽（Mask）宾语：将句子中的宾语\"the lazy cat\"遮蔽（Mask）掉，然后让预训练语言模型去预测这个位置的单词或短语。通过这种方式，模型可以学习到宾语通常是哪些类型的单词或短语，并且可以学习到它们与其他单词或短语之间的语义关系。 通过这些操作，预训练语言模型可以通过自监督学习任务来学习到语法知识，例如主语、谓语、宾语等。这些知识可以帮助模型更好地理解自然语言，从而在实际的下游任务中表现更加优秀。 对比学习就是让模型通过看一些相似的和不相似的东西，来学习它们的特点。比如，你可以通过看一些猫和狗的图片，来学习它们的样子，而不需要别人告诉你哪个是猫，哪个是狗。你只需要知道哪些图片是同一种动物，哪些图片是不同种动物，然后让模型尽量把同一种动物的图片放得近一点，把不同种动物的图片放得远一点。这样，模型就可以自己学习到猫和狗的特点。 优点： - 可以利用大量的无标签数据来学习数据的特征，提高模型的泛化能力和性能。 - 可以用于预训练模型，为下游任务提供一个良好的初始化，减少训练时间和数据需求。 - 可以用于多种领域和任务，比如图像，文本，音频等，以及分类，聚类，检索等。 缺点： - 需要设计合适的数据增强方法，以生成有效的正样本对和负样本对。 - 需要选择合适的批次大小，以保证有足够的负样本对进行对比。 - 需要避免一些假负样本对的影响，比如不同类别但相似的数据点。 - 需要平衡对比损失函数和下游任务损失函数之间的权重，以避免过拟合或欠拟合。 不同的 losses 的作用： - ContrastiveLoss: 假设我们想训练一个模型，让它能够判断两个句子是否有相同的意思。我们可以给模型一些句子对，比如： 我们可以使用 ContrastiveLoss 来让模型学习，使得相同意思的句子对之间的距离尽可能小，不同意思的句子对之间的距离尽可能大。 - MultipleNegativesRankingLoss: 假设我们想训练一个模型，让它能够从多个句子中选择最合适的一个。我们可以给模型一些句子列表，比如： 我们可以使用 MultipleNegativesRankingLoss 来让模型学习，使得锚点和正确候选句子之间的距离尽可能小，锚点和错误候选句子之间的距离尽可能大。 - CosineSimilarityLoss: 假设我们想训练一个模型，让它能够计算两个句子之间的相似度分数。我们可以给模型一些句子对和分数，比如： 我们可以使用 CosineSimilarityLoss 来让模型学习，使得模型预测的相似度分数与人工标注的相似度分数尽可能接近。 最小二乘法比较直观，很容易解释，但不具有普遍意义，对于更多其他机器学习问题，比如二分类和多分类问题，最小二乘法就难以派上用场 交叉熵损失函数是一种常用于分类问题中的损失函数，它的大小表示两个概率分布之间的差异。交叉熵损失函数可以通过最小化交叉熵来得到目标概率分布的近似分布。 举例说明交叉熵损失函数，假设我们需要对数字1，2，3进行分类，它们的label依次为： [1,0,0]， [0,1,0]， [0,0,1]。当输入的图像为数字1时，它的输出和label为： [0.3,0.4,0.3] , [1,0,0]。接下来我们就可以利用交叉熵计算网络的 loss = - (1log (0.3) + 0 + 0) = 1.20。随着训练次数的增加，模型的参数得到优化，这时的输出变为： [0.8,0.1,0.1]。则 loss = - (1log (0.8) + 0 + 0) = 0.22。可以发现loss由1.20减小为0.22，而判断输入图像为数字1的概率由原本的0.3增加为0.8，说明训练得到的概率分布越来越接近真实的分布，这样就大大的提高了预测的准确性 例子 假设我们有一个模型用于识别手写数字的文本序列，输入是一系列图像，输出是对应的文本序列。我们使用CTC损失函数来训练这个模型。 在这个例子中，模型的输出是一个三维张量，形状为。我们假设有两个样本，为2，序列长度为3，类别数量为6（0-5表示数字0-5，最后一个类别为CTC的空白标记）。 对应的标签序列是一个二维张量，形状为。第一个样本的标签序列为[1, 2]，第二个样本的标签序列为[4, 5]。 CTC（Connectionist Temporal Classification）是一种用于序列标注的算法，它可以在不需要对齐输入和输出的情况下，直接从输入序列（如声音或图像）预测输出序列（如文字或标签）。CTC的核心是定义一个概率分布，用于计算输入序列和输出序列之间的对应关系。 CTC中的解码（decoding）是指根据模型预测的概率分布，找出最可能的输出序列的过程³。CTC中有三种常用的解码方法：greedy decode，prefix beam decode和beam search⁴。其中，beam search是一种基于全局搜索的解码方法，它维护一个固定大小的候选集合（beam），每个候选项都是一个完整的输出序列。在每个时间步上，它会根据当前的候选集合和预测概率，生成新的候选集合，并保留概率最高的若干个输出序列。这种方法的优点是可以找到全局最优或者近似最优的输出序列，缺点是计算量非常大，需要遍历所有可能的输出序列和概率。 举一个简单的例子，假设我们有一个四分类问题，即$n=4$，要判断一段语音是A、B、C还是D。我们用0表示空白符（blank），1表示A，2表示B，3表示C，4表示D。假设一段语音的真实标签是ABCD，即$y_0=1,y_1=2,y_2=3,y_3=4$，而模型预测的概率分布如下： | 时间步 | 0 | 1 | 2 | 3 | 4 | | ------ | ---- | ---- | ---- | ---- | ---- | | 1 | 0.6 | 0.2 | 0.1 | 0.05 | 0.05 | | 2 | 0.5 | 0.3 | 0.1 | 0.05 | 0.05 | | 3 | 0.4 | 0.35 | 0.15 | 0.05 | 0.05 | | 4 | 0.3 | 0.25 | 0.25 | 0.1 | 0.1 | | 5 | 0.2 | 0.15 | 0.25 | 0.2 | 0.2 | | ... | 假设我们设置beam size为2，即每个时间步只保留两个最优的候选序列。那么beam search的过程如下： - 时间步1：我们从第一行中选择概率最大的两个类别作为初始候选序列，即(0)和(1)，它们对应的概率分别为$P(0)=\\log(0.6)=-0.51$和$P(1)=\\log(0.2)=-1.61$。 - 时间步2：我们从第二行中选择每个类别，并将其添加到前一步的候选序列中，形成新的候选序列。例如，对于类别1，我们可以得到两个新的候选序列：(01)和(11)，它们对应的概率分别为$P(01)=P(0)+\\log(0.3)=-1.41$和$P(11)=P(1)+\\log(0.3)=-2.51$。同理，我们可以得到其他八个新的候选序列：(02)，(12)，(03)，(13)，(04)，(14)，(00)，(10)。然后我们从这十个候选序列中选择概率最大的两个作为当前时间步的最优候选序列，即(01)和(02)，它们对应的概率分别为$P(01)=-1.41$和$P(02)=-1.71$。 - 时间步3：我们重复上一步的操作，从第三行中选择每个类别，并将其添加到前一步的候选序列中，形成新的候选序列。例如，对于类别1，我们可以得到两个新的候选序列：(011)和(021)，它们对应的概率分别为$P(011)=P(01)+\\log(0.35)=-2.31$和$P(021)=P(02)+\\log(0.35)=-2.61$。同理，我们可以得到其他八个新的候选序列：(012)，(022)，(013)，(023)，(014)，(024)，(010)，(020)。然后我们从这十个候选序列中选择概率最大的两个作为当前时间步的最优候选序列，即(011)和(012)，它们对应的概率分别为$P(011)=-2.31$和$P(012)=-2.51$。 - 时间步4：我们继续重复上一步的操作，从第四行中选择每个类别，并将其添加到前一步的候选序列中，形成新的候选序列。例如，对于类别1，我们可以得到两个新的候选序列：(0111)和(0121)，它们对应的概率分别为$P(0111)=P(011)+\\log(0.25)=-3.21$和$P(0121)=P(012)+\\log(0.25)=-3.41$。同理，我们可以得到其他八个新的候选序列：(0112)，(0122)，(0113)，(0123)，(0114)，(0124)，(0110)，(0120)。然后我们从这十个候选序列中选择概率最大的两个作为当前时间步的最优候选序列，即(0112)和(0113)，它们对应的概率分别为$P(0112)=-3.11$和$P(0113)=-3.31$。 - 时间步5：我们继续重复上一步的操作，从第五行中选择每个类别，并将其添加到前一步的候选序列中，形成新的候选序列。例如，对于类别1，我们可以得到两个新的候选序列：(01121)和(01131)，它们对应的概率分别为$P(01121)=P(0112)+\\log * * 预训练模型是如何存储的？不同的预训练模型可能有不同的存储方式。但是，一般来说，预训练模型都是通过一些参数来存储的，这些参数是在大量的数据上训练得到的，可以反映出模型学习到的知识。 这些参数通常是一些数值，可以用矩阵或张量的形式来表示。比如，一个预训练模型可能有几百万或几十亿个参数，每个参数都有一个数值，这些数值就可以组成一个很大的矩阵或张量。这样，预训练模型就可以用一个文件来存储这个矩阵或张量，文件的大小取决于参数的数量和精度。 当我们使用预训练模型时，我们就可以加载这个文件，把参数读取到内存中，然后用这些参数来初始化我们的模型。这样，我们就可以利用预训练模型已经学习到的知识，而不需要从头开始训练。 模型 all-MiniLM-L6-v2 是一个用于句子相似度的预训练模型，它是基于MiniLM的一个变种，使用了6层的自注意力机制（Attention）和384维的隐藏层。它是在一个包含10亿个句子对的数据集上使用对比学习（Contrastive Learning）的方法进行训练的。它可以将句子或段落映射到一个384维的稠密向量空间，可以用于聚类或语义搜索等任务。 举个例子，假设我们有以下三个句子： A: 我喜欢吃苹果 B: 苹果是我的最爱 C: 我讨厌吃香蕉 我们可以使用 all-MiniLM-L6-v2 模型来计算这三个句子的向量表示，然后计算它们之间的余弦相似度（Cosine Similarity），得到以下结果： sim(A, B) = 0.87 sim(A, C) = 0.12 sim(B, C) = 0.11 从这个结果可以看出，句子A和B非常相似，因为它们都表达了喜欢苹果的意思，而句子A和C，以及句子B和C都很不相似，因为它们表达了不同的情感和食物偏好。这样，我们就可以利用 all-MiniLM-L6-v2 模型来判断句子之间的语义关系。 Transformer 模型是一种基于自注意力机制的深度学习模型，主要用于自然语言处理和计算机视觉领域。Transformer 模型由编码器（Encoder）和解码器（Decoder）两个部分组成，可以处理变长的输入序列。Transformer 模型训练出来的模型是指模型在训练过程中学习到的参数和权重，以及模型在测试数据上的表现和评估。 Transformer 模型的训练过程大致如下： * 数据准备：收集、标注、清洗、划分、转换等操作，使得数据符合模型的输入要求。使用子词分词器（subwords tokenizer）对文本进行分词，并添加开始和结束标记。 * 模型定义：选择合适的网络结构、损失函数、优化器等组件，构建模型的计算图。使用位置编码（Positional encoding）表示单词在序列中的位置。使用多头注意力（Multi-head attention）和前馈网络（Feed forward network）构成编码器和解码器的层。 * 模型训练：使用训练数据不断地更新模型的参数和权重，使得模型的误差变小。使用遮挡（Masking）操作防止解码器看到未来的信息。使用学习率调度（Learning rate scheduling）策略控制优化速度。 * 模型测试：使用测试数据评估模型的泛化能力，如准确率、召回率、F1值等指标。使用贪心搜索（Greedy search）或集束搜索（Beam search）生成翻译结果。 * 模型保存：将训练好的模型保存为文件，以便后续使用或部署。 注意力机制（英语：attention）是人工神经网络中一种模仿认知注意力的技术。这种机制可以增强神经网络输入数据中某些部分的权重，同时减弱其他部分的权重，以此将网络的关注点聚焦于数据中最重要的一小部分。数据中哪些部分比其他部分更重要取决于上下文。可以通过梯度下降法对注意力机制进行训练。 优缺点： 优点： 可以提高模型的性能和准确度，尤其是对于长或复杂的数据，可以减少信息的丢失和混淆。 可以减少模型的计算量和内存消耗，因为模型不需要处理所有的数据，而只需要关注最重要或最相关的部分。 可以增强模型的可解释性和可视化性，因为模型可以显示出它关注的部分和权重，方便人们理解模型的行为和决策。 缺点： 可能会导致模型过于依赖注意力机制，忽略其他重要的因素或特征。 可能会增加模型的训练难度和时间，因为注意力机制需要额外的计算和参数。 可能会存在一些局限性或问题，如注意力机制如何适应不同类型和领域的数据，如何处理多头或多层的注意力机制，如何避免注意力机制的冗余或冲突等。 Transformer 模型使用了残差连接（residual connection）和层归一化（layer normalization）的技术，可以有效地防止梯度消失或爆炸的问题。残差连接可以让信息在深层网络中更容易地传递，层归一化可以稳定梯度的分布和大小。 >> 残差连接就是让模型的输入和输出相加，这样可以让模型保留输入的信息，不会丢失或改变太多。你可以想象模型是一个大盒子，里面有很多小盒子，每个小盒子都要对输入的信息做一些处理，然后传给下一个小盒子。但是有些小盒子可能处理得不好，或者处理得太多，导致信息变得不清楚或不完整。为了避免这种情况，我们可以让每个小盒子在处理完信息后，再把原来的信息加回去，这样就相当于给了信息一个保险，让它不会丢失或改变太多。 层归一化就是让模型的输出在每一层都保持一个合适的大小，不会太大或太小。你可以想象模型是一个大水管，里面有很多小水管，每个小水管都要对水流做一些调节，然后传给下一个小水管。但是有些小水管可能调节得不好，或者调节得太多，导致水流变得太大或太小。为了避免这种情况，我们可以让每个小水管在调节完水流后，再把水流调整到一个合适的范围内，这样就相当于给了水流一个标准，让它不会超出或低于正常的水平。 Transformer 模型使用了多头注意力机制（multi-head attention），可以增强模型的表达能力和泛化能力，同时也可以并行计算，提高训练效率。多头注意力机制可以让模型关注到不同子空间的信息，捕捉到更加丰富的特征信息。 >>多头注意力机制就是让模型可以同时注意到不同的信息，提高模型的能力和效率。你可以想象模型是一个大眼睛，里面有很多小眼睛，每个小眼睛都要对输入的信息做一些观察，然后传给下一个层。但是有些小眼睛可能观察得不好，或者观察得太少，导致信息变得不清楚或不完整。为了避免这种情况，我们可以让每个小眼睛在观察完信息后，再把不同小眼睛的观察结果合并起来，这样就相当于给了信息一个全面的视角，让它不会遗漏或重复。 Transformer 模型使用了位置编码（positional encoding），可以将词序信息加到词向量上，帮助模型学习这些信息。位置编码可以让模型知道每个词在句子中的相对和绝对的位置信息，而不需要依赖于循环神经网络结构。 >>Transformer 模型是一种可以处理文字和声音的模型，它有一个特别的功能，就是可以让模型知道每个词在句子中的位置，帮助模型理解句子的意思。你可以想象模型是一个大耳朵，里面有很多小耳朵，每个小耳朵都要对输入的信息做一些听取，然后传给下一个层。但是这些小耳朵只能听到每个词的声音，不能听到每个词在句子中的顺序。为了让模型知道每个词在句子中的顺序，我们可以给每个词加上一个标签，比如第一个词是1，第二个词是2，以此类推。这样就相当于给了模型一个位置编码，让它能够听到每个词在句子中的相对和绝对的位置信息。而且这种位置编码不需要像其他模型那样一个一个地处理词，所以模型可以更快地学习和处理信息。 | 词 | 词向量 | 位置编码 | 词向量+位置编码 | |:--:|:------:|:--------:|:--------------:| | 我 | [0.1, 0.2, 0.3] | [1, 0, 0] | [1.1, 0.2, 0.3] | | 爱 | [0.4, 0.5, 0.6] | [0, 1, 0] | [0.4, 1.5, 0.6] | | 你 | [0.7, 0.8, 0.9] | [0, 0, 1] | [0.7, 0.8, 1.9] | 这里我们假设每个词的词向量和位置编码都是三维的，位置编码用一个单位矩阵表示，每个词在句子中的位置对应矩阵的一行。然后我们把词向量和位置编码相加，得到一个新的词向量，这个新的词向量就包含了词序信息。这样模型就可以知道每个词在句子中的相对和绝对的位置信息了。 假设你要找一座山上的最低点，你可以用梯度下降算法来帮你。梯度下降算法的基本思想是，你先随机选择一个位置，然后看看你周围的坡度，沿着最陡的下坡方向走一步，然后再看看你周围的坡度，再沿着最陡的下坡方向走一步，重复这个过程，直到你找到最低点。这里，你走的步长就是学习率，你周围的坡度就是梯度。 但是，这个过程中可能会遇到一些问题或挑战，比如： * 学习率问题：如果你走的步长太大，可能会导致你在最低点附近来回跳动，或者跳过最低点，无法停下来；如果你走的步长太小，可能会导致你走得太慢，或者卡在一个不太低的地方，无法继续下降。 * 局部最小值问题：如果这座山不是光滑的，而是有很多凹凸不平的地方，那么可能存在很多低洼的地方，但不一定是最低的地方。如果你碰巧卡在一个低洼的地方，而周围都是上坡，那么你就无法继续下降，而错过了真正的最低点。 * 鞍点问题：如果这座山不是单峰的，而是有很多平缓的地方，那么可能存在很多平坦的地方，但既不是最高也不是最低。如果你碰巧卡在一个平坦的地方，而周围都是平坦或微弱的上下坡，那么你就无法确定该往哪个方向走，而停滞不前。 * 梯度消失或爆炸问题：如果这座山非常高或非常低，那么可能存在很多极端的坡度。如果你碰到一个非常陡或非常缓的坡度，那么你可能会走得非常快或非常慢，导致你无法控制好自己的速度和方向。 假设您想要爬上一座山的最高峰，但是您不知道具体的路线，只能根据眼前的情况来决定下一步怎么走。您的目标就是让自己的高度尽可能地增加，也就是最大化高度函数。 梯度就是您在某个位置能够看到的最陡峭的方向，它表示了高度函数在这个位置上的变化率。如果您沿着梯度的方向走，就能够让自己的高度增加得最快。如果您沿着梯度的反方向走，就能够让自己的高度减少得最快。 损失函数就是您和最高峰之间的距离，它表示了您离目标有多远。您的目标就是让损失函数尽可能地减小，也就是最小化损失函数。 梯度和损失函数的关系就是：如果您沿着梯度的反方向走，就能够让损失函数减小得最快。如果您沿着梯度的方向走，就能够让损失函数增加得最快。 所以，在机器学习中，我们通常使用梯度下降法来求解损失函数的最小值，也就是找到最佳的参数。我们从一个随机的参数开始，计算损失函数和梯度，然后沿着梯度的反方向更新参数，使得损失函数逐渐减小，直到达到一个局部最小值或者收敛。 例子： 假设您想要找到一个一元函数的最小值，也就是 y = f(x) ，其中 x 是输入变量， y 是输出变量。您不知道函数的具体形式，只能通过观察函数在不同位置的值来判断。 损失函数就是您和目标之间的距离，它表示了您当前位置和最低点之间的高度差。在这个例子中，我们可以使用 y 作为损失函数，它的定义是： $$ L(x) = y = f(x) $$ 梯度就是损失函数在某个位置上的变化率，它表示了损失函数在这个位置上沿着不同方向（也就是不同 x ）的变化程度。在这个例子中，梯度是一个一维数值，它是损失函数对 x 的导数，它的定义是： $$ \\\\frac{dL}{dx} = \\\\frac{dy}{dx} = f'(x) $$ 梯度和损失函数的关系就是：如果您沿着梯度的反方向更新 x ，就能够让损失函数减小得最快。如果您沿着梯度的方向更新 x ，就能够让损失函数增加得最快。 所以，在数学中，我们通常使用梯度下降法来求解损失函数的最小值，也就是找到最佳的 x 。我们从一个随机的 x 开始，计算损失函数和梯度，然后沿着梯度的反方向更新 x ，使得损失函数逐渐减小，直到达到一个局部最小值或者收敛。 具体来说，梯度下降法的步骤如下： - 初始化 x 为任意值 - 选择一个学习率 \\\\alpha ，它表示每次更新 x 的步长 - 重复以下步骤直到收敛或者达到最大迭代次数： - 计算损失函数 L(x) - 计算梯度 \\\\frac{dL}{dx} - 更新 x = x - \\\\alpha \\\\frac{dL}{dx} 下图展示了一个简单的例子，其中函数是 y = x^2 + 2x + 1 ，我们初始化 x = -3 ，并设置学习率为 0.1 。我们可以看到随着迭代次数的增加， x 不断接近真实值（也就是 -1 ），而损失函数不断减小，最终收敛到一个很小的值。 近似最近邻搜索（Approximate Nearest Neighbor Search）是一种在高维空间中快速找到跟一个给定点最相似的点的方法。它不一定能找到最准确的最近邻，但是可以节省很多时间和空间。 近似最近邻搜索有很多种算法，其中一种比较常用的是基于局部敏感哈希（Locality Sensitive Hashing，LSH）的方法。它的基本思想是，把高维空间中的点映射到低维空间中的哈希值，使得相似的点有更大的概率映射到相同或者相近的哈希值，而不相似的点有更大的概率映射到不同或者相远的哈希值。 具体来说，LSH方法有以下几个步骤： - 定义一个哈希函数族，每个哈希函数可以把一个高维向量映射到一个标量值。 - 从哈希函数族中随机选取k个哈希函数，组成一个复合哈希函数，它可以把一个高维向量映射到一个k维向量。 - 对于每个数据点，用复合哈希函数计算它的哈希值，并把它存储在一个哈希表中，哈希表的键是哈希值，值是对应的数据点。 - 对于一个查询点，用同样的复合哈希函数计算它的哈希值，并在哈希表中查找是否有相同或者相近的哈希值，如果有，就返回对应的数据点作为候选的最近邻。 - 对于候选的最近邻，用真实的距离度量（如欧氏距离）来计算它们与查询点的相似度，并返回最相似的一个或者几个作为近似的最近邻。 这种方法可以大大减少搜索空间和计算量，因为它只需要比较少数的候选点，而不是所有的数据点。当然，这种方法也有一些缺点，比如它需要调整合适的参数（如k和哈希函数族），它可能会丢失一些真正的最近邻（如映射到不同哈希值的点），它可能会返回一些不太相似的点（如映射到相同或者相近哈希值但实际距离较远的点）。 简单例子： LSH的方法是把这些数字变成一些字母，比如A，B，C，D，E，F，G，H，I，J。它用了一种特别的规则来变换，使得接近的数字变成相同或者相近的字母，而不接近的数字变成不同或者相远的字母。比如它可以用这样的规则： - 如果数字是奇数，就变成A - 如果数字是偶数，并且小于等于6，就变成B - 如果数字是偶数，并且大于6，就变成C 按照这个规则，我们可以得到这样的结果： - 1 -> A - 2 -> B - 3 -> A - 4 -> B - 5 -> A - 6 -> B - 7 -> C - 8 -> C - 9 -> A - 10 -> C 然后我们把这些结果放在一个表格里面，表格的第一列是字母，第二列是对应的数字。比如这样： | 字母 | 数字 | | --- | --- | | A | 1, 3, 5, 9 | | B | 2, 4, 6 | | C | 7, 8, 10 | 对于一个给定的数字，我们先用同样的规则把它变成一个字母。然后我们在表格里面找有没有相同或者相近的字母。如果有，就返回对应的数字作为候选的最接近的数字。然后我们再用真正的距离来比较候选的数字和给定的数字有多接近，并返回最接近的一个或者几个作为近似的最接近的数字。 举个例子吧，假设我们要找跟6最接近的数字。我们先用规则把6变成B。然后我们在表格里面找有没有B或者跟B相近的字母。我们发现有B和C两个字母符合条件。所以我们返回B和C对应的数字作为候选的最接近的数字。也就是2, 4, 6, 7, 8, 10这几个数字。然后我们再用真正的距离来比较它们和6有多接近，并返回最接近的一个或者几个作为近似的最接近的数字。显然最接近6的是6本身和4。 这种方法可以大大减少搜索和计算的次数，因为它只需要比较少数的候选数字，而不是所有的数字。当然，这种方法也有一些缺点，比如它需要想好合适的规则（如怎么变换和怎么判断相近），它可能会丢失一些真正最接近的数字（如5和7都跟6很接近但是被变成了A和C），它可能会返回一些不太接近的数字（如10跟6不太接近但是被变成了C）。 >问：请用小白能懂的方式解释如下一段话：RandomForestClassifier 的 fit 方法是用于训练随机森林分类器的方法。在训练过程中，随机森林会从数据集的不同子样本中拟合多个决策树分类器，并使用平均值来提高预测准确性和控制过度拟合。每个决策树都是在不同的随机样本和随机特征集上训练的，这有助于减少过度拟合并提高模型的泛化能力？ 假设你有一堆动物的照片，你想让电脑能够识别出每张照片上是什么动物，这就是一个分类问题。你可以用随机森林分类器来解决这个问题。 随机森林分类器就像是一个团队，里面有很多个小伙伴，每个小伙伴都是一个决策树分类器。决策树分类器就像是一个问答游戏，它会根据照片上的一些特征，比如颜色、形状、大小等，来问一些是非问题，比如“这个动物有没有毛发？”，“这个动物有没有尾巴？”，“这个动物的耳朵是圆的还是尖的？”等，最后根据回答来判断这个动物是什么。 但是如果只有一个决策树分类器，它可能会问得太细或者太死板，导致只能识别出一些特定的照片，而对于其他的照片就不太准确了，这就叫做过度拟合。为了避免这种情况，我们可以让多个决策树分类器一起工作，每个决策树分类器都从所有的照片中随机挑选一些来学习，而且每次问问题的时候也只从所有的特征中随机挑选一些来问，这样每个决策树分类器都会有不同的视角和侧重点，这就叫做随机森林。 当我们要识别一张新的照片时，我们就把它给每个决策树分类器看一遍，让他们都给出自己的判断，然后我们看看他们中间哪种判断最多，就认为那种判断是正确的。比如说有10个决策树分类器，其中有6个说这张照片上是猫，3个说是狗，1个说是兔子，那么我们就认为这张照片上是猫。这样我们就可以利用多数投票的方式来提高预测的准确性和可靠性。 所以，RandomForestClassifier 的 fit 方法就是用来训练随机森林分类器的方法，它会让随机森林里面的每个决策树分类器都从数据集中学习一些知识，并且保持一定的多样性和随机性。 假设你要做一个蛋糕，你有一些原料，比如面粉、鸡蛋、牛奶、糖等，你也有一个烤箱，但是你不知道怎么配比这些原料，也不知道怎么调节烤箱的温度和时间，你只知道最后的蛋糕要好吃。这时候，你可以用网格搜索或者随机搜索来帮你找到最佳的配方和烘焙方法。 网格搜索的思路是这样的：你先列出每种原料和烤箱的可能取值，比如面粉可以是100克、200克、300克等，鸡蛋可以是1个、2个、3个等，牛奶可以是50毫升、100毫升、150毫升等，糖可以是10克、20克、30克等，烤箱的温度可以是150度、180度、210度等，烘焙的时间可以是10分钟、15分钟、20分钟等。然后你把所有可能的组合都试一遍，比如100克面粉+1个鸡蛋+50毫升牛奶+10克糖+150度+10分钟，200克面粉+2个鸡蛋+100毫升牛奶+20克糖+180度+15分钟，等等。每做一个蛋糕，你就尝一下味道，给它打一个分数。最后你找到分数最高的那个组合，就是最佳的配方和烘焙方法。 随机搜索的思路是这样的：你不用列出每种原料和烤箱的可能取值，而是给它们一个范围或者一个分布，比如面粉可以是0到500克之间的任意值，鸡蛋可以是0到5个之间的任意整数值，牛奶可以是0到200毫升之间的任意值，糖可以是0到50克之间的任意值，烤箱的温度可以是100到250度之间的任意值，烘焙的时间可以是5到30分钟之间的任意值。然后你随机地从这些范围或者分布中抽取一些组合，比如123克面粉+2个鸡蛋+87毫升牛奶+18克糖+167度+12分钟，456克面粉+4个鸡蛋+32毫升牛奶+45克糖+223度+27分钟，等等。每做一个蛋糕，你也尝一下味道，给它打一个分数。最后你找到分数最高的那个组合，就是最佳的配方和烘焙方法。 网格搜索和随机搜索的区别在于，网格搜索是把所有可能的组合都试一遍，而随机搜索是只试一部分随机的组合。网格搜索的优点是可以保证找到最佳的组合，但是缺点是需要花费很多时间和资源。随机搜索的优点是可以节省时间和资源，而且有时候也能找到很好的组合，但是缺点是不能保证找到最佳的组合。 假设你有以下六个数据点： 你想要将它们分成两个组，也就是说，n_clusters = 2。KMeans 的工作流程如下： 1. 随机选择两个数据点作为初始的中心点，比如说 [1, 2] 和 [10, 4]。 2. 计算每个数据点到两个中心点的距离，然后将每个数据点分配给距离最近的中心点所在的组。这样，我们就得到了两个组，分别是 [[1, 2], [1, 4], [1, 0]] 和 [[10, 2], [10, 4], [10, 0]]。 3. 计算每个组的数据点的平均值，然后用这个平均值作为新的中心点。这样，我们就得到了两个新的中心点，分别是 [1, 2] 和 [10, 2]。 4. 重复步骤 2 和步骤 3，直到中心点不再变化或者达到最大迭代次数。在这个例子中，由于中心点已经不变了，所以我们就停止迭代，得到最终的聚类结果。 一个例子： Sure, I can show you an example of using sklearn.cluster Kmeans. Here is a code snippet that imports the class, creates an instance with 3 clusters, and fits it on some sample data: Output: You can see that the model has assigned each observation to one of the three clusters based on their similarity. You can also use the method to assign new observations to existing clusters: Output: 给定初始化质心的例子 k-means++ K-means++算法的过程是： 假设你有一堆小球，有红色的，绿色的和蓝色的。你想把它们分成三堆，每堆颜色都差不多。你可以用K-means++算法来做这件事，步骤如下： 1. 你随便拿一个小球，假设是红色的，放在一边，作为第一堆的开始。 2. 你再拿一个小球，但是这次你不是完全随便拿，而是根据它跟已经拿出来的小球的颜色不同来决定。比如说，如果你拿到了一个绿色的小球，它跟红色的小球颜色不同，所以你更有可能拿它；如果你拿到了一个粉色的小球，它跟红色的小球颜色差不多，所以你不太可能拿它。假设你最后拿了一个绿色的小球，放在另一边，作为第二堆的开始。 3. 你再拿一个小球，同样根据它跟已经拿出来的小球的颜色不同来决定。这次你要考虑它跟两个小球的颜色不同，选择跟它们都不太像的小球。比如说，如果你拿到了一个黄色的小球，它跟红色和绿色的小球都有一点颜色不同，所以你有一点可能拿它；如果你拿到了一个蓝色的小球，它跟红色和绿色的小球都有很大颜色不同，所以你更有可能拿它。假设你最后拿了一个蓝色的小球，放在第三边，作为第三堆的开始。 4. 现在你已经有了三堆小球的开始，分别是红色、绿色和蓝色的小球。接下来你要把剩下的小球分配给这三堆。你可以用一个简单的方法：对于每个剩下的小球，看看它跟哪堆开始的小球颜色最像，就把它放到那堆里面。比如说，如果你拿到了一个紫色的小球，它跟蓝色的小球颜色最像，就把它放到蓝色堆里面；如果你拿到了一个橙色的小球，它跟红色的小球颜色最像，就把它放到红色堆里面。 5. 当你把所有剩下的小球都分配完后，你会发现每堆里面有一些不太合适的小球，比如说红色堆里面可能有一些粉色或者棕色的小球。这时候你可以用一个简单的方法来调整一下：对于每堆，找出所有属于这堆的小球中颜色最中间（或者最普通）的那个作为新的开始；然后再重复步骤4，把所有的小球重新分配给新的开始。这样你就可以让每堆里面的小球颜色更加一致。 6. 你可以重复步骤5，直到你觉得每堆里面的小球颜色都很合适，或者你觉得再调整也没有太大的改变。这样你就完成了用K-means++算法把小球分成三堆的任务。 (t-distributed Stochastic Neighbor Embedding) > 什么是 t-分布随机近邻嵌入。 TSNE 是一种让我们看到高维数据的方法，它可以把高维数据变成二维或三维的图像，让我们更容易理解数据的特点。TSNE 的方法是这样的： 首先，它会看高维数据中每两个数据点有多相似，相似的数据点会有更大的概率成为邻居，不相似的数据点会有更小的概率成为邻居。这个概率也受到一个参数的影响，这个参数表示每个数据点想要有多少个邻居。 然后，它会在二维或三维空间中随机放一些点，这些点就是高维数据的映射。它也会计算这些点之间的相似性，用同样的方法，只是换了一个公式。 接着，它会比较高维空间和低维空间中的相似性，如果不一样，就会调整低维空间中的点的位置，让它们更接近高维空间中的相似性。它会重复这个过程，直到找到一个比较好的位置。 最后，它会把低维空间中的点画出来，不同类别的数据点用不同颜色表示。这样我们就可以看到高维数据在低维空间中的分布和聚类情况了。 对象存储不是存储在云服务器的本地，而是存储在分布式的存储集群中，云服务器只是通过网络协议访问对象存储。 分布式的存储集群是对象存储的底层实现，它可能使用文件存储或者块存储来存储对象的数据和元数据，但是对外提供的接口仍然是对象存储的接口，不是文件存储的接口。 简单来说，ONNX.js 会将 ONNX 模型中的每个算子（operator）转换成一个 WebGL 着色器（shader），然后将这些着色器编译成 GPU 可执行的代码。着色器是一种用于控制图形渲染过程的程序，它可以对顶点、像素、纹理等进行计算和处理。ONNX.js 会将模型的输入和输出数据存储在 WebGL 纹理（texture）中，然后通过调用着色器来执行模型的计算。纹理是一种用于存储和映射图像数据的对象，它可以被绑定到着色器中的变量上，从而实现数据的传递和访问。 ONNX.js 还使用了一些优化技术，来提高 WebGL 后端的性能和稳定性。例如，ONNX.js 会根据浏览器和设备的支持情况，自动选择合适的精度和格式来存储纹理数据。ONNX.js 还会缓存已经编译好的着色器和已经分配好的纹理，以避免重复的开销。此外，ONNX.js 还支持一种叫做 pack/unpack 的模式，可以将多个低维度的张量（tensor）打包成一个高维度的张量，从而减少纹理切换和着色器调用的次数。 简单来说，ONNX.js 会将 ONNX 模型中的每个算子（operator）转换成一个 WebAssembly 模块（module），然后将这些模块编译成二进制格式的字节码（bytecode）。字节码是一种可以被浏览器中的 WebAssembly 虚拟机（virtual machine）快速加载和执行的代码格式。ONNX.js 会将模型的输入和输出数据存储在 WebAssembly 的内存（memory）中，然后通过调用 WebAssembly 模块中的函数（function）来执行模型的计算。WebAssembly 的内存是一种可以被 JavaScript 和 WebAssembly 代码共享访问的线性数组（linear array）。ONNX.js 会使用 JavaScript 的类型化数组（typed array）来读写 WebAssembly 的内存。 ONNX.js 还使用了一些优化技术，来提高 WebAssembly 后端的性能和稳定性。例如，ONNX.js 会根据浏览器和设备的支持情况，自动选择合适的精度和格式来存储数据。ONNX.js 还会缓存已经编译好的 WebAssembly 模块，以避免重复的开销。此外，ONNX.js 还支持一种叫做 SIMD 的特性，可以让 WebAssembly 代码同时对多个数据进行相同的操作，从而提高并行计算的效率5。 为了理解这句话，你需要知道以下几个概念： - WASM是WebAssembly的缩写，它是一种在浏览器中运行二进制代码的技术，它可以提高代码的执行效率和安全性。 - WebGL是一种在浏览器中使用GPU进行图形渲染的技术，它可以提高图形处理的性能和质量。 - WebGPU是一种在浏览器中使用GPU进行通用计算的技术，它可以提高计算密集型任务的性能和兼容性。 - ONNX是一种表示机器学习模型的开放标准，它可以让不同的框架和平台之间共享和转换模型。 - ONNX运算符是一种定义模型中各种数学操作的标准，它可以让不同的后端实现和执行模型。 这句话的原因是： - WASM后端是通过将原生的ONNX Runtime引擎编译成WASM格式来实现的，因此它可以保留所有ONNX Runtime支持的功能和特性。 - WebGL和WebGPU后端是通过将ONNX模型转换成相应的图形或计算着色器来实现的，因此它们需要根据不同的硬件和驱动来适配和优化。由于这些后端还处于开发阶段，它们还没有完全覆盖所有ONNX运算符。 * New Bing",
    "url": "/blog/小白术语说明",
    "path": "tech/ai/小白术语说明.mdx"
  },
  {
    "title": "指针网络",
    "content": "Pointer Network（指针网络）是一种特殊类型的神经网络，它在输出中直接指向输入序列中的位置，而不是通常的固定大小的词汇表。这种结构特别适合于那些输出序列依赖于输入序列中元素位置的问题，例如排序问题。 经典的旅行推销员问题（TSP）。在这个问题中，给定一组城市，任务是找出穿过所有城市一次、且总旅行距离最短的路线。 在传统的神经网络中，你需要预定义所有可能的路线，然后让模型选择。但这几乎是不可能的，因为可能的路线数量随城市数量的增加呈现组合爆炸。 这就是指针网络派上用场的地方。Pointer Network通过观察所有的城市（输入序列），然后输出一个序列，该序列是城市的索引，指明了应当访问城市的顺序。其关键在于网络的每一个输出元素都是根据上下文来直接指向输入序列中的某个位置。 指针网络工作的简化步骤： 1. 输入序列编码： 使用RNN/LSTM/Transformer等编码输入序列（城市的坐标）。 2. 注意力机制： 训练一个注意力模型，可对输入序列中的不同部分分配不同的权重。 3. 输出序列解码： 利用注意力权重，生成指向输入序列中元素位置的输出序列（对城市访问的顺序索引）。 通过这种方式，指针网络可以有效地处理那些其输出是输入序列的排序或重组的任务，既提高了任务的处理速度，又避免了不切实际的输出空间设计。 \"不切实际的输出空间设计\"这个短语通常指的是在某些机器学习任务中，如果我们试图预定义所有可能的输出，会遇到输出空间巨大到难以处理的问题。 许多机器学习任务，特别是在自然语言处理中，有一个固定且有限的输出空间。例如，在一个语言模型中，我们可能有一个由所有单词组成的词汇表，模型在给定上下文时预测下一个单词。这个单词列表是预定义的，尽管其可能包含成千上万的单词，它仍然是固定的。 与固定输出空间不同，某些任务涉及的输出空间随输入动态变化。以排序问题为例，假设我们有一个包含10个不同整数的列表，并希望网络输出它们升序排列的结果。输出的每一个位置应该是指向输入列表中某个元素的指针。如果我们尝试像处理固定词汇表一样处理这个问题，我们会遇到几个问题： - 组合爆炸：随着输入列表的大小增加，可能的排序结果（输出空间）是阶乘级增长的（例如，10个数有10!种排列方式），远超常规词汇表的大小。 - 输出依赖于输入：每个不同的输入列表会有一个不同的、特定的排序结果。因此，我们无法预先定义一个能够覆盖所有可能输入的排序结果的静态词汇表。 - 效率低下：预定义所有可能的排列会导致巨大的输出空间，使得模型不仅训练效率低，而且在执行时需要大量的计算资源。 这就是为什么我们说在这种情况下进行\"不切实际的输出空间设计\"是不合理的。作为解决方案，指针网络提出了使用指针的概念。它不是在一个固定的词汇表中选择输出，而是生成指向输入序列中各个元素位置的索引。这种方式在处理变化的输出空间时既高效又有效。它能够根据输入动态调整其输出，从而克服了传统方法在面对这类问题时的局限性。 *",
    "url": "/blog/指针网络",
    "path": "tech/ai/指针网络.md"
  },
  {
    "title": "晕3D",
    "content": "晕3D（Motion Sickness）是许多玩家在玩3D游戏时遇到的常见问题，主要是由于视觉、前庭系统（内耳平衡器官）和身体感觉之间的冲突所引起的。这种冲突会导致大脑混淆，从而引发一系列不适症状，如头晕、恶心、出汗等。以下是详细的原因解释： - 视觉感知：在玩3D游戏或使用虚拟现实（VR）设备时，玩家的眼睛看到的是快速移动的图像和场景变化。 - 前庭感知：前庭系统通过内耳的半规管感知身体的实际运动和姿势变化。当玩家坐在固定的位置上玩游戏时，前庭系统感受到的是静止状态或非常有限的运动。 - 冲突：当视觉系统感知到快速运动而前庭系统感知到静止时，产生了感知上的冲突。这种冲突使大脑难以协调两者的信息，从而引发晕动症。 - 延迟（Latency）：在VR环境中，系统可能会有轻微的延迟（延迟几毫秒到几百毫秒），这会导致图像更新与头部运动不一致，增加晕动症的可能性。 - 帧率不稳定（Frame Rate Instability）：帧率的突然变化或不稳定会导致画面卡顿或不流畅，增加视觉与身体感知之间的冲突。 - 视角变化：游戏中的快速视角变化或频繁的视角切换会增加视觉信息的复杂性，容易引发头晕和不适。 - 运动方式：某些游戏中的运动方式，如第一人称视角的快速移动、旋转或突然加速和减速，容易引发晕动症。 - 屏幕刷新率：低刷新率的屏幕会导致画面拖影和闪烁，这会增加视觉疲劳和晕动症的可能性。 - 显示器尺寸和分辨率：较小的显示器和低分辨率的画面容易引发视觉不适，增加晕动症的风险。 - 敏感度：不同个体对晕动症的敏感程度不同，有些人天生对视觉和前庭系统的冲突更敏感。 - 适应能力：有些玩家经过一段时间的适应，可以逐渐减少晕动症的症状，而另一些人可能持续感到不适。 - 光线和环境：在昏暗的环境中玩游戏或使用VR设备可能会增加眼睛的疲劳度，进而增加晕动症的风险。 - 姿势和位置：玩家的坐姿、头部和身体的位置也会影响前庭系统的感知，如果姿势不自然或长时间保持一种姿势，可能会增加不适感。 晕3D的主要原因是由于视觉系统和前庭系统之间的感知冲突，以及由此产生的感知不一致。各种因素，如延迟、帧率不稳定、视角变化、屏幕刷新率、个人敏感度和环境因素等，都会影响晕动症的发生。 AI可以实时分析玩家的反应和行为，动态调整游戏摄像机的视角和运动方式，以减少晕动症的发生。例如： - 平滑摄像机移动：AI可以通过预测玩家的移动方向和速度，提前调整摄像机的运动，使其更加平滑和自然，减少突兀的摄像机变化。 - 减少摄像机抖动：AI可以检测和减少摄像机的抖动和剧烈运动，尤其是在玩家快速移动或旋转时。 AI可以根据玩家的反馈和生理信号（如眼动追踪数据、头部运动传感器等），自适应调整游戏的视野（Field of View, FOV）以减少晕动症。例如： - 动态视野调整：在玩家感觉不适时，AI可以自动缩小视野，减少边缘视觉的影响，帮助缓解晕动症。 - 智能缩放：AI可以在玩家需要集中注意力时，智能放大重要的游戏区域，减少视觉杂乱。 AI可以在游戏中添加平滑过渡效果，减少画面切换的突兀感。例如： - 淡入淡出效果：在场景切换时，AI可以自动添加淡入淡出的过渡效果，使画面切换更加平滑。 - 智能加载：AI可以预测玩家的下一步行动，并提前加载相关场景，减少加载时的卡顿和突兀感。 AI可以分析玩家的游戏历史和反馈，推荐个性化的游戏设置，以减少晕动症。例如： - 个性化视角设置：AI可以根据玩家的偏好和反馈，推荐最佳的视角和摄像机设置。 - 游戏优化建议：AI可以根据玩家的硬件配置和游戏设置，提供优化建议，以提高游戏流畅度和画面稳定性。 在虚拟现实（VR）游戏中，AI可以通过多种方式减少晕动症。例如： - 智能运动预测：AI可以预测玩家的头部和身体运动，提前调整虚拟世界中的视角和物体位置，减少延迟和不自然的运动。 - 生理反馈监测：AI可以实时监测玩家的生理信号（如心率、皮肤电反应等），在玩家出现不适迹象时，自动调整游戏设置或提供休息提示。 许多现代游戏已经开始采用智能摄像机控制系统，例如《《The Legend of Zelda: Breath of the Wild》》中的摄像机系统，通过平滑移动和智能调整视角，减少了晕动症的发生。 在一些高端VR设备中，已经开始采用AI技术来预测和补偿玩家的头部和身体运动。例如，Oculus和HTC Vive的一些应用程序中，AI可以实时调整视角，减少延迟和不自然的运动，从而减少晕动症。 AI技术在解决晕3D问题上具有巨大的潜力。通过动态摄像机控制、自适应视野调整、平滑过渡效果、个性化设置推荐和虚拟现实中的应用，AI可以显著减少玩家的晕动症，使3D游戏和VR游戏体验更加舒适和愉快。",
    "url": "/blog/晕3D",
    "path": "tech/ai/晕3D.md"
  },
  {
    "title": "机器与人类的视角",
    "content": "人类视角中一切都是符号的排列组合，通过符号沟通并理解世界；万物（人类能够理解的）的符号化都是逐步抽象演变出来的，比如从早期的象形文字，到当代的丰富多彩的多民族语言文字；再比如从 0/1 计算机编码规则，到更加抽象的编程语言；都是从底层的规则定义开始逐步演化组合出更加庞大的抽象符号 机器视角中一切都是像素的分布排列组合，人类通过投喂足够多的标注的图片跟数据来让机器逐步学习到抽象的规则（像素的不同分布代表不同的意义，可能是猫狗，也可能是人类）；理论上机器视角的世界是更加丰富的，而且更可能演化出人类从未定义过的符号（人类制定的符号化路径只是演化路径之一，肯定还有其他符号规则演化的可能性），所以理论上机器可以模拟一切人类世界的东西（能理解的或者不能理解的），机器学习中的 VAE/GAN 等可以通过潜在空间分布来生成逼近真实世界（人类理解的真实世界）的样本正是这种理论的证明；而像素也是人类符号定义的一种，应该有更加本质的存在，等挖掘出后可能更加容易模拟人类视角中的真实世界 written 2024/7/5",
    "url": "/blog/机器与人类的视角",
    "path": "tech/ai/机器与人类的视角.md"
  },
  {
    "title": "机器学习实践记录",
    "content": "业务目标是解决自动化场景遇到的验证码识别 看完本文的收获：你将了解到机器学习在业务中的一种应用场景。分享将从非专业的机器学习开发的视角重现了以业务场景为核心，逐步介绍工具调研、模型调研、训练和部署等过程中所遇到的问题以及解决思路。 识别变长不规则的图片验证码（包含字母或数字） * 遇到的问题 * 付费且不准 * 原因：结果很不准，需要付费 * 成熟开源的 OCR 只能识别比较规整的字母跟数字 * 结果：不选择，决定找开源模型，自己训练 * 网上模型很多，需要甄选并改造成合适的模型 寻找到并调研了的，赞数比较多，应该靠谱 阅读源码后判断只能做定长识别 模型实现：Tensorflow 框架的 CNN * CNN（Convolutional Neural Network 卷积神经网络） 做图片特征提取 * 定长编码：one hot编码，将离散的分类数据转换为神经网络等模型可以处理的向量表示。如果编码是 4 位，则是一个识别 4 位字符的分类任务 * 交叉熵损失函数推动反向传播 问题1 * 我们的目标验证码是变长的 * 处理变长可能方案：裁剪，切割，并一个个识别文本 * 结果：放弃直接的 CNN 模型，寻找更合适的模型 * 原因：图片背景有噪声，导致对图片进行切割困难，会损失一些文本细节导致识别不准。 寻找到并调研的，赞数相对 CNN 少了一个量级，看起来很顺眼，有论文支持 阅读源码后判断可以做变长序列识别 Pytorch 框架的 CRNN 流程 * CNN 做图片特征提取 * LSTM(RNN)(Long-short term memory) 对 CNN 提取的特征序列建模，利用上下文信息提高识别的准确性； * 对于 RNN 的作用 * 作用1（字符内部像素序列）：对于常见的随机顺序验证码，RNN 通过处理构成字符的像素序列并捕捉序列中的特征信息，学习到字符的局部和全局特征，包括形状、纹理、笔画等信息，从而提升判定字符的准确率。 * 作用2（字符序列）：对于非随机顺序验证码，对于一些相似或者易混淆的字符，RNN可以根据前后的字符来判断最可能的结果。例如，如果验证码中有一个字符“l”，它可能是字母“l”或者数字“1”，但是如果前面的字符是“o”，并且训练数据中经常出现 ol，那么RNN就可以推断出它更可能是字母“l”；也即 RNN 能够在大样本中寻找到生成验证码序列的一些可能得规律，从而增加准确性 * PS：定长任务中可以理解成是对 CNN 识别能力的增强？ * CTC（Connectionist temporal classification 连接时序分类） 对 RNN 的输出序列对齐 + 计算损失驱动梯度下降反向传播 * CTC 算法不需要训练数据对齐（降低人工对齐工作量），它会把所有相同输出的对齐合并。帮助模型学习字符级别的对齐和映射关系，尤其在没有明确字符分隔符的情况下 * 其他应用：适用于音频到文字的转码任务（音频的时长 t0->t1 可能对应一个字符） 问题1 * 样本量评估（经验问题）：到底多少数据才能训练出可观效果？知道量级后才能给同事时发出定量的帮忙请求，做到有的放失 * 实践步骤 1. 参考开源库 * 基本都在 >=五位数 2. 测试量级；找到最小量级同时又能得出不错效果的大概样本数 * 找到合适的自动样本生成库，用自动生成的样本测试 5 位数能达到 90%；降低量级测试到 6k 数据能达到 80% 左右准确性；于是暂定为 6k 的目标标注数据 问题2 * 真实样本数不足导致学习慢（损失率下不去）：1k 多的标注数据不足以直接训练出效果（发动人民群众标注前提是看到效果（死锁），后面知道，标注本身让人没啥好感） * 当时标注一个样本大致需要 10s，刷新 -> 下载 -> 打开 -> 标注；还是比较费时间 * 标注加速（解决标注慢问题）：脚本实现样本批量下载，批量合并 * 数据清洗：对大家帮忙标注的数据进行简单核对，例如：对包含 0，1，o，l 等的可能出错的标注进行简单核对（ls sample/train | grep 'l'） 实践步骤 * 微调 * 猜想（微调）：迁移学习跟特征共享，用少量样本训练出效果 * 对比：九年义务教育中老师口中的举一反三，先学到如何使用公式（底层特征），然后套公式（知识迁移）；机器学习也可以通过其他样本（与真实样本共享一些特征）的学习积累底层特征，然后应用到少量样本加速学习过程 * 猜想实践： 1. 用第三方库使用同样的字符集生成6万张验证码样本，4,5,6长度分别是2万张；划分为训练 57k + 测试集 30k，花费17个小时（M1 CPU，在更高级版本的 pytorch 中能够支持 M1 的 GPU）训练出参数模型 A.pt * 效果：识别自身测试集准确率 90% 左右 2. 基于 A.pt 的参数，使用 1k 多的标注数据进行训练模型 B.pt（体积是 31M） * 效果：损失率下降很快，3个小时，达到准确率 44% 左右 3. 改造数据加载器, 调参 4. 大家帮忙持续标注更多真实数据并优化训练模型 5. 数据修正 6. 最后将模型识别部署成 python 服务（目前达到准确率 75% 左右（训练集3k，测试集 250）） 感悟 * 给定模型下，能搞到合适的训练数据集很重要 是否可以通过已有样本进行变换生成新的样本？ * 数据集较小 * 本次应对：增加真实数据量标注 * 后续尝试： 0. （已经实践）通过借助模型实际应用进行数据收集，识别准确的数据可以直接扩充标注样本库，识别失败的数据进行人工标记后喂给模型”刻意练习“ * 刻意练习后得到的新权重有时候无法识别旧权重识别准确的数据 * 思考：只做错题，而不去重复做会做的题不一定会提升模型能力 * 实践：在可以刻意练习基础上进行重复的题目训练以维持旧能力（通过旧权重持续产生正确的数据，然后投喂给新权重） * 实践后的思考： * 实际应用收集的准确数据是能提升模型准确率的 * 猜想：模型的识别由数据的多个特征综合判定的，比如人脸有耳鼻眼嘴 4 个特征，如果只有耳朵认识的准确率比较低（其他特征识别准确率比较高），则模型还是有可能判定成功一张脸，通过收集模型判定为脸的数据来训练模型，则可能提高识别耳朵特征的识别准确率，从而提升整体综合准确率 * 只是刻意练习而不去强化准确的认知，会导致模型忘记之前会做的题 * 类似的鸡生蛋，蛋生鸡：SAM 模型的数据也并非全都是标注的，后面大量的训练数据都是通过早期的 SAM 模型分割出，然后训练出更强大的 SAM 模型 1. 使用数据增强技术，如随机裁剪、旋转、翻转等，扩充训练数据的多样性。 2. 使用 VAE（变分自编码器） 模型根据现有数据生成类似样本（因为训练 VAE 本身就需要比较多的样本数据，并且 VAE 也需要知道生成样本的Label） 3. GAN? 1. 尝试了一般 cgan ，3k 张图训练，效果不佳（LOSS 会震荡，使用 Adam 优化器） 2. 后续尝试 wgan（训练更平滑，使用 RMSprop 优化器） 4. stable diffusion 图生图？ * 已经尝试 1. 5k 张长度为 4 的类似样本的 * cgan 训练，结果：训练后生成数字比较模糊 * wgan 训练，结果：生成数字还是比较模糊；原因：样本太少（真实样本只标记了 3k ，估计真实） * 模型复杂度过高：过多的参数和复杂的模型结构会使模型在训练数据上表现很好，但在新数据上泛化能力较差。 * 本次应对：使用比较靠谱的论文模型；PS：模型使用的 LSTM 在每个 Step 共享参数能避免过拟合（RNN 的升级版，RNN 会有梯度消失跟爆炸问题（而正是由于共享参数导致）） * 方案汇总：模型参数缩减（剪枝（去除重要要的参数），量化（eg:缩减精度），知识蒸馏等），但是这个度不太好把控；例子：使用 LLaMa 模型初始权重（通过无监督学习获得）微调出更小的但是效果还不错的新模型 Alpaca 跟 Vicuna * 噪声和异常样本：当训练数据中存在噪声、异常样本或标注错误时，CRNN可能会过拟合这些不代表真实数据分布的异常情况。模型会试图适应这些异常数据，导致在新数据上的表现下降。 * 本次应对：手动清理；模型去预测测试集，对测试集预测错误的样本进行归纳，再反向应用到训练集 * 其他方案：基于统计，聚类，邻近度，机器学习，时间序等；异常检测方法的选择和调整需要根据具体问题和数据的特点进行 * 不平衡的类别分布：如果训练数据中的类别分布不平衡，即某些类别的样本数量远远多于其他类别，模型可能会倾向于过拟合训练数据中较多的类别。这会导致模型对于少数类别的泛化能力较差。 * 本次应对：真实的数据抓取，符合真实数据的生成规律分布，规避问题 * 过度训练：CRNN进行过多的训练轮次或使用过小的学习率，模型可能会过度拟合训练数据。过度训练会导致模型过度适应训练数据中的细节和噪声，而无法泛化到新数据。 * 应对：早停法：验证集上监控模型性能，当性能不再提升时停止训练。 * * * * * * * * * *",
    "url": "/blog/机器学习实践记录",
    "path": "tech/ai/机器学习实践记录.md"
  },
  {
    "title": "监督学习",
    "content": "在监督学习中，训练数据既包含输入特征也包含输出标签（或称为目标值）。算法通过学习这些数据，建立输入和输出之间的映射关系，以便能够预测新输入数据的输出。监督学习常用于分类和回归问题。常见的监督学习算法包括线性回归、逻辑回归、决策树、支持向量机、朴素贝叶斯、神经网络等。 在半监督学习中，训练数据包含少量的带有标签的样本和大量的无标签样本。这种方法结合了监督学习和无监督学习的优势，可以在标签数据有限的情况下提高模型的性能。 不依赖于标注过的训练数据来学习数据的特征。算法能够自动地从数据中发现结构和模式，而不需要任何外部的指导或标签。无监督学习特别适合于那些难以获取标注数据的场景，或者当我们对数据的结构和关系知之甚少时。 * 聚类分析：通过聚类算法，如K-means或层次聚类，将数据分组，使得同一组内的数据点相似度高，不同组间的数据点相似度低。这有助于识别数据中的自然分布和结构。 * 降维技术：如主成分分析（PCA）、核PCA、t-SNE等，通过降低数据的维度来提取关键特征，同时尽量保留数据的重要信息。 * 自编码器（动态训练参数）：一种神经网络，通过重构输入数据来学习数据的有效表示。自编码器的隐藏层可以捕捉数据的关键特征。 * 例子：MNIST 数据的压缩与解压中，对于每一张图像，编码器的函数都是试着去学习一种可以将图像的数千个像素点的信息压缩到一个32维的向量中的方法，而解码器则试图从这个32维的表示中重新构造出原始图像。训练过程中要达到的目标就是使得解码后的图像尽可能的接近原始的输入图像。 输入图像数据本身就是训练目标 * 对比学习：通过比较数据点之间的相似性和差异性，学习数据的特征表示。提供了更多的灵活性和效率（比如服务于迁移学习），尤其是在处理大规模未标注数据集时 * 对比学习的目标（可以被视作迁移学习的准备步骤，以下给定的 SimpleCNN 例子可以在训练好后在最后一层加上全连接层进行图像分类的迁移学习，具体可以查看迁移学习的示例代码例子） * 表征学习： 对比学习主要用于无监督的场景，因此它最主要的目标是学习出能够捕捉数据本质特征的表达（表征），而不是像监督学习那样直接用于分类或回归任务。这些特征可以用于下游任务，例如图像识别、目标检测或自然语言处理任务等。 * 数据增强的效果理解： 利用数据增强创建的不同视图，模型学习到的特征应该是对这些视图改变具有不变性的。这意味着无论图像如何旋转或变色，模型都可以识别出图像中的同一对象。 对比学习示例 PyTorch实现的对比学习的简单代码示例。模型的目标是学习一个特征表示，这个表示能够区分不同的数据样本。具体来说，模型通过最大化正样本（经过数据增强的相似图像）之间的相似度来实现这一点，同时最小化与负样本（不同的图像）之间的相似度。不依赖于外部的标注信息。 强化学习是一种通过与环境的交互来学习的方法。在这种学习方式中，算法通过尝试不同的行为来探索环境，并根据每次行为的结果（奖励或惩罚）来调整其策略，以最大化长期累积的奖励。强化学习常用于控制问题，如机器人导航、游戏AI等。（Python Gym库提供了一种简单有效的方式来设计和评估强化学习算法，特别适用于那些需要快速了解和应用新技术的场合。） 三种方式： * 基于价值：例如 Q-learning * 基于策略 * 价值与策略结合 迁移学习是机器学习中的一种策略，它涉及将在一个任务上已经学习到的知识应用到另一个任务上。换句话说，你拿一个在大量数据上经过预训练的模型，然后将其应用到一个与之相关但数据相对较少的新任务上。这种做法可以加快模型的训练速度，提高模型的性能，尤其是当新任务的数据有限时。 使用Python中的PyTorch库对新的图像分类任务进行微调。这里所使用的预训练模型是ResNet，它经过在ImageNet数据集上的预训练，我们将其应用到一个新的分类任务上（如猫狗分类）。 * gpt *",
    "url": "/blog/机器学习的种类",
    "path": "tech/ai/机器学习的种类.md"
  },
  {
    "title": "柏拉图表征假说与 AI 关系",
    "content": "Ilya Sutskever在离开OpenAI后不久点赞了一篇由MIT团队发表的AI论文，这篇题为《The Platonic Representation Hypothesis》的论文引起了广泛关注。这篇论文探讨了AI模型在不同数据和模态上的训练是否趋向于收敛成一个共享的现实世界统计模型。本文将深入解析该论文的核心思想，并探讨其对未来AI发展的重要意义。 要理解柏拉图表征假说，首先需要了解柏拉图的洞穴寓言。在柏拉图的著作《理想国》中，他描述了一群囚犯从出生就被锁在洞穴中，只能看到身后物体在墙上的影子。这些影子成为了他们眼中的“现实”，但实际上这些影子只是现实的投影。哲学家被比喻为从洞穴中走出来，看到阳光下真实世界的人，他们通过逻辑、数学和自然科学等手段理解更高层次的“现实”。今天，这个目标传递到了AI科学家的手中。 柏拉图表征假说的核心是：不同的神经网络模型在不同的数据和模态上训练，最终会在其表征空间中收敛成一个共享的现实世界统计模型。换句话说，随着AI模型规模和训练数据的增加，不同的AI模型会趋向于对现实进行统一的表征。 论文中提出了一种验证柏拉图假说的方法——模型拼接（Model Stitching）。该方法通过将两个模型的中间表示层连接起来，形成一个新的“缝合”模型，如果这个缝合模型表现良好，说明两个原始模型的表征是兼容的。通过这种方法，研究者分析了78个计算机视觉（CV）模型，发现性能越强的模型其表征相似度越高。 论文指出，AI模型表征收敛主要有以下三大原因： * 任务通用性：当一个AI模型需要同时完成多种任务时，其表征空间会收敛到一个小范围，因为每个任务目标都会对模型施加额外的约束。 * 模型容量：模型越大，越容易逼近全局最优表征，从而推动表征收敛。更大的模型能够找到一个共享的全局最优解，实现跨模态的通用性和适应性。 * 简单性偏见：深度神经网络倾向于选择最简单的解决方案，这种倾向随着模型变大而更加显著。这种简单性偏见符合奥卡姆剃刀原则，促使模型找到更高质量的解决方案。 实验结果表明，模型性能越强，其表征相似度越高。论文作者提出了多种方法来验证柏拉图表征假说的有效性，并发现强大的模型往往在表征上趋同。 这给我们的启示是：随着模型参数、任务多样性和算力的增加，AI模型的表征会逐渐收敛趋同。这是否意味着只要扩大模型规模就可以实现通用人工智能（AGI）呢？答案是复杂的。尽管扩大模型规模能实现表征收敛，但对于某些独立任务而言，单独训练一个专用模型可能更具经济价值。 柏拉图表征假说为我们提供了一个新的视角来审视多模态数据之间的关系。即使训练集中不存在跨模态的配对数据，不同模态的数据也会对模型训练有直接帮助。未来，随着AI技术的不断进步，我们能否借助AI的力量走出洞穴，理解高维度的真正现实呢？ * 原文翻译带图： * * 解读： 柏拉图表征假说（Plato's theory of representation）是柏拉图关于知识和现实的哲学理论，强调了抽象理念（或“形式”）和感知现实之间的关系。这一假说在柏拉图的对话中得到了广泛的讨论，特别是在《理想国》和《斐多篇》中。 柏拉图认为，感知世界中的所有事物都是不完美的、暂时的，它们只是更高层次的完美和永恒的“理念”或“形式”的影像或表征。这些理念是唯一真正的存在，而感知世界只是对这些理念的模仿或反映。以下是对柏拉图表征假说的具体解释和例子： 1. 理念世界（World of Forms）：这是一个抽象的、非物质的、完美和永恒的世界。所有的理念（如“美”、“善”、“正义”）都存在于这个世界中。 2. 感知世界（World of Senses）：这是我们日常生活中体验到的物质世界，充满了不完美和变化。这里的所有事物都是理念的模仿或表征。 1. 洞穴比喻（Allegory of the Cave）： - 在《理想国》第七卷中，柏拉图描述了一个洞穴中的囚徒，他们从出生起就被锁链锁住，只能看到洞穴墙壁上的影子。这些影子是洞外物体在火光下投射到墙上的影像。 - 这些影子代表感知世界中的事物，而洞外的真实物体代表理念。 - 当一个囚徒被解放并走出洞穴，他首先会被阳光（真理和知识的象征）刺痛眼睛，但逐渐适应后，他会看到真实的事物和它们的完美形式。 - 这个比喻说明，人类的感知只是对真实理念的影像或表征，而通过哲学和知识的追求，我们可以理解和接近这些真实的理念。 2. 床的例子： - 在《理想国》第十卷中，柏拉图讨论了工匠制作的床和画家绘制的床之间的区别。 - 工匠制作的床是对“床”的理念的表征，它在感知世界中存在。 - 画家绘制的床是对工匠制作的床的模仿，是感知世界的二次表征。 - 因此，画家的床与真实的“床”的理念相去甚远，因为它只是模仿的模仿。 - 现实世界中的事物：我们在日常生活中感知到的所有事物（如树、桌子、人）都是理念的影像或表征。 - 理念或形式：这些是永恒和完美的原型，它们存在于一个超越物质世界的抽象层面。 - 知识和感知：真正的知识是对理念的理解，而感知只是对表征的认识。 柏拉图的表征假说对西方哲学的发展产生了深远影响，特别是在形而上学和认识论领域。它提出了关于现实、知识和真理的基本问题，这些问题在现代哲学和科学中仍然具有重要意义。例如： - 科学理论：科学家们试图通过实验和观察揭示自然界的基本法则，这些法则可以被视为对自然界“理念”的探索。 - 艺术和文学：艺术家和作家通过他们的作品表征和探索人类经验和理念，反映了柏拉图的表征假说的概念。 柏拉图表征假说认为，感知世界中的事物只是永恒和完美的理念的影像或表征。通过哲学和知识的追求，我们可以超越感知世界，接近这些真实的理念。这个假说不仅是理解柏拉图哲学的关键，也是认识论和形而上学的基础理论之一。 * GPT-4o",
    "url": "/blog/柏拉图表征假说",
    "path": "tech/ai/柏拉图表征假说.md"
  },
  {
    "title": "模型压缩",
    "content": "* 知识蒸馏 * 通过训练一个较小的“学生”模型来模仿一个大的“教师”模型的行为，从而达到压缩模型的目的。 * 低秩分解 * 效果 * 低秩分解不仅减少了模型的计算和存储需求，还有助于提高模型的泛化能力（参数少，有助于降低过拟合，则泛化能力越强？）。 * 压缩：图像矩阵的压缩 How-to：权重矩阵 W 可以表示为两个低秩矩阵A和B的乘积：W = A B^T。这里A和B的秩远小于W，从而降低了GPU显存占用。 * reference * * 量化 * 将权重和激活函数的精度从单精度浮点数（32位）降低到更低位的表示，如16位、8位甚至更低。 * 在极端的情况下，可以进行二值化或三值化，此时权重和激活只取{-1, 0, 1}这几个值。 * 权重剪枝 * 稀疏剪枝：移除模型中权重的一部分，通常是那些接近于零的权重。 * 结构化剪枝：按照某种模式移除权值，比如剪枝整个卷积核或卷积滤波器。 * 共享权重： * 在网络的不同部分共享同一组参数，这在循环神经网络（RNNs）中很常见。 * 紧凑型卷积核： * 使用更小的卷积核或组合小卷积核来替代大卷积核，如深度可分离卷积。 小波变换： * 使用小波变换替代一部分卷积层来实现特征提取与降维。 * 使用更高效的架构： * 设计或使用特别为效率优化的网络架构，如MobileNet、SqueezeNet、EfficientNet等。 重参数化不仅仅指低秩分解，它指的是用一种不同的方式重新表达模型中的参数，以达到某种目的，比如减少参数总数、增加模型的稳定性或是改善性能，或者让模型能够进行求导梯度下降（例如 VAE 的训练）等。 低秩分解是重参数化的一种方法。 * gpt",
    "url": "/blog/模型压缩",
    "path": "tech/ai/模型压缩.md"
  },
  {
    "title": "正则化",
    "content": "正则化是一种防止机器学习模型过拟合的重要技术，通过在模型训练过程中引入某些限制或惩罚，使模型更具泛化能力。正则化方法有多种，具体可以分为以下几类： L1正则化 (Lasso) - L1正则化：通过在损失函数中添加权重绝对值的和作为惩罚项，鼓励模型的权重稀疏化，从而实现特征选择。 - 公式：$ \\text{Loss} = \\text{Original Loss} + \\lambda \\sum_i |w_i| $ L2正则化 (Ridge) - L2正则化：通过在损失函数中添加权重平方和作为惩罚项，防止模型的权重值过大，增加模型的稳定性。 - 公式：$ \\text{Loss} = \\text{Original Loss} + \\lambda \\sum_i w_i^2 $ 数据增强 (Data Augmentation) - 数据增强：通过对训练数据进行各种变换（如旋转、裁剪、翻转等），生成新的训练样本，从而增加数据的多样性，防止模型过拟合。 Dropout - Dropout：在每次训练迭代中，随机“丢弃”一部分神经元，使模型在训练过程中每次都使用不同的网络结构，增强模型的泛化能力。 - 公式：在每次训练迭代中，将每个神经元以概率 $ p $ 置为 0。 Label Smoothing - Label Smoothing：通过对标签分布进行平滑处理，防止模型对训练数据中的特定标签过度自信，提高模型的泛化能力。 - 公式：$ y_{\\text{smoothed}} = (1 - \\epsilon) \\cdot y_{\\text{one-hot}} + \\frac{\\epsilon}{k} $ Early Stopping - Early Stopping：在训练过程中监控验证集的表现，当验证集误差不再下降时，提前停止训练，防止模型过拟合。 Weight Sharing - Weight Sharing：在网络结构中共享部分权重，减少模型的自由参数，从而防止过拟合。例如，在卷积神经网络中，共享卷积核的权重。 - 对抗训练：通过在训练过程中加入对抗样本，使模型在面对恶意样本时也能表现良好，从而提高模型的鲁棒性。 - Batch Normalization：在每个小批量数据上对网络的输入进行标准化，使得每一层的输入在训练过程中保持稳定，从而加速训练并提高模型的泛化能力。 VQ 正则化（Vector Quantization Regularization）在某种程度上结合了多种正则化技术的特点，但它最接近于模型结构正则化和数据正则化的混合体。具体来说： * 模型结构正则化：通过在模型结构中引入向量量化层，强制潜在表示离散化，减少模型的自由度，从而防止过拟合。 * 数据正则化：通过离散化潜在表示，实际上增加了数据表示的稀疏性和鲁棒性，类似于数据增强技术。 例子： 假设你拍摄了一张照片，照片中有许多不同的颜色，我们需要对照片中的颜色进行量化，即我们采用一组有限的颜色来近似表示照片中的所有颜色。 具体步骤如下： 1. 选择代码簿（Codebook）：为了量化颜色，我们首先需要一个颜色的代码簿。代码簿就是我们选择用来表示所有颜色的那组颜色，比如我们选择红色，蓝色，绿色，黄色，黑色和白色作为我们的代码簿，即这六种颜色作为我们用来表示所有颜色的基本颜色集合。 2. 将照片中的每个像素颜色映射到最接近的代码簿颜色：然后我们遍历照片中的每个像素，找到距离这个像素颜色最近的代码簿中的颜色，然后用这个最近的颜色来替代原来的像素颜色。例如，一个淡紫色的像素在我们的代码中可能会被替换为蓝色，因为淡紫色在距离上最接近蓝色。 3. 保存映射信息：为了重构原图像，我们需要保存颜色的映射信息，即每个像素颜色对应的代码簿中的颜色序号。有了这个信息，我们可以从代码簿中找到正确的颜色来重新构建图像。 向量量化（VQ，Vector Quantization）的代码簿一般通过无监督学习方法获得，其中最常见的就是k-means聚类算法。例子： 创建一个VQ层通常涉及以下几个步骤： 1. 初始化一个码本（Codebook）：这是一组可以学习的向量，模型在训练中会逐渐更新这些向量，以便它们代表数据中的通用模式。 2. 前向传播（Forward Pass）：在前向传播期间，模型接收输入的连续数据，并将其映射到码本中最接近的向量上，这一步称为量化。 3. 损失函数（Loss Function）：VQ层需要一个损失函数来指导学习过程。通常有两部分组成——量化损失和码本损失。量化损失鼓励输入靠近它们对应的码本向量，码本损失则保证码本向量靠近输入数据。 代码例子 这个例子中的类定义了VQ层的核心功能。在前向传播中，输入被量化到最接近的码本向量，计算损失后返回量化的输出、损失以及一个度量码本使用情况的复杂度(perplexity)指标。在训练中，这个损失将被用来更新码本向量以及可能的其他网络参数。 Label Smoothing 是一种正则化技术，用于减少深度学习模型在训练过程中对特定标签的过度自信，从而提高模型的泛化能力。通过将一个类别的标签分布稍微平滑化，使得模型不会过度拟合训练数据中的噪声。 在分类任务中，标准的一个热编码（one-hot encoding）将目标标签表示为一个向量，其中正确的类别被编码为 1，其他类别被编码为 0。例如，对于一个 4 类分类任务，如果正确类别是第 2 类，标签向量是 [0, 1, 0, 0]。 使用 Label Smoothing，我们将目标标签从一个热编码转换为一个平滑分布。例如，对于一个 4 类分类任务，我们可以将标签向量 [0, 1, 0, 0] 转换为 [0.1, 0.7, 0.1, 0.1]，其中0.1表示的是平滑因子的一部分。 假设我们有一个 3 类分类任务： 1. 类别 A 2. 类别 B 3. 类别 C 标准一个热编码标签（没有 Label Smoothing）： - 类别 A 的标签： [1, 0, 0] - 类别 B 的标签： [0, 1, 0] - 类别 C 的标签： [0, 0, 1] 使用 Label Smoothing 后的标签（假设平滑参数为 0.1）： - 类别 A 的标签： [0.9, 0.05, 0.05] - 类别 B 的标签： [0.05, 0.9, 0.05] - 类别 C 的标签： [0.05, 0.05, 0.9] 假设我们有一个类别数为 $ k $ 的分类任务，平滑因子为 $ \\epsilon $。对每个标签的转换公式为： $ y_{\\text{smoothed}} = (1 - \\epsilon) \\cdot y_{\\text{one-hot}} + \\frac{\\epsilon}{k} $ 其中，$ y_{\\text{one-hot}} $ 是一个热编码标签向量，$ y_{\\text{smoothed}} $ 是平滑后的标签向量。 在图像分类任务中，使用 Label Smoothing 可以防止模型在训练过程中对特定类别的过度自信，尤其是当训练数据中存在噪声时。这有助于提高模型的泛化能力，从而在测试数据上的表现更好。 在机器翻译、文本分类等任务中，Label Smoothing 也被广泛使用。它可以帮助模型在处理词汇表较大的情况下，更好地进行分类和预测。 Label Smoothing 是一种有效的正则化技术，通过平滑标签分布，减少模型对训练数据的过度拟合，提高泛化能力。它在图像分类、自然语言处理等任务中都有广泛应用，能够显著提升模型在测试集上的表现。",
    "url": "/blog/正则化",
    "path": "tech/ai/正则化.md"
  },
  {
    "title": "WebGPU",
    "content": "WebGPU shaders定义了具体的渲染操作指令，而pipelines则组织了这些指令和状态，整个WebGPU渲染流程则负责执行这些指令，最终完成图形渲染。 实际例子：vite-site/src/pages/machine-learning/webgpu/util.ts 是WebGPU中的一个核心概念，它是一种资源绑定机制，允许开发者将资源（如缓冲区、纹理、采样器等）与着色器的绑定点（binding points）关联起来。在着色器程序中，这些绑定点可以通过特定的标识符访问对应的资源。理解的关键在于以下几点： 1. 资源封装：封装了一组资源，这些资源可以是缓冲区、纹理、采样器等。这些资源在着色器中用于各种目的，例如存储顶点数据、纹理映射、统一缓冲区（uniforms）等。 2. 布局（Layout）：每个都有一个对应的，它定义了中资源的类型、数量和绑定方式。这个布局必须与在管线创建时指定的布局相匹配，这样资源才能被正确地绑定和使用。 3. 绑定点（Binding Points）：资源在中通过绑定点索引来引用。这些索引对应着色器代码中的绑定标识符，确保着色器可以正确地访问绑定的资源。 4. 管线兼容性：必须与管线兼容，这意味着它的布局要与管线中预期的布局一致。如果管线期望在绑定点0上有一个缓冲区，那么也应该在绑定点0上提供一个缓冲区。 5. 性能优化：使用可以优化性能，因为它允许开发者一次设置多个资源，然后在多个绘制调用中重用相同的，无需在每次绘制时都重新绑定资源。 6. 着色器访问：在着色器代码中，通过绑定点索引访问中的资源。例如，如果一个缓冲区绑定到的绑定点0，那么在着色器中就可以通过定义的绑定点0来访问这个缓冲区。 7. 命令编码：在WebGPU的命令编码过程中，被设置到渲染或计算管线上。这样，当管线运行着色器时，它会使用这些绑定的资源。 举个例子，如果你有一个计算着色器需要两个输入缓冲区和一个输出缓冲区，你会创建一个，在其中绑定这三个缓冲区到指定的绑定点。然后，你可以将这个设置到计算管线上，当计算命令执行时，着色器就可以读取输入缓冲区的数据并将结果写入输出缓冲区。 总之，是WebGPU中管理和优化资源访问的一种机制，它确保了资源可以被高效地组织和重用，同时为着色器提供了必要的数据和参数。",
    "url": "/blog/理解 webGPU",
    "path": "tech/ai/理解 webGPU.md"
  },
  {
    "title": "LSP（Language Server Protocol）",
    "content": "Cursor 是在 LSP 基础上构建了智能中间层 1. 传统 LSP 架构 2. Cursor 的增强架构 1. 请求预处理（发给 LSP 前） 2. 响应后处理（LSP 返回后） 3. 上下文增强 1. 智能补全优化 2. 错误诊断增强 3. 代码导航优化 1. 中间件模式 2. 上下文感知 1. 兼容性：继承了整个 LSP 生态系统 2. 可扩展性：可以为任何支持 LSP 的语言添加 AI 能力 3. 渐进增强：即使 AI 层失效，基础 LSP 功能仍然可用 4. 性能优化：智能过滤减少了不必要的信息噪音 理解从 code index 的建立开始，到 diff code review 可能的流程 假设有人修改了 ： 最终发送给 AI 的上下文： javascript async register(userData) { return this.userService.createUser(userData); // 需要确保传入的 userData 有 username } javascript async create(req, res) { const userData = req.body; const user = await this.userService.createUser(userData); // 需要验证前端是否传入 username } javascript static create(userData) { return new User(userData); // 确认 User 模型是否包含 username 字段 } ` 基于这个上下文，AI 能够发现： 1. 潜在问题：新增的 检查可能导致 为 undefined 时报错 2. 影响范围： 和 都需要确保传入 username 3. 建议： - 添加 username 存在性检查 - 更新相关的测试用例 - 检查前端是否正确传递 username 字段 整个流程就是： 1. 建立索引：分析代码结构和依赖关系 2. 解析变更：识别修改了什么 3. 查找相关：从索引中找到相关的文件和函数 4. 提取片段：只取相关的代码片段，不是整个文件 5. 构建上下文：组装成 AI 能理解的格式进行分析",
    "url": "/blog/理解Cursor Indexing",
    "path": "tech/ai/理解Cursor Indexing.md"
  },
  {
    "title": "神经网络",
    "content": "神经网络可以分为不同的类型，如前馈网络，卷积网络，循环网络等，用于解决不同的问题，如图像识别，自然语言处理，时间序列预测等。 * 前馈神经网络（Feedforward Neural Network）：这类神经网络中，信息只从输入层向输出层传递，没有反馈或循环的连接。每一层的神经元只与下一层的神经元相连，不与同层或上一层的神经元相连。前馈神经网络的代表有全连接神经网络（FCN）、卷积神经网络（CNN）、生成对抗网络（GAN）等。 * 反馈神经网络（Feedback Neural Network）：这类神经网络中，信息可以在不同层之间反馈或循环，形成有向或无向的环路。这样可以使神经网络具有记忆功能，在不同时刻具有不同的状态。反馈神经网络的代表有循环神经网络（RNN）、长短期记忆网络（LSTM）、Hopfield网络、玻尔兹曼机等。 * 自组织神经网络（Self-organizing Neural Network）：这类神经网络中，信息的传递和处理是根据输入数据的特征自动调整的，而不是由预先设定的规则或参数决定的。这样可以使神经网络具有自适应和自学习能力，能够发现输入数据中的模式和规律。自组织神经网络的代表有自组织映射（SOM）、学习向量量化（LVQ）、自适应共振理论（ART）等。 * 混合神经网络（Hybrid Neural Network）：这类神经网络中，信息的传递和处理是由不同类型的子网络组合而成的，每个子网络可以有不同的结构和功能，以实现更复杂和高级的任务。混合神经网络的代表有变分自编码器（VAE）、Transformer、BERT等。 LSTM和RNN的区别主要体现在以下几个方面： - LSTM引入了细胞状态（cell state），它是一条贯穿整个链式结构的水平线，可以在不同时间步之间传递信息，而不受梯度消失或爆炸的影响。 - LSTM通过三个门结构（遗忘门、输入门、输出门）来控制细胞状态中的信息流动，可以选择性地添加或删除信息，从而实现对长期和短期信息的记忆和忘记。 - LSTM的激活函数不仅使用了tanh函数，还使用了sigmoid函数，并结合求和操作，使得梯度更容易保持稳定。 - LSTM可以更好地处理不同长度的序列数据，而RNN通常需要固定长度的输入和输出。 参数共享的好处之一是减少了LSTM模型的参数量。如果每个时间步都有不同的参数，模型的参数量将随着时间步数的增加而成倍增加，导致模型变得非常庞大且难以训练。通过共享参数，LSTM模型可以更高效地学习和表示序列数据的长期依赖关系。 * 学习共享模式和规律：参数共享使得模型能够在整个序列中共享相同的门控参数，从而更好地学习序列中的共享模式和规律。这意味着模型能够捕捉到序列数据中的共性特征，而不仅仅是对单个时间步的输入进行建模。通过共享参数，模型可以更好地理解序列数据中的长期依赖关系，并且在新数据中也能够适应这些共性特征。 * 减少过拟合的风险：参数共享有助于降低模型的复杂性和参数量，从而减少过拟合的风险。当模型的参数量过多时，容易导致过拟合，即在训练数据上表现良好，但在新数据上表现较差。通过参数共享，模型的参数量得到控制，使得模型更加简洁，减少了过拟合的可能性，并提高了模型的泛化能力。 * 提取通用特征表示：参数共享有助于模型学习到更通用的特征表示。通过在不同时间步共享参数，模型能够将重要的信息从一个时间步传递到下一个时间步，形成连续的特征表示。这样，模型能够更好地捕捉序列中的关键特征，并将其应用于未知数据的推断和泛化。 因为参数少，所以避免了过拟合，所以提取的才是更加通用的特征？ - CNN 是卷积神经网络，它可以有效地提取图像的局部特征，但是它对图像的整体结构和顺序不敏感，因此不适合处理变长的数据。 - RNN 是循环神经网络，它可以处理序列数据，比如文本、语音、视频等，它有记忆功能，可以保存前面的信息，并影响后面的输出。但是它也有一些问题，比如梯度消失或爆炸、计算复杂度高、难以并行等。 - CNN 和 RNN 的结合可以克服各自的缺点，发挥各自的优势。一种常见的结合方式是先用 CNN 对图像进行特征提取，然后用 RNN 对提取出来的特征进行序列生成或分类。这样可以利用 CNN 的速度和轻量，以及 RNN 的顺序敏感性。 - 识别变长的字母是一个典型的场景，它需要同时考虑图像和文本的信息。一个可能的解决方案是先用 CNN 对图像进行卷积和池化，得到一个二维的特征图，然后将特征图切分成多个一维的向量，每个向量对应一个字母的位置。接着用 RNN 对这些向量进行编码或解码，得到最终的字母序列。这样可以实现对变长字母的识别。 CTC，即 Connectionist Temporal Classification，是一种用于解决序列标注问题的方法。它的设计目标是允许输入序列和输出序列的长度不完全匹配，并且不需要对齐信息。CTC 在深度学习中的应用较为广泛，特别是在语音识别、文本识别（OCR）和语言翻译等领域。 CTC 的工作原理如下： * 输入序列经过 CNN 进行特征提取和降维处理，得到特征序列。 * 特征序列经过 RNN 进行时序建模，得到隐藏状态序列。 * 隐藏状态序列经过全连接层映射到输出空间，如字符或标签的概率分布。 * CTC 损失函数根据输出序列和标签序列之间的对应关系计算损失，用于训练模型。 CTC 适用于序列标注问题，允许处理不完全对齐的输入和输出序列，而交叉熵损失函数适用于分类问题，用于衡量模型输出的概率分布与真实标签之间的差异。 - LSTM+CTC：这种方法使用了循环神经网络（LSTM）和联结时间分类器（CTC）来处理序列数据，可以处理不定长的文字，不需要对齐输入和输出。LSTM可以提取文字的时序特征，CTC可以解决重复字符和空白字符的问题。这种方法适用于印刷文字、验证码等场景。 - CRNN：这种方法是卷积神经网络（CNN）和循环神经网络（RNN）的结合，使用CNN来提取图像的局部特征，然后使用RNN来提取文字的序列特征，并通过全连接层生成最终的标签预测。这种方法也可以配合CTC来处理不定长的文字。这种方法适用于场景文字、手写文字等场景。 - CnOCR：这是一个开源的Python OCR工具包，支持简体中文、繁体中文（部分模型）、英文和数字的常见字符识别，支持竖排文字的识别。它自带了20多个训练好的识别模型，适用于不同应用场景，安装后即可直接使用。它还集成了场景文字检测功能，可以处理各种图片。它追求的目标是使用简单，而不是实现最新的模型算法。 输入向量 [ 1 , 2 , 3 , 4 , 1 , 2 , 3 ] 对应的Softmax函数的值为 [ 0.024 , 0.064 , 0.175 , 0.475 , 0.024 , 0.064 , 0.175 ] 。 输出向量中拥有最大权重的项对应着输入向量中的最大值“4”。这也显示了这个函数通常的意义：对向量进行归一化，凸显其中最大的值并抑制远低于最大值的其他分量。 反向传播是一种用来训练神经网络的常见方法，它的基本思想是通过导数链式法则，计算损失函数对网络参数的梯度，并根据梯度更新参数，以达到最小化损失函数的目的。 反向传播的过程可以分为以下几个步骤： 1. 前向传播：将训练数据输入到神经网络的输入层，经过隐藏层和激活函数，最后到达输出层并输出预测结果。 2. 计算误差：将预测结果与真实标签进行比较，得到损失函数的值，例如均方误差或交叉熵等。 3. 反向传播误差：从输出层开始，逐层计算损失函数对每个神经元的输出和输入的偏导数，即误差信号。这一步利用了导数链式法则，即如果Z = g(Y)而Y = f(X)，那么∂Z/∂X = (∂Z/∂Y)·(∂Y/∂X)。 4. 更新参数：根据损失函数对每个神经元的输入的偏导数，即误差信号和激活函数的导数，计算损失函数对每个权重和偏置的偏导数，即梯度。然后根据梯度和学习率，更新每个权重和偏置的值，使损失函数减小。 * * * * * * * * * LSTM * *",
    "url": "/blog/神经网络",
    "path": "tech/ai/神经网络.mdx"
  },
  {
    "title": "压缩稀疏行（Compressed Sparse Row, CSR）格式",
    "content": "数组的生成是基于每行非零元素的累积数量来确定的。对于每一行，存储的是第i行第一个非零元素在数组中的起始位置。具体步骤如下： 1. 初始化数组： - 创建一个长度为行数加一的数组。额外的一个元素用于标记所有非零元素之后的位置。 2. 累计每行的非零元素数量： - 遍历稀疏矩阵，每遇到一个非零元素就增加相应行的非零元素计数。 3. 填充数组： - 从第0行开始，累计每行的非零元素计数，并将其存储到数组中。 假设我们有以下稀疏矩阵A： 我们生成和数组如下： - values： - col_indices： 现在生成数组的步骤如下： 1. 初始化： 2. 累计每行的非零元素数量： - 第0行有2个非零元素（1和2） - 第1行有2个非零元素（3和4） - 第2行有1个非零元素（5） - 第3行有1个非零元素（6） 3. 填充数组： - = 0 （第0行的非零元素起始于） - = + 第0行的非零元素数量 = 0 + 2 = 2 （第1行的非零元素起始于） - = + 第1行的非零元素数量 = 2 + 2 = 4 （第2行的非零元素起始于） - = + 第2行的非零元素数量 = 4 + 1 = 5 （第3行的非零元素起始于） - = + 第3行的非零元素数量 = 5 + 1 = 6 （这是所有非零元素之后的位置） 最终生成的数组为： 在还原稀疏矩阵时，数组起到重要作用。它帮助我们确定每行非零元素在和中的范围。具体步骤如下： 1. 初始化还原矩阵： - 创建一个与原始矩阵A尺寸相同的零矩阵。 2. 遍历每一行的非零元素： - 使用数组确定每行的非零元素在和中的起始和结束位置。 - 将这些非零元素填充回零矩阵的对应位置。 假设我们要还原上面的矩阵A： 1. 初始化还原矩阵： 2. 遍历每一行的非零元素： - 第0行：, - 第1行：, - 第2行：, - 第3行：, 最终还原的矩阵为： 是的，除了压缩稀疏行（Compressed Sparse Row, CSR）格式外，还有其他几种常见的稀疏矩阵压缩方法。下面介绍几种常见的方法及其特点： CSC格式与CSR格式类似，但它按列而不是按行存储非零元素。它使用三个数组来存储稀疏矩阵： - values：存储非零元素。 - row_indices：存储每个非零元素的行索引。 - col_pointers：存储每列的非零元素在数组中的起始位置。 对于矩阵A： - values： - row_indices： - col_pointers： BCSR格式是CSR格式的扩展，通过将稀疏矩阵划分为固定大小的块来进行压缩。这种方法可以提高存储和计算的局部性，从而提高性能。 对于矩阵A： 可以划分为2x2的块： - values：包含所有块中的非零元素。 - block_row_ptr：指示每个块行的起始位置。 - block_col_ind：指示每个块列的索引。 DIA格式用于对角线为主的稀疏矩阵，通过存储矩阵的对角线来压缩数据。它使用两个数组： - values：存储对角线元素。 - offsets：存储每条对角线的偏移。 对于矩阵A： - values： - offsets： COO格式通过存储非零元素的行索引、列索引和对应值来表示稀疏矩阵。它使用三个数组： - row_indices：存储非零元素的行索引。 - col_indices：存储非零元素的列索引。 - values：存储非零元素的值。 对于矩阵A： - row_indices： - col_indices： - values： CSB格式是一种块压缩格式，适用于具有局部密集块的稀疏矩阵。它将矩阵划分为小块，并仅存储非零块的信息。CSB格式在并行计算中表现良好。 假设矩阵A划分为2x2的块： 只存储含非零元素的块。 不同的稀疏矩阵压缩方法适用于不同的应用场景和矩阵结构。选择合适的压缩方法可以大大提高存储效率和计算性能。 BCSR格式是CSR格式的扩展，通过将稀疏矩阵划分为固定大小的块来进行压缩。它在块级别上存储数据，从而提高存储和计算的局部性。 假设我们有以下稀疏矩阵A： 我们将矩阵A划分为2x2的块： BCSR格式会存储这些块中的非零元素及其索引： - values： - block_row_ptr：（每个块行的起始位置） - block_col_ind：（每个块列的索引） CSB格式通过将稀疏矩阵划分为大小相等的小块，并仅存储含有非零元素的块。它适用于矩阵具有局部密集块的情况，尤其是在并行计算中表现良好。 我们使用相同的矩阵A，并将其划分为2x2的块： CSB格式会存储每个非零块及其位置： - values：存储非零块的内容： - block_indices：存储每个非零块的位置（块的行和列索引）： 1. 数据存储方式： - BCSR：按固定大小的块存储，适用于较为均匀分布的稀疏矩阵。每行块的起始位置和列索引分别存储在和中。 - CSB：按包含非零元素的块存储，适用于具有局部密集块的稀疏矩阵。每个非零块的内容和位置分别存储在和中。 2. 适用场景： - BCSR：适用于较为均匀分布的稀疏矩阵，通过块级别的存储优化提高存储和计算的局部性。 - CSB：适用于具有局部密集块的稀疏矩阵，在并行计算中表现良好，因为可以独立处理每个块。 3. 实现复杂度： - BCSR：相对简单，因为只需处理固定大小的块。 - CSB：相对复杂，因为需要动态确定每个非零块的位置和内容。 还原BCSR矩阵： - 根据和确定每个块的位置。 - 从中提取每个块的内容，填充到原始矩阵中相应位置。 还原CSB矩阵： - 根据确定每个非零块的位置。 - 从中提取每个非零块的内容，填充到原始矩阵中相应位置。",
    "url": "/blog/稀疏矩阵压缩",
    "path": "tech/ai/稀疏矩阵压缩.md"
  },
  {
    "title": "视频发展 AI ing",
    "content": "* 已有图片 + 文字描述 * 多图片各自短视频生成合并成长视频 * 已有视频 + 风格替换 RNN -> GAN -> 自回归(transformer) -> diffusion -> sora/V-JEPA * RNN：循环神经网络，是一种能够处理序列数据的神经网络结构，可以捕捉视频帧之间的时间依赖关系，生成连续的视频序列。RNN的代表模型有LSTM、GRU等。 * GAN：生成对抗网络，是一种无监督的生成模型框架，通过让两个神经网络相互博弈来进行机器学习。GAN可以生成视觉逼真度高的视频，但控制难度大、时序建模较弱。GAN的代表模型有DCGAN、WGAN、Pix2Pix、PatchGAN等。 * 自回归（transformer）：自回归模型是一种基于概率的生成模型，可以根据已有的数据预测下一个数据的概率分布。自回归模型可以实现细粒度语义控制，时序建模能力强，但计算量大。自回归模型的代表模型有Transformer、BART、GPT等。 * ：扩散模型是一种基于能量最小化的生成模型，可以将数据从高维空间扩散到低维空间，再从低维空间还原到高维空间。扩散模型可以生成高质量、高分辨率的视频，但生成速度慢。扩散模型的代表模型有DDPM、DVAE、Score-based等。 * dVAE 的基本思想是，首先使用 VAE 将原始数据编码成离散的潜在变量，然后使用扩散模型将潜在变量逐步加入噪声，最后使用逆向过程从噪声中重建潜在变量和原始数据 * sora/V-JEPA：sora是一种基于扩散模型的视频生成方法，可以从文本生成视频，具有高效、稳定、可控的特点。V-JEPA是一种基于自回归模型的视频生成方法，可以从图片生成视频，具有高质量、高分辨率、高连贯性的特点。 V-JEPA 是一种视频自监督学习的方法，它可以通过观看视频来学习视觉表示。它的特征预测方法是，让模型能够预测视频中一个区域（称为目标区域y）的特征表示，这个预测基于另一个区域（称为源区域x）的特征表示。这样，模型可以学习视频中不同时间步骤之间的语义关系，以及视频中的高级概念信息。 Sora 架构 * 什么是 Patches？ - ViT * 为什么可以做到多种分辨率？- 动态分辨率 NaViT 多尺寸分辨率训练及特征融合训练 * 视频数据怎么提取 patches？- 空间 patches - ViViT - 针对视频数据 patches 提取 * 主体架构？ -扩散 transformer - Dit - UNet 采用归纳偏置保持空间局部性和平移等边性，transformer 逐步学习更远像素点 * 提示词是否优化？- 提示词扩写 - GPT4",
    "url": "/blog/视频发展 AI ing",
    "path": "tech/ai/视频发展 AI ing.md"
  },
  {
    "title": "理论",
    "content": "MIT的这篇论文通过详细的实验和案例研究，挑战了语言是思维基础的传统观点，为人工智能的发展提供了新的思路。未来的AI研究可能需要更多地关注非语言形式的数据和方法，探索语言之外的思维形式。这不仅为大语言模型的改进提供了新的方向，也为实现更强大、更智能的人工智能系统奠定了基础。 * as a powerful tool for transmitting knowledge, language reflects the sophistication of human cognition — but does not give rise to it. * * *",
    "url": "/blog/语言是交流工具而不是思维工具",
    "path": "tech/ai/语言是交流工具而不是思维工具.md"
  },
  {
    "title": "软件2.0",
    "content": "概念：软件2.0通常指的是那些在其核心集成了机器学习模型的软件系统。这些系统的行为不仅仅是由传统的编程代码决定的，而是由数据驱动的模型生成的。 以下是对文章一些观点的理解 Modules can meld into an optimal whole >原文：普通软件通常被分解成多个模块，各个模块中间通过共有函数、API 或者端到端的方式通信。然而，对于软件2.0，如果一开始2个相交互的模块是独立训练的，我们之后也很容易在整个系统中进行反向传播。想想看，如果你的浏览器可以自动设计底层指令，从而提高加载页面的速度；或者说你导入的计算机视觉库（比如 OpenCV）可以根据你的特定数据，自动调整行为； 理解： 软件2.0指的是那些利用机器学习模型作为其核心组件的软件系统。关键特点是它能够通过收集数据和反向传播来持续学习和自我优化。这种方式使得软件能够更加智能和适应性强，因为它不再是静态的代码集合，而是一个能够随着时间和数据的积累而变得更加高效的动态系统。 举例： 1. 浏览器自动设计底层指令： 在这个例子中，我们可以想象一个浏览器，它不仅仅是静态地执行预定义的指令来加载页面。相反，它具备了通过机器学习来优化这些指令的能力。例如，浏览器可以监控用户的上网习惯、页面加载时间、系统资源使用情况等数据，并且使用这些数据来训练一个模型，这个模型能够预测并调整底层的网络请求、资源分配策略，甚至是JavaScript引擎的工作方式，以提高页面加载的速度。这种优化是动态进行的，能够适应不同用户的使用环境和习惯。 2. 计算机视觉库自动调整行为： 在另一个例子中，传统的计算机视觉库，比如OpenCV，提供了一系列预定义的算法和函数来处理图像和视频数据。在软件2.0的情况下，这些库可以进一步智能化。假设你正在使用这样的库来进行面部识别，库本身可以包含一个或多个机器学习模型，这些模型可以根据你提供的特定数据（比如，你的应用中的用户面部图像）自动调整其内部参数。这样，面部识别的准确性不仅仅取决于通用算法，还取决于它如何通过学习特定的数据集来自我优化。 >原文：软件2.0的奇怪特性也在不断出现。比如，对抗样本和攻击样本的存在，使得软件2.0的不可解释性问题变得更加突出。 理解： 对抗样本是经过特别设计的输入数据，它们对人类来说可能看起来和正常数据没有区别，但是能够欺骗机器学习模型，使其做出错误的预测或分类。这些情况突出了软件2.0的不可解释性问题，因为即使是模型的开发者也很难完全理解或解释为什么模型会被这些对抗样本所欺骗。这种不可解释性不仅挑战了模型的可靠性，也给安全性带来了风险，尤其是在那些对精确性和可靠性有着极高要求的应用场景中。 例如： 1. 自动驾驶汽车： 在自动驾驶汽车中，使用了大量的机器学习模型来处理视觉识别任务，例如识别交通标志。一个对抗样本的例子可能是一个轻微修改过的停车标志，虽然对于人类来说这个标志看起来没有任何问题，但是修改可能会导致自动驾驶系统无法正确识别这个标志，从而可能忽略停车指令，引发安全问题。 2. 面部识别系统： 在安全性要求高的面部识别系统中，对抗样本可能是经过精心设计的图像或者是特定的化妆或配饰，这些对于人眼来说不会影响识别，但可能会导致面部识别系统无法正确识别或者将一个人错误地识别为另一个人。 - 模块演化 - 初始：单一大型服务（单体架构）。 - 演化：服务分解为小型、独立的模块，这些模块相互配合工作（微服务架构）。 - 设计哲学： 基于规则的编程，开发者需要手动编写代码来明确规定软件的每一步行为。 - 开发方式： 往往是水平式的，需要事先规划架构和未来可能的扩展点。 - 更新迭代： 更新通常涉及整个系统的重新部署或至少是较大模块的重新部署。 - 可解释性： 高，因为行为是由清晰的代码逻辑决定的。 - 示例： 传统的ERP系统，银行交易系统等，这些系统都是具有固定逻辑和复杂业务规则的系统。 - 模块演化 - 初始：独立的专用模型完成各自的任务，然后将结果整合（如多个单独的模型串联处理任务）。 - 演化：单个端到端模型直接处理整个任务，没有中间转换步骤（如一个模型同时处理语音识别、自然语言理解和语音合成）。 - 设计哲学： 基于数据和学习的编程，软件通过对大量数据进行学习来自我优化和改进，而非仅仅依据预编程的规则。 - 开发方式： 往往是垂直式的，能够从数据中自主学习并适应新任务，不需要开发者预见到每一种可能。 - 更新迭代： 在很多情况下，模型可以通过持续学习实现自我更新而无需人为干预。 - 可解释性： 相对较低，因为模型决策过程复杂且不总是透明的。 - 示例： 机器学习驱动的推荐系统（如YouTube或Netflix的推荐算法），自然语言处理系统（如GPT-3），自动驾驶车辆。 软件1.0更倾向于模块化和微服务架构，其设计是为了解决特定的、已知的问题而构建的。而软件2.0则向着使用数据驱动的方法，通过端到端的模型自动学习解决问题，这常涉及人工智能和机器学习特别是深度学习领域。 水平式开发通常指的是传统的软件开发过程，这里的\"水平\"可以类比于搭建房屋的每一层。这种方法的特点包括： - 层次结构：软件以层的形式构建，每一层依赖于下面的一层。比如，用户界面层依赖于业务逻辑层，而业务逻辑层则依赖于数据访问层。 - 模块化：系统被分解为功能模块，每个模块负责特定的功能或服务。 - 规划先行：在开发之前需要详尽的规划，了解系统所有层和模块如何相互作用。 - 硬编码：逻辑是通过编写具体的代码指令来实现的，对于新的问题和需求通常需要开发人员手动进行修改和更新。 垂直式开发则更多地与数据驱动的机器学习方法相联系，\"垂直\"可以类比于从基础设施一直向上生长的树木。这种方法的特点包括： - 端到端学习：任务从输入到输出通过一个端到端的系统来完成，没有必须人为编写的规则。 - 数据为中心：系统主要通过分析和学习大量数据来获得知识和解决问题的能力。 - 动态适应：系统可以不断从新的数据中学习，从而适应新的问题和场景。 - 自主性：因为模型可以自我学习，所以对于未知问题的适应性强，这减少了人工编写代码的需要。 在机器学习（尤其是深度学习）中，垂直式的发展体现在从底层特征直接学习到高层的决策，而无需人类专家制定复杂的规则或者手动设计特征。 总体来说，水平式开发注重于开发者明确规划各个模块的交互和功能，而垂直式开发依赖于从数据中自动学习模式和规律，这样的系统通常能自适应未见过的数据或情况。 (30篇论文，就能掌握当今科技90%的知识！ILYA公布神级论文清单(上）)[https://mp.weixin.qq.com/s?__biz=Mzk0NzQzOTczOA%3D%3D&mid=2247507514&idx=3&sn=6f90564bb8a07f39d360aec1968e6b14&ref=openi.cn] (下)[https://mp.weixin.qq.com/s?__biz=Mzk0NzQzOTczOA==&mid=2247507451&idx=1&sn=d586514585870f4e3205199a8f6f2f5f&chksm=c3744ceef403c5f883ea5a495e58764ffa57cbf7a7c5c0fa30b4493057f58ef288774b0768a4&scene=21#wechat_redirect] * 翻译 *",
    "url": "/blog/软件 2.0",
    "path": "tech/ai/软件 2.0.md"
  },
  {
    "title": "逆变换采样",
    "content": "逆变换采样（Inverse Transform Sampling）是一种用于从任意概率分布中抽样的方法。它的基本思想是利用均匀分布的样本生成符合目标分布的样本。这个过程基于目标分布的累积分布函数（CDF，Cumulative Distribution Function）和逆累积分布函数（Inverse CDF，或Quantile Function）。 步骤1：从均匀分布中抽样 首先，从均匀分布 \\(U(0, 1)\\) 中抽取样本。均匀分布意味着每个数字在0到1之间出现的概率相等。 步骤2：计算逆变换 使用目标分布的逆累积分布函数（Inverse CDF）将从均匀分布中抽取的样本转换为符合目标分布的样本。 假设我们想从指数分布中抽样。指数分布的概率密度函数（PDF）和累积分布函数（CDF）分别如下： - 概率密度函数（PDF）： $$f(x; \\lambda) = \\lambda e^{-\\lambda x} $$ - 累积分布函数（CDF）： $$F(x; \\lambda) = 1 - e^{-\\lambda x} $$ 对于指数分布： - 累积分布函数（CDF） \\(F(x)\\) 给出了 \\(X\\) 小于或等于某个值的概率。 - 逆累积分布函数（Inverse CDF） \\($F^{-1}(p)$\\)，其中 \\($p \\in [0, 1]$\\)，给出了一个概率值对应的下限。 对于指数分布，其逆累积分布函数可以通过解以下方程得到： $$p = 1 - e^{-\\lambda x} $$ 将这个等式解为 \\(x\\)： $$x = -\\frac{1}{\\lambda} \\ln(1 - p) $$ 这里 \\(p\\) 是从均匀分布 \\( U(0, 1) \\) 中抽取的样本。 步骤1：从均匀分布 \\(U(0, 1)\\) 中抽样 假设我们抽取 \\(p = 0.5\\) （任何介于0和1之间的均匀随机数）。 步骤2：计算逆变换 针对特定的 \\(\\lambda\\) 值（假设 \\(\\lambda = 1\\)），我们使用逆累积分布函数 \\(F^{-1}(p)\\) 来转换 \\(p\\)： $$x = -\\frac{1}{1} \\ln(1 - 0.5) = - \\ln(0.5) \\approx 0.693 $$ 所以，使用逆变换采样，我们得到了一个符合指数分布的样本 \\(x \\approx 0.693\\)。 Box-Muller 通过角度和半径大小两个分量的独立性分别单独生成并转换成 (x, y) 对。 无法直接通过逆变换生成一维正态分布的主要原因是正态分布的累积分布函数 (CDF) 没有解析形式，这使得逆变换方法不太实用。 没有解析形式的函数是指无法通过有限个基本函数（如多项式、指数函数、对数函数、三角函数等）的代数运算和复合运算来表示其精确值的函数。换句话说，这些函数无法用一组封闭形式的表达式来表示，通常需要以数值方法、级数展开或图形来近似表示或解析。 以下是一些常见的没有解析形式的函数类型： 许多特殊函数通常没有简单的解析形式。例如： - 误差函数（Error Function, \\(\\operatorname{erf}(x)\\)）：这是一个出现在概率、统计和偏微分方程中的常见函数。虽然它有定义和性质，但无法用基本初等函数的有限组合表示。 $$ \\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2} \\, dt $$ 隐函数是通过多个变量间的关系定义的，并且无法显式解出。例如： - 兰伯特 \\(W\\) 函数：通过关系 \\(We^W = x\\) 定义。对于大多数 \\(x\\) 它没有闭式表达。 - 指数方程：例如 \\(x = e^{-x}\\)，无法通过代数方法得到显式解。 在某些情况下，无法解析地表达某些无理或者超越方程的解。举个例子： - 代数方程级数解：如一般五次及更高次多项式的根，根据阿贝尔-鲁芬尼定理，无法用根式表示。 一些积分无法通过单一的基本函数或其简单组合进行表达。例如： - 椭圆积分：这些积分经常出现于物理学和工程学中，并不能通过基本函数的具体表达来表示。 $$ E(x) = \\int_0^x \\sqrt{1 - t^2} \\, dt $$ 某些微分方程具有解，但这些解不能以初等函数的形式表达。例如： - 艾里函数（Airy function, \\(\\operatorname{Ai}(x)\\) 和 \\(\\operatorname{Bi}(x)\\)）：这些函数出现在量子力学和光学等领域中，经常用来描述某些物理现象。 $$ \\frac{d^2y}{dx^2} - xy = 0 $$ 总体而言，当函数的定义涉及复杂的代数运算、特定的积分、微分方程的解，或者当它们的定义关系本质上是隐含的，这些函数往往没有简单的解析形式。现代数学中，尽管没有解析形式，但我们可以通过数值方法、级数展开或其他方法对它们进行近似分析和计算。这对实用计算和理论研究都是非常重要的。",
    "url": "/blog/逆变换采样",
    "path": "tech/ai/逆变换采样.md"
  },
  {
    "title": "Go简介",
    "content": "--- title: Go 简介 description: --- Goroutine 可以看作对 thread 加的一层抽象，它更轻量级，可以单独执行。因为有了这层抽象，Gopher 不会直接面对 thread。 1. 创建一个 goroutine 的栈内存消耗为 2 KB，实际运行过程中，如果栈空间不够用，会自动进行扩容。创建一个 thread 则需要消耗 1 MB 栈内存，而且还需要一个被称为 “a guard page” 的区域用于和其他 thread 的栈空间进行隔离。 2. 对于一个用 Go 构建的 HTTP Server 而言，对到来的每个请求，创建一个 goroutine 用来处理是非常轻松的一件事。而如果用一个使用线程作为并发原语的语言构建的服务，例如 Java 来说，每个请求对应一个线程则太浪费资源了，很快就会出 OOM 错误（OutOfMermoryError）。 1. Thread 创建和销毀都会有巨大的消耗，因为要和操作系统打交道，是内核级的，通常解决的办法就是线程池。 2. 而 goroutine 因为是由 Go runtime 负责管理的，创建和销毁的消耗非常小，是用户级。 1. 当 threads 切换时，需要保存各种寄存器，以便将来恢复： 一般而言，线程切换会消耗 1000-1500 纳秒，一个纳秒平均可以执行 12-18 条指令。所以由于线程切换，执行指令的条数会减少 12000-18000。 2. Goroutine 的切换约为 200 ns，相当于 2400-3600 条指令。 因此，goroutines 切换成本比 threads 要小得多。 Go runtime 会负责 goroutine 的生老病死，从创建到销毁，都一手包办。 Runtime 会在程序启动的时候，创建 M 个线程（CPU 执行调度的单位），之后创建的 N 个 goroutine 都会依附在这 M 个线程上执行。这就是 M:N 模型 1. 使用关键字 go * go创建一个新的 goroutine 2. GC * GC 需要在M 上进行所以会调度 3. 系统调用 4. 内存同步访问 * atomic, mutex, channel 等操作 * https://golangbot.com/goroutines/ * https://rakyll.org/scheduler/ * https://blog.nindalf.com/posts/how-goroutines-work/ * https://speakerdeck.com/retervision/go-runtime-scheduler?slide=27 * https://povilasv.me/go-scheduler/# * https://morsmachine.dk/netpoller * https://morsmachine.dk/go-scheduler * https://golangbot.com/structs-instead-of-classes/ * https://zhuanlan.zhihu.com/p/80853548",
    "url": "/blog/Go简介",
    "path": "tech/backend/Go简介.md"
  },
  {
    "title": "JSON序列化",
    "content": "问：easyjson 跟 golang 原生 JSON 方法对比 * easyjson是一个用于快速进行json序列化和反序列化的工具包，它通过为结构体生成方法来避免使用反射，从而提高性能和效率。 * 与golang原生的json方法相比，easyjson的性能可以提高2～3倍。 * easyjson的使用方法是先安装它，然后为需要序列化的结构体添加注释，然后运行easyjson命令生成对应的解析代码。 问： 一些easyjson的优缺点？ 答：easyjson的优点是： * 它可以提高json序列化和反序列化的性能和效率，比golang原生的json方法快2～3倍。 * 它可以避免使用反射，从而减少内存分配和GC压力。 * 它可以通过注释和命令行工具自动生成解析代码，使用方便。 easyjson的缺点是： * 它不会校验json的正确性，可能导致解析错误或异常。 * 它的代码风格可能不太优雅，有些重复或冗余。 * 它需要为每个解析字段指定FieldTag，否则会使用变量名作为解析字段2。 问：easyjson 是什么原理 答： * 它通过给我们要进行序列化的 struct 生成方法来实现不通过反射进行json序列化。 * 它只针对预先定义好的 json 结构体对输入的json字符串进行纯字符串的截取，并将对应的json字段赋值给结构体。 * 它提供了代码生成工具，可以一键生成go文件中定义的结构体对应的解析代码。 问：举个例子 答： 好的，我可以给你一些示例代码来说明 easyjson 跟 golang 自带 JSON 工具的不同。 假设我们有一个结构体如下： 如果我们使用 golang 自带的 JSON 工具，我们可以这样序列化和反序列化： 如果我们使用 easyjson，我们需要先安装 easyjson 工具： 然后在结构体上加上注释 ，并使用命令 生成对应的序列化和反序列化方法： 生成的方法会在一个新的文件 中，我们可以这样使用： 从这些示例代码中，我们可以看到 easyjson 跟 golang 自带 JSON 工具的不同。 通过 GPT 生成",
    "url": "/blog/JSON序列化",
    "path": "tech/backend/JSON序列化.md"
  },
  {
    "title": "napi 宏的理解",
    "content": "Rust 宏的核心之一是通过生成代码来简化重复任务，而 DSL（领域专用语言）则是在宏的帮助下，为特定领域问题设计的一种更易读、易用的语法。通过宏，可以把 Rust 的代码变得像定制的小语言一样，让开发者专注于问题本身，而不是细节的实现。 以下是几个示例，展示如何用 Rust 宏构建 DSL： --- 以下宏实现了一个简单的 HTML 构建器，用于生成 HTML 文本： 输出： 分析： - 这段代码通过宏 提供了一个类似 HTML 的 DSL。 - 用户无需关心如何拼接字符串，只需描述 HTML 结构。 - 嵌套语法与 HTML 自然对应。 --- 以下宏提供了一种简化命令行参数解析的 DSL： 运行示例： 分析： - 通过 宏定义命令行参数的格式和类型。 - 用户只需描述参数，而无需手动解析或处理错误。 --- 以下宏用来定义一个简单的有限状态机： 输出： 分析： - 通过 宏，用户可以简单地定义状态和转移规则。 - 宏将 DSL 编译成 Rust 代码，完成状态机的实现。 - 代码清晰且具有实际用途。 --- Rust 宏在构建 DSL 时具有以下优点： 1. 可读性：用自然的语法描述特定领域的规则，用户无需了解底层实现。 2. 简化代码：减少样板代码，专注于核心逻辑。 3. 强类型保障：结合 Rust 的类型系统，确保生成代码的安全性。 这些 DSL 示例涵盖了 HTML 构建、命令行解析和状态机定义，是 Rust 宏在领域专用语言设计中的常见用法。 下面是之前手写的简化实现版本，并为其添加了详细注释，帮助理解各部分的功能和逻辑： --- - 这是 Rust 编写的 N-API 模块的入口点。 - Node.js 在加载 文件时，会查找并调用这个函数。 关键参数： - ：当前 N-API 的上下文，保存模块运行时的状态。 - ：代表模块的导出对象，相当于 Node.js 中的 。 函数作用： - 注册模块的所有导出方法，并将它们绑定到 。 --- - 这个部分通过 创建一个符合 N-API 标准的函数。 - 的关键参数： - ：N-API 环境上下文。 - ：JavaScript 中函数的名字。 - ：指向 Rust 中实际的函数实现。 --- 这是具体的 Rust 函数逻辑，通过 FFI 暴露给 Node.js 调用。 - 参数解析： - 使用 获取传递给函数的参数，期望两个参数。 - 每个参数需要用 解析并转换为 Rust 类型。 - 结果处理： - 计算两个整数的和。 - 使用 创建一个 JavaScript 的数值对象，将结果返回给调用者。 --- 1. Node.js 加载 文件时，调用 。 2. 使用 创建函数并将其绑定到模块的导出对象。 3. 在 JavaScript 中调用 ，会触发 函数的执行： - 获取参数。 - 调用 Rust 函数。 - 将结果转换为 JavaScript 类型并返回。 --- - 优点： - 直接操作 N-API，可以完全控制模块行为。 - 适用于需要手动优化性能或有特殊需求的场景。 - 缺点： - 编写 N-API 代码繁琐且容易出错。 - 需要处理参数解析、类型转换、错误管理等大量细节。 - 难以维护，代码量大。 --- 手写实现展示了 Rust 和 Node.js 交互的基础，但在实际项目中，推荐使用类似 宏来简化开发。属性宏会自动生成上述复杂逻辑，让开发者专注于核心功能。 - gpt",
    "url": "/blog/napi 宏的理解",
    "path": "tech/backend/napi 宏的理解.md"
  },
  {
    "title": "rust Arena内存分配",
    "content": "在 Rust 中， 是一种内存分配策略，主要用于高性能场景中高效地分配和管理一组对象。相比 、、 和 等智能指针， 的优势在于它可以集中管理对象的内存，从而避免频繁的分配与释放操作。 （内存池）是一种分配器，通过在一块连续的内存区域中分配多个对象来提升性能。这些对象的生命周期与 本身绑定。当释放 时，所有分配的对象会统一释放，无需逐一销毁。 Rust 的一些库（如 、）提供了 的实现。 --- 特别适合构建复杂的递归数据结构（如树、图等），因为它能有效避免复杂的生命周期管理。 示例：语法树解析器 优势： - 不需要显式管理节点的生命周期。 - 分配和释放都非常高效。 --- 当程序需要频繁创建并销毁对象，且这些对象的生命周期较短时， 可以避免频繁调用系统内存分配器，提高性能。 示例：批量计算临时对象 优势： - 分配性能优于堆分配（如 ）。 - 对象销毁集中在 的生命周期结束时，减少了单个对象的析构开销。 --- 当多个对象需要共享同一批数据时，使用 可以避免 或 的引用计数开销。 示例：共享数据结构 优势： - 统一管理节点的内存，避免循环引用。 - 不需要手动清理，生命周期简单。 --- 对于游戏开发或其他需要频繁更新大量小对象的场景（如粒子系统、实体管理）， 可以显著提升分配和清理性能。 示例：粒子系统 优势： - 避免了频繁的分配和释放，尤其是在帧更新频繁的场景中。 - 更高的缓存命中率。 虽然 确实在某些场景下具有显著优势，但它并不是万能的工具，也不适合所有场景。使用 需要权衡以下几个关键点： --- 的核心理念是将分配的对象生命周期绑定到 自身。这带来了两个问题： - 无法灵活控制对象的释放时间：对象的内存只能在整个 被销毁时统一释放，无法提前释放单个对象。如果你的场景需要精细控制对象的生命周期（比如按需释放某些资源）， 就不合适。 - 容易造成内存浪费：如果 存在一些对象长时间不被使用，而 本身又不能销毁，这些对象占用的内存会被浪费。 --- 分配的对象通常是不可移动的，因为它们存储在 的内存块中。这样会导致以下问题： - 不能直接使用一些需要所有权移动的特性（例如对象放入 或其他容器中）。 - 必须使用引用来访问 分配的对象，可能会导致生命周期管理变得复杂。 --- 大多数 实现（如 ）并不是线程安全的，如果需要跨线程共享对象，还需要额外的同步机制。这时，线程安全的智能指针（如 ）会更合适。 例外： 支持跨线程，但需要特别小心使用。 --- 由于 使用内存池进行分配，所有对象会一直占用内存直到 释放。这种方式可能导致内存占用的峰值比智能指针高，尤其是分配了很多临时对象但未及时销毁 时。 --- 的对象不具有 Rust 所强调的所有权语义（比如 和 ），这可能导致以下问题： - 安全性降低：如果程序逻辑复杂，可能出现逻辑错误（例如悬空引用）。 - 借用规则变得模糊：你可能需要手动确保引用的正确性，而不是依赖编译器自动检查。 --- 虽然 性能非常高，但它的优势只有在以下场景下才会显现： - 大量的频繁对象分配和释放。 - 对象的生命周期高度统一。 对于普通的业务逻辑（如 CRUD 应用），智能指针（如 、、）提供了更直观的代码语义和更安全的内存管理方式。 --- | 场景 | 使用智能指针 | 使用 Arena | |-----------------------------------|-------------------------------|---------------------------------| | 复杂生命周期管理 | ✅ 可以用 或 | ❌ 生命周期过于统一可能不适用 | | 按需释放对象 | ✅ 可以释放单个对象 | ❌ 只能统一释放所有对象 | | 多线程环境 | ✅ 用 | ❌ 多数 Arena 不支持线程安全 | | 高内存利用率（避免浪费） | ✅ 更细粒度释放内存 | ❌ 长期运行可能导致内存浪费 | | 轻量级分配和高性能需求 | ❌ 频繁分配释放会增加开销 | ✅ 高效分配与释放 | | 递归结构或大量小对象 | ❌ 智能指针管理较为复杂 | ✅ 简化生命周期和管理 | --- 是一个强大的工具，但它的使用场景非常明确： - 高性能需求：大量对象的快速分配和释放。 - 生命周期统一：所有对象的生命周期可以和 绑定。 - 复杂递归结构：例如语法树、图等需要高效管理的结构。 在大多数常见场景中（如 Web 应用、后端服务等），智能指针（、、 等）提供了更安全、更灵活的内存管理方案。选择是否使用 时，应该基于你的应用需求、生命周期复杂性以及性能瓶颈来决定。 结论：不要过度优化，除非确实需要。 如果你的应用性能瓶颈明确在内存分配上，且生命周期统一， 才是最佳选择。否则，智能指针更符合 Rust 的所有权和安全性设计理念。 的原理可以简单理解为一次性分配一大块连续的内存，然后从这块内存中逐步划分小块来分配给对象。分配的对象不会单独释放，而是等到整个 生命周期结束时，统一回收整块内存。这种方法减少了频繁调用操作系统内存分配器的开销，从而提升了性能。 下面通过一个简单的例子手动实现一个 的简化版本来说明其工作原理： --- --- 1. 初始化 ： - 创建一个大小为 1024 字节的内存池，用于存储分配的对象。 2. 分配对象： - 调用 方法时， 会检查是否有足够的剩余内存。 - 它将对象写入内存池中的下一个空闲位置，同时满足类型的对齐要求。 3. 访问对象： - 返回的指针是对象在 中的地址，因此可以像普通引用一样访问它们。 4. 释放内存： - 对象的内存不会单独释放，只有在 的生命周期结束时，整个内存池会统一释放。 --- --- - 分配：通过维护一个偏移量 ()，每次从内存池中分配指定大小的块。 - 对齐：确保分配的内存地址符合对象的对齐要求。（编译器会确保结构体的大小是它最大对齐要求的倍数） - 统一释放： 本身的内存由 管理，当 被销毁时，所有分配的内存会自动释放。 --- - 减少内存分配调用：传统分配器需要频繁调用系统分配内存，而 只需要一次性分配大块内存。 - 分配速度快：分配内存只需简单地增加偏移量。 - 释放简单：对象的内存无需单独回收，只需释放整个 。 --- 这个简单的 演示了其核心思想。在实际应用中，比如使用 或 等库，内部实现会更复杂，处理对齐、更高效的内存管理等问题，但基本原理是相似的。 结构体占用 8 字节 而不是 7 字节，是因为内存对齐和填充 (padding) 的原因。计算结构体大小时，编译器会确保结构体的大小是它最大对齐要求的倍数。(每种数据类型在内存中的地址必须是其对齐要求的倍数。) --- - - 占用地址 。 - 起始地址是 0，符合对齐要求（4 的倍数）。 - - 紧接着 ，理论上起始地址是 4，但需要满足 的对齐要求（2 的倍数）。 - 地址 ，符合对齐要求。 - - 紧接着 ，理论上起始地址是 6。 - 地址 ，对 没有特殊对齐要求。 - 填充 (padding)： - 由于 的最大对齐要求是 （由 决定），整个结构体的大小必须是 的倍数。 - 当前结构体占用了 共 7 字节。 - 为了满足对齐要求，编译器会在末尾填充 1 个字节，使总大小达到 8 字节。 --- 填充是为了让结构体的每个实例在数组或内存中都符合其对齐要求。如果不填充，数组的第一个元素对齐了，第二个元素可能会错位，从而导致性能问题或未定义行为。 如果没有填充， 的大小是 7 字节，而不是 8 字节： - 第一个元素占用 。 - 第二个元素的起始地址是 ，但 的对齐要求是 ，地址 不符合要求。 - 这种情况下，程序会崩溃或性能受损。 通过填充使结构体大小变为 8 字节： - 第一个元素占用 。 - 第二个元素从地址 开始，满足 的对齐要求。 --- 可以通过以下代码验证： --- 1. 最大对齐要求： 的最大对齐要求是 （由 决定）。 2. 结构体大小必须是最大对齐要求的倍数：为了使结构体对齐，编译器在末尾填充 1 字节，使总大小变为 8。 3. 原因：填充的目的是为了保证结构体实例在数组中对齐，避免跨平台问题和性能下降。 --- 如果想手动优化内存布局，可以尝试调整字段顺序。例如： 调整后，结构体的布局如下： - -> 。 - -> 。 - -> 。 此时 的大小仍然是 8 字节，但没有额外的填充。 当 Arena 的内存不足时，它不会自动扩容，而是通过新的大块内存来继续分配对象：Arena 会分配一块新的内存区域，原有的对象依然保留，新的对象被分配到新区域中。 * 这和动态数组扩容时需要移动已有数据的方式不同。 * Arena 是批量分配与集中释放，而动态数组是按需分配与按需释放。 * gpt",
    "url": "/blog/rust Arena内存分配",
    "path": "tech/backend/rust Arena内存分配.md"
  },
  {
    "title": "作用域规则",
    "content": "Rust 的作用域规则是其内存安全性的基础之一，主要围绕所有权、借用和生命周期来确保内存的管理安全且高效。以下通过几个简单例子说明 Rust 的作用域规则。 --- - 变量 的所有权存在于定义到作用域结束之间。 - 离开作用域时，Rust 会自动调用 ，释放内存。 --- - 的所有权被转移给 后， 在作用域内被标记为无效。 - 这种设计避免了两者指向同一内存而导致潜在的错误。 --- --- - 通过 借用一个变量，借用不会转移所有权。 - 作用域结束后，借用结束，原变量仍然有效。 --- - 一个变量在同一时间只能有一个可变借用，避免数据竞争。 - 可变借用结束后，变量才能被再次使用。 --- - Rust 禁止同时存在不可变借用和可变借用，防止并发修改的错误。 --- - 通过作用域嵌套，解决不可变借用和可变借用的冲突。 --- 生命周期（）表示引用的作用域，Rust 编译器通过生命周期标注检查引用是否有效。 示例： - 的生命周期只存在于内层作用域， 的生命周期更长。 - Rust 编译器不允许引用超出被引用对象的作用域，避免悬垂指针。 --- 当函数涉及引用参数时，需要显式标注生命周期，表示引用的有效范围。 解读： 1. 表示 和 的生命周期有关联，并且返回值的生命周期与它们相同。 2. 编译器确保 的生命周期不超过 和 中较短的那一个。 --- 如果参数的生命周期不同，Rust 会限制返回值的引用。 --- 如果结构体包含引用，必须显式声明生命周期，表明其有效性。 解读： - 约束 的生命周期，确保 的引用有效。 --- 静态生命周期表示引用在程序整个运行期间都有效（例如，字符串字面值）。 --- 在某些情况下，编译器能自动推断生命周期，无需显式标注。 推断规则： 1. 每个引用参数都分配一个生命周期。 2. 如果只有一个引用参数，返回值与该参数共享生命周期。 3. 如果有多个引用参数且返回值使用其中一个，编译器无法推断时需要显式标注。 --- 当涉及多个引用参数时，需要明确区分生命周期。 解读： - 表示 和 可能有不同的生命周期。 - 返回值不涉及引用，因而无需与 或 关联。 --- 1. 生命周期的核心目标：确保引用在有效范围内安全使用。 2. 常用规则： - 返回值的生命周期不能超过参数的生命周期。 - 多个参数涉及时，明确生命周期关系非常重要。 3. 场景分类： - 函数参数与返回值。 - 结构体中的引用。 - 静态生命周期或复杂引用场景。 Rust 的生命周期机制虽然看起来复杂，但通过明确的规则和编译器检查，确保了内存安全。 - - gpt",
    "url": "/blog/rust 作用域规则",
    "path": "tech/backend/rust 作用域规则.md"
  },
  {
    "title": "rust 智能指针与可变性",
    "content": "Rust 的智能指针是一种封装了指针行为的数据结构，它不仅能像普通指针那样访问数据，还附加了更多的功能，比如所有权管理、引用计数或内存自动回收。常见的智能指针有 、、 和 。下面通过具体例子讲解这些智能指针及其使用场景。 --- 是最简单的智能指针，用于将数据存储在堆上而不是栈上。 使用场景： - 当数据的大小在编译时不确定，或者需要递归数据结构（如链表、树等）。 --- （Reference Counted）允许在单线程中多个所有者共享数据。它通过引用计数来跟踪数据的所有权。 使用场景： - 当需要共享不可变数据，且数据的生命周期不明确时（如在图或树结构中多个节点共享数据）。 --- （Atomic Reference Counted）是线程安全的引用计数智能指针，适合多线程环境。 使用场景： - 当需要在多线程之间共享数据时使用。 --- 提供了一种“内部可变性”机制，即即使外部不可变，也能在运行时借用可变引用（动态检查）。 使用场景： - 当你需要在不可变数据结构中修改数据，但无法使用可变引用时。 注意： 只适用于单线程，且在运行时借用规则违反时会触发 panic。 --- 在实际项目中，常常需要将这些智能指针组合使用。例如，使用 创建一个共享的、可变的树节点。 --- - ：堆上存储单一所有者的数据。 - ：单线程下共享不可变数据。 - ：多线程下共享不可变数据。 - ：单线程中提供内部可变性。 - 组合使用：例如 用于共享和修改数据。 选择合适的智能指针取决于具体的需求：是否需要多线程支持？是否需要可变性？是否需要共享？根据这些因素进行决策即可。 - gpt",
    "url": "/blog/rust 智能指针与可变性",
    "path": "tech/backend/rust 智能指针与可变性.md"
  },
  {
    "title": "rust 的 trait",
    "content": "Rust 中的 是一种抽象机制，它定义了某些类型必须实现的行为集合。 的核心思想是通过行为定义接口，并实现面向接口编程，从而实现灵活性、可扩展性和类型安全。 我们通过几个例子来讲解 Rust 中 的核心思想。 --- 的本质是描述一组行为或功能，任何实现该 的类型都必须提供具体实现。 - 定义行为： 定义了通用行为 。 - 多态性：不同的类型（ 和 ）通过实现 可以表现出各自的行为。 - 面向接口编程：调用代码只关心对象是否实现了 ，而不关心具体类型。 --- Rust 中的 是静态分发的。这意味着编译器在编译时会根据具体类型生成特定的实现代码，从而避免运行时开销。 - 静态分发：编译器在 函数中内联了 的 方法，没有运行时成本。 - 泛型约束：通过 ，确保传入的类型必须实现 。 --- 通过 ，我们可以在运行时动态决定使用哪个类型的实现。动态分发使用指针（如 或 ），并带有一定的运行时开销。 - 动态分发：使用 ，使得程序在运行时通过虚表查找调用正确的方法。 - 扩展性： 中可以容纳任意实现了 的类型。 - 灵活性与性能的权衡：相比静态分发，动态分发更灵活，但有运行时开销。 --- Rust 的 支持组合和扩展，通过将多个 组合，或者定义继承关系，创建更强大的接口。 - 组合： 由 和 组合而成，定义了更多的行为。 - 扩展：通过实现 ，自动继承了 和 的能力。 --- 可以为方法提供默认实现，类型在实现 时可以选择覆盖这些默认实现。 - 默认实现：减少了重复代码， 可以直接使用默认实现。 - 灵活覆盖： 通过重载提供了自定义实现。 --- Rust 中 的核心思想包括： 1. 定义行为接口：通过 明确类型必须实现的功能。 2. 静态与动态分发：提供零成本的静态分发，或支持灵活的动态分发。 3. 组合与扩展：通过组合或继承构建强大的接口。 4. 默认实现：为通用功能提供默认实现，减少重复代码。 是 Rust 实现类型安全、多态性和灵活扩展的关键机制，同时保持了性能的高效性。 - gpt",
    "url": "/blog/rust 的 trait",
    "path": "tech/backend/rust 的 trait.md"
  },
  {
    "title": "rust 的泛型",
    "content": "Rust 泛型的核心思想是 “编写与类型无关的代码，同时保留类型的安全性和性能”。通过泛型，我们可以实现代码复用，而不用牺牲 Rust 的零成本抽象原则。 下面通过几个例子深入讲解这一思想。 --- 泛型允许我们定义一个逻辑，能够处理多种类型。以一个简单的栈（）为例： - 逻辑独立于具体类型：栈的功能代码 和 与具体类型无关，只需定义一次。 - 类型安全：泛型 确保 只能存储 ， 只能存储字符串，防止类型混淆。 - 性能无损：Rust 在编译时会为每种类型生成具体实现（单态化），无运行时开销。 --- 泛型不仅可以表示任意类型，还可以通过trait 约束限定类型的行为。例如，我们可以要求某些泛型类型必须实现特定的接口。 我们定义一个函数，接受两个参数并返回较大的那个值。这个函数需要比较操作，只有实现了 （支持比较操作）的类型才能使用它： - 泛型与行为绑定： 表示泛型 必须实现 ，这样函数中才可以安全地使用比较操作。 - 灵活性和安全性兼顾：虽然 函数适用于任意类型，但通过 限制，避免了不支持比较的类型（如复杂对象）导致编译错误。 --- Rust 的泛型通过单态化，在编译时为每种具体类型生成特定实现，从而消除运行时的类型检查或多态分派。 如果查看生成的汇编代码，会发现编译器生成了两个独立的实现： - 一个处理 的 函数。 - 一个处理 的 函数。 结果： - 泛型函数的性能等同于手动为每种类型编写函数。 - 泛型提供了零成本的灵活性，无需运行时的开销。 --- 泛型可以与其他 Rust 特性（如枚举、trait 等）结合，创建高度抽象、灵活的代码。 Rust 标准库的 就是一个泛型枚举，用于表示一个可能有值或无值的类型： - 使用泛型 ，可以适用于任意类型。 - 通过 ，我们可以表达 “值可能为空”的语义，而无需每种类型都重新设计逻辑。 --- Rust 泛型的核心思想在于： 1. 参数化类型：将类型作为参数，使代码逻辑独立于具体类型。 2. 类型安全：通过 trait 约束明确泛型的行为边界。 3. 零成本抽象：在编译时单态化，既保留灵活性又无运行时开销。 4. 高效复用：与 Rust 的其他特性结合，简化常见模式的实现。 泛型使 Rust 在灵活性、类型安全和性能之间取得了很好的平衡。 - gpt",
    "url": "/blog/rust 的泛型",
    "path": "tech/backend/rust 的泛型.md"
  },
  {
    "title": "rust 设计思想",
    "content": "Rust 的设计理念贯穿其语言的核心特性，从安全性到性能，再到开发者体验，形成了一个强大且平衡的编程生态。 * 泛型：编译器时单态化，编译器为每种具体类型生成特定实现，从而消除运行时的类型检查或多态分派。Rust 的泛型通过单态化，在编译时为每种具体类型生成特定实现，从而消除运行时的类型检查或多态分派。 * trait：在泛型和 trait 静态分发的情况下，Rust 编译器会为每个具体类型生成对应的函数实现。这就是所谓的 单态化（monomorphization）。 --- Rust 的主要目标是避免 内存安全问题（如空指针解引用、缓冲区溢出、悬垂指针等）。 - 设计理念：通过编译时检查确保程序不会出现未定义行为，而不依赖垃圾回收（GC）。 - 具体实现： - 所有权系统：每块内存有且仅有一个所有者。 - 借用和生命周期：通过借用（引用）和生命周期标注避免悬垂指针。 - 不可变性默认：默认不可变数据结构，避免数据竞争问题。 - 意义：以 零运行时开销 的方式实现内存安全。 示例： --- Rust 通过编译时检查，防止常见的并发错误（如数据竞争和死锁）。 - 设计理念：通过类型系统在编译阶段确保并发代码的安全。 - 具体实现： - 线程安全的所有权： 和 trait 定义了跨线程传递和共享的能力。 - 不变量保护：通过锁（如 ）和原子操作（如 ），确保线程安全。 - 意义：开发者可以编写高效的并发代码，而不必害怕数据竞争和其他隐晦的并发问题。 示例： --- Rust 的类型系统通过静态检查，确保类型正确性，并为开发者提供丰富的表达能力。 - 设计理念：类型系统不仅用于错误检测，还要提高代码可读性和表达力。 - 具体实现： - 枚举（Enum）和模式匹配（Pattern Matching）：提供强大的分支逻辑控制。 - 泛型和 trait：实现灵活的代码复用，同时保持类型安全。 - Option 和 Result 类型：避免空值和错误处理中的未定义行为。 - 意义：减少运行时错误，提高代码的鲁棒性和可读性。 示例：Option 和模式匹配： --- Rust 的性能接近 C/C++，但不以安全性为代价。 - 设计理念：提供接近硬件级的性能，同时保持内存和线程安全。 - 具体实现： - 无垃圾回收：通过所有权系统管理内存，避免 GC 带来的延迟。 - 高效的并发模型：通过零开销抽象，允许开发者实现高性能并发。 - LLVM 优化：Rust 使用 LLVM 编译器后端生成高效机器代码。 - 意义：适合系统级开发和性能敏感的场景。 --- Rust 通过工具和设计降低了复杂语言的学习成本和开发门槛。 - 设计理念：即使是复杂的语言，也应尽可能友好。 - 具体实现： - 友好的编译器错误信息：详细的错误信息帮助开发者快速定位问题。 - Cargo 工具链：集成包管理、构建、测试和发布功能。 - 社区驱动生态：官方文档和社区资源齐全。 - 意义：帮助开发者以更低的心智负担编写高质量代码。 示例：编译器的友好提示： 错误信息： --- Rust 提供了强大的模块系统和依赖管理工具，方便开发者组织代码和复用现有的解决方案。 - 设计理念：模块化设计提高可维护性和复用性。 - 具体实现： - 模块和包管理：通过模块和 系统实现代码组织。 - Cargo：标准化的包管理器，提供统一的依赖管理和版本控制。 - trait 和泛型：实现灵活的代码复用。 - 意义：减少重复代码，提高团队协作效率。 --- Rust 强调显式的错误处理，避免隐式失败。 - 设计理念：通过类型系统强制处理错误，提升程序可靠性。 - 具体实现： - Result 类型：通过 和 明确表示操作成功或失败。 - 操作符：简化错误传播。 - panic! 和不可恢复错误：用于处理极端情况。 - 意义：提高错误处理的可读性和安全性。 示例：Result 和 操作符： --- Rust 的核心设计理念是多个方面的综合平衡，它们共同构成了 Rust 的独特性： 1. 零成本抽象：高效且灵活的语言特性。 2. 内存安全：无 GC 的情况下杜绝内存错误。 3. 并发安全：让多线程编程更安全、更高效。 4. 类型安全与表达性：静态类型系统提升可靠性和开发体验。 5. 性能优先：接近 C/C++ 的性能表现。 6. 开发者体验：友好的编译器和工具链。 7. 模块化与代码复用：强大的模块和依赖管理。 8. 显式错误处理：类型驱动的错误处理机制。 这些理念让 Rust 成为一门兼具安全性、性能和开发效率的现代系统级编程语言。 - gpt",
    "url": "/blog/rust 设计思想",
    "path": "tech/backend/rust 设计思想.md"
  },
  {
    "title": "rust 跨语言调用",
    "content": "Rust 能与 C 交互的原因在于它和 C 都可以遵循 C ABI（Application Binary Interface，应用二进制接口）。C ABI 是一种语言无关的标准，规定了函数调用、参数传递、内存对齐等机制，确保不同语言生成的二进制代码能够互操作。 --- C ABI 定义了一种“规则”，让不同的编程语言按照统一的方式调用函数或传递数据。 就好比语言之间的“翻译协议”，只要两边都遵循同样的协议，就能互相理解。 --- 我们先写一个 C 函数，然后用 Rust 调用它。 第一步：写 C 的代码 然后我们用 把这个 C 文件编译成一个共享库（比如 或 ）： 第二步：写 Rust 的代码调用这个 C 函数 运行步骤： 1. 用 Rust 编译运行这个文件时，告诉它要链接 （动态库）： 输出结果为：。 --- 1. C 编译器的作用： - C 编译器把 函数编译成机器码，并存到共享库里（）。 - 这个共享库对外暴露了函数 的地址和调用方式。 2. Rust 的作用： - Rust 通过 语法，告诉编译器：我想调用一个遵循 C ABI 的函数。 - Rust 的编译器会在运行时找到这个共享库，并调用 函数。 3. C ABI 的作用： - 确保函数的参数和返回值按约定的方式传递。比如： - 参数 和 被放在固定的寄存器里（或者栈上）。 - 返回值被放在另一个固定位置。 --- 这次我们写一个 Rust 函数给 C 调用。 第一步：写 Rust 的代码 然后编译成共享库： 第二步：写 C 的代码调用这个 Rust 函数 运行步骤： 1. 编译 C 代码并链接 Rust 的共享库： 输出结果为：。 --- 1. Rust 并不是依赖 C，而是遵循 C 的规则（C ABI）。 - 关键字就是告诉 Rust 编译器，函数调用要遵循 C 的规则。 - C ABI 是一个约定，规定了参数和返回值怎么传递、内存怎么对齐等。 2. Rust 和 C 是“直接用机器语言对话”的，不需要翻译成 C 代码。 - Rust 和 C 的交互只发生在编译后的机器代码层。 - Rust 编译后的共享库（）和 C 编译的可执行程序用相同的协议调用函数。 3. Rust 通过 FFI 提供了跟其他语言交互的能力： - 只要其他语言也遵循 C ABI，比如 Python 的 或 JavaScript 的 ，它们都能和 Rust 互操作。 这个例子可以看出，C 和 Rust 的交互是靠 ABI 约定，而不是中间语言或运行时的“翻译”。 --- 初步理解：napi-rs 将 rust 代码编译成了 N-API 协议能够识别的 .node 文件，所以 js 能够直接加载，如果是 rust 编译出的 .so 文件（遵循 C-ABI ） 则需要 C或者 C++ 编写胶水语言 做成符合 N-API 的 .node 文件，然后 才可以被 js 调用？基于 N-API：注重 跨平台性 和 稳定性，适合长期维护的项目。 其他调用方式： * rust -> .wasm（跨平台，可以 node 跟浏览器环境，比 .node 性能稍低） -> js * neon （针对 v8 引擎，跳过 NAPI 抽象层，性能比 napi-rs 转换的代码更高效，但是可维护性也更差），基于 V8 深度绑定：追求 性能极致 和 灵活定制，适合对底层优化要求极高的项目。 --- - 关键点：Rust 代码直接编译成符合 N-API 标准 的 文件。 - 工作流程： 1. Rust 使用 或 库，通过 N-API 暴露函数。 2. 在编译过程中自动生成符合 N-API 标准的动态库（ 文件）。 3. Node.js 加载这个 文件，直接调用 Rust 的函数。 > 特点：这一过程完全自动化，不需要手写 C/C++ 胶水代码。Rust 代码编译后直接可以被 Node.js 使用，开发体验优良。 --- - 关键点： 文件是标准的 C ABI 动态库格式，JavaScript 环境（如 Node.js）本身不支持直接加载 C ABI 的库。 - 问题： - JavaScript 和 文件之间缺少直接的桥梁。 - 需要手写胶水代码，用 C 或 C++ 把 封装成 N-API 模块（ 文件）。 1. Rust 编译出 文件： - 使用 类型的 编译出 C ABI 动态库： 2. 用 C/C++ 编写 N-API 模块： - 使用 C/C++ 代码封装 文件的函数为 N-API 模块： 3. 编译 C++ 胶水代码成 文件： - 使用 或其他工具链编译生成 文件： > 特点：这一过程繁琐，需要编写和维护额外的 C/C++ 代码。 --- | 特点 | 使用 | 使用 + C/C++ 胶水代码 | |--------------------------|--------------------------------------------------------|----------------------------------------------------| | 开发效率 | 高：无需手写胶水代码，Rust 直接编译成 文件 | 低：需要手动编写 C/C++ 胶水代码封装 文件 | | 依赖性 | 只需 ，完全用 Rust 开发 | Rust + C/C++，需要额外工具链 | | 可维护性 | 高：Rust 和 Node.js 开发者可以直接维护 | 低：Rust 和 C/C++ 开发需要协作 | | 适用场景 | 适合绝大多数 Node.js 插件开发场景 | 适合已有 文件需要快速封装到 N-API 的场景 | | 性能 | 性能优良，直接生成 N-API 模块 | 性能同样优良，但手工编写代码可能增加维护成本 | --- - 如果你的目标是为 Node.js 开发插件，使用 Rust 的 N-API 工具链（如 ）是最佳选择，它能让你直接生成 文件，省去了手动编写 C/C++ 胶水代码的麻烦。 - 如果你手头已有 文件（比如是用其他语言编写的库），需要编写 C/C++ 胶水代码将其封装成 文件，才能被 Node.js 加载和使用。 最终，N-API 是核心桥梁，Rust 和 C/C++ 都是为了生成能符合 N-API 的模块。 - gpt",
    "url": "/blog/rust 跨语言调用",
    "path": "tech/backend/rust 跨语言调用.md"
  },
  {
    "title": "关于node的addon",
    "content": "Node.js 对 native addon 开发侧暴露的是 ABI（In computer software, an application binary interface (ABI) is an interface between two binary program modules） 以下是摘抄，还有待琢磨原理 NAN 将 v8/libuv 相关的 API 进行了封装，对外是稳定的抽象层 API (但却无法保证是 ABI 稳定)。用 NAN 进行封装的 native addon 几乎无法通过预编译的方式进行分发，因为跨 Node 版本底层 v8/libuv API 变化之后需要对源码进行重新编译才能使用。所以这就是为什么很多 native addon 在 npm install 后还要调用一堆工具链在本地进行编译才能使用，以及为什么有时候 node 版本升级之后之前安装好的 node_modules 就无法直接使用了。 一句话理解：V8底层的封装，估计需要理解V8才能开发 自从 Node.js v8.0.0 发布之后，Node.js 推出了全新的用于开发 C++ 原生模块的接口，N-API。本质其实是将 NAN 这层抽象挪到了 node 源码中，在 node 编译的时候就编译好这层对外抽象，这样 N-API 对外就是稳定的 ABI 了。 一句话理解：v8之上node层的封装，跟随Node编译，独立于操作系统 基本步骤： ./demo.cc 模块配置跟编译 ./binding.gyp 生成.node文件（动态链接库） 调用.node文件 node扩展方式 * 插件开发 * WebAssembly 哪种更快些？或者分别适用于哪些场景？ Wasm被保证与可以运行您的Electron应用程序的任何操作系统兼容。 C，C++和Rust将没有此保证。除了从JavaScript调用的开销之外，它们将更加高效。 正在积极地处理从JavaScript调用Wasm的开销，并且正在减少或完全消除这种开销。 * * Rust bindings for writing safe and fast native Node.js modules. *",
    "url": "/blog/关于node的addon",
    "path": "tech/backend/关于node的addon.md"
  },
  {
    "title": "内存分析",
    "content": "--- title: 内存分析 --- 长时间（一般是几天时间）一条内存只增不减的曲线就标志着内存泄露了 0. node引入 require('heapdump') 1. 启动node, eg: npm run start 3. lsof -i:8080 4. kill -USR2 <pid>（第三步查看到的pid） 5. 根据情况调整压测参数 autocannon -c 10 -a 10 http://localhost:8080 6. 重复执行3次步骤 4，5 7. 浏览器打开控制台memory栏目，按照生成顺序载入生产的内存快照 heapdump开头的文件 * https://marmelab.com/blog/2018/04/03/how-to-track-and-fix-memory-leak-with-nodejs.html * https://github.com/aliyun-node/Node.js-Troubleshooting-Guide/blob/master/0x03_%E5%B7%A5%E5%85%B7%E7%AF%87_%E6%AD%A3%E7%A1%AE%E6%89%93%E5%BC%80%20Chrome%20devtools.md",
    "url": "/blog/内存分析",
    "path": "tech/backend/内存分析.md"
  },
  {
    "title": "列存储",
    "content": "什么是列存储？跟传统存储啥区别？为啥列存储能更加快速响应复杂查询？ 列存储（Columnar Storage）是一种数据存储方式，与传统的行存储（Row Storage）有显著区别。列存储在处理分析型工作负载时，尤其是涉及复杂查询的情况下，表现得更加高效。 在列存储中，数据按列存储，而不是按行存储。每一列的数据存储在一起，所有同一列的数据都连续存储在磁盘上。例如，对于一个有四个列（A、B、C、D）的表，数据将按如下方式存储： 在传统的行存储中，数据按行存储，每一行的数据存储在一起。对于同样的四列表，数据将按如下方式存储： 1. 数据存储方式 - 行存储：所有列的值按行存储在一起。 - 列存储：同一列的值按列存储在一起。 2. 适用场景 - 行存储：适用于OLTP（在线事务处理）工作负载，例如频繁的插入、更新和删除操作。 - 列存储：适用于OLAP（在线分析处理）工作负载，例如复杂的查询和报表分析。 1. 查询效率 - 在分析型查询中，通常只涉及少量的列。例如，计算某个统计数据只需访问特定的列。列存储仅读取需要的列数据，减少了I/O操作。 - 行存储则需要读取整行数据，即使只需要其中一两列，也要读取整行，导致不必要的I/O开销。 2. 数据压缩 - 列存储的同一列数据类型相同且经常有相似的值，压缩效果更好。例如，一个包含许多重复值的列可以通过RLE（Run-Length Encoding）等压缩算法显著减少存储空间。 - 行存储的数据类型混杂在一起，压缩效率低下。 3. 向量化处理 - 列存储系统可以利用向量化处理技术，同时处理一批列数据，提高CPU使用效率。 - 行存储处理每行数据时需要多次加载和存储，效率较低。 4. 缓存利用 - 由于列存储按列连续存储，缓存命中率高，减少了对磁盘的访问。 - 行存储按行存储，缓存命中率较低，需要频繁访问磁盘。 列存储通过优化I/O操作、提高压缩效率、利用向量化处理和更好的缓存利用，显著提高了复杂查询的响应速度。因此，它特别适用于需要进行大量读取和分析的场景，如数据仓库、商业智能和实时分析系统。 假设有一个大型销售数据表，包含日期、产品ID、销售数量和销售额四个字段： 行存储： 列存储： 查询总销售额时，列存储只需读取“销售额”列的数据，而行存储则需读取每一行的所有字段，明显增加了I/O开销和处理时间。这就是列存储在复杂查询中表现更高效的原因。",
    "url": "/blog/列存储",
    "path": "tech/backend/列存储.md"
  },
  {
    "title": "性能分析",
    "content": "--- title: Profile --- 最近通过 alinode 的CPU火焰图找出了项目中吞吐率阻塞点，简单分享 * profile的原理？ * 如何对profile结果做可视化？ * 火焰图如何分析？ * 如何development/production进行profile Chrome profiler 为了找到那些耗时最多的代码，Chrome 分析器每 100μs 捕获一个堆栈跟踪。 这意味着，如果一个函数只需要 50μs 的执行时间，就可能不会在分析器中显示出来！ 当你分析几毫秒以上的时间时，可以准确了解应用程序在何时花费最多的时间。 但是，当你放大 profiler 面板想看更精准的时间时，信息会变得不太准确。 分析器也会不一致。 每次运行时，会产生一个稍微不同的结果。 有时可能会记录非常短的函数调用，而在其他时间再次运行这些函数调用信息可能会丢失。 * Scripting * Rendering * Painting * Other * Idle * CPU Profiles 基于对StackTrace的“采样”进行实现，在一时间段内（至少得5min保证样本数足够多）对堆栈做快照采样 * 优点: >简单，源代码侵入性不大 * 劣势: >采样数据主要体现在调用次数上，对CPU的占有时间不够精准 * 要求： >1. 样本必须足够多。 >2. 程序中所有正在运行的代码点都必须以相同的概率被Profiler采样。 >>JVM 只能在采样,是否就违背了第二条原则？最终导致profile不太准 * 适合场景 >Sampling由于低开销的特性，更适合用在CPU密集型的应用中，以及不可接受大量性能开销的线上服务中。 >也是一般Profiler的实现机制，典型的就是alinode Instrumentation方式对几乎所有方法添加了额外的AOP（Aspect Oriented Programming 中文翻译为面向切面编程）逻辑，这会导致对线上服务造成巨额的性能影响 这个有点像函数埋点 * 优点 >绝对精准的方法调用次数、调用时间统计。 * 缺点 >性能消耗比较大，侵入性也比较高 * 适合场景 >Instrumentation则更适合用在I/O密集的应用中、对性能开销不敏感以及确实需要精确统计的场景中。 火焰图统计的是次数，所以属于CPU采样类型 * sampling >y 轴表示调用栈，每一层都是一个函数。调用栈越深，火焰就越高，顶部就是正在执行的函数，下方都是它的父函数。 >x 轴表示抽样数，如果一个函数在 x 轴占据的宽度越宽，就表示它被抽到的次数多，即执行的时间长。注意，x 轴不代表时间，而是所有的调用栈合并后，按字母顺序排列的。 火焰图就是看顶层的哪个函数占据的宽度最大。只要有\"平顶\"（plateaus），就表示该函数可能存在性能问题。 上面代码中，start_thread是启动线程，调用了func_a。后者又调用了func_b和func_d，而func_b又调用了func_c。 合并后 有了这个调用栈，就能生成对应的图，灵魂火焰图 <!-- | func_c | a | | --- | --- | | func_d | func_b | | func_a | a | | start_thread | a | --> 目前v8-profiler在 node12的时候安装构建失败，node12版本自带的build不成功； 替代方案是选用 v8-profiler-next，步骤： 1. npm i -D v8-profiler-next 2. 参照v8-profiler-next文档，在dev启动5分钟左右 3. 压测（例如：autocannon -c 10 -d 300 -p 1 http://localhost:8080） 4. 导出结果到chrome就可以看到具体的 cpu占用情况 在chrome 调试工具里找到 Javascript Profiler即可导入 xx.cpuprofile 文件进行分析，有百分比跟倒立火焰图 目前alinode用起来还比较流畅，如果使用的是Docker部署，可以走以下流畅 1. 直接使用alinode镜像构建自己的dockerfile （为了降低Docker镜像的大小，node12 对应的alinode版本看起来基于alpine linux，所以有些需要注意，例如apt需要替换apk） 2. 按照官方文档创建 app-config.json 文件 3. 部署上线，然后进行 cpu在线Profile，转储后在线分析（支持火焰图跟dev tool模式） 感兴趣的可以参考 1. 跳转到 2. 点击 cpu-profiling 3. 打开console，点击Javascript Profiler，点击record，6秒后点击stop 4. 再次点击 cpu-profiling 更多精细demo, 本地运行这个，修改pages/test/use-test.ts里面的参数，再次走上面流程，看下100微秒的捕获丢失效果，以及最小的时间单元是否是 0.1毫秒（也就是100微秒） * https://github.com/brendangregg/FlameGraph * https://www.mattzeunert.com/2016/08/06/show-native-functions-in-js-profile.html * https://nodesource.com/blog/diagnostics-in-NodeJS-2 * https://www.ruanyifeng.com/blog/2017/09/flame-graph.html * https://tech.meituan.com/2019/10/10/jvm-cpu-profiler.html * https://www.jianshu.com/p/c79c5e02ebe6 <u>2021-7-28</u>",
    "url": "/blog/性能分析",
    "path": "tech/backend/性能分析.md"
  },
  {
    "title": "理解 rust 无 GC",
    "content": "自己的理解：rust 无 GC应该指的是没有集中式的 GC，而是分散式的，随着代码的执行而自动的 回收？ Rust 中没有传统意义上的 集中式垃圾回收（Garbage Collector, GC），而是通过 分散式的、编译时静态分析和运行时的引用计数机制 来管理内存。这种方式避免了传统 GC 的集中式工作方式，回收是随着代码逻辑执行自然发生的。 --- 1. 所有权系统（Ownership System） Rust 的内存管理依赖 所有权机制，在编译时完成内存分配和释放的分析。 - 每个值都有一个所有者（）。 - 当值的所有者超出作用域时，内存会立即释放。 - 在编译时，Rust 检查所有权转移、借用和生命周期，确保内存安全。 例子： 2. 借用和引用 - 借用（ 和 ）允许多个地方访问一个值，而不改变所有权。 - Rust 编译器通过借用检查器确保没有数据竞争（数据同时被修改和读取）。 例子： 3. 引用计数（ 和 ） - 对于需要多个所有者的场景，可以使用引用计数（ 或 ）。 - 运行时跟踪引用计数，所有引用都被丢弃时，内存会被自动释放。 例子： 4. 线程安全的引用计数（） - 是 的线程安全版本，用于多线程环境。 - 依赖原子操作管理引用计数。 --- - 性能更高： - 没有 GC 暂停（GC Pause），因此实时性较好。 - 在作用域结束时立即释放内存，不需要等待垃圾回收器触发。 - 内存管理可控： - 开发者通过所有权和借用机制对内存管理有更多的明确控制。 - 更小的运行时开销： - Rust 没有语言级别的垃圾回收器，这减少了程序运行时的内存和 CPU 开销。 --- - Rust 的内存回收不是集中管理，而是随着代码执行逐步完成的： - 编译时：分析作用域和所有权，静态确定释放时机。 - 运行时：通过引用计数（如 和 ）动态管理共享数据。 例子： --- | 特性 | Rust | 传统 GC (如 Java, Go) | |-----------------------|--------------------------|-------------------------------| | 回收触发时机 | 编译时分析，运行时即时回收 | 运行时批量回收（触发时暂停） | | 运行时开销 | 无集中管理，开销更小 | 需跟踪对象、回收开销较高 | | 暂停问题 | 无 GC 暂停，实时性更好 | 存在 GC 暂停问题 | | 内存安全性 | 编译时强保证 | 运行时依赖语言特性和检查机制 | --- Rust 的内存管理依赖 所有权机制 和 引用计数（如果需要），在代码执行的同时完成内存的分散式回收，不依赖集中式 GC。它避免了传统 GC 的性能问题，同时提供了强大的内存安全性。 - gpt",
    "url": "/blog/理解 rust 无 GC",
    "path": "tech/backend/理解 rust 无 GC.md"
  },
  {
    "title": "理解并发",
    "content": "* 同步阻塞 例如：计算一个 对于CPU计算类型的，会阻塞单线程 * 异步非阻塞 例如： request请求，数据库处理等异步I/O 本质上node还是为异步I/O创建了新的线程，例如：对于数据库的非阻塞异步操作，实际上是开启了独立的数据库操作线程去处理，然后把结果返回给主线程 * 单线程模型（Nodejs）: 服务员A处理不同顾客的点单请求,多个I/O厨师准备多个订单上的菜肴->然后服务员A端给不同顾客； * 多线程模型（例如GO）: 里面有许多个厨师，每个厨师的一条龙服务：处理顾客的点单请求->准备菜肴->然后端给顾客 * https://stackoverflow.com/questions/34855352/how-in-general-does-node-js-handle-10-000-concurrent-requests",
    "url": "/blog/理解并发",
    "path": "tech/backend/理解并发.md"
  },
  {
    "title": "Docker 镜像",
    "content": "Docker 镜像的层次关系原理基于联合文件系统（Union File System），这种文件系统允许多个文件系统层次叠加在一起，表现为一个单一的文件系统。Docker 利用这一特性来构建和管理镜像和容器。 联合文件系统（如 AUFS、OverlayFS、btrfs 等）是 Docker 的核心技术之一。它允许多个目录（称为层）叠加在一起，使其看起来像一个单一的目录。每一层都是只读的，只有最顶层是可写的。 Docker 镜像由多个只读层组成，每一层对应 Dockerfile 中的一条指令。这些层是增量的，即每一层只包含相对于前一层的变化。镜像层次结构的基本原理如下： 1. 基础镜像层：这是镜像的最底层，通常是一个操作系统的基础镜像（如 、、 等）。它提供了运行应用所需的基本环境。 2. 中间层：每一条 Dockerfile 指令（如 、、 等）都会创建一个新的层。每个中间层只包含相对于前一层的变化。例如，安装软件包、复制文件等。 3. 顶层：这是镜像的最顶层，通常包含应用程序的代码和配置文件。 当你启动一个容器时，Docker 会在镜像的基础上添加一个可写层，这个可写层称为容器层。容器层是唯一可写的层，所有对文件系统的修改（如创建新文件、修改现有文件等）都会发生在这一层。 1. 高效存储：由于每一层是只读的，并且是增量的，Docker 可以通过共享相同的基础层来节省存储空间。例如，多个基于同一基础镜像的容器可以共享该基础镜像的层。 2. 快速构建：构建镜像时，如果某一层已经存在且没有变化，Docker 可以直接重用该层，而不需要重新构建。这大大加快了镜像的构建速度。 3. 缓存机制：Docker 使用缓存机制来加速镜像的构建和部署。如果某一层已经存在于缓存中，并且没有变化，Docker 可以直接从缓存中获取该层，而不需要重新执行指令。 假设我们有如下的 Dockerfile： 构建镜像时，每一条指令都会创建一个新的层： 1. 基础镜像层： 2. 工作目录层： 3. 复制依赖文件层： 4. 安装依赖层： 5. 复制应用程序代码层： 6. 暴露端口层： 7. 运行应用程序层： 当你运行容器时，Docker 会在这些只读层的基础上添加一个可写层。所有对文件系统的修改都会发生在这个可写层中。 联合文件系统（Union File System）如何允许多个文件系统层次叠加在一起，表现为一个单一的文件系统。例子： 假设我们有三个不同的目录，每个目录代表一个文件系统层： 1. Layer 1：包含基本的操作系统文件 2. Layer 2：包含一些应用程序文件 3. Layer 3：包含用户的数据文件，容器运行时的可写层 - Layer 1（基础操作系统层） - Layer 2（应用程序层） - Layer 3（用户数据层） 当我们使用联合文件系统将这三个层叠加在一起时，它们会表现为一个单一的文件系统： 让我们将这个概念应用到 Docker 镜像和容器上： 1. 基础镜像（Layer 1）：假设我们使用一个 Ubuntu 基础镜像，它包含了操作系统的基本文件。 2. 应用程序层（Layer 2）：我们在基础镜像上安装了一个应用程序，这会创建一个新的层。 3. 用户数据层（Layer 3）：容器运行时，用户在容器内创建或修改文件，这些变化会记录在一个可写层中。 当我们启动容器时，Docker 会将这些层叠加在一起，表现为一个单一的文件系统： 用户在容器内看到的文件系统就像是一个完整的文件系统，尽管它实际上是由多个层叠加而成的。 在写时复制机制（Copy-on-Write, CoW）下，可能存在两份 文件，一份在只读层（lower layer），一份在可写层（upper layer）。但是，从文件系统的视图来看，用户只会看到一份文件。具体来说，这样的设计是为了在保证只读层数据不变的同时允许用户修改文件。以下是更详细的解释： 联合文件系统通过叠加多个层来形成一个合并视图，用户只会看到合并后的文件系统视图，而不是底层的具体实现。 1. 查找文件： - 当用户访问 文件时，联合文件系统首先在可写层（upper layer）中查找。 - 如果可写层中找不到该文件，则继续查找只读层（lower layer）。 2. 写时复制过程： - 如果用户对 文件进行修改，并且该文件在只读层中存在（可写层中不存在），则联合文件系统会将该文件从只读层复制到可写层。 - 所有的修改操作都会在可写层中的副本上进行。 这种机制确保了只读层的文件不会被直接修改，从而保持了其完整性。同时，用户对文件的修改操作仍然可以进行，只是这些修改会反映在可写层中。 假设有一个联合文件系统，其中只读层和可写层的初始状态如下： - 只读层（lower layer）： - 可写层（upper layer）： 当用户修改 文件时，文件系统执行以下步骤： 1. 查找： - 在可写层中查找 ，未找到。 - 在只读层中查找 ，找到该文件。 2. 复制： - 将 文件从只读层复制到可写层。 3. 修改： - 在可写层中的副本上进行修改。 修改后的文件系统层次结构如下： - 只读层（lower layer）： - 可写层（upper layer）： 用户通过文件系统看到的合并视图如下： 从用户的角度来看，他们看到的只是一个文件系统，其中 文件已被修改，而不会意识到底层存在两份 文件。 - Docker容器：Docker容器的文件系统利用联合文件系统叠加多个只读层（来自Docker镜像的各层）和一个可写层（容器的读写层）。容器启动后，所有对文件的修改都在可写层进行，而镜像层保持不变。 - Linux Live CD：Live CD系统通常是只读的，但用户可以在会话期间对文件进行修改。这些修改被写入到一个临时的可写层（如RAM），原始的CD内容不变。 在联合文件系统中，通过写时复制机制，文件系统可以在只读层中保持文件的原始状态不变，同时允许用户对文件进行修改。这种机制在保持数据完整性和一致性的同时，提供了灵活的文件管理能力。虽然底层可能存在两份文件（原始和副本），但用户看到的是一个统一的、合并的文件系统视图。",
    "url": "/blog/Docker 的理解",
    "path": "tech/frontend/Docker 的理解.md"
  },
  {
    "title": "SSR原理浅析",
    "content": "--- title: Vue SSR 经验 description: --- 基本猜想： 1. 按照层级匹配，只在不匹配的时候报错，层次正确即可 2. 只在data-server-rendered=\"true\"的时候hydrate，随后移除data-server-rendered server.html will match client but not match client * 服务端： 1. hydrate=true -> 根据component 渲染出来的正常 html * 客户端： 将组件封装成 async component，在条件成熟的时候resolve（浏览器空闲/dom可见/事件触发） 原理上不是真实意义上的hydrate，而是异步组件的渲染； 所以如果一个组件在转换成 async的时候会出问题（例如：会有跟激活的组件的交互），那就不适用这个package 原理：vue-client-only 组件的 render 函数简单封装 * 服务端返回h(false) 占位 * 客户端：根据parent._isMounted 返回正常组件 （在浏览器mounted的时候） * in dev : true(代表plugin middleware等文件会在每个请求进入的时候重新载入) * in production: false （代表不会重新载入plugin，只会反复执行返回的函数） 1. bundle code ./vue-ssr.js * * * * *",
    "url": "/blog/SSR原理浅析",
    "path": "tech/frontend/SSR原理浅析.md"
  },
  {
    "title": "vite/webpack 之 vue ssr 图解",
    "content": "",
    "url": "/blog/SSR工程结构图解",
    "path": "tech/frontend/SSR工程结构图解.md"
  },
  {
    "title": "Float32Array 应用场景",
    "content": "是 JavaScript 中的一种类型数组，它代表平台字节顺序为 32 位的浮点数型数组（对应于 C 浮点数据类型）。让我们探讨一下 的应用场景和特性： 1. 数值计算和底层二进制数据处理： - 存储的是单精度浮点数，每个元素占据 4 个字节。这使得它在处理大量数值计算时更加节省内存。 - 连续的内存空间存储使得在进行数学运算时效率更高，从而提高性能. 2. 音频和图像处理： - 在音频处理中， 可以存储音频样本数据，例如音频波形、频谱分析等。 - 图像处理中的像素值也可以使用 存储（利用其高精度），例如图像滤波、变换等。 3. WebGL 和 WebGPU： - 在图形渲染领域非常重要。WebGL 和 WebGPU 使用 来处理图形数据，例如顶点坐标、纹理坐标、颜色等。 - 在这些图形库中，使用 可以高效地传递数据给 GPU 进行并行计算和渲染。 4. 二进制数据传输和解析： - 可以用于处理二进制数据，例如网络传输中的数据包、文件解析等。 - 通过 和 ，可以将二进制数据转换为 ，并进行解析。 当然可以。 的优势主要体现在以下几个方面： 是固定大小的，并且在内存中是连续存储的。它直接映射到计算机的浮点数表示，这意味着它可以非常高效地使用内存。相比之下，普通的 JavaScript 数组是动态的，可以包含不同类型的元素，因此它们需要更多的内存开销来存储额外的信息，比如元素类型和指针。 由于 中的数据是类型化的（即数组中的每个元素都是相同类型的），JavaScript 引擎可以对操作这些数组的代码进行优化。编译器知道数组中的每个元素都是32位浮点数，因此它可以在底层使用更快的算法和指令集来处理这些数据。 保证了数组中的每个元素都是32位浮点数。这种一致性确保了当你处理科学计算、图形渲染或者音频处理等需要精确浮点运算的应用时，数据的表现是可预测的。 更接近硬件层面的表示，因为现代计算机硬件通常都是针对固定大小和类型的数据进行优化的。这意味着读取和写入这些数组的操作可以非常迅速，因为它们可以直接映射到硬件操作。 使用 可以直接读取和写入二进制数据流，这在处理网络传输和文件 I/O 操作时非常有用。例如，你可以直接从一个 创建一个 ，而不需要进行任何转换。这使得在客户端和服务器之间传输浮点数数据变得更加简单和快速。 在网络传输中，使用 可以直接发送和接收二进制数据，而不需要将浮点数转换为字符串或其他格式。这减少了数据的大小，因此可以加快传输速度并减少带宽使用。 许多 Web API 都是围绕类型化数组设计的，这意味着 可以直接用于这些 API，无需额外的转换或包装。这使得在使用 WebGL、Web Audio API 等技术时， 成为处理和传输数据的自然选择。 一个手动创建的二进制序列化过程，它类似于Protocol Buffers（Protobuf）的工作原理；例子： 从一个二进制WebSocket流中接收数据，这个流包含了一个复合结构的数据包，其中既有字符串也有浮点数。 假设服务器发送的数据包格式如下： - 4个字节的整数，表示用户ID - 32个字节的字符串，表示用户名 - 4个字节的单精度浮点数，表示用户的账户余额 这个数据包总共占用40个字节，我们的任务是从这个二进制流中解析出这三个字段。 首先，我们会接收到一个 对象，其中包含了这40个字节的数据。我们需要按照正确的顺序和数据类型来解析这些数据。 发送方有以下数据： 发送方将执行以下步骤来序列化这些数据： 接收方 在这个例子中： - 是原始的二进制数据。 - 被用来解析整数和浮点数字段。 - 被用来获取用户名的字节，然后使用 将这些字节解码成字符串。 - 没有直接使用，但如果我们有多个浮点数需要处理，我们可以创建一个 视图来直接操作这些浮点数。 与手动处理二进制数据相比，Protobuf提供了以下优势： * 效率： Protobuf设计用来在网络上高效传输数据，它的编码通常比JSON小很多。 * 跨平台： Protobuf支持多种编程语言，可以轻松地在不同的系统和语言之间传输数据。 * 可扩展性： Protobuf设计了向后和向前兼容，你可以更改数据结构而不破坏已部署的程序之间的通信。 * 自动代码生成： Protobuf提供了代码生成工具，可以自动为多种语言生成序列化和反序列化代码，减少了手动编码的错误和工作量。 * GPT",
    "url": "/blog/Type Array 应用场景",
    "path": "tech/frontend/Type Array 应用场景.md"
  },
  {
    "title": "Vite简介与原理",
    "content": "--- title: vite 简介与原理 --- 讲解范围 0. 背景 1. vite是什么 2. 特征: 冷启动 + 热更新 3. C/S 基本架构 3. 预处理: 解决的问题 + 原理 4. 热替换：如何使用 + 原理 5. 其它：插件机制 + 竞品走势 + 总结 + QA 目前的webpack development 场景也会把所有可能用到的代码全部进行打包构建，这样打包出来的代码是十分庞大的，很多时候其实我们在开发过程中并不需要全部代码的功能，而是一小部分，这个时候大量的构建时间都是多余的，我们需要一个能够真正意义上实现懒加载的开发工具。 Vite 是一个由原生ESM 驱动的 Web 开发构建工具。在开发环境下基于浏览器原生ES imports 开发，在生产环境下基于Rollup打包。 在浏览器端使用 export、import 的方式导入和导出模块，在 script 标签里设置 type=\"module\"，浏览器会识别所有添加了type='module'的script标签，对于该标签中的import关键字，浏览器会发起http请求获取模块内容。 vite之前 当冷启动开发服务器时，基于打包器的方式启动必须优先抓取并构建你的整个应用，然后才能提供服务。 vite后 灰色部分是暂时没有用到的路由，甚至完全不会参与构建过程，随着项目里的路由越来越多，构建速度也不会变慢。 vite之前 * 第一阶段： 打包器的开发服务器将构建内容存入内存，修改后重新构建并重载页面（代价很高 + 重新加载页面会消除应用的当前状态） * 第二阶段： HMR 热更新（主流的webpack），允许一个模块 “热替换” 它自己，而对页面其余部分没有影响。 实践中：不过HMR也会随着应用规模的增长而显著下降 vite后 HMR 是在原生 ESM 上执行的。当编辑一个文件时，Vite 只需要精确地使已编辑的模块与其最近的 HMR 边界之间的链失效（大多数时候只需要模块本身），使 HMR 更新始终快速，无论应用的大小。 1. server 预构建 -> 启用服务 node + connect + ws 2. 浏览器请求 -> server中间件处理 -> server插件调用（（路径改写，hot注入，依赖图生成） -> 返回 client 基于 esbuild 的依赖预打包 1. 减少模块/请求数量； 2. 支持 CommonJS 依赖。 预打包只有在依赖变动时才需要执行，但在有大量依赖的项目中，每次执行还是可能会需要很长时间。Vite 之前是使用 Rollup 来执行这个过程，在 2.0 中切换到了 esbuild，使这个过程加快了几十倍。 Vite 通过在一开始将应用中的模块区分为 依赖 和 源码 两类，改进了开发服务器启动时间。 1. 依赖 大多为在开发时不会变动的纯 JavaScript，例如 node_modules 等第三方依赖 2. 源码 通常包含一些时常被编辑的文件 * 场景 import { debounce } from 'lodash' 导入一个命名函数的时候，debounce 函数的模块内部又依赖了很多其他函数，形成了一个依赖图。 当浏览器请求 debounce 的模块时，又会发现内部有 2 个 import，再这样延伸下去，这个函数内部可能带来了几百次请求。 * 解决方案 折中 利用 Esbuild 快速的构建编译速度，在没有感知的情况下在启动的时候预先帮你把 debounce 所用到的所有内部模块全部打包成一个传统的 js bundle（构建好的bundle会放在node_modules/.vite下），后续对 A module的引用都直接返回 .vite/A 文件内容 Esbuild 使用 Go 编写，并且比以 JavaScript 编写的打包器预构建依赖快 10-100 倍。 1. self accept * demo .vue css * demo .vue template * demo .vue script 2. array accept (not self accept) * demo https://github.com/wizardpisces/lerna-repo/tree/master/packages/mini-vite/template-vue-ts/src/mimic-store * * server build moduleGraph 分析 源文件 import -> build 父子关系图 分析 源文件 import.meta.hot -> build 热替换依赖图 client build hotModulesMap 在 rollup 插件基础上 扩展出 vite 的 plugin API，以及执行机制。好处：一套插件可能在 rollup 跟 vite 同时兼容使用（如果并未使用vite独有的插件hook），扩大 vite的生态圈，直接运行大部分的 rollup插件 * vite已经出到版本2，目前spa项目用起来没发现什么特别问题 * 内部项目(admin)可以开始先尝试 Vite * SSR 目前处在实验阶段，还需等待 目前看起来不会 原因： * webpack能覆盖更多的奇特的场景，生态也更丰富 1. 服务端渲染 (SSR) 支持还处在实验阶段，产线使用仍需等待 不会 ，供初入坑 原因： * 由于预构建，node_modules里面的源代码已经被打包到 node_modules/.vite下面，目前此路径并未注册到热替换图里面 多种构建工具的比对 1. webpack :强调对web开发的支持，尤其是内置了HMR的支持，插件系统比较强大，对各种模块系统兼容性最佳(amd,cjs,umd,esm等，兼容性好的有点过分了，这实际上有利有弊,导致面向webpack编程），有丰富的生态，缺点是产物不够干净，产物不支持生成esm格式， 插件开发上手较难，不太适合库的开发。 2. rollup: 强调对库开发的支持，基于ESM模块系统，对tree shaking有着良好的支持，产物非常干净，支持多种输出格式，适合做库的开发，插件api比较友好，缺点是对cjs支持需要依赖插件，且支持效果不佳需要较多的hack，不支持HMR，做应用开发时需要依赖各种插件。 3. esbuild: 强调性能，内置了对css、图片、react、typescript等内置支持，编译速度特别快（是webpack和rollup速度的100倍+),缺点是目前插件系统较为简单（框架作者还在迭代中），生态不如webpack和rollup成熟。 * https://github.com/wizardpisces/lerna-repo/tree/master/packages/mini-vite * https://vitejs.dev/guide/ * https://github.com/evanw/esbuild * https://segmentfault.com/a/1190000039264055 vite ssr简介",
    "url": "/blog/Vite简介与原理",
    "path": "tech/frontend/Vite简介与原理.md"
  },
  {
    "title": "为什么需要隔离",
    "content": "防止控件之间样式覆盖，特别在微前端应用场景（例如 qiankun ），良好的隔离能够降低样式污染 Vue 中的 属性是一个特殊的属性，可以添加到 标签上，以确保样式只应用于当前组件的元素，而不泄露到外部。其工作原理基于以下几个步骤： 1. 唯一属性标识：当你在 标签上加上 属性后，Vue 会为每个组件生成一个唯一的 data 属性（例如 ）。这个属性是随机的，并且对于每个组件都是独一无二的。 2. 后处理 CSS：Vue 通过其构建工具（如 webpack 加上 vue-loader）在编译过程中处理这些 样式。它会自动将上述生成的唯一 data 属性作为选择器添加到所有的 CSS 规则中。这意味着，这些样式只会匹配带有相应 data 属性的元素。 3. 更新模板：在编译组件模板时，Vue 会自动将相应的 data 属性添加到所有模板中的元素上。因此，这些元素将与 样式中的选择器匹配。 4. 防止样式泄露：由于这些 data 属性是唯一的，因此样式只能匹配到拥有相应 data 属性的元素。这就确保了样式只会应用于当前组件内的元素，而不会影响到其他组件的元素。 这样的处理方式确保了组件样式的封闭性，避免了全局样式污染。但是，这种方法也有它的局限性，比如它不能很好地处理子组件的样式覆盖问题，因为子组件的根节点不会被父组件的 scoped 样式所影响。对于这种情况，Vue 提供了深度选择器 或 （在某些 SCSS 预处理器中使用 ），以便可以强制样式穿透到子组件中。 总的来说，Vue 的 CSS 功能是通过自动添加唯一的属性选择器到组件的样式规则，并将这些选择器应用于带有对应属性的元素，从而实现样式封装和隔离的。 CSS Modules 是一种 CSS 文件模块化方案，它的主要目标是解决全局 CSS 命名冲突的问题，从而实现样式的局部作用域。这种方法在 React 社区中非常流行。其工作原理基于以下几个步骤： 1. 生成唯一类名：CSS Modules 在构建过程中，会为每个 CSS 类名生成一个唯一的、全局唯一的标识符。这个标识符通常是由原始类名和一些其他信息（如文件名、路径、哈希值等）组成的，确保了在全局范围内的唯一性。 2. 构建 CSS 文件：CSS Modules 将修改过的类名写入构建后的 CSS 文件。这样，每个组件的样式都有自己的唯一类名，避免了不同组件间的样式冲突。 3. 导出类名映射：CSS Modules 会导出一个 JavaScript 对象，这个对象是原始类名到生成的唯一类名的映射。这就是为什么我们可以使用 这样的语法来导入 CSS Module， 对象就包含了这个映射。 4. 使用类名：在 React 组件中，我们可以使用这个映射来为元素应用样式。例如，，这里的 就是生成的唯一类名。 这种方法的优点是可以确保样式的封装性，避免了全局样式污染。缺点是需要配置构建工具（如 webpack），并且不支持直接在 CSS 中使用其他组件的样式，因为每个组件的样式都是封装的。 CSS Modules 的原理就是通过在构建过程中为每个 CSS 类名生成一个全局唯一的标识符，然后在 JavaScript 中使用这个标识符来应用样式，从而实现样式的局部作用域。 不方便做样式覆盖 例如：需要对 ant-table 做样式覆盖，则需要 的 jsx 显示覆盖，更期望只对外层的 css 做类似 Vue 的 scoped，而不是每个使用的地方都写 ； 是一个在 React 和其他 JavaScript 库中实现 CSS-in-JS 的库。它使用 JavaScript 来编写 CSS，使得组件的样式和逻辑可以在同一个地方定义，而且可以利用 JavaScript 的全部功能来创建动态的样式。 的样式隔离原理基于以下几个步骤： 1. 创建样式化组件：首先，你会使用 提供的 API 来创建一个样式化组件。这个 API 接收一个模板字符串，其中包含你的 CSS 规则，然后返回一个新的 React 组件。这个新的组件会将这些 CSS 规则应用到它呈现的元素上。 2. 生成唯一类名：当你创建一个样式化组件时， 会为这个组件生成一个唯一的类名。这个类名是全局唯一的，因此不会与其他组件的样式发生冲突。 3. 插入样式： 会将你的 CSS 规则插入到一个样式标签中，并且将生成的唯一类名添加到这些规则中。这个样式标签通常被插入到 元素中。 4. 应用样式：当你使用样式化组件时，它会将生成的唯一类名添加到呈现的元素的 属性中。这样，这个元素就会接收到你在样式化组件中定义的样式。 这种方法的优点是，它不仅可以实现样式的封装性，而且可以利用 JavaScript 的全部功能来创建动态的、复杂的样式。它的缺点是，可能会增加运行时的复杂性，因为样式是在运行时插入到文档中的，而不是在构建时。此外，它也需要一些学习成本，因为你需要习惯在 JavaScript 中编写 CSS。 总的来说， 的原理是通过在 JavaScript 中创建样式化组件，然后为每个组件生成一个唯一的类名，将这个类名和对应的 CSS 规则插入到文档中，从而实现样式的封装和隔离。",
    "url": "/blog/css 隔离",
    "path": "tech/frontend/css 隔离.md"
  },
  {
    "title": "为什么 ref 没法直接传递？",
    "content": "可能原因： 1. 封装：React 强调组件的封装，即组件应该管理和控制自己的状态和行为。 是访问组件内部 DOM 节点的一种方式，如果 能够像普通的 prop 那样传递，那么组件的内部节点就可以被外部的父组件随意访问和修改，这破坏了封装性。 2. 所有权原则：在 React 中，每个 有一个拥有者 —— 创建它的组件。这个原则确保了 的使用是明确和可控的。如果 能够像普通的 prop 那样自由传递，那么它的所有权就会变得模糊，可能导致不同组件意外地共享对同一个 DOM 节点的引用。 3. 抽象泄漏：组件应该暴露出一个明确的 API，而 直接关联到 DOM 节点或组件实例，这意味着组件的内部实现细节被暴露给了父组件。这种抽象泄漏可能会导致父组件过分依赖子组件的内部结构，使得重构变得困难。 4. 使用 的明确性：通过 明确地传递 ，组件库的作者可以决定哪些内部元素可以被父组件访问。这样做可以确保 的使用是有意为之，并且对组件的内部结构有清晰的认识。 总之， 没法直接传递是为了保持组件的封装性，维护组件间清晰的边界，并确保组件的抽象不被破坏。当需要将 传递给子组件时，React 提供了 API 作为一种明确和有控制的方式来实现这一点。 在这个简化的实现中， 函数返回一个新的组件，这个组件接收一个特殊的属性 。这个属性是父组件传递下来的 。然后， 函数中的类组件将 作为参数传递给 函数。 在 组件中，我们创建了一个 ref () 并将其作为 属性传递给 。因为 是用我们的简化版 创建的，它能够接收并使用这个 。 这个简化的实现演示了 的核心概念，但请注意，这并不是 React 实际上如何实现 的。React的实现包含更多的细节和优化，以确保正确处理生命周期方法、hooks 等。如果你想要在生产环境中使用 ，请使用 React 提供的官方 API。",
    "url": "/blog/forwardRef 的思考",
    "path": "tech/frontend/forwardRef 的思考.md"
  },
  {
    "title": "框架设计的思考",
    "content": "（时间：2021-12-2，最后更新时间2022-1-13） 文章内容主要基于 + 自身对后的思考总结 大致种类： 1. Mutable vs Immutable 2. Dependency Tracking vs Dirty Tracking 3. Reactivity vs Simulated Reactivity eg: * vue偏向OOP(Object Oriented Program)编程模型; 驱动视图偏向data mutable tracking * React偏向FP(Functioanl Program)编程模型；驱动视图偏向data immutable快照更新 * Angular Dirty Tracking * Svelte 偏向编译时的 Dirty Tracking 大致种类： 1. JSX vs Template （动态渲染函数和基于静态字符串的复杂的 Vue 表达式） 2. Expressiveness vs Raw Perf （表现力和原生性能） 3. Runtime Scheduling vs AOT（运行时调度和提前优化） JSX 表现力强，但是没法做静态编译优化 Template 失去了很多表现力，受限于模板语法，但是可以做静态编译优化 eg: * vue jsx/Virtual-DOM + template，编译成render函数 * 主要方向是 template 的静态优化 * react jsx/Virtual-DOM，编译成render函数 * 主要方向是 JSX + 运行时调度优化，提升用户感知性能，例如：时间切片（调度也耗时） * angular 基于template的，它们将模板编译成相对较低级别的指令来进行内容渲染，基于Incremental DOM更新（相对Virtual-DOM diff节省内存，同时更费CPU，所以是一个CPU跟内存的取舍） * 主要方向是：待补充 * svelte（一种编译器框架） 编译生成指令更新，基本无runtime调度 * 缺点：包大小会随着工程增长而成比较陡峭线性增长；受限于模板语法，所以有很多心智负担？级别越低的编译输出，很难将你的自定义操作与它进行挂钩，就好比你无法使用 C 语言去调试你的汇编代码 * 主要方向 AOT优化，缩小编译体积，例如对编译指令大小的优化 react < vue编译时 < svelte react > vue运行时 > svelte 运行时模板一次性依赖收集 ~~运行时关联变量更新操作与组件的更新函数，变量变动时触发对应组件的更新函数~~ 1. reactive只能传入对象，方便数据的访问； 2. ref 能传任意值，会创建一个 ref 对象，主要暴露 .value 属性，对value是Object的情况会用 reactive再次封装； 所以 ref 可以理解成对 reactive 的再次封装，处理了 primary reactive场景，同时解决了 对象整体替换的烦恼 1. reactive 是 mutable 模式的数据 tracking； 2. ref 是 immutable 模式的数据 tracking；能传 ref 细粒度的热更新：在同一组件层面还能区分只对样式、模板或者js逻辑更新 轻编译重运行时 运行时反复依赖处理 ~~运行时通过hooks调用顺序关联hooks(eg:setData)跟函数组件，做后续hook调用触发对应函数组件更新~~ 重编译轻运行时 编译时模板一次性依赖收集 ~~编译时通过收集模板对变量的引用顺序编译出真实事件函数与更新函数，做后续事件精准更新~~ 利用浏览器的 microTask 或者 macroTask 做组件树更新的批量处理 1. 基本调度单元? >组件的更新函数 2. 如何调度父组件更新又触发子组件的更新？ > 动态修改调度的数组，保证子组件的update函数能在父组件的更新job中实时push到调度的queue，保证了更新的一致性 >>坑：queue的遍历不建议通过静态遍历方式，例如：forEach，而需要通关过动态遍历方式来做，例如：\"for let of\"；（ps:动静指是否实时对数组长度做求值，只有实时计算才能保证动态改变的调度队列也能被执行） * 优势可能演变成劣势，缺点也可能演变成某种优势；分久必合合久必分； * Vue跟React最大的不同点可能就在于数据的Reactivity（是Immutable还是Mutable）；后续的演变都离不开这点，例子： 1. Vue3的Hooks为了解决.vue文件更贴近web开发者却导致逻辑视图割裂问题；顺带有更好的逻辑复用 + 更好的ts 2. React fiber为了解决 数据Immutable更安全更好预测却导致的更新性能问题 3. React + Mobx 跟 Vue3 Composition-API如此类似 * https://zhuanlan.zhihu.com/p/35046696 * https://zhuanlan.zhihu.com/p/76622839 * https://github.com/wizardpisces/experiment/blob/master/packages/mini-react * https://github.com/wizardpisces/experiment/blob/master/packages/mini-svelte * https://github.com/wizardpisces/experiment/blob/master/packages/mini-vue * https://github.com/vuejs/vue-next * https://github.com/google/incremental-dom",
    "url": "/blog/框架设计的思考",
    "path": "tech/frontend/框架设计的思考.md"
  },
  {
    "title": "hiring",
    "content": "--- title: Hiring --- * 开发经验不限 * 了解小程序跨端加分 * 在自己感兴趣的领域研究的比较深入加分 * 熟悉 Vue SSR，React SSR，小程序 加分 * 英文6级及其以上 加分 * 有一定沟通能力，能清楚表达自己的想法 * 工作时间 10/7/5 * 项目中可能接触到的相关技术 * 打包构建：webpack, rollup, vite 等 * 开发框架：vue, koa, uniapp, Nuxtjs, eggjs/midwayjs 等 * 服务相关：nginx 等 * 部署监控：docker, pm2, alinode 等 * 其他 * 每周羽毛球 * 年度体检 * 带薪年假十天起 * 等 caprice@klook.com",
    "url": "/blog/hiring",
    "path": "thoughts/life/hiring.mdx"
  },
  {
    "title": "个人简介",
    "content": "* 刘泽/男/1992 * github: https://github.com/wizardpisces * email: 1275839779@qq.com * 电话：18514428886 * 工作经历：7年多（去哪儿网 -> 美团网 -> 客路旅行 -> Shopee） * 意向：前端Leader * 对技术感兴趣：会主动了解技术的发展，例如：Vite，Svelte等，同时做一些小的概念实现； 对DSL编译有一定研究，实现过具备一定完备性的SASS编译器； * 热爱分享：有组织过大大小小70多场分享，顺带推动Typescript Vite等技术在公司的落地 * 接触过的技术关键词：Vue Typescript Uniapp Nuxt Koa Eggjs Nginx SASS Webpack Gulp Vite Rollup Docker PM2 Yarn Pnpm React Jquery Backbone etc. * 生活：喜欢羽毛球运动 * 职位：Expert Engineer * 入职时间：2022.04-至今 * 业务：Affiliate * 技术栈：React Typescript Nestjs Vite Webpack Python etc. * 主要负责 Partnership跟ChinaBu两条业务线（Whitelabel Affiliate 小程序 AMP）以及部分基础设施建设 * 人员培养：截止到2022.01，陆续培养了3名manager，分别担任Hotel，WebInfra，ChinaBu前端负责人；（其中2人是较低级别招进来而后培养起来） * 技术栈：Vue Typescript Uniapp Nuxt Koa Eggjs Nginx SASS Webpack Gulp Vite Rollup Docker PM2 Npm Yarn Pnpm etc. * 负责业务：小程序，AMP（ToB分销），Affiliate（ToC引流），Whitelabel（ToB Partner解决方案） * Infra：Klook-UI（ToC组件库） Storybook（产研协作平台），Nuxt（Vue SSR）落地 * 负责业务：小程序，AMP，Affiliate，Whitelabel，HotelIpad项目后面停止，Merchant交接给了其它部门 * Infra：Klook私有库搭建，推进Typescript * 其它：绩效S后晋升 Senior Manager * 负责业务：AMP，Merchant，HotelIpad * Infra：推动Vue在公司内取代Jquery，推动Webpack替换Gulp * 相关业务：部分主站业务，AMP，Merchant * 其它：主要做系统的业务迭代+技术重构 * 职位：平台事业群-前端工程师 * 入职时间：2015.06-2016.04 * 业务：店铺推广，商家入驻 * 技术：Gulp React Backbone Jquery * 职位：酒店事业部-前端工程师 * 入职时间：2014.06-2015.06 * 业务：供应商系统，admin * 技术：Fekit Backbone Jquery Zepto * 时间：2021.08-2022.01 * 功能：affiliate合作联盟渠道（主要包括portal，widget，server三部分），主要为C端引流， * 目的：提升系统的可维护性、稳定性、dev效率 * 角色：前端负责人（技术选型，方案设计，平稳上线） * 拆分Affiliate为多包仓库（同时配合改动dev流程，部署流程） * Widget投放方式升级：从纯静态切换成动静结合 * widget全面单元测试覆盖，同时解决一些node性能问题：拆分middleware，多语言深拷贝去除等 * 成果：稳定上线，达到升级目的 * 时间：2021.01-至今 * 功能：为商家提供类似C端购买能力，但需要满足商户的配置要求，例如主题等 * 目的：保证业务功能更新的及时性、节省人力 * 角色：前端负责人（方案设计，项目推动） * 设计whitelabel的平台化方案 * 寻找契机推动各个业务线配合接入 * 成果：已经接入2个业务线，ToC功能更新直接反应到whitelabel平台 * 时间：2019.12-至今 * 功能：web前端组件库 * 目的：提升开发跟设计效率，人员培养 * 角色：组件库负责人（技术选型，初版落地，后面交接给团队成员迭代，培养潜在Infra负责人） * 带团队从0到n建立web组件库，支持mobile/PC，支持SSR * 实行边迭代边落地路线 * 推动组件库在各个业务线落地（设计+开发侧） * 推动设计流程配合组件库的迭代以及设计稿的应用 * 成果：组件已经应用到各个业务线上千次，设计团队也产生一份对应的组件库 * 时间：2020.03-2021.06 * 功能：技术改造 * 目的：普及新的开发思路，提升开发效率，统一技术栈 * 角色：框架升级负责人之一（技术选型，先行试点，推动落地） * 落地Nuxt框架的工程结构，有发现Nuxt库的BUG并提过 * 推动TS的渐进式替换 * 推动自适应开发理念取代PC/Mobile完全分离协作模式，节省人力 * 成果：各业务线完成技术升级，新的开发思维得到普及，代码健壮性增强 * 时间：2019.06-2019.08 * 功能：面向中国大陆市场的销售渠道 * 目的：补全销售渠道 * 角色：前端负责人（技术选型，初版上线，后续交接给团队成员迭代，培养潜在ChinaBU人才） * 带团队从0到1创建klook小程序 * 推动设计开发同步进行 * 考虑后续whitelabel MVP的上线时间 * 成果：初版超预期顺利发布，技术选型推动后续跨端水到渠成，（），简单改造后Whitelabel MVP也上线 华中农业大学 本科 计算机科学与技术 2010-2014 英语六级",
    "url": "/blog/self-introduction",
    "path": "thoughts/life/self-introduction.mdx"
  },
  {
    "title": "todo",
    "content": "* 龙脊：东南端 * 南丫島：最南端，看起来需要坐船过去 * 鳳凰徑：西南端 * 大帽山：中部 * 鶴咀 登山步道 石澳 （跟龙脊一个点）",
    "url": "/blog/todo",
    "path": "thoughts/life/todo.mdx"
  },
  {
    "title": "世界趋同",
    "content": "--- - 过去，手机形态多种多样，有滑盖、翻盖、直板、全键盘。 - 现在，几乎所有手机都是黑色/白色玻璃+直板+全面屏，连摄像头排列都趋同。 - 折叠屏虽然是创新，但也只是回归“翻盖”形态。 - 过去，ThinkPad、MacBook、VAIO 各有风格。 - 现在，大多数高端轻薄本都类似MacBook：铝合金机身 + 超窄边框 + 大触控板，甚至连 Windows 电脑的 UI 也趋向 macOS 风格。 - 过去，车企设计风格多样，有肌肉车、流线型、方盒子等。 - 现在，电动车因空气动力学需求，普遍采用流线型+隐藏式门把手+封闭式前脸，例如特斯拉、比亚迪、极氪的设计越来越像。 - 过去，智能手表形态多样（方形、圆形、异形）。 - 现在，大多数高端手表要么趋向 Apple Watch（方形）、要么趋向三星 Galaxy Watch（圆形）。 --- - 早期：Facebook、Twitter、Instagram、TikTok 界面各有不同。 - 现在：几乎所有 App 都有短视频推荐流（Instagram Reels、YouTube Shorts、微博 Story）。 - 各大社交平台开始采用类似的“推荐+算法”机制。 - 过去，VS Code、JetBrains、Vim、Emacs 等编辑器风格各异。 - 现在，几乎所有主流编辑器都支持： - AI 补全（Copilot, Tabnine, Cursor） - Agent 交互 - GitHub 代码搜索集成 - 过去：搜索引擎主要是基于关键字索引的传统网页搜索。 - 现在：所有搜索引擎（Google, Bing, 百度等）都在转向AI 搜索+摘要生成+对话式回答，最终形态趋同。 --- - 过去：游戏有单机买断制、点卡制、免费试玩等多种商业模式。 - 现在：越来越多游戏采用免费+内购（微交易）模式，例如： - 《英雄联盟》《王者荣耀》《原神》等。 - 甚至许多原本卖断制的游戏（如 FIFA、使命召唤）都转向“战斗通行证”+ 赛季收费模式。 - 过去：短视频和直播是分开的，如 YouTube 主要是长视频，Twitch 主要是游戏直播。 - 现在：所有平台都在做短视频+直播+带货，比如： - 抖音和快手直播带货 - B 站加了短视频、直播 - YouTube Shorts - 过去：电影和电视剧在电影院、电视台播放。 - 现在：Netflix、Disney+、HBO Max、爱奇艺等平台都采用订阅制+独家内容，形成趋同。 --- - 过去：电影往往是独立作品，系列较少。 - 现在：几乎所有大 IP 都在拍续集、重启、宇宙化，比如： - 漫威宇宙、DC 宇宙、星战宇宙、哈利波特宇宙 - 国产剧也开始流行“季播化”（《庆余年》《隐秘的角落》） - 过去：欧美流行音乐占据主流，各国风格迥异。 - 现在：K-Pop 影响力扩大，全球流行音乐逐渐向 K-Pop 的舞蹈+视觉包装+团体模式靠拢，比如： - 中国、日本的偶像团体开始模仿 K-Pop 练习生制度。 - 欧美流行歌手（如 Lady Gaga, BTS 合作）借鉴 K-Pop 营销方式。 - 过去：美剧更快节奏，国产剧更慢热。 - 现在：国产剧越来越采用“快节奏+美剧式叙事”，例如： - 《开端》《隐秘的角落》借鉴美剧的紧凑叙事。 - 《三体》剧版采用好莱坞式的制作水准。 --- - 过去：不同国家的建筑风格迥异（如中国四合院、日本町屋、欧洲哥特式建筑）。 - 现在：现代建筑越来越趋同，高楼大厦以玻璃幕墙+极简线条为主，如： - 迪拜、上海、纽约的摩天大楼风格越来越像。 - 连国内的住宅小区也趋向统一的现代风格（高层+玻璃窗）。 - 过去：不同国家的流行趋势差异大。 - 现在： - 全球流行的极简风、机能风（Techwear）、复古 Y2K几乎同步。 - 运动品牌风潮全球一致，如 Nike、Adidas、New Balance 在全球大热。 - 过去：不同国家的饮食文化差异很大。 - 现在： - 全球范围内咖啡文化流行，星巴克、瑞幸等品牌让咖啡成为日常饮品。 - 各国快餐文化趋同，麦当劳、肯德基的菜单全球相似，甚至出现“亚洲风味汉堡”等融合趋势。 --- 1. 市场竞争 → 低成本、高效率的解决方案会成为行业标准（如黑色汽车、电动车设计）。 2. 技术进步 → 开源技术、算法驱动，导致产品功能和设计趋同（如 AI 补全、搜索 AI 化）。 3. 全球化与信息传播 → 互联网让流行趋势迅速传播，导致文化、审美趋同（如 K-Pop、建筑设计）。 4. 用户习惯养成 → 习惯一旦形成，厂商不愿意冒险改变（如短视频 UI、推荐算法）。 5. 路径依赖 → 过去的成功模式容易被复用，如战斗通行证、流媒体订阅、电影续集化。 世界趋同是一种效率最大化的结果，但也会带来创新空间受限的问题，因此有时候“逆趋同”的小众创新（如赛博朋克风格、复古科技）反而会成为新的趋势突破点。 - gpt-4o",
    "url": "/blog/世界趋同",
    "path": "thoughts/life/世界趋同.md"
  },
  {
    "title": "显示器",
    "content": ">整体上人眼的分辨率大约相当于 7K-8K，但在中央视野的分辨率要高于 8K。意味着 8K 是显示器发展的极限？ 可以用一些物理和感知上的衡量标准来估计人眼的“分辨率”。 人眼的分辨率通常用“角分辨率”来衡量，它指的是眼睛能分辨的最小角度。正常视力（20/20 视力）下，典型的角分辨率是 1 弧分（1 arcminute），也就是 1/60 度。 这意味着，人眼能分辨出 1 弧分的两个点之间的差别。 要将角分辨率转化为类似显示器的“像素”概念，我们可以进行一些估算。如果假设视角宽度为 120 度，视场高度为 60 度，并且视距约为 25 cm（阅读距离），以下是简单的计算： - 水平分辨率：人眼水平视角为 120 度，每度 60 弧分，因此眼睛的理论水平分辨率是 120 × 60 = 7200 弧分。 - 像素等效：如果用像素来衡量，可以假设每弧分为一像素。这样，视野宽度约为 7200 像素。同理，垂直视野为 60 度，分辨率约为 3600 像素。 所以，从分辨率角度看，整个人眼的视野大约相当于 7200 × 3600 像素，约等于 26 兆像素（MP）。 然而，人眼并非在整个视野上都有均匀的分辨率。中央凹是视网膜上分辨率最高的区域，位于眼睛的中心，用于处理精细的视觉信息。在中央凹区域，视网膜上的感光细胞（视锥细胞）密度极高，分辨率也更高。 在中央凹，视力最高的部分可以达到大约 500-600 PPI（像素每英寸） 的感知精度。因此，尽管人眼的总分辨率可能在 26 MP 左右，但在最集中的区域分辨率更高。 人眼和大脑协同工作，通过眼睛的快速移动（称为“扫视”）和大脑的感知机制，将局部高分辨率的图像拼接在一起。这意味着人眼并不总是以固定的分辨率“拍摄”世界，而是通过快速聚焦和大脑补偿形成清晰的感知。 - 角分辨率：1 弧分（1/60 度）。 - 像素等效：大约相当于 7200 x 3600 像素（约 26 MP）。(8K 是 7680 x 4320) - 中央凹：中央视野分辨率较高，达到 500-600 PPI。 - 大脑处理：大脑通过感知和整合提高视觉精度。 > 意味着带鱼屏比 16:9 的屏幕更耗费眼睛？ 视觉系统是大脑中最复杂的系统之一，大约消耗了大脑整体能量的 30%。处理视觉信息需要大脑整合来自视网膜的信号，进行识别、分析、和解码。因此，大脑为了处理复杂的视觉输入，确实需要大量能量来支持： - 信息处理：眼睛捕捉的每一个视觉场景都会生成大量的信息，大脑需要通过视觉皮层和其他认知功能区域进行处理。这包括边缘检测、颜色识别、深度感知等复杂任务。 - 补偿机制：大脑不仅处理即时的视觉输入，还需要进行复杂的推测、模式识别、补偿盲点等操作，这进一步增加了能量需求。 扫视（saccades）是眼睛快速跳动的过程，大约每秒发生3-4次。每次扫视都会让眼睛重新获取新信息，并让大脑重新处理这些新输入。相比长时间盯着一个地方，频繁的扫视会增加以下方面的能量消耗： - 眼部肌肉活动：扫视需要眼部肌肉的持续运动，而长时间保持不动的凝视（fixation）则相对耗费更少的肌肉能量。眼睛的快速移动本身需要动用一系列眼外肌来控制方向和焦点。 - 大脑视觉处理负荷：每次扫视时，大脑都要重新处理新的视觉输入，并整合之前的信息。这增加了视觉皮层的处理负担。而当我们固定盯着某个地方时，大脑只需维持当前视觉图像的稳定性，而无需处理大量的新信息。 - 抑制机制：在扫视期间，视觉输入会被短暂抑制（称为扫视抑制，Saccadic Suppression），以防止大脑处理模糊或跳跃的画面。这种抑制机制也是大脑额外的工作，并需要一定的能量。 因此，频繁的扫视相比固定注视，确实更耗费能量，因为它不仅需要眼部肌肉的运动，还会给大脑带来更多处理负荷。 - 长时间注视一个点： - 眼睛的肌肉活动最小，只需维持轻微的聚焦。 - 大脑不需要处理太多新信息，只需维持当前视觉图像的稳定性，降低信息处理量。 - 整体能量消耗较低。 - 频繁扫视： - 每次扫视都会涉及眼部肌肉的剧烈运动，消耗更多能量。 - 每次扫视会带来大量新视觉信息，大脑需要不断重新处理、分析和整合，增加能量消耗。 - 需要视觉抑制机制来避免画面模糊，大脑需要进一步协调视觉皮层的活动。 虽然扫视会消耗更多能量，但大脑也有一些高效的机制来节省资源。比如，在固定注视的过程中，大脑会将不太重要的部分模糊化或降低处理优先级，集中精力处理感兴趣或重要的区域。在不需要大量精细处理的情况下，大脑可以调整资源分配，节省能量。 此外，注意力的集中也会增加能量消耗。当你盯着一个特定的点或物体时，大脑需要调动更多的资源来处理该区域的细节，这也可能导致能量消耗增加。但如果注意力较低或视觉任务较简单，盯着一个地方的能量消耗仍然比频繁扫视低。 - 扫视比固定注视更耗能：由于眼球运动和大脑处理新信息的复杂性，扫视会比盯着一个地方消耗更多能量。 - 大脑的视觉处理耗费大量能量：即便是静止不动，大脑也需要持续处理视觉输入，并维持视觉的稳定。 - 补偿机制进一步增加能量消耗：大脑通过整合、预测和填补视觉信息来优化感知，这些复杂的处理过程也增加了能量需求。 > 当刷新率达到某个高水平后，比如超过 144Hz，视觉系统很难再区分帧与帧之间的差异 视觉残留（Persistence of Vision）是指当物体的图像在视网膜上消失后，图像的感知会在短时间内继续保留，大脑会对该图像的印象延续数毫秒。这个现象帮助我们将一系列快速切换的静态图像感知为连续运动。 1. 视网膜滞留：当光线刺激眼睛时，视网膜上的感光细胞会产生电信号，传递到大脑。然而，光线消失后，这些信号并不会立即停止，而是持续一小段时间（通常是几毫秒）。这意味着，即使物体不再被直接看到，它的图像仍会在视网膜上“停留”片刻。 2. 运动感知：视觉残留是我们感知连续运动的基础。比如在电影或动画中，每秒只展示24帧静态图片，但由于视觉残留，我们会感知到流畅的连续画面，而不是一帧一帧的闪动。 3. 视觉补偿：大脑利用视觉残留来弥补快速运动或切换的画面空隙，让我们感知到一个连贯的世界。这是大脑补偿机制的一部分，有助于避免画面跳动或断裂感。 - 电影和电视：电影通常播放速度为每秒24帧。视觉残留让这些帧不会显得分散，而是形成平滑的运动效果。 - 光迹现象：在夜晚挥舞光源时，会看到光的轨迹，这是因为光的图像在视网膜上残留了一段时间。 视觉残留是人眼和大脑的一种生理现象，帮助我们将快速切换的静态图像整合为连续的运动感知。它在视频、动画以及现实世界的运动感知中起到关键作用，避免了我们看到闪烁的画面或不连续的影像。 > 人体的残留现象不仅限于视觉和听觉，还涉及触觉、嗅觉、味觉、运动及情绪等多个方面。这些现象反映了身体感官系统在处理信息时的复杂性，以及我们如何整合和体验感知世界。 听觉残留是一种声音在消失后仍能被感知的现象，类似于视觉残留，它对我们理解语言、欣赏音乐和维持听觉流畅性具有重要作用。 听觉残留比视觉残留更加短暂，并非完全因为信息量的不同，而是由于以下几个因素： 视觉和听觉在信息处理上存在本质差异。视觉通常以较大的空间分辨率接收大量的静态或动态图像，且每个视觉帧包含丰富的细节。相比之下，听觉处理的是时间维度上的连续波形，这种信息的传递和感知速度比视觉更快。 - 视觉通常需要大脑更长时间来整合和处理图像，因此视觉残留时间稍长。 - 听觉需要实时、精确地处理声音，尤其在语音和音乐中，音频信号必须被快速解析和传递给大脑，这使得听觉残留时间更短，以避免混淆连续的声音信号。 听觉是一个极为敏感的时间依赖感官。声音的频率、节奏和持续时间决定了我们如何理解语言和音乐，因此需要快速、精准的处理。 - 听觉残留时间较短，避免产生音符或语言音节间的混淆。 - 如果听觉残留时间过长，可能会导致声音重叠或延迟感，影响听觉感知的清晰度和流畅性。 为了处理不同类型的感知信息，大脑对听觉和视觉信号采取了不同的优化策略： - 视觉可以容忍较大的时间延迟，特别是在电影、动画等领域，通过残留现象让画面流畅。 - 听觉必须保持精确的时间节奏。短暂的残留允许听觉系统在快速接收新的声音信号的同时保持信息传递的连贯性。 虽然声音在时间维度上是动态的，但它的信息密度通常低于视觉。例如，一幅静态图像可能包含数百万像素的信息，而一段音频信号在相同时间内传递的细节较少。因此，听觉系统的处理时间较短。 听觉残留较短并非仅仅因为信息量较小，而更多是由于感官处理机制、时间敏感性、大脑优化策略等多方面的因素共同作用。听觉系统需要快速准确地处理声音，以确保信息传递的清晰和连贯性。",
    "url": "/blog/人眼 - 晕3D",
    "path": "thoughts/life/人眼 - 晕3D.md"
  },
  {
    "title": "因果思维型与目标导向型",
    "content": "比如说今天中午得少吃点，A 同学问是因为肚子不舒服么，B 同学问是因为下午有团建得留肚子吃大餐么； | 思维方式 | 核心区别 | |---|---| | 因果型思维（A 同学） | 关注 「过去或当前的原因」，试图找出导致某个事件发生的根本原因。 | | 目标导向思维（B 同学） | 关注 「未来的目的或影响」，试图理解某个行为背后的目标或意图。 | --- | 维度 | 因果型思维 | 目标导向思维 | |---|---|---| | 思考方向 | 从已发生的事实出发，推导其原因。 | 从未来的目标出发，推导当前行为的目的。 | | 关注点 | 过去和当前：为什么会这样？ | 未来：接下来会怎样？ | | 典型问题 | 「为什么你今天中午吃得少？是不是肚子不舒服？」 | 「你是不是为了下午团建少吃点？」 | | 应用场景 | 追根溯源、问题分析（如故障排查、历史复盘）。 | 规划目标、预判趋势（如产品设计、市场战略）。 | | 思维特点 | 以事实和数据为依据，强调因果关系。 | 以可能性和目的为依据，强调行动的意义。 | | 优点 | 严谨可靠，有助于理解过去和当前的问题。 | 前瞻性强，有助于做出面向未来的决策。 | | 缺点 | 容易局限在已有经验，缺乏创新性。 | 可能忽视现实条件，导致计划不切实际。 | --- --- 🔹 归纳思维（Inductive Thinking） → 从具体案例总结一般规律 🔹 演绎思维（Deductive Thinking） → 从一般规律推导具体结论 | 维度 | 归纳思维 | 演绎思维 | |---|---|---| | 核心逻辑 | 从个别事实推导出普遍结论 | 从已有规则推导出具体结论 | | 思考方向 | 具体 → 抽象 | 抽象 → 具体 | | 例子 | 看到很多公司因 AI 转型成功，推测 AI 会是大趋势 | 知道 AI 是大趋势，所以建议公司投资 AI | | 优点 | 适用于创新、发现新趋势 | 适用于逻辑推理、系统性决策 | | 缺点 | 可能得出错误的普遍结论（以偏概全） | 依赖前提的正确性，如果基础假设错了，结论也错 | ✅ 应用：数据分析（归纳），法律推理（演绎） 🎯 最佳策略：归纳找规律，演绎验证推理 --- 🔹 第一性原理思维（First Principles Thinking） → 追本溯源，拆解问题到最基本的事实 🔹 类比思维（Analogical Thinking） → 通过已有经验和案例进行推测 | 维度 | 第一性原理思维 | 类比思维 | |---|---|---| | 核心逻辑 | 拆解问题到最基本的事实和原理，再从底层重新构建 | 借鉴已有经验，通过相似案例推测 | | 思考方式 | 重新发明轮子 | 改良已有轮子 | | 例子 | 造电动汽车时，重新思考汽车的本质，优化能源结构（特斯拉） | 造电动汽车时，借鉴燃油车的设计逻辑（传统车企） | | 优点 | 适用于突破性创新 | 适用于快速决策和优化 | | 缺点 | 需要较高的认知成本 | 可能受到路径依赖，限制创新 | ✅ 应用：创业创新（第一性原理），产品优化（类比思维） 🎯 最佳策略：创新时用第一性原理，执行时用类比思维 --- 🔹 发散思维（Divergent Thinking） → 追求多种可能性，强调创造力 🔹 聚合思维（Convergent Thinking） → 追求最佳解决方案，强调逻辑性 | 维度 | 发散思维 | 聚合思维 | |---|---|---| | 核心逻辑 | 产生多个不同的解决方案 | 从多个选项中选择最优解 | | 思考方向 | 横向拓展 | 纵向收敛 | | 例子 | 头脑风暴：如何提高产品用户体验？（不限方案） | 评估所有方案，选出最可行的执行方案 | | 优点 | 适用于创新和创造性问题 | 适用于优化和执行 | | 缺点 | 可能缺乏可行性 | 可能局限于已有的方案，错失创新 | ✅ 应用：设计思维（发散），决策分析（聚合） 🎯 最佳策略：先发散，再聚合，即先创造多个方案，再筛选出最佳方案 --- 🔹 快思考（System 1 Thinking） → 直觉反应，依赖经验和情绪 🔹 慢思考（System 2 Thinking） → 深度分析，依赖逻辑和理性 | 维度 | 快思考（直觉） | 慢思考（理性） | |---|---|---| | 核心逻辑 | 自动化、直觉判断，快速决策 | 深度思考、逻辑推理，慢速决策 | | 特点 | 省时省力，但容易受偏见影响 | 精确严谨，但耗费认知资源 | | 例子 | 突然看到蛇，会立刻躲开（本能） | 评估投资策略，需要计算收益风险 | | 优点 | 适用于日常决策、紧急情况 | 适用于复杂决策、需要深度分析的情况 | | 缺点 | 可能受到情绪、偏见影响 | 可能决策过慢，消耗精力 | ✅ 应用：驾驶、对话（快思考），战略决策、科学研究（慢思考） 🎯 最佳策略：大部分决策用快思考，关键决策用慢思考 --- | 对比维度 | 思维方式 1 | 思维方式 2 | 适用场景 | |---|---|---|---| | 因果 vs. 目标 | 关注「原因」（过去） | 关注「目的」（未来） | 问题分析 vs. 目标规划 | | 归纳 vs. 演绎 | 具体 → 规律 | 规律 → 具体 | 数据分析 vs. 逻辑推理 | | 第一性原理 vs. 类比 | 追本溯源，重构思维 | 借鉴经验，优化方案 | 突破创新 vs. 快速执行 | | 发散 vs. 聚合 | 产生新想法 | 筛选最佳方案 | 创意构思 vs. 方案落地 | | 快思考 vs. 慢思考 | 直觉、快速决策 | 逻辑、深度思考 | 日常判断 vs. 复杂问题 | 如何运用这些思维方式？ 🔹 解决问题时，结合 因果分析 + 目标导向 🔹 创新时，用 第一性原理 思考，再用 类比思维 执行 🔹 做决策时，先 发散思维 产生选项，再 聚合思维 选择最优方案 🔹 日常决策用快思考，但面对重要决策时要切换慢思考",
    "url": "/blog/因果思维型与目标导向型",
    "path": "thoughts/life/因果思维型与目标导向型.md"
  },
  {
    "title": "行为决定思想",
    "content": "* 斯坦福监狱实验：一个人的行为和心态，并不是完全由自己说的算；所处情境对行为的塑造，比我们想象中的更加深刻；外部环境，也许会让人背弃很多他曾经的价值观 * 洞见 1. 人对一个东西的态度，不是想法决定的，而是行为决定的。一旦有了一个持续的行为，他就会觉得这个行为是对的。 2. 选择性注意：人们只会留意那些自己原本就认同的信息。这也是为什么改变一人那么难。 3. 想法无法改变想法，环境才能改变想法。 * 结论 * 人不是先有想法，才有行为，而是先有行为，进而产生支持这种行为的想法。 * 不要低估环境对你的影响，它有时可以决定你是谁；更不要低估你自己对自己的影响，别让别人决定你是谁。 * 案例： 1. 《黑豹纠察队》：拥有超人能力的阿祖小时候被刻意培养成表演型人格（情感忽视），长大后特别需要别人的认可，假如失去这些，对他来说比死还难受，公司以此来从精神控制阿祖，预防失控；人是社会性动物，只要他身在群体中，就无时不刻不受环境的影响，超人也不例外 费斯廷格认为，当人们的行为与他们的信念或态度发生冲突时，他们会感到不适（即“认知失调”），为了消除这种不适，他们会调整自己的想法来合理化行为。例如，一个人如果做出了某个行为，他们可能会改变之前的态度或信念以使行为看起来合理。 情绪的产生往往是对身体行为的反应。例如，人们先笑，才感到快乐，而不是先感到快乐才笑。 1. 建立写作习惯 * 设定小的、可实现的目标：从每天写几百字开始，而不是设定过于宏大的目标。通过实际行动（即使是小步）不断强化“我是作家”的身份认同。 * 规律性写作：每天在固定时间段安排写作，通过重复行为让大脑逐渐接受并习惯这一活动。这种重复行为会慢慢塑造内心的信念，即“我是一名作家”。 2. 使用认知失调来强化动力 * 根据费斯廷格的认知失调理论，当人们的行为与信念不一致时，他们会感到不适。例如，如果一个人认为自己想成为作家，但又从不写作，这种不协调感会促使他要么调整想法（放弃成为作家的目标），要么调整行为（开始写作）。为了减少这种不适，可以鼓励他先进行写作行为，久而久之，他会调整内心的信念，进一步强化作家身份。 3. 奖励和反馈机制 * 自我奖励：为每次完成写作任务设置奖励，比如看一部喜欢的电影，或者吃一顿好吃的餐食。通过这种行为与奖励的关联，强化写作的积极意义。 * 外部反馈：让他将写作成果分享给朋友、写作群体，获得外部的认可和反馈。正面的评价和认可会进一步促使他保持写作的行为，并强化成为作家的自我认知。 4. 情境控制和行为提示 * 行为暗示：在写作环境中放置一些与作家身份相关的物品，比如书籍、文具，或与写作相关的目标清单等，通过这些视觉暗示刺激写作行为。 * 消除干扰：为自己设定一个专注的写作环境，减少外界干扰。这样可以增加实际写作行为发生的频率，进一步培养写作习惯。 通过这些方式，想成为作家的人可以从日常的小行为入手，逐步通过行为带动内心的想法，进而形成良好的写作习惯和作家身份认同。 表演型人格（Histrionic Personality Disorder, HPD）是一种人格障碍，特征在于个体极度寻求他人关注和赞美，往往通过夸张的情感表达和表演来引起他人的注意。表现出这种人格特质的人通常有以下特点： 1. 过度的情绪表达：他们通常以戏剧化、夸张的方式表达情绪，可能对日常事件作出强烈的反应，似乎比实际情况更加情绪化。 2. 寻求注意：这类人经常会感到不被关注时很不安，因此会通过各种手段，如穿着夸张、行为浮夸，甚至是制造戏剧性事件，来确保自己成为焦点。 3. 对他人情感的敏感：尽管他们很关注自己是否得到关注，却往往难以真正体会他人的情感，而是将他人的反应视为自己吸引注意力的一部分。 4. 人际关系的肤浅和短暂：表演型人格的人可能会试图通过快速建立密切关系来获得他人的关注，但这些关系通常不够稳固，容易破裂，因为他们更多地关心外在的关注，而非关系本身的深度。 5. 易受外界影响：他们通常容易受到他人意见和环境的影响，因为他们的行为往往是为了取悦他人，而非基于自己的内心需求。 这种人格障碍属于B型人格障碍，与自恋型、边缘型人格障碍属于同一类型。表演型人格障碍的根本问题在于缺乏内在的自我认同，个体需要通过外界的认可来确认自己的价值。 治疗通常包括心理治疗，尤其是认知行为疗法（CBT），帮助患者意识到其行为模式并学会更健康的情绪表达方式。 表演型人格障碍（Histrionic Personality Disorder, HPD）的形成可能受到多种因素的影响，通常是遗传、早期生活经历和环境的复杂交互结果。以下是主要的原因和机制： 有研究表明，表演型人格障碍在家庭中有一定的遗传性。如果家族成员中有人患有类似的情感障碍或人格障碍，其他成员更容易发展出表演型人格。这表明，某些人格特质（如情绪不稳定或外向性格）可能通过基因遗传影响人格形成。 早期儿童时期的经历对人格发展有重要影响，尤其是情感的表达和处理方式。常见的经历包括： - 父母的过度溺爱：如果孩子在成长过程中被父母过度关注，或父母不断强化他们的情感表达，孩子可能会学会通过夸张的行为和情绪来获得关注。 - 情感忽视：相反，某些孩子可能在情感需求上被忽视，因而学会通过引起他人关注的方式，来填补他们内心的空虚感。 - 不稳定的家庭关系：家庭中的不稳定因素，如冲突、忽视或不一致的养育方式，可能促使孩子发展出过度依赖外界认可的行为模式。 社会和文化的影响也可以促进表演型人格的形成。某些文化或社会环境可能强化了外表、形象和表现自我重要性的价值观。例如，现代媒体和社交平台强调“被看到”以及通过夸张或戏剧化的方式展示自我，可能会助长这一类人格特质的发展。",
    "url": "/blog/心理学 - 行为决定思想",
    "path": "thoughts/life/心理学 - 行为决定思想.md"
  },
  {
    "title": "改革的目标",
    "content": "改革在一定程度上涉及到降低交易成本和组织成本。具体来说，改革旨在通过优化资源配置、提升效率、减少不必要的制度性障碍，来推动经济和社会的进步。以下是关于改革如何影响交易成本和组织成本的简要分析： 交易成本是指在市场交易过程中发生的各种成本，包括信息成本、谈判成本、监督成本等。改革可以通过多种方式降低交易成本： - 制度创新：通过简化繁琐的行政程序、减少不必要的审批和监管环节，使得市场主体的交易更为高效，减少不确定性。 - 透明度和法制建设：建立和完善法律法规，增强市场透明度和可预见性，降低信息不对称带来的风险。 - 技术进步：推行数字化、信息化等技术手段，简化交易流程，减少交易过程中的中介成本。例如，电子政务的推行使得政府与企业之间的交易更加高效。 例子：放松对某些行业的管制，允许更多市场主体进入，可以减少由于政府垄断带来的交易壁垒，使得市场交易更加活跃和高效。 组织成本是指企业或政府在组织内部运作中为了协调生产、管理、监督而发生的各种成本。改革可以通过优化管理、改善制度设计来降低这些成本： - 简政放权：政府通过减少对经济活动的过度干预，可以让企业在市场中更加灵活地运营，减少过于集中的管理成本。 - 结构调整：通过改革政府或企业的组织结构，使得决策链条更加简化、管理层级更加精简，从而降低内部协调和管理的成本。 - 鼓励创新与效率：通过推动市场化改革，促使企业提升内部管理效率，减少资源浪费。 例子：国有企业改革通过引入市场机制，削减冗余人员、改进治理结构，能够减少组织成本，提升企业竞争力。 除了交易成本和组织成本，改革还包括诸多其他目标： - 提高生产力：通过改革释放生产要素的潜力，推动技术进步和创新。 - 促进公平和包容性发展：通过教育、医疗、养老等方面的社会改革，减少社会不平等，促进可持续发展。 - 环境可持续性：推动绿色改革，减少对环境的破坏，促进资源节约型和环境友好型社会的建设。 虽然改革确实可以显著降低交易成本和组织成本，但其本质在于通过系统性变革来提高整个经济和社会体系的效率、适应性和公平性。",
    "url": "/blog/改革的目标",
    "path": "thoughts/life/改革的目标.md"
  },
  {
    "title": "精英过剩：起源与历史镜鉴",
    "content": "当然可以。以下是整理后的文章： “精英过剩”（Elite Overproduction）是指一个社会中受过良好教育、有抱负并希望获得高社会地位的人数，远超出实际能够容纳的职位或资源总量。这会导致精英群体之间的激烈竞争，最终引发政治分裂、社会动荡，甚至革命与政权更替。 这一现象并非仅发生在当代，而是有深厚的历史根基。现代社会的高等教育扩张、大量培养“准精英”人群，加剧了这种结构性矛盾。 --- 以美国为例，早在2010年前后，一些社会学家与历史学家就开始警告：美国培养出来的律师、博士、媒体人、政客数量远超市场所需。结果： * 高学历者找不到匹配岗位； * 大量年轻人背负高额学生贷款； * 中产阶级“下沉”； * 社会上升通道受阻，焦虑蔓延。 这种情况并非美国独有，中国近年来也显现类似趋势： * 高校扩招多年后，每年数百万应届大学生涌入就业市场； * “考公热”“考研热”成为多数人的出路； * 很多人即使获得研究生学历，也只能做基层工作或转行。 这类现象的背后，其实是精英培养与岗位供给严重错配，资源不足，“蛋糕不够分”。 --- 精英过剩通常源于以下几个方面： 1. 教育通胀：高等教育大众化后，“本科”不再是门槛，而是起点。人们不断通过学历升级维持竞争力，造成教育军备竞赛。 2. 岗位结构滞后：社会并没有同步创造那么多高质量的、能匹配“精英期望”的岗位。 3. 上升通道受阻：传统的晋升路径（比如考编、升职）变得狭窄或被既得利益集团垄断。 4. 路径依赖问题：上一代的成功经验（考大学、考研究生）仍被沿用，但社会已无法承接如此规模的“上升预期”。 --- 结构性矛盾会酿成深层危机，主要表现为： * 社会撕裂：精英内部的竞争逐渐转化为政治、意识形态上的分裂。 * 去合法性化：部分精英质疑现有体制的公平性，出现激进派、民粹派、革命派。 * 反制度情绪上升：精英无法向上，就可能掀桌子、煽动基层群众不满。 * 政治周期动荡：旧精英集团受到新精英挑战，社会陷入政权不稳或制度更替的循环。 --- 解决精英过剩，并非简单的“减少教育”，而是需要结构性改革和路径多样化。 1. 职教分流（职普分流） 高中阶段将一部分学生引导至职业教育体系，试图打造“技术精英”“实用型人才”，缓解高考内卷和大学挤兑。 2. 扩展基层岗位、城乡均衡政策 通过扩张基层公务员、乡村振兴项目，为精英提供新的落脚点。 3. 鼓励创新创业、产业多元化发展 让人才向二三线城市、数字经济、制造业、技术服务等新兴领域扩散。 * 德国“双元制教育”：早期将学生分流至普通高中与职业高中，后者与企业深度合作，就业对口率极高。 * 日本“终身雇佣制”的松动：让人才流动更加自由，鼓励中小企业吸纳人力。 * 美国社区大学体系：提供更便宜、更灵活的教育路径，避免所有人盲目涌向名校。 --- 当前问题的棘手之处在于：很多既得利益者本身就是通过旧路径（大学—研究生—官僚/学术/媒体）成功上位的，因此改革面临制度内阻力。 这也意味着：继续走“小学→初中→高中→大学→研究生→博士→高位”的道路，将变得更加拥挤和激烈。未来可能需要： * 开辟“横向路径”：鼓励更多人成为产业精英、技能人才、数字劳动力。 * 打破学历崇拜：构建“去学历中心化”的社会评价体系。 * 改革教育资源分配：真正实现因材施教，而非一刀切的升学率导向。 --- 这一理论的重要支撑来自历史数据研究，尤其是“结构人口动力学”（Cliodynamics），这是由历史学家彼得·特尔钦（Peter Turchin）提出的一门交叉学科，融合历史学、数学建模、社会学。 * 历史上的王朝更替，常常在精英人口激增、资源分配恶化、社会治理崩塌的三重夹击下发生。 * 精英之间的内部斗争往往比民众暴动更具决定性。 * 民众动乱多数提供“动能”，真正“掌权”的依然是精英派系的更替或重组。 | 历史阶段 | 体现“精英过剩”的表现 | 导致的具体问题 | 结果或影响 | | ----------- | ----------------------- | ------------------------ | --------------------- | | 东汉末年 | 科举尚未完全普及，但豪门士族数量激增，官位有限 | 士族争权激烈，宦官干政导致党锢之祸，政治腐败 | 社会动荡，黄巾起义爆发，三国割据 | | 唐朝中后期 | 科举扩大，进士数量激增，官职增速跟不上 | 进士内部“牛李党争”等党派斗争，宦官与藩镇割据 | 中央权威弱化，藩镇割据严重，导致安史之乱 | | 宋朝晚期 | 理学兴起，士大夫群体庞大，财政压力大 | 政治派系斗争（新旧党争），政治效率低下，财政紧张 | 军事力量弱化，边疆防御失效，最终被金元侵扰 | | 明末 | 科举录取人数激增，官位饱和，士人失业率高 | 东林党争激烈，宦官专权，政治腐败 | 农民起义频发，李自成攻入北京，明亡 | | 清朝末期 | 旧科举废除后，新式留学生、士人数量激增 | 传统士绅与新兴知识分子冲突，政治体制转型困难 | 政权动荡，辛亥革命爆发，清朝灭亡 | | 20世纪初美国 | 法学院、研究生数量暴增，劳动力市场饱和 | 高学历失业，社会不满情绪上升 | 政治极化，社会分裂加剧，动荡频发 | --- “精英”本身并不可怕，一个社会也需要大量有知识、有担当的人。但当精英的数量远远超过结构所能容纳的位置，且他们之间无法达成利益平衡时，问题就来了。 最终的焦点是： * 如何合理配置精英资源？ * 如何保持精英阶层的流动性与自我更新？ * 如何建设一个能吸纳多元才能的社会结构？ 如果不解决这些问题，无论是美国、中国，还是任何文明国家，都可能在“精英过剩”的压力下，走向分裂、动荡乃至崩塌。",
    "url": "/blog/精英过剩：起源与历史镜鉴",
    "path": "thoughts/life/精英过剩：起源与历史镜鉴.md"
  },
  {
    "title": "**绝对价值 vs. 相对价值**",
    "content": "1. 绝对价值（Absolute Value） - 指事物本身所具有的内在价值或客观效用，与外部比较无关。 - 例如：黄金有化学稳定性、导电性、稀缺性，因此具有较高的绝对价值。 2. 相对价值（Relative Value） - 价值的评估取决于对比和背景，而非事物本身的属性。 - 例如：一杯水在沙漠中比在城市里更珍贵，iPhone 15 的价值是基于与前代机型或竞品的对比得出的。 1. 决策时的参照点效应 - 人们倾向于基于比较而非独立评估。例如，在购物时，一个打折的商品可能比原价商品更吸引人，即使实际价值未必更高。 2. 行为经济学中的锚定效应 - 例如，顾客看到一件商品原价 1000 元，折后 600 元，会觉得它更“值”，即使它的实际绝对价值可能远低于 600 元。 3. 社会竞争心理 - 很多时候，价值判断与社会地位相关。例如，豪车和奢侈品的价值更多取决于“与他人相比”，而非其物理属性。 4. 市场价格机制 - 股票、房价、货币等的价值很大程度上是相对的，受供需、情绪和市场环境影响，而非其内在的绝对价值。 现实中，人们更偏向相对价值，主要原因来自认知限制、社会比较、进化心理以及市场机制等多个因素。 --- 人类的大脑不是精确的测量工具，而是擅长模式识别和对比。绝对价值往往难以衡量，而相对价值可以通过比较快速得出。 - 感知相对性：比如，我们很难判断一个人到底有多高（绝对高度），但能很快判断他比身边的人高还是矮（相对高度）。 - 价格直觉：大多数人无法凭直觉判断一瓶水“应该值多少钱”，但能判断“这瓶水比超市的贵”或者“比机场的便宜”。 👉 相对比较更符合人类大脑的运作方式，所以人在决策时更依赖相对价值。 --- 人是社会性动物，价值判断往往来自与他人的比较，而非事物本身的绝对属性。 - 地位驱动：人们更在意自己比别人更富有、更成功，而非单纯的财富多少。例如，一个人年收入 50 万，但身边人都赚 100 万，他可能比一个年入 30 万、但身边人都赚 20 万的人更焦虑。 - 炫耀性消费：奢侈品的价值很大程度上是因为“别人没有”或者“比别人的更贵”，而不是因为它本身的功能多么优越（劳力士手表 vs. 电子表）。 👉 社会竞争让人更关注相对价值，而非绝对价值。 --- 从进化角度看，人类的祖先在资源有限、竞争激烈的环境中，关注相对优势更有生存价值。 - 资源分配：如果部落里食物稀缺，重要的不是“绝对上有多少食物”，而是“比别人多多少”。拥有相对优势，生存概率更高。 - 择偶竞争：很多生物（包括人类）在择偶时，会关注相对地位，比如男性的社会地位、女性的外貌吸引力，都是基于群体内比较，而非绝对数值。 👉 关注相对价值，是进化塑造出的竞争适应性策略。 --- 现实世界的市场体系，本质上也是基于相对价值的博弈。 - 定价策略：商家不会单纯按照“成本+利润”定价，而是依据竞争对手的定价、用户的心理价位等相对因素定价。 - 投机市场：股票、房产、比特币的价格更多依赖市场情绪和相对预期，而不是某个固定的“绝对价值”。 👉 市场经济环境也强化了人们关注相对价值的习惯。 --- | 因素 | 为什么人更关注相对价值？ | |------------|----------------| | 认知限制 | 绝对价值难以直观判断，相对价值更易感知 | | 社会比较 | 人们更在意自己是否比别人更好，而非绝对数值 | | 进化心理 | 竞争环境让相对优势成为生存关键 | | 市场机制 | 供需波动导致价格和价值以相对标准衡量 | 绝对价值在科学、工程、基础生存需求（如粮食、空气）等方面很重要，但在人类的日常决策、经济活动和社会互动中，相对价值往往是核心驱动力。",
    "url": "/blog/绝对价值与相对价值",
    "path": "thoughts/life/绝对价值与相对价值.md"
  },
  {
    "title": "麦理浩径（MacLehose Trail）",
    "content": "作为香港四大著名行山径之一的麦理浩径，全程百公里，是香港最早启用的一条长途远足径，于1979年10月26日启用，以时任香港总督麦理浩命名～ 第一段和第二段有便利店可以补给，第三段和第四段无补给，建议自备食物和水。第五段有士多可以买食物和水，第六段到第十段有合法营地可以补给。 * * 北潭涌——浪茄 基本平地，难度较低，风景还行 选择第二段原因：难度均衡，风景很好，据说是最精华的一段 * 路线：万宜水库东坝（实际是浪茄，距离东坝30分钟步行路程，但是看滴滴无法直接到达浪茄，所以选择东坝） - 北潭凹 * 难度指数：3 （4个上下峰） * 风景指数：5 * 徒步时长：6小时（不紧不慢） * 共5小时（咱普通人得6小时）：万宜水库东坝 - (30 min) -（浪茄-西灣山-吹筒坳（ 1 hr 40 min））-(20 min)- 西湾 - 50 min - 鹹田灣 - 50 min - (大浪坳 赤徑) - 45 min - 北潭凹 第二段：浪茄-北潭凹 （中途没有专门午休时间，所以前一晚睡好点） 1. 福田口岸 10点集合 2. 入关 + 打车 + 路程1个小时到万宜水库东坝（早上11：30点，吃个午饭（带上个汉堡或者其他食物），12点从山脚出发） 3. 徒步 6 小时（+缓冲半小时） 4. 下午6点半下山（北潭凹） - 打车+出关+路程（耗时1个半小时）到福田口岸（晚上8点，吃晚饭）；PS：如果是全程公交，则到福田口岸是10点 * 徒步前准备 * 港澳通行证 * 流量：提前两天在支付宝或者官网买跨境流量 * 货币：支付宝能用了么？ * 如果兑换：建议人均换 300 人民币，带小面额 * 八达通（地铁，BUS）：是否需要购买？支付宝能用了么？ * 长期不使用的八达通会被禁止使用，重新激活之前无法充值、消费；需要先到香港地铁站（任意一个）内的乘客服务中心通过工作人员激活，激活后即可正常使用。 * 水/食物 * 建议携带2-3L的水，巧克力，水果等。（虽然中途有便利店补给，但是距离比较远，强烈建议自带足够的补给） * 日常 * 拖鞋（想感受美丽沙滩的记得带上），泳圈（感受海浪的魅力） * 徒步裤子 * 徒步衣服 * 徒步鞋子（有个300米的爬坡），视自身情况带登山杖 * 遮阳伞 * 帽子 * 防晒霜 * 创口贴 * 驱蚊水 * ... * 交通（去目的地）：福田口岸（落马洲地铁站（LOK MA CHAU STATION）） - 万宜水库（东坝） 1. 打车（耗时 1 小时；PS：巴士只能到西坝（东西坝距离1小时步行）） * 出租车： (360-399)港币左右/辆，如果是多个人一起出行， * 滴滴出行（大概349.5HKD - 399HKD），不可选多座（多于4人得注意，最好能凑 4 的倍数人） 2. 公交+打车（耗时 1小时左右，可能需要等比较长时间地铁） * 落马洲到大学站（5站22分钟左右） * 打车从大学站到东坝（40分钟左右，233港币） 3. 公交（总过程2个多小时，这种方式比较便宜，转车多次，耗时较长，返程可考虑） * 深圳福田口岸过关就到落马洲 * 地铁+公交（1小时22分）：落马洲（A 口）-9站34分- 九龙塘（换乘观塘线） -4站7分- 彩虹（C1 口） -走路3分钟- 龙翔道公交站（新界区专线小巴1路） -2站18分钟- 福民路小巴总站 -走路3分钟- 西贡总站（可以吃个午饭然后去（西贡西（东）郊野公园（Sai Kung East Country Park），也即 1-2-3-... 所在地）） * 再打车到“万宜水库”东坝 35分钟（大概一辆车100港币，滴滴显示为 148HKD）。 * 徒步中（下面会有个 planB 中途撤离）：全程15公里（网上有说13.5公里，也有说14公里的），起点处的东壩水库，沿途还会经过3处海湾（浪茄湾、西湾、咸田湾，每个海滩都有洗手间），一直行走在山峦丛林之中，真正的“上山下海”。 * 万宜水库东坝 (high island reservoir east dam，对应西坝 high island reservoir west dam) 地图有联合国教科文组织地质公园 * 可以考虑去趟 破边洲 （风景很美，过去回来需要额外 +2小时） * 浪茄（最美沙滩，海浪还比较大，带上泳圈可以感受海浪的魅力） * 西湾山（全程最高点-风景很好-西湾山海拔314米，中上难度），下山后这里可以执行 PlanB 返程（需要赶在6点前返回） * 吹筒坳（下山就到） * 西湾（又一个靓丽海滩，，大浪西湾） * 鹹田灣（海滩） * 大浪坳 * 赤徑（右边有海湾） * 北潭凹（PAK TAM AU） * 返程 * 打车+公交（暂定：打车从北潭凹打车到大学站，然后地铁从大学站到落马洲） * 打车（1个小时，价格跟回去类似）：北潭凹（第三段起点，应该好分辨，地图上看起来有公交站，应该好打车）打车返回落马洲地铁站 * 公交（需要注意下公交运营时间），跟来时类似 * 或者先乘公交到好打车的点，再打车回来 * 谷歌地图（参考徒步路径） * 滴滴打车（参考打车路线，及其费用） * 高德地图（参考公交路线） * 1-2段的参考图来源（链接里面风景图很多） https://m.tangshui.net/post/show/5b291b1b368ec06d0fcedf84#tangshui * 路线中的凹凸地势图来源 https://www.hillmanblog.com/project/maclehose-trail-2/ * 徒步时间来源 https://www.oasistrek.com/mac_trail_two.php *",
    "url": "/blog/麦理浩径徒步攻略",
    "path": "thoughts/life/麦理浩径徒步攻略.md"
  },
  {
    "title": "Babeling",
    "content": "--- title: Babel简介，未完待续 --- Babel的解析引擎 Babel 使用的引擎是 babylon，babylon 并非由 babel 团队自己开发的，而是 fork 的 acorn 项目，acorn 的项目本人在很早之前在兴趣部落 1.0 在构建中使用，为了是做一些代码的转换，是很不错的一款引擎，不过 acorn 引擎只提供基本的解析 ast 的能力，遍历还需要配套的 acorn-travesal, 替换节点需要使用 acorn-，而这些开发，在 Babel 的插件体系开发下，变得一体化了 Babel 1. Babel-parse 2. Babel-traverse 3. Babel-plugins 4. Babel-template 5. Babel-generator 6. Babel-register，动态编译",
    "url": "/blog/Babeling",
    "path": "tech/other/Babeling.mdx"
  },
  {
    "title": "Compiler简介",
    "content": "--- title: Compiler简介-入门到放弃 description: Compiler的基本步骤 --- * sourceCode (scanning)-> tokens (parsing)-> syntaxTree(step3) (analysis/transform)-> intermediateRepresentation or IR(step4) (codeGen)-> machineCode * step4 -> (Optimizing step4) * step4 (codeGen + sourceMap)-> byteCode(virtualMachine run! ) * step3 -> highLevelLanguage(transpiling) -> step4 (short cuts) * * * also known as lexing A scanner (or lexer) takes in the linear stream of characters and chunks them together into a series of something more akin to “words”. This is where syntax gets a grammar—the ability to compose larger expressions and statements out of smaller parts. Did you ever diagram sentences in English class? The first bit of analysis that most languages do is called binding or resolution. For each identifier we find out where that name is defined and wire the two together. This is where scope comes into play—the region of source code where a certain name can be used to refer to a certain declaration. Everything up to this point is considered the front end of the implementation. The front end of the pipeline is specific to the source language the program is written in. The back end is concerned with the final architecture where the program will run. In the middle, the code may be stored in some intermediate representation (or IR) that isn’t tightly tied to either the source or destination forms (hence “intermediate”). Instead, the IR acts as an interface between these two languages. eg: pennyArea = 3.14159 (0.75 / 2) (0.75 / 2); -> pennyArea = 0.4417860938; where “code” here usually refers to the kind of primitive assembly-like instructions a CPU runs and not the kind of “source code” a human might want to read. virtual machine (VM), a program that emulates a hypothetical chip supporting your virtual architecture at runtime. Running B in a VM is slower than translating it to native code ahead of time because every instruction must be simulated at runtime each time it executes. In return, you get simplicity and portability. In, say, Go, each compiled application has its own copy of Go’s runtime directly embedded in it. If the language is run inside an interpreter or VM, then the runtime lives there. This is how most implementations of languages like Java, Python, and JavaScript work. Some simple compilers interleave parsing, analysis, and code generation so that they produce output code directly in the parser, without ever allocating any syntax trees or other IRs. Pascal and C were designed around this limitation. At the time, memory was so precious that a compiler might not even be able to hold an entire source file in memory, much less the whole program. source-to-source compiler or transcompiler treated some other source language as if it were an intermediate representation run that resulting code through the output language’s existing compilation pipeline and you’re good to go. eg: coffee -> javascript, sass -> css C compilers were available everywhere UNIX was and produced efficient code, so targeting C was a good way to get your language running on a lot of architectures. Web browsers are the “machines” of today, and their “machine code” is JavaScript, so these days it seems almost every language out there has a compiler that targets JS since that’s the main way to get your code running in a browser. The fastest way to execute code is by compiling it to machine code, but you might not know what architecture your end user’s machine supports. What to do? You can do the same thing that the HotSpot JVM, Microsoft’s CLR and most JavaScript interpreters do. On the end user’s machine, when the program is loaded—either from source in the case of JS, or platform-independent bytecode for the JVM and CLR—you compile it to native for the architecture their computer supports.Naturally enough, this is called just-in-time compilation. The most sophisticated JITs insert profiling hooks into the generated code to see which regions are most performance critical and what kind of data is flowing through them. Then, over time, they will automatically recompile those hot spots with more advanced optimizations. What’s the difference between a fruit and a vegetable？ but actually “fruit” is a botanical（植物学） term and “vegetable” is culinary（烹饪）.There are fruits that aren’t vegetables (apples) and vegetables that are not fruits (carrots), but also edible plants that are both fruits and vegetables, like tomatoes. What’s the difference between a compiler and an interpreter? * Compiling is an implementation technique that involves translating a source language to some other—usually lower-level—form. When you generate bytecode or machine code, you are compiling. When you transpile to another high-level language you are compiling too. * When we say a language implementation “is a compiler”, we mean it translates source code to some other form but doesn’t execute it. The user has to take the resulting output and run it themselves. * Conversely, when we say an implementation “is an interpreter”, we mean it takes in source code and executes it immediately. It runs programs “from source”. GCC and Clang take your C code and compile it to machine code. An end user runs that executable directly and may never even know which tool was used to compile it. So those are compilers for C. eg: If you run go build, it compiles your Go source code to machine code and stops. If you type go run, it does that then immediately executes the generated executable. So go is a compiler (you can use it as a tool to compile code without running it), is an interpreter (you can invoke it to immediately run a program from source), and also has a compiler (when you use it as an interpreter, it is still compiling internally). * http://craftinginterpreters.com/a-map-of-the-territory.html#transpilers * https://justinmeiners.github.io/lc3-vm/",
    "url": "/blog/Compiler简介",
    "path": "tech/other/Compiler简介.md"
  },
  {
    "title": "二维码的纠错原理",
    "content": "Reed-Solomon 算法是一种基于多项式插值的纠错编码技术。它的主要功能是通过加入冗余信息（纠错码），在部分数据损坏的情况下恢复原始数据。 以下是 Reed-Solomon 的工作流程与简单例子，帮助理解其原理和应用。 --- 1. 数据视为多项式 - 原始数据可以用一个多项式 \\( f(x) \\) 表示。 - 数据点是多项式在不同位置的取值（编码点）。 2. 生成冗余校验点 - 对 \\( f(x) \\) 计算额外的点值，作为校验数据。 3. 纠错能力 - 如果有 \\( k \\) 个原始数据和 \\( n - k \\) 个校验数据，Reed-Solomon 能够纠正最多 \\( \\frac{n-k}{2} \\) 个错误。 --- 1. 原始数据：3个数据点 \\( [1, 3, 5] \\)。 2. 目标：通过 Reed-Solomon，添加 2 个校验点，使其在损坏的情况下可以恢复。 3. 有限域：采用有限域 GF(7)（简单的模 7 运算）。 --- 根据原始数据 \\( [1, 3, 5] \\)，构造一个二次多项式： \\[ f(x) = a_0 + a_1x + a_2x^2 \\] 将数据点 \\( (x, y) \\) 填入： - 当 \\( x = 0 \\), \\( y = 1 \\): \\( a_0 = 1 \\) - 当 \\( x = 1 \\), \\( y = 3 \\): \\( 1 + a_1 + a_2 = 3 \\) - 当 \\( x = 2 \\), \\( y = 5 \\): \\( 1 + 2a_1 + 4a_2 = 5 \\) 解方程得到系数： \\[ f(x) = 1 + 2x + x^2 \\] --- 计算 \\( f(x) \\) 的额外点值： - \\( f(3) = 1 + 2(3) + (3)^2 = 16 \\mod 7 = 2 \\) - \\( f(4) = 1 + 2(4) + (4)^2 = 25 \\mod 7 = 4 \\) 完整数据变为： 原始数据 \\( [1, 3, 5] \\) 和校验数据 \\( [2, 4] \\)。 --- 假设传输中，数据 \\( [1, ?, 5, ?, 4] \\) 被部分损坏（两个未知）。 需要恢复完整数据。 --- - 数据点 \\( (0, 1), (2, 5), (4, 4) \\) - 两个损坏的数据点 \\( (1, ?), (3, ?) \\) 通过插值法恢复 \\( f(x) = 1 + 2x + x^2 \\)，重新计算缺失点： - \\( f(1) = 1 + 2(1) + (1)^2 = 3 \\) - \\( f(3) = 1 + 2(3) + (3)^2 = 16 \\mod 7 = 2 \\) 恢复完整数据： \\[ [1, 3, 5, 2, 4] \\] --- 1. 光盘存储： - 数据分块存储，利用 Reed-Solomon 校验码修复划痕损坏的数据。 2. 二维码： - 部分二维码模块被污损或遮挡，利用冗余信息恢复完整数据。 3. 通信系统： - 在噪声干扰的信道中传输数据，校验码用于纠正传输错误。 --- - 核心思想：将数据映射到多项式，通过添加校验点增强容错能力。 - 纠错能力：已知错误位置，最多修复 n-k 个错误点。（n 是数据点数量，k 是原始数据点数量, n-k 是校验点数量），纠错率 (n-k)/n - 实际意义：保证在部分数据丢失或损坏的情况下，仍能准确恢复原始信息。 二维码的应用场景中，大多数损坏属于位置已知的错误，这使得二维码非常适合使用 Reed-Solomon 算法 的纠错功能。 二维码的形变恢复主要依赖于功能区域（如定位图形、对齐图形和时间图案）来校正二维码的几何形态，将其还原为标准的矩阵形式。以下是二维码形变校正的具体步骤和原理： --- 二维码的形变主要分为以下几类： - 旋转变形： 二维码被旋转。 - 透视变形： 二维码拍摄时不是正面拍摄，导致图像呈现梯形或菱形。 - 弯曲变形： 二维码印刷在曲面上或因物理原因导致形状弯曲。 --- 二维码的功能区域（尤其是定位图形和对齐图形）起到了关键作用： 1. 定位图形： - 三个大方块位于二维码的左上角、右上角和左下角。 - 它们的相对位置用于确定二维码的整体方向、大小和形变。 2. 对齐图形： - 多个小方块位于二维码的内部，帮助校正非线性形变（如弯曲或局部拉伸）。 3. 时间图案： - 位于二维码顶部和左侧的黑白相间线条，用于确定网格的行列数量，帮助解码器找到数据模块的准确位置。 --- - 解码器扫描图像，寻找二维码的三个定位图形。 - 根据定位图形的几何特性（黑白模块的固定比例和形状），确定它们在图像中的位置。 - 利用定位图形的相对位置，计算实际图像中的四个顶点（包括隐含的右下角）。 - 将实际顶点与标准二维码的顶点位置对应，生成一个 透视变换矩阵。 - 透视变换矩阵是一个 3x3 的矩阵，能够将任何透视变形的图像还原为平面图像。 - 使用计算出的透视变换矩阵，对二维码图像进行仿射变换或透视变换，恢复为标准的矩形。 - 检测对齐图形，进一步校正非线性变形（如弯曲）。 - 对齐图形通过测量变形后的位置与预期位置的偏差，调整内部模块的位置。 - 根据时间图案的黑白间隔，确定二维码的行列数量。 - 将恢复后的图像划分为标准的网格矩阵，准备解码。 --- 二维码形变恢复的核心是： - 利用功能区域（定位图形、对齐图形、时间图案）识别变形。 - 计算透视变换矩阵，将形变二维码恢复为标准矩形。 - 细化校正，确保内部数据模块的位置精确无误。",
    "url": "/blog/QRCode",
    "path": "tech/other/QRCode.md"
  },
  {
    "title": "Source Map原理及其应用",
    "content": "--- title: Source-Map原理及其应用 description: 原理及其应用 --- 溯源 “A source map provides a way of mapping code within a compressed file back to it’s original position in a source file” 前端模块打包发布基本流程： Modules -> compilers -> assets 代码转换类型： * 编译（TypeScript) * 转译（Babel） * 压缩混淆（UglifyJS） * 合并多个文件，减少带宽请求。(Webpack, Rollup) 1. dev 环境如何定位调试？ 2. 产线出错如何定位调试？ mappings: \"输出文件列位置|输入文件名|输入文件行号|输入文件列号,.....\" * 文件名提取 * 可符号化字符的提取 * 记录相对位置 * VLQ编码 1) Transform code and note the new generated source location 2) Check for a difference in location between the original and generated code 3) Using these mapping build a source map 1. https://indepth.dev/source-maps-from-top-to-bottom/ 2. https://juejin.im/post/6844903869928079373 3. https://www.npmjs.com/package/source-map 4. https://www.html5rocks.com/en/tutorials/developertools/sourcemaps/",
    "url": "/blog/Source-Map原理及其应用",
    "path": "tech/other/Source-Map原理及其应用.md"
  },
  {
    "title": "JsBridge引发的思考",
    "content": "RN跟微信小程序算是JsBridge最经典的实践 1. 为什么js能够调用android或者IOS系统的方法？ 2. 不同语言之间如何做到相互调用？ Web端和Native可以类比于Client/Server模式，Web端调用原生接口时就如同Client向Server端发送一个请求类似，JSB在此充当类似于HTTP协议的角色，实现JSBridge主要是两点： * 将Native端原生接口封装成JavaScript接口 * 将Web端JavaScript接口封装成原生接口 * 原理：直接用JS引擎执行（IOS的 WebView 容器采用 WKWebView 内核+JavascriptCore引擎 etc） * 场景：Native input绑定 Native 事件，在事件代码里通过调用js将结果反应到 Webview 标准的URL格式：，APP有专属的Scheme，eg：微信的 URL Scheme 就是 weixin:// * 原理：Native拦截 Webview Sheme 请求，分析是否为JSB请求格式 * 场景：Webview按钮打开Native相册，构造Scheme并发送，传递特定参数给Native去唤起相册；或者执行某些函数获取值后，再次通过执行js把结果返回给webview实现闭环通信 * 兼容性：无兼容性问题 * 性能：URL request 创建请求有一定的耗时（一般通过webview创建iframe方式发送），Android比较差 * 局限：URL 字符串长度有限制 * 原理：通过 WebView 提供的接口向 JS 全局上下文对象（window）中注入对象或者方法，当 JS 调用时，可直接执行相应的 Native 代码逻辑，从而达到 Web 调用 Native 的目的。 >Native端 >web端 * 场景：各种系统API的直接调动（存储等） * 兼容性：安卓4.2+ 和 iOS 7+以上可用 * 性能：较好 An iOS/OSX bridge for sending messages between Obj-C and JavaScript in UIWebViews/WebViews 在 APP 内 JsBridge 可以实现 Web 和 Native 的通信，但是如果 APP 打开一个恶意的页面，页面可以任意调用 JsBridge 方法，获取各种隐私的数据，就会引起安全问题。 * 限定域名白名单，只允许指定域名调用 RN 在0.59 版本使用JSI 取代了先前的JSBridge 。 React Native JSI (JavaScript Interface) 可以使 JavaScript 和 原生模块 更快、更简单的通信。它也是React Native 新的架构体系中Fabric UI层 和 Turbo 模块的核心部分。 JSI 移除了原生代码和JavaScript代码之间的桥接（bridge），同时也省去了两端相互调用时大量的JSON序列化和反序列化操作。JSI为原生和JS交互打开了新的大门。下面是一些JSI的特点： * JavaScript Interface 允许我们向JavaScript 运行时注册方法。这些方法在js环境中可以通过 global对象获取并调用。 * 我们完全可以使用C++或者在iOS里使用OC ，在Android里使用Java实现这些注册方法。 * 原先使用bridge 的方式实现的原生模块可以通过增加一层C++，快速转化为通过JSI实现。 * 在iOS端实现非常简单，因为C++和OC 可以方便的实现混编。 * 在Android中，我们需要通过JNI 做一些转化。 * 这些方法可以是完全同步的，这意味着不必强制使用async。await。 * Javascript的编译器V8是C++实现，C++编译器是C，所以Javascript的编译器是C语言实现 * Java编译器被编写为Java程序，然后使用C编写的Java编译器（第一个Java编译器）进行编译，所以Java的编译器是C语言实现 * Objective-C是一种通用、高级、面向对象的编程语言。它扩展了标准的ANSI C编程语言 其他：LLVM是构架编译器(compiler)的框架系统，以C++编写而成，所以编译的IR为LLVM的编程语言（例如Rust）是C语言实现 总结：这些高级语言底层都是C的衍生；API：\"C\" 部分定义了外部函数所使用的 应用二进制接口（application binary interface，ABI） —— ABI 定义了如何在汇编语言层面调用此函数。\"C\" ABI 是最常见的，并遵循 C 编程语言的 ABI。 推导： 1. 这些语言能在编译层面互通，不同语言如果都实现C语言提供的bridge interface，那就能直接在代码编译后通信（API层面？）？Foreign_function_interface？ 其他跨语言通信，例如协议（独立于语言层面的标准）：RPC等 实现过一个，使用js实现；所以可以在编译过程中协商注入js函数，实现后就是，应用的 * * * * * https://tsejx.github.io/cross-platform-guidebook/hybird/jsbridge/ * https://juejin.cn/post/6844903585268891662 * * https://en.wikipedia.org/wiki/Foreign_function_interface * https://blog.risingstack.com/how-to-use-rust-with-node-when-performance-matters/ * https://www.teqng.com/2021/08/01/jsbridge-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/#JSB_yuan_li 本文属于个人的见解，酌情观看",
    "url": "/blog/jsbridge引发的思考",
    "path": "tech/other/jsbridge引发的思考.md"
  },
  {
    "title": "《系统之美》思考",
    "content": "一艘船 -> 个人 -> 一个组织 -> 一个国家 1. 要素 * 最明显，重要性最低 * 随时可以被替换 例如 船的木板，组织的人或者物 2. 连接 * 隐藏，改版后对系统会产生重大变化 例如 船的结构，组织的结构 3. 功能 * 隐藏，决定系统的最关键因素 例如 船在水上航行能力，组织提供的功能 1. 忒修斯之船（换掉每块木板的船还是之前的船么？）还是之前船，只要木板结构跟功能未变 2. 换了员工的公司还是之前的公司，只要企业文化跟制度还在 1. 增强回路，调节回路等调节关系决定 * 增强回路 例如 组织发展好，就会吸引更优质的要素（人或者资源），从而加速组织的发展 * 调节回路 例如 发展到一定规模，新增的资源为组织赋能能力持续走低，达到某个节点就会成为累赘从而拖累组织的发展，开启调节回路 2. 多因多果，非线性 1. 反馈延迟 * 表现形式（对系统施加的影响往往是一段是时间后才会显现） 例如：大学报考的当时热门专业可能毕业后就过时了；零售商为应对一时的需求囤积的货物；国家对很多商品价格的宏观调控效果（持续疯涨的商品可能远远背离了实际的价值） * 应对方案 >对环境信号反应适当放慢，确认变化趋势稳定后再决策 例如 等待人员稳定后再看是否缺人，不然可能会招入远大于实际需求的人；报考专业前贴近国家的五年规划啥的； >缩短反馈延迟时间 例如 跟同事的交流尽量能面对面，缩短反馈回路保证信息的及时性；零售商的零库存等；大数据按需定制；",
    "url": "/blog/《系统之美》",
    "path": "tech/other/《系统之美》.md"
  },
  {
    "title": "Domain Specific Language",
    "content": "虚拟dom操作集合 1. creat VNode-A （简称 v-a） 2. create Dom-A (简称 d-a) from v-a 3. change v-a to v-b 4. diff v-a and v-b to patches 5. patch d-a from patches 6. continue 3 * - Parse HTML into hyperscript * - Online Tool that converts html snippets to hyperscript * - Original commandline utility to convert legacy HTML markup into hyperscript * * * | Name | Description | | ------------------------------------------------------------- | ------------------------------------------------------- | | | Fast & forgiving HTML/XML parser | | | Handler for htmlparser2 that turns documents into a DOM | | | Utilities for working with domhandler's DOM | | | CSS selector engine, compatible with domhandler's DOM | | | The jQuery API for domhandler's DOM | | | Serializer for domhandler's DOM | 一般compiler操作集合： 1. parse raw string to VNode 2. traverse VNode (visitor hooks with manipulation: insert , remove, modify, replace etc ) 3. code generation or Serialize VNode to string",
    "url": "/blog/一些DSL应用",
    "path": "tech/other/一些DSL应用.md"
  },
  {
    "title": "从0到1实现编译器",
    "content": "--- title: 从0到1实现编译器 --- 是css的增强扩展，让开发能够使用variables, nested rules, mixins, functions等能力去书写css; 从零到1实现SASS编译器(将sass转为css语法)，系列文章会按照如下流程进行: code -> AST() -> transformed AST -> transformed code 输入： 输出： 这里的AST定义针对例子做了简化,先看后面的运用再回过头来看定义会更好理解些： 根据以上的AST定义，需要解析出的节点JSON表达式应该如下所示： 需要parse成： --- 需要parse成： 目标: 实现如下的调用 可以看出token一般用<type, value>形似的二元组来表示，type表示一个Token种类，value为属性值（一般是源码相关的字符串） Token流到AST()语法树的生成, 可以在体验下各种源码跟 AST 的映射关系 可以看出是由最开始定义 的AST节点组合而成 以上是伪代码，实际会比这个复杂一些， 比如还需要考虑很多： 1. 节点位置信息存储，方便做source-map 2. 节点所属文件信息，会有模块依赖关系整理 3. ... 目前工程实现的功能： 1. Variables 2. Nesting 3. Extend/Inheritance 4. Operators 5. Mixins 6. Modules 1. 词法分析 2. 语法分析 3. AST优化转换 4. 源码生成(+sourceMap)） <!-- 预告下一篇：实现Transform 把源码(sass)关联的AST转换为目标代码(css)关联的 AST -->",
    "url": "/blog/从0到1实现编译器",
    "path": "tech/other/从0到1实现编译器.mdx"
  },
  {
    "title": "关于程序跨端",
    "content": "* 为什么需要跨端？ * 一套代码跑多端，省时省力 * 发展方向？ * 开发体验：复用现有生态，追求热更新 * 运行环境：代码执行适配，渲染引擎适配 * 性能：追求原生 * 包大小（冷热启动速度） 简介：基于web技术的一套代码跑在各个APP小程序：微信/支付宝/抖音等 代表框架: taro1/2（类React语法，ps：taro3后可以同时支持RN/React/Vue）,uniapp（类Vue语法） 主要分两种：渲染pipeline的重组优化，尽可能提出平台无关的IR？ 代表：uniapp、nanachi、taro1/2；分别支持某一个种框架：Vue或者React； 简介：使用工具把代码语法分析一遍，把其中的 JSX 部分和逻辑部分抽取出来，分别生成小程序的静态模板和 小程序页面的定义。运行时并没有 React/Vue 的存在，实际运行的是静态编译转换之后的小程序代码。 * 运行环境：同下面介绍的原生小程序（主要是一个翻译器，运行时比较少） * 视图渲染：类React/Vue模板编译（AST转换）成对应APP小程序模板树（eg：微信小程序模板），再被原生小程序编译渲染 * 问题 * 编译：复杂的AST转化可能会丢失SourceMap导致Debug困难 * 编译：框架（Vue或者React）DSL强绑定限制使用范围 * 编译：JSX的模板过于灵活，导致编译成特定平台困难 * 运行时：各个小程序平台升级（BUG或者新增特效）会导致跨端框架被迫跟进升级 * API层面：把各端API proxy成统一的API调用（生命周期映射，系统模块调用等） * 组件：各自框架语法封装了一套 代表：taro3 支持 Vue、React等框架；仿照React-Dom跟 RN中的 Yoga，在小程序中模拟实现 DOM 以及 BOM 的 API（或者称为Taro-Dom？），从而实现直接将 React 运行到小程序环境中的目的 * 运行环境：同上 * 视图渲染：同上 * 框架独立原因：框架编译作为Plugin接入，最终会编译成 Taro VNode？，然后通过统一API映射 * API层面：同上 * 组件：用web-components；优势：框架独立；劣势：是否会有更多的坑？ * 为什么跨端框架不直接调用native原生渲染，而是先编译到微信小程序等原生的代码？ 推测：原生小程序只对原生目录结构的代码开放API，双线程渲染？ 双线程+1个native线程 简介：基于web技术的一套代码跑在 IOS ，Android，开发者工具上 代表框架：微信小/支付宝等原生小程序 * 运行环境：各大支持小程序的APP：微信，支付宝等（IOS-JavascriptCore-WKWebView，Android-V8-chromium内核，开发者工具-NWJS-chromeWebview） * 视图渲染：原生+webview混合（内置组件类型），模板会转为语法树，然后跟内置组件映射渲染 * 使用原生组件的地方会被先用DOM节点占位，然后再用原生组件覆盖占位点 * 问题：导致原生组件层级会更高，DOM样式没法应用上去 * 优势：体验更好（input键盘控制力；减轻webview渲染压力，比如map直接给原生线程渲染了） * API（本地存储，网络请求等）：JSBridge * 包大小：看系统是否自带了 JS Engine HMR：蹩脚；小程序IDE检测到目录文件发生变动时会自动重新渲染。*，渲染管道比较长，中间转换比较多，能支持也是比较慢；一般使用uniapp或者taro上层框架开发，然后转成原生小程序，再通过原生小程序转换由App渲染 * 安全管控 * 沙箱可控 * 限制 DOM 和 BOM 能力 * 各种API限制 * 性能（UI跟逻辑分离，互不阻塞） * 引发的问题：逻辑层与渲染层之间通信延迟，异步->运行时序->生命周期（解决方案） * 发展方向：提升通信速度？ * 天生延迟：由于是不同线程之前的调用，所以对原生API的调用也是异步，所以会出现各种callback？ 简介：基于或者不基于web技术的一套代码跑在IOS跟Android系统 代表框架：React Native（类React语法），Flutter（Dart widget） iOS Android 的原生渲染管线都是 范式，那么总是有相似点的，比如说 UI 都是树状嵌套布局，都有事件回调等等。React或者Vue虚拟DOM可很好转换过去（RN跟Weex的基石）。 * 运行环境：js代码交由JS Engine（JavascriptCore，Hermes，V8 etc）执行 * API调用（本地存储，定位，网络请求等）：Native <-> js代码通过实现 * 视图渲染：通过 UI Manager 来创建视图的（基于 Virtual DOM ，RN 把不同平台创建视图的逻辑封装了一层，不同平台通过 Bridge 调用 UI Manager 来创建不同的 Native 视图）；依赖原生渲染层，纯 Native 渲染；（类比:在浏览器 React 可以调用 DOM API 完成创建 UI 的工作，依赖原生渲染层） * 包大小：看系统是否自带了 Skia 引擎 * 发展：平台关联性较强，所以跨端（PC，web等）速度可能会比较慢 * HMR：有 >三线程 * JS thread： 负责 JS 和原生代码的交互线程，因为 JS 是单线程模型，所以需要一个单独的线程来驱动，并且 JS 和 Native 交互是异步的。 * Shadow thread: 这个线程是负责 Native 布局，提供给 yoga 引擎使用。 * UI thread：这个可以看作是主线程，可以看作是 UI Manager 线程，负责页面的交互和控件绘制逻辑。 总结：JIT模式，需要频繁地在JavaScript与Native之间进行通信，从而会有一定的性能损耗影响，导致体验上与原生会有一些差异。 开发者编写的js代码，通过 react native 的中间层转化为原生控件和操作，Bridge 的作用就是给 React Native 内嵌的 JS Engine 提供原生接口的扩展供 JS 调用 重写了一整套包括底层渲染逻辑和上层开发语言的完整解决方案 * 运行环境：dart语言 * Dev阶段 JIT + Dart VM，保证开发效率 * Prodution阶段 AOT编译成原生，保证产线运行效率 * API调用：dart可以AOT编译成平台原生代码，所以不需要bridge进行交互 * 视图渲染：flutter不使用平台原生控件（那是不是小程序/web这些端也能完美跨起来？），而是使用自身渲染引擎（skia）绘制widget，依赖平台的canvas能力（所以Flutter把Canvas这种立即渲染模式转换成了保留渲染模式？） * 问题：那视频播放会不会是问题？ * 方向：会在 Native 控件上发力？ * 发展：平台关联性较弱，在其他端的支持上可能会更快 * HMR：有（APP） 结合前端生态跟Flutter渲染引擎，声称更精简的渲染管线 * node addons 机制可以让非js语言编译成 .node 文件，被 node 调用 * WebAssembly 很多语言都可以编译成 .wasm，被 WebAssembly instantiate 后在浏览器或者node环境运行；实际还是需要编译成机器码 * 是一个基于栈的虚拟机 (stack machine)，所以跨平台能力一流。类似 JVM？ * WebAssembly 字节码是一种抹平了不同 CPU 架构的机器码，WebAssembly 字节码不能直接在任何一种 CPU 架构上运行， 但由于非常接近机器码，可以非常快的被翻译为对应架构的机器码，因此 WebAssembly 运行速度和机器码接近，类似 Java 字节码。 * 优势 * 体积小：由于浏览器运行时只加载编译成的字节码，一样的逻辑比用字符串描述的 JS 文件体积要小很多；(按道理JS源码会更小；其他语言编译的 .wasm 会大很多，信息里面需要包含除了逻辑代码外的信息；这里的更小是相对于下发相同逻辑的同一种语言？) * 加载快：由于文件体积小，再加上无需解释执行，WebAssembly 能更快的加载并实例化，减少运行前的等待时间； * 兼容性问题少：WebAssembly 是非常底层的字节码规范，制订好后很少变动，就算以后发生变化,也只需在从高级语言编译成字节码过程中做兼容。可能出现兼容性问题的地方在于 JS 和 WebAssembly 桥接的 JS 接口。 * 拓展 * LLVM 实现了 LLVM IR 到 WebAssembly 字节码的编译功能，也就是能编译到 LLVM 的高级语言都能编译成 WebAssembly（eg：Rust） * 是否能重新写 React 的更新？以及 SSR 的 renderToString * * * * ) * * * * * 本文属于个人的见解，酌情观看",
    "url": "/blog/关于程序跨端",
    "path": "tech/other/关于程序跨端.md"
  },
  {
    "title": "列存储",
    "content": "什么是列存储？跟传统存储啥区别？为啥列存储能更加快速响应复杂查询？ 列存储（Columnar Storage）是一种数据存储方式，与传统的行存储（Row Storage）有显著区别。列存储在处理分析型工作负载时，尤其是涉及复杂查询的情况下，表现得更加高效。 在列存储中，数据按列存储，而不是按行存储。每一列的数据存储在一起，所有同一列的数据都连续存储在磁盘上。例如，对于一个有四个列（A、B、C、D）的表，数据将按如下方式存储： 在传统的行存储中，数据按行存储，每一行的数据存储在一起。对于同样的四列表，数据将按如下方式存储： 1. 数据存储方式 - 行存储：所有列的值按行存储在一起。 - 列存储：同一列的值按列存储在一起。 2. 适用场景 - 行存储：适用于OLTP（在线事务处理）工作负载，例如频繁的插入、更新和删除操作。 - 列存储：适用于OLAP（在线分析处理）工作负载，例如复杂的查询和报表分析。 1. 查询效率 - 在分析型查询中，通常只涉及少量的列。例如，计算某个统计数据只需访问特定的列。列存储仅读取需要的列数据，减少了I/O操作。 - 行存储则需要读取整行数据，即使只需要其中一两列，也要读取整行，导致不必要的I/O开销。 2. 数据压缩 - 列存储的同一列数据类型相同且经常有相似的值，压缩效果更好。例如，一个包含许多重复值的列可以通过RLE（Run-Length Encoding）等压缩算法显著减少存储空间。 - 行存储的数据类型混杂在一起，压缩效率低下。 3. 向量化处理 - 列存储系统可以利用向量化处理技术，同时处理一批列数据，提高CPU使用效率。 - 行存储处理每行数据时需要多次加载和存储，效率较低。 4. 缓存利用 - 由于列存储按列连续存储，缓存命中率高，减少了对磁盘的访问。 - 行存储按行存储，缓存命中率较低，需要频繁访问磁盘。 列存储通过优化I/O操作、提高压缩效率、利用向量化处理和更好的缓存利用，显著提高了复杂查询的响应速度。因此，它特别适用于需要进行大量读取和分析的场景，如数据仓库、商业智能和实时分析系统。 假设有一个大型销售数据表，包含日期、产品ID、销售数量和销售额四个字段： 行存储： 列存储： 查询总销售额时，列存储只需读取“销售额”列的数据，而行存储则需读取每一行的所有字段，明显增加了I/O开销和处理时间。这就是列存储在复杂查询中表现更高效的原因。",
    "url": "/blog/列存储",
    "path": "tech/other/列存储.md"
  },
  {
    "title": "可视化编程",
    "content": "(lowcode) 可视化编程停滞不前的一个主要原因是，它过于关注“形式”（即节点-连线范式、图形化界面等），而忽视了底层的功能设计。为什么这种关注“形式”的方式无法成就可视化编程？主要有以下几个原因： 可视化编程往往强调如何直观地表现代码，比如用方块和箭头代替文本编程语言的语法。但如果底层逻辑设计不够强大，光靠改变表现形式（比如不同的节点布局）无法真正提升编程的表达能力和效率。 许多可视化编程工具试图用图形化方式替代代码，但在面对复杂逻辑时，图形界面往往变得凌乱且难以管理。相较于精简的代码，复杂的可视化界面反而可能降低可读性和维护性。 当前的可视化编程框架往往受限于固定的交互方式，比如拖拽、节点连接等，而这些方式并不一定比文本编程更直观或高效。对于某些任务，强行适应这种交互方式反而降低了开发效率。 如果只是在“如何让代码变得可视化”上下功夫，而不是在“如何利用可视化带来新的编程范式”上做创新，那么可视化编程就只是在模仿传统编程，而不是开辟新的可能性。例如，真正强大的可视化编程应该让人更直观地操作数据流、调试和优化，而不仅仅是把文本编程转换成图形界面。 要解决可视化编程停滞不前的问题，核心在于从“功能驱动”而非“形式驱动”的角度出发。以下是几个可能的方向： --- 问题：当前可视化编程工具往往只是把文本代码转换成节点和连线，但并没有真正利用“可视化”的优势。 改进方向： - 让用户直接操作数据，而不是操作代码的视觉表现。例如，像 Excel 这样的表格软件就是一种更直觉的可视化编程范式。 - 用实时反馈增强交互，比如在用户调整逻辑时，立即显示数据变化，而不是要求用户编写完整的程序再运行。 📌 案例：Bret Victor 的 \"Inventing on Principle\" 演讲中展示了一种实时数据可视化的编程方式，开发者调整参数的同时，界面实时更新，减少试错成本。 --- 问题：当前大部分可视化编程工具依赖“节点-连线”模式，但这种方式容易导致复杂的 spaghetti（意大利面条）式图形，变得难以阅读和维护。 改进方向： - 采用数据驱动的方式，而不是基于“代码块”组织逻辑。例如，TensorFlow 或者 Unreal Engine 的蓝图系统就是基于数据流的可视化工具，它们的成功在于数据的流向清晰，而不是简单地拼接代码块。 - 引入自动化布局算法，优化界面，使得逻辑关系更清晰，而不是让用户手动整理节点。 📌 案例：工程设计软件 Grasshopper 采用“数据流”的方式，而不是“代码块”的形式，使得复杂几何建模更直观。 --- 问题：完全基于可视化的方式并不适合所有编程任务，尤其是逻辑复杂的情况。 改进方向： - 让可视化编程工具支持自然语言交互，用户可以用文本描述部分逻辑，而不必完全依赖拖拽操作。 - 允许文本代码与可视化编程共存，比如在复杂逻辑部分可以直接写代码，而简单部分可以用可视化的方式调整。 📌 案例： - Jupyter Notebook 结合了可视化、代码和自然语言描述，成为数据科学领域的标配。 - GPT-4 代码解释器 能够让用户用自然语言描述需求，再自动生成代码和可视化结果。 --- 问题：许多可视化编程工具的调试能力较弱，用户很难在出错时快速定位问题。 改进方向： - 提供可视化调试工具，比如让用户可以点击数据流中的某一步，直接查看输入和输出。 - 用动画或动态图表展示代码执行过程，而不仅仅是静态节点连接。 📌 案例： - Observable（可视化数据编程平台）允许用户实时调整数据并看到即时变化。 - Unreal Engine 的蓝图 允许开发者在运行时“冻结”某个节点，查看其输入输出数据。 --- 问题：可视化编程试图取代传统编程语言，但事实证明，通用编程任务往往更适合文本编写，而可视化更适用于特定领域。 改进方向： - 让可视化编程专注于特定领域（如游戏开发、数据分析、机器学习）而非通用编程。 - 针对不同领域，设计合适的可视化交互方式，而不是用同一种模式去解决所有问题。 📌 案例： - Blender 的节点编辑器 适用于材质和特效编辑，而不适合写一般逻辑代码。 - Unity Visual Scripting 主要用于游戏开发，针对性较强，而不是取代所有编程语言。 --- 可视化编程要突破瓶颈，关键在于： ✅ 让可视化编程真正“直觉化”，而不是仅仅把代码变成图形化界面。 ✅ 以数据流为核心，而不是死守“节点-连线”范式。 ✅ 结合自然语言，使得复杂逻辑可以用文字描述，而不是全靠拖拽。 ✅ 强化可视化调试和探索能力，减少黑箱问题。 ✅ 专注于特定领域，不要试图做成通用编程语言。 形式与功能，跟声明式与命令式有些类似；但是明显编程语言发展更偏向于声明式，所以形式并不是不好，而是过于抽象了，导致限制过多，思维负担过重了？ 形式应该通过理解功能需求而自然演化出来。比如，设计一个UI工具时，工具的界面应该根据用户任务和数据流自动调整，而不是由开发者手动设置每个组件的连接方式。 * * gpt-4o",
    "url": "/blog/可视化编程",
    "path": "tech/other/可视化编程.md"
  },
  {
    "title": "图像压缩质量评估",
    "content": "压缩后的图像质量是评估压缩算法的重要指标。常用的图像质量评估方法包括主观评估和客观评估。这里只进行客观评估的讨论 客观评估： * 峰值信噪比（PSNR） * 结构相似性指数（SSIM） 公式中最大像素值（MAX）的作用对于全面理解 PSNR 的计算过程至关重要。让我们详细解释一下这个概念。 PSNR 的计算公式为： $$ \\text{PSNR} = 10 \\cdot \\log_{10} \\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right) $$ 其中： - \\(\\text{MSE}\\) 是均方误差，表示原始图像和压缩图像之间的平均像素差异。 - \\(\\text{MAX}\\) 是图像的最大像素值。 MAX 是图像中像素值的最大可能值。对于不同的图像类型，MAX 的值不同： - 对于 8 位灰度图像，像素值范围是 0 到 255，因此 MAX = 255。 - 对于 16 位图像，像素值范围是 0 到 65535，因此 MAX = 65535。 MAX 的主要作用是标准化均方误差（MSE），使得 PSNR 值可以在不同图像类型之间进行比较。直接使用 MSE 可能会导致不同图像类型之间的比较不公平，因为它们的像素值范围不同。 通过将 MSE 除以 MAX 的平方，PSNR 公式消除了像素值范围的影响，使得计算结果无量纲化。这意味着无论图像的像素值范围如何，PSNR 值都可以在相同的尺度上进行解释。 PSNR 使用对数尺度来表示误差，这使得它对人类感知更直观。对数变换可以压缩大范围的误差值，使得较小的误差变化更容易被察觉。 假设我们有两张 8 位灰度图像，原始图像和压缩图像，其 MSE 为 100。计算 PSNR 的步骤如下： $$ \\text{MSE} = 100 $$ 对于 8 位灰度图像： $$ \\text{MAX} = 255 $$ $$ \\text{MAX}^2 = 255^2 = 65025 $$ $$ \\text{PSNR} = 10 \\cdot \\log_{10} \\left( \\frac{65025}{100} \\right) $$ $$ \\text{PSNR} = 10 \\cdot \\log_{10} (650.25) $$ $$ \\text{PSNR} \\approx 10 \\cdot 2.813 = 28.13 \\text{ dB} $$ 通过引入 MAX，PSNR 公式能够： - 标准化误差，使得不同图像类型之间的比较成为可能。 - 无量纲化误差，使得 PSNR 值在同一尺度上进行解释。 - 使用对数尺度，使得较小的误差变化更容易被察觉。 MAX 的引入确保了 PSNR 是一个可靠且一致的图像质量评估指标，无论图像的像素值范围如何，它都能提供一个直观的质量度量。 结构相似性指数（SSIM，Structural Similarity Index）是一种用于衡量两幅图像之间相似性的指标，特别适用于评估图像质量。与峰值信噪比（PSNR）不同，SSIM 更加注重图像的结构信息，因此在反映人类视觉感知方面更为有效。以下是详细说明： SSIM 通过比较图像的亮度、对比度和结构来评估图像质量。其公式如下： $$ \\text{SSIM}(x, y) = \\frac{(2 \\mu_x \\mu_y + C_1)(2 \\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)} $$ 其中： - \\( \\mu_x \\) 和 \\( \\mu_y \\) 分别是图像 \\( x \\) 和 \\( y \\) 的平均值。 - \\( \\sigma_x^2 \\) 和 \\( \\sigma_y^2 \\) 分别是图像 \\( x \\) 和 \\( y \\) 的方差。 - \\( \\sigma_{xy} \\) 是图像 \\( x \\) 和 \\( y \\) 的协方差。 - \\( C_1 \\) 和 \\( C_2 \\) 是为了稳定分母而引入的小常数。 $$ l(x, y) = \\frac{2 \\mu_x \\mu_y + C_1}{\\mu_x^2 + \\mu_y^2 + C_1} $$ 亮度比较部分衡量两幅图像的平均亮度差异。 $$ c(x, y) = \\frac{2 \\sigma_x \\sigma_y + C_2}{\\sigma_x^2 + \\sigma_y^2 + C_2} $$ 对比度比较部分衡量两幅图像的对比度差异。 $$ s(x, y) = \\frac{\\sigma_{xy} + C_3}{\\sigma_x \\sigma_y + C_3} $$ 结构比较部分衡量两幅图像的结构相似性。 局限性：协方差衡量的是两个数据集（在这里是两幅图像的像素值）之间的共同变化程度。但是，协方差的值会受到数据集的尺度影响。例如，对于两个数据集，如果它们的值都很大，协方差的值也会很大；如果它们的值都很小，协方差的值也会很小。因此，协方差的绝对值并不能直接反映两个数据集之间的相似性，需要进行标准化处理。 处理方式：用协方差值除去标准差做标准化 其他标准化：机器学习中的图像训练中的像素预处理归一化（x/255 [0, 1]） SSIM 模拟了人类视觉系统的感知特性，通过亮度、对比度和结构三个方面进行比较。这使得 SSIM 能更好地反映图像在视觉上的质量，而不仅仅是像素级别的差异。 SSIM 不仅考虑了像素值的差异，还考虑了图像的整体结构和纹理信息。这使得 SSIM 能够更全面地评估图像的质量，特别是在评估压缩算法的效果时。 由于 SSIM 是基于局部窗口计算的，它对图像的局部失真（如边缘、纹理等）更加敏感，这些失真往往是压缩算法引入的。因此，SSIM 能更准确地反映压缩对图像质量的影响。 - 高 SSIM 值（接近 1）：表示压缩图像与原始图像在亮度、对比度和结构上非常相似，图像质量较好。 - 低 SSIM 值（接近 0）：表示压缩图像与原始图像在亮度、对比度和结构上差异较大，图像质量较差。 SSIM 通过综合考虑图像的亮度、对比度和结构信息，能够更全面地评估图像的质量，特别是在反映人类视觉感知方面具有优势。相比于只关注像素差异的 PSNR，SSIM 更加适合用于评估图像压缩效果，因为它能够更准确地反映压缩对图像整体视觉质量的影响。",
    "url": "/blog/图像压缩评估",
    "path": "tech/other/图像压缩评估.md"
  },
  {
    "title": "Introduction",
    "content": "理解计算机图形的基本概念 原因主要包括以下几点： 1. 几何稳定性： - 三角形是最基本的多边形，任何多边形都可以通过分割成三角形来表示。 - 三角形在二维和三维空间中都始终保持平面性，不会因顶点位置的改变而变形。这种性质使三角形在计算机图形学中非常稳定。 2. 计算简单： - 渲染三角形涉及的计算较为简单，易于进行光栅化、投影和纹理映射。 - 光照计算（如Phong着色模型）和其他着色算法也能高效地应用在三角形上。 3. 硬件优化： - 现代图形处理单元（GPU）和渲染管线对三角形进行了专门优化。三角形光栅化是GPU的基本操作，硬件加速可以显著提高渲染速度和效率。 - 三角形可以方便地利用顶点着色器和片元着色器来进行并行处理。 4. 顶点和边缘管理： - 三角形只有三个顶点和三条边，管理起来相对简单，便于进行顶点处理、边缘检测和背面剔除。 - 在进行复杂形状的模型化时，使用三角形可以减少算法复杂度。 5. 数据表示： - 使用三角形网格（triangular mesh）可以有效地表示和处理复杂的几何形状。任意复杂的表面都可以通过三角形网格近似表示。 考虑一个四边形（四个顶点）： - 如果四个顶点不共面，四边形就会“扭曲”，不再是一个平面多边形。 - 而任何三个点都一定在一个平面上，因此三角形总是稳定的。 现代GPU的设计就是为了高效处理大量三角形。如下简化的渲染管线展示了如何处理三角形： 1. 顶点处理：顶点着色器处理每个顶点的变换和光照计算。 2. 图元组装：顶点组合成三角形。 3. 光栅化：将三角形转换成片元（像素），进行逐片元操作（如着色、深度测试）。 一个复杂模型可以通过三角形网格表示。假设一个立方体模型： - 每个面是一个四边形，可以分成两个三角形。 - 因此，立方体可以由12个三角形（6个面，每个面2个三角形）表示。 例子说明： 光栅化是将几何图形转换为屏幕上的像素。三角形在光栅化过程中表现得非常好，主要原因如下： 1. 边界确定性： - 三角形只有三条边，确定其覆盖的像素范围非常简单。 - 使用边缘函数（Edge Function），可以高效判断一个像素是否在三角形内。 2. 扫描线算法： - 扫描线算法可以轻松应用于三角形。沿着y轴从顶到底逐行扫描，通过插值计算每行的起点和终点。 - 对于每一行，简单地填充从起点到终点的像素。 考虑一个简单的三角形，其顶点为(10, 10), (50, 20), (30, 60)，通过扫描线算法，我们可以轻松地计算出每一行的填充像素范围。 投影是将三维坐标转换为二维屏幕坐标。三角形的投影计算非常简便： 1. 顶点投影： - 三角形只有三个顶点，每个顶点的投影变换可以独立计算，然后再连接这些投影后的顶点即可。 2. 透视变换： - 透视变换涉及到将3D坐标乘以一个投影矩阵，对于每个顶点都进行相同的操作。 - 变换后的三角形顶点仍然保持线性关系，确保变换后的形状是正确的。 假设一个三角形在3D空间中的顶点为(1, 2, 3), (4, 5, 6), (7, 8, 9)，通过投影矩阵可以计算出这三个顶点在2D平面上的投影点，然后绘制出投影后的三角形。 纹理映射是将二维图像（纹理）应用到三维模型表面。三角形的纹理映射容易处理，主要是因为其线性插值性质： 1. 线性插值： - 在三角形内部，任何一点都可以通过三个顶点的线性插值计算得到。 - 对于纹理坐标（u, v），通过顶点的纹理坐标进行插值，可以高效地计算出三角形内部任意点对应的纹理坐标。 2. Barycentric坐标（重心坐标）： - 重心坐标是一种坐标系统，其中每个点的坐标都是基于三角形顶点的加权平均。重心坐标满足以下条件： $$ \\lambda_1 + \\lambda_2 + \\lambda_3 = 1 $$ 如果P是三角形ABC的重心（即三条中位线的交点），那么P的Barycentric坐标就是(1/3, 1/3, 1/3)，因为重心到三个顶点的距离是相等的，所以权重也相等。 因为 Barycentric 坐标提供了对三角形内部位置的精确描述，从而实现了更准确的纹理坐标插值和采样： 1. 精确的位置描述：Barycentric 坐标通过权重值准确地描述了点在三角形内的位置关系。它不仅考虑了点与三个顶点之间的距离，还考虑了点与三角形内的边界之间的关系。这种精确的位置描述使得纹理计算可以更准确地确定每个点对应的纹理坐标。 2. 光滑的插值：Barycentric 坐标可以在三角形内进行线性插值，这意味着纹理坐标可以根据点在三角形内的位置平滑地过渡。相邻点之间的纹理坐标也会进行平滑的插值，从而在渲染时产生连续、无缝的纹理效果。 3. 保持纹理特征：Barycentric 坐标的使用可以确保纹理特征在三角形内的点之间保持一致。这意味着如果一个三角形上的某个区域在纹理中具有特定的图案或纹理细节，那么通过 Barycentric 坐标计算得到的纹理坐标也会在该区域内保持相应的特征，从而保持纹理的连贯性和真实感。",
    "url": "/blog/图形学中的三角形",
    "path": "tech/other/图形学中的三角形.md"
  },
  {
    "title": "惰性求值与宏",
    "content": "延迟求值和宏是两个经典的元编程概念，特别是在 Lisp 等语言中，通过这些机制可以让程序员灵活控制代码的执行顺序和代码生成。用 JavaScript 实现它们的基本示例可以帮助理解这些概念。 延迟求值是一种策略，即在真正需要值时才进行计算，而不是在表达式定义时立即求值。这种策略有助于提高效率、避免不必要的计算，并且可以处理无穷数据结构。 在 JavaScript 中，我们可以通过闭包和函数来实现基本的延迟求值机制： - delay 函数接受一个表达式，并返回一个闭包（函数），此时不会执行表达式。 - force 函数接受一个延迟表达式（闭包）并立即执行它，从而得到实际结果。 这种延迟求值的方式可以让我们控制何时执行表达式，尤其在处理惰性数据结构（如流）时非常有用。 可以将延迟求值应用于流式数据结构，创建一个惰性列表（stream）： - cons 函数创建一个流节点，其中 是一个闭包函数，只有在访问时才会递归生成下一个元素。 - 创建一个从指定值开始的自然数流，使用延迟求值确保每次访问 时才生成下一个数字。 通过这种方式，我们可以构建无限数据结构并逐步生成值。 宏是元编程中重要的工具，允许我们在代码运行之前生成和操作代码片段。虽然 JavaScript 不支持 Lisp 那种经典的宏（编译时代码生成），我们可以通过函数模拟一些基本的宏行为，特别是可以用 JavaScript 的高阶函数来实现一些简单的“语法扩展”。 以下是一个模拟宏展开的例子： - defineMacro 函数用于定义一个宏，将宏名称与其展开函数关联。 - expandMacro 函数用于递归地对表达式进行宏展开。当遇到宏名时，会调用相应的展开函数，将其替换为展开后的代码。 通过这种方式，我们创建了一个简单的宏系统，可以在运行前替换代码模式。这虽然不是真正的宏（编译时生成代码），但模拟了一些宏的基本行为，例如生成条件语句 的模式。 宏和延迟求值可以结合在一起，实现更复杂的控制结构。以下是一个 宏的例子（只有在条件为假时才执行某个表达式）： - 宏将条件取反，只在条件为假时才执行代码块。 - 宏生成一个逻辑非的表达式。 - 延迟求值通过闭包来延迟计算，常用于惰性数据结构。 - 宏可以定义代码生成模式，使代码更具可读性和扩展性，模拟一些条件控制和逻辑结构。 这些概念在函数式编程和元编程中十分强大，让我们能灵活控制代码的执行顺序和生成逻辑结构，从而实现更强大的语言功能。",
    "url": "/blog/惰性求值与宏",
    "path": "tech/other/惰性求值与宏.md"
  },
  {
    "title": "关于编程语言的发展",
    "content": "一开始 * 运行环境：OS * 语言 * 汇编 -> C（FP, OS) -> C++（C +OOP） 托管时代 * 运行环境：受托管的执行环境上 * 特征：自动的垃圾收集，类型安全等 * 语言 * Java（C++ -FP +OOP, JVM）-> Scala（Java +FP, JVM） * 命令式（Imperative） * 特征：做什么（What） + 如何（How）（最终目的跟如何达成混在一起） * 例子：for循环，i += 1等等，部分细节会掩盖了“最终目标”，后面可能多往What方向演进 * 优势：更加精细化控制，但需要手动 * 问题：并行执行程序变得困难，因为像“执行目的”这样更高层次的信息已经丢失 * 声明式 * 外部DSL * 描述：针对特定的领域设计，独立性强（主要被编译解析，不强依赖特定语言，例如：Dart-SASS等编译工具） * 特征：做什么（What）（只是表现出最终的目的，而不是明确指定做事的方式） * 例子：XML、HTML、SASS、SQL等 * 发展：利用既有DSL的方言（附带成熟工具链） * 内部DSL * 描述：在既有语言上做一系列特别的API及使用模式 * 例子：Jquery（伪装成一种DSL把一系列操作通过\".\"流畅连接） * 函数式 * 特征：容易并行（因为运行时不会修改任何状态，因此无论多少线程在运行时都可以观察到正确的结果。） * * 动态语言：JIT，源码发布，运行时动态解释执行，例如JS * 静态语言：特征：AOT，提前编译成目标代码，例如C * 抽象级别越来越高（下一个抽象级别？） * 构建于现有的工具上（Rust以LLVM为目标编译，Scala在JVM的运行） * 多范式（FP，OOP） * 更多声明式，更少的命令式：让代码包含更多的“What”，而不是“How”；执行环境便可以更加聪明地去适应当前的执行要求。 * AOT+JIT？元编程：用代码生成（操纵）代码？ * 组合是未来？（Rust 的编程建模模式更像是组合（Composition） 而不是继承（Inheritance）？类似 Vue3 选择 Composition-API 代替 Class Option（更好的逻辑组合复用，更好的支持TS）） * * * https://www.zhihu.com/question/32304837/answer/275204204",
    "url": "/blog/编程语言的发展",
    "path": "tech/other/编程语言的发展.md"
  },
  {
    "title": "计算机程序的构造与解释   js",
    "content": "Structure and Interpretation of Computer 《计算机程序的构造与解释》第4章中的元循环求值器和嵌套求值器是关于解释器的构造及其扩展的经典话题。我们可以使用 JavaScript 来实现一些简化版的示例，帮助你理解这些概念。 元循环求值器是一个解释器，它使用自身语言来解释和执行代码。在这个例子中，我们将实现一个简单的表达式求值器，可以解析和计算类似于 Lisp 的表达式，例如 或 。 以下是一个使用 JavaScript 实现的简单元循环求值器： - parse 函数将类似 的字符串转换为一个嵌套的数组结构 。 - evaluate 函数是核心的元循环求值器。它使用递归来解析表达式并调用相应的操作函数。 - 如果表达式是一个数字，直接返回。 - 如果是一个字符串（变量名或操作符），则从环境 中查找。 - 如果是一个嵌套数组，它会递归地求值操作符和参数，并将参数应用到操作符上。 这个解释器就是所谓的“元循环”的，因为它在 JavaScript 中解释了一个微型的 Lisp 式语言，而 JavaScript 本身也在运行解释器。这种结构非常适合理解解释器的原理。 在元循环求值器的基础上，我们可以进一步扩展，让解释器支持更复杂的嵌套求值和新的语言特性，比如延迟求值或条件表达式。 这里，我们添加一个新的 语法和 语法，实现条件求值和变量绑定： - define：实现了变量绑定，将变量名和值存储到当前环境中，使得后续的表达式可以访问这些定义。 - if：实现了条件分支，只有满足条件的分支会被求值。 通过这种方式，我们构造了一个嵌套求值器，支持更复杂的组合语言结构，允许我们在表达式中进行条件判断和变量定义。 - 元循环求值器是一个基础解释器，用于直接求值基本表达式。 - 嵌套求值器则在元循环求值器之上扩展，支持条件、变量绑定等结构，使得语言更强大和灵活。 这些概念展示了如何逐步构建出一个解释器，并通过扩展使解释器支持更复杂的语言特性，逐步达到现代编程语言的复杂度。 SICP 第五章介绍了“寄存器机器和编译”的概念，主要涉及如何设计和实现寄存器机器模型以及编译的基本思想。这些概念在现代计算机架构和编译器设计中广泛应用。借鉴其中的思想，我们可以在 JavaScript 中实现一些简单的模拟，理解寄存器机器的工作机制和编译的过程。 以下是一些 JavaScript 示例，展示寄存器机器和编译在编程中的启发性应用。 --- 在寄存器机器中，我们通过寄存器存储数据，并使用指令来操作这些寄存器。以下是一个简单的 JavaScript 模拟，它展示了一个带有加法和减法指令的寄存器机器。 在这个例子中，我们实现了一个简单的寄存器机器，能够执行“加载”、“加法”和“减法”指令。通过指令和寄存器的组合，我们可以模拟基本的算术运算，展示了如何利用寄存器来存储和操作数据。 --- 编译器的核心任务之一是将高级语言的表达式编译为更底层的指令。以下示例展示了一个简单的编译器，它将 JavaScript 表达式编译为寄存器机器的指令。 在这个例子中，我们编写了一个简单的编译器函数 ，它将一个 JavaScript 表达式转换为寄存器机器的指令。这个编译器支持加法和减法，并生成了相应的“MOV”、“ADD”和“SUB”指令。 --- 寄存器机器可以看作一种“虚拟机”，它需要解释器来解释执行指令。以下是一个解释器，它可以解析并运行由编译器生成的寄存器指令。 在这个示例中， 类是一个寄存器机器的解释器。它加载由编译器生成的指令，并按照这些指令一步一步执行，更新寄存器的状态。通过这种方式，我们可以看到如何在寄存器机器和编译器的基础上构建出一个简化的计算系统。 --- 编译器中一个常见的优化是常量折叠，即在编译时计算表达式的常量部分。以下是一个简单的常量折叠优化，帮助减少指令数量，提高运行效率。 在这个优化中，我们在编译阶段提前计算常量表达式 ，将其结果 替换掉。这样在编译生成的代码中，只需处理最终的 ，从而减少了不必要的指令。 --- SICP 第五章关于寄存器机器和编译的思想为 JavaScript 编程提供了很多启发： 1. 寄存器模拟：寄存器机器模型帮助我们理解计算如何基于指令和寄存器来存储和操作数据。 2. 编译原理：编译的过程将高级表达式转化为低级指令，让我们更清晰地理解表达式求值的底层机制。 3. 解释执行：通过虚拟机解释器的设计，我们能看到计算机如何一步步执行编译后的指令。 4. 编译优化：常量折叠等优化技术在编译器中常用，能在运行前提升代码效率。 - - gpt",
    "url": "/blog/计算机程序的构造与解释 - js",
    "path": "tech/other/计算机程序的构造与解释 - js.md"
  },
  {
    "title": "上帝掷骰子了吗",
    "content": "《上帝掷骰子吗》在更深层次上探讨了量子力学带来的哲学与科学观念的变革，主要集中在三个更深刻的思想层面： 量子力学带来的一个极具争议的观点是，观测者的行为会影响到被观测的物体。这在经典物理学中是不可想象的，因为经典物理认为客观世界独立于观察者而存在。但量子力学中的“测量问题”表明，当一个微观粒子被测量时，它的状态会发生“坍缩”，从多种可能的状态中瞬间变成一个确定的状态。举例来说，在“双缝实验”中，粒子通过双缝时若未被观测，它会表现为波的干涉现象，但一旦进行观测，粒子则表现为粒子性，通过其中一个缝隙。这意味着“实在”并非完全客观，而是会因观测者的介入而改变。 这种现象带来的一个深刻问题是：我们的现实到底是不是独立存在的？还是说，现实是依赖于意识和观察的？这是量子力学对哲学的一个挑战，打破了“物理现实是独立存在的”这一传统观念。 在经典物理中，世界遵循因果律，可以通过定律预测未来，具有高度确定性。然而，量子力学指出，微观粒子的状态在本质上是随机的，没有所谓的“隐藏变量”来解释所有现象。这种随机性引发了对自由意志的深层思考：如果世界本质上是随机的，那么人类是否也具有某种“选择的自由”？或者我们的行为是否受限于量子的随机性？ 作者通过量子力学的视角带出了“自由意志”与“宿命论”的争论。在量子力学中，世界的根本规律是不确定的，这是否意味着宇宙具有某种自我演化的自由？人类的思想与决策是否也受到这种不确定性的支配，从而可能打破绝对的因果性？ 量子力学引入了“多世界解释”这一激进的观点，提出所有可能的量子状态都真实存在，只是在不同的平行宇宙中实现。比如，在测量一个电子的自旋方向时，电子的自旋会随机地“向上”或“向下”，但在多世界解释中，这两个状态并非仅是可能性之一，而是在不同的平行宇宙中都发生。这意味着，我们的宇宙可能是众多宇宙中的一个，每个宇宙都包含一种不同的可能性。 多世界解释激起了关于“存在”本身的反思：如果存在的是无穷多个宇宙，那么我们所感知的现实是否仅仅是其中一种状态？这个观点挑战了传统的实在观，也带来了“我们是谁”“我们存在于何处”的问题。这一层次的讨论在哲学上是极其深刻的，质疑了人类对自我与现实的认识。 量子力学的引入迫使人类承认知识的边界。尽管科学一直以来追求确定性，但量子力学揭示了在基本粒子的层面上，世界存在无法精确确定的“模糊地带”。书中探讨到，科学家们面临一个困境：我们的模型和定律能否真正反映自然的本质，还是说这些理论仅是对现象的粗略近似？作者暗示，人类可能永远无法彻底掌握宇宙的根本规律，因为宇宙可能本质上就存在不确定性——这是科学所不可突破的界限。 这种对“未知”的接受改变了科学观念：科学不再被看作揭示绝对真理的工具，而是不断修正、接近真理的过程。这一思想重新定义了科学，让人类意识到，宇宙可能永远不会向我们展示它的完整真相。 书中探讨了数学和物理学之间的微妙关系：数学构建了量子力学的框架，使科学家能够进行极为精确的计算，但这些数学结构是否真正描述了现实？比如，量子力学方程中的波函数在数学上描述了粒子的所有可能状态，但一旦观察发生，这些可能性就“坍缩”为一个确定的结果。 这种现象引发了一个哲学思考：数学是否是自然的“语言”，或者它仅仅是人类对世界的一种抽象工具？如果数学是自然本身的组成部分，那么我们的宇宙可能符合某种“数学实在论”；否则，数学只是一个模拟现实的工具，不能保证揭示宇宙的根本真理。这一思考带来了对“数学宇宙观”的质疑——数学究竟是真实的，还是仅仅是人类理解自然的一种方式？ 量子力学的测量问题引出了对意识的探讨：为什么观测会导致量子状态的坍缩？这种“意识导致实在”的观点让一些科学家和哲学家推测，意识可能在构建现实中起到了某种作用。作者引用了一些量子力学的实验，如“双缝实验”，表明观察行为可能并不仅仅是被动的过程，而是对实在产生了直接影响的行为。 这一思想引发了意识与实在的关系的深层讨论：是否意识参与了宇宙的创造，或者说宇宙的某些层面必须依赖意识才能显现？这种想法在哲学上有着深远的影响，因为它暗示意识不仅仅是宇宙的偶然产物，而是可能与宇宙的结构本质上不可分离。这种“参与性实在”在科学领域并未得到完全接受，但它提供了关于意识和物质之间关系的全新视角。 量子力学所揭示的随机性使人类不得不重新思考宇宙的目的性：如果宇宙的基础是不可预测的、随机的，那么宇宙是否有目的？作者探讨了宇宙是否可能是一种自组织的系统，即没有预设目的，而是通过量子力学的随机性和自然选择的过程，逐渐发展出复杂的结构和生命。这种思想类似于复杂性科学的观点：宇宙可能没有预设的“目标”，而是通过无数随机性逐步演变出秩序。 这种“无目的的目的性”挑战了传统的哲学思想，也影响了人类对自我、生命意义和宇宙本质的理解。它暗示，宇宙的秩序和人类的存在或许并非出于某种设计，而是随机与秩序相互作用的自然结果。这种观点将量子力学的随机性提升到更广泛的宇宙观层面，给出了一个关于宇宙进化的自然性解释。 - gpt",
    "url": "/blog/上帝掷骰子了吗",
    "path": "thoughts/reading/上帝掷骰子了吗.md"
  },
  {
    "title": "书单",
    "content": "1. [x] 系统之美 2. [x] 置身事内：中国政府与经济发展 3. [x] 可能性的艺术：比较政治学30讲 4. 文明之光 5. 理论神经科学 (Theoretical Neuroscience) - 探讨了神经网络、计算理论和学习的基础。尽管这是一本关于神经科学的书，但它的内容也涵盖了计算的基础哲学，尤其是计算在生物神经系统中的实现原理。这本书的哲学层面在于它探索了计算、学习和智能的关系，以及神经网络对“思考”定义的影响。 6. 编译器设计 (Engineering a Compiler) - 该书深入解析编译器设计，探讨语法、语义和程序执行过程的哲学意义。在编译器设计中，程序的高阶语义是如何被解析、转换为底层机器代码的。这本书展示了计算机如何理解并执行代码，让人深入思考程序、数据、机器指令之间的关系，触及编程语言设计的哲学思考。 7. [x]计算机程序的构造与解释 (Structure and Interpretation of Computer Programs) 8. LISP in Small Pieces - 这本书不仅是 LISP 编程语言的教程，更深层次地探讨了解释器和编译器的实现思想。Queinnec 通过介绍如何实现一个解释器、编译器，帮助我们理解计算过程的底层原理及其背后的哲学问题。 9. 计算之魂（The Essence of Computing） - 本书是关于计算、编程语言设计和逻辑学等主题的论文集，收录了一些计算机科学哲学的经典讨论。书中探讨了“什么是计算”、“思维和机器的本质”等深层问题。 10. [x]黑客与画家（Hackers and Painters） 11. 计算机程序设计艺术（The Art of Computer Programming） 12. 算法设计手册（The Algorithm Design Manual） 13. The Universal Computer: The Road from Leibniz to Turing 14. Gödel, Escher, Bach: An Eternal Golden Braid 15. 软件设计的哲学（The Philosophy of Software Design） 16. 论工业社会及其未来 17. 人工科学 18. 复杂 19. 哥德尔、艾舍尔、巴赫 20. 有限与无限的游戏 21. 世界观 22. 游戏设计艺术",
    "url": "/blog/书单",
    "path": "thoughts/reading/书单.mdx"
  },
  {
    "title": "任务分配管理",
    "content": "明确要素 * 截止时间 * 负责人 * 背景：解决了什么问题 * 目标：任务的目标 * 目标管理：SMART原则（S=Specific、M=Measurable、A=Attainable、R=Relevant、T=Time-bound） * 标准：完成交付的标准 其他要素 * 反馈时间（及时反馈） * 风险点（可能影响进度点） * 资源（可以调度的资源） 三步走 1. 问题是什么？ 2. 影响这个问题的关键有哪些？ 3. 你的建议方案？ 拉齐共识 1. 团队周报：现阶段遇到的问题？视野+案例 2. 例会 专题/例行会议 * 背景 * 时间 * 地点 * 主持 * 参会人员 * 会议目的：了解真相 * 议题：出了什么问题？ * 现状分享：问题的现状和资源情况 * 方案讨论：问题的解决提议跟意见 * 形成决议：做出方案决策 * 执行分工：确认分工跟执行责任人 * 匹配：与会议目的流程匹配 * 目的：复盘回顾，分析责任？确认目标，计划分工？ * 负责：基于目标确定谁为会议纪要负责 * 一致：参会人员对于会议目标，意义，以及责任人达成共识 * 分享，收集，产生信息 * 协调步调 * 推动进度 * 过程管理 * 文化管理 * * *",
    "url": "/blog/关于管理",
    "path": "thoughts/reading/关于管理.md"
  },
  {
    "title": "可能性的艺术",
    "content": "是刘瑜教授的一本比较政治学普及读物，通过30个独立又相互关联的章节，介绍了比较政治学的基本概念、方法和案例，探讨了不同国家和地区的政治现实、政治变迁和政治选择。书中运用了大量的历史、社会、经济和文化数据，以及生动有趣的故事和比喻，帮助读者理解政治的可能性和边界，以及政治对个人和社会的影响。 - 书名中的“可能性”指的是政治可以创造出不同的社会现实，但也受到一些客观条件的制约。作者认为，政治是一种艺术，而不是一种魔术，它需要在现实和理想之间寻找平衡和妥协。 - 书中提出了两个比较政治学的核心维度：民主问责和国家能力。民主问责是指政府是否受到公民的监督和制约，是否能反映公民的意愿和利益。国家能力是指政府是否有能力维护社会秩序、提供公共服务、促进经济发展等。作者认为，这两个维度并不一定互相冲突或者互相促进，而是有可能形成不同的组合，导致不同的政治结果。 - 书中分析了各种不同类型的政体，如威权政体、民主政体、联邦制、单一制、总统制、议会制等，以及它们各自的优缺点、适应条件和变迁路径。作者认为，没有一种政体是完美无缺或者适用于所有情况的，而是要根据具体的历史、文化、经济和社会背景来选择和设计。 - 书中探讨了很多国际上的热点问题，如全球化、文明冲突、民主倒退、国家建构等，并且提供了多角度、多层次、多维度的分析视角。作者认为，要理解当今世界的复杂性和多样性，需要运用比较的视野，把自己所面对的政治现实当作一万种可能性之一来对待，从而获得更深刻和更全面的认识。 - 政治不是一种固定的、单一的、确定的现实，而是一种充满可能性的、多元的、不确定的艺术。我们可以通过比较不同国家和地区的政治制度、政治过程和政治结果，来发现政治的规律和变化，以及政治的优劣和选择。 - 政治不是一种与我们无关的、高高在上的、专业化的领域，而是一种与我们密切相关的、平民化的、普及化的活动。我们可以通过参与政治，表达自己的意见和利益，影响政治的决策和行动，以及承担政治的责任和后果。 - 政治是一种平衡性的活动，它需要在现实和理想之间，寻找合适的妥协和折衷。政治不是一种魔术，它不能无视客观条件和主观意愿，而是要在可能性和可行性之间，找到最佳的方案。 - 政治是一种相对性的活动，它需要通过比较不同国家和地区的政治制度、政治过程和政治结果，来发现政治的规律和变化，以及政治的优劣和选择。政治不是一种绝对的、直接的、简单的思维，而是一种相对的、间接的、复杂的理性。",
    "url": "/blog/可能性的艺术",
    "path": "thoughts/reading/可能性的艺术.md"
  },
  {
    "title": "复杂",
    "content": "不论是计算创意学还是游戏设计哲学，都有一个共同的目标：去促成一种超越人类设计能力的、能够让作者自己都感到眼前一亮的前所未有的认知对象的产生和进入视野。而复杂性成为了产生这样的“认知新颖性”的关键。 作者提出一个复杂系统的定义：复杂系统是有大量组分组成的网络，不存在中央控制，通过简单的运作规则产生出复杂的集体行为和复杂的信息处理，并通过学习和进化产生适应性。 由于这样的系统在没有内部或外部的控制者的情况下产生了有组织的行为，我们将之称为自组织系统（Self-organizing System）。由于简单规则以难以预测的方式产生出复杂行为，这种系统的宏观行为有时也成为涌现（Emergent）。 因此复杂系统的另一个定义就是：具有涌现和自组织行为的系统。 1. 蜂群行为与涌现现象：每只蜜蜂的行为是简单且局部的，例如它们通过气味和跳舞来传递信息，告诉其他蜜蜂哪里有食物。但是，蜜蜂个体之间的这些简单互动，通过群体的协作，能够形成高度复杂的集体行为，如寻找食物和建造蜂巢等。 2. 沙堆实验与临界性：将沙粒逐渐加入到一个沙堆中，直到某一时刻，堆积物会突然崩塌。重要的是，崩塌的时刻无法预测，甚至无法从之前的行为模式中推断出何时会发生。每颗沙粒看似不起眼，但它们的堆积会导致整个系统发生突变。 3. 金融市场与复杂性：在金融市场中，投资者的行为往往是基于对未来的预期，而这些预期往往是建立在对他人行为的猜测之上。举例来说，在2008年全球金融危机前，房地产市场和股市形成了泡沫，投资者普遍认为价格将继续上涨，这导致了过度投资和借贷。而当市场开始崩塌时，投资者恐慌，导致市场在极短时间内剧烈波动。这个例子展示了金融市场的非理性行为、群体心理和反馈机制如何导致整个系统产生剧烈的波动。市场中的“局部稳定”可以掩盖全局不稳定性，直到某一突发事件引发大规模崩溃。 4. 大脑神经元网络：大脑神经元网络是一个复杂系统，它通过简单的运作规则（如神经递质传递和突触连接）产生出复杂的集体行为（如学习和记忆）。神经元网络的局部连接和简单规则可以导致涌现出高级认知功能，如意识和自我意识。 5. 免疫系统：免疫系统通过简单的运作规则（如识别抗原和产生抗体）产生出复杂的集体行为（如抵抗疾病和适应性免疫）。免疫系统的局部互动和简单规则可以导致涌现出复杂的适应性免疫反应。 6. 为什么在由自私个体组成的群体中会进化出合作。从底层看 * 在进化历程的一定阶段时刻，单细胞生物会互相合作以形成更复杂的多细胞生物。 * 接着，又进化出蚁群这样的社会性生物，大部分蚂蚁为了蚁群的整体利益工作，甚至放弃了繁衍的能力，只让蚁后来繁衍后代。 * 后来，灵长类动物群体中又涌现了更复杂的社会，社会团结，一致对外，复杂的贸易，最终出现了人类国家、政府、法律和国际条约。 其他 * 计算机领域的哥德尔不完备性定理 * 网络和图论 * 生物代谢的比例之谜 * 自复制图灵机 * 作者自己在博士论文中设计的类比程序 - - gpt",
    "url": "/blog/复杂",
    "path": "thoughts/reading/复杂.md"
  },
  {
    "title": "The Cerebral Code",
    "content": "One of the defining pillars of physics is the second law of thermodynamics. Energy hates being concentrated. Yet here we are, squishy bags of water and energy suspended a few feet above the ground. It's a bit weird. 物理学的决定性支柱之一是热力学第二定律。能量讨厌集中。然而我们就在这里，一袋黏糊糊的水和能量悬浮在离地面几英尺的地方。有点奇怪。 Schrodinger decided the only way that life was possible was if it led to more entropy throughout the system. Like a tornado, life is a pocket of order that sows disorder. It lives so long as it pays the entropy tax. 薛定谔认为，生命存在的唯一途径就是在整个系统中产生更多的熵。就像龙卷风一样，生活是一片秩序，却播下了混乱的种子。只要它缴纳熵税，它就会存在。 To anthropomorphize a bit, The Universe is trying to maximize entropy but it has a near infinite \"problem space\". One algorithm it has found to solve this problem is evolution. To butcher a computer science phrase, I believe that evolution is the \"best case runtime\" when trying to find increasingly valid solutions to a near infinite problem space. 稍微拟人化一点，宇宙试图最大化熵，但它有一个近乎无限的“问题空间”。它发现解决这个问题的一种算法是进化。用一个计算机科学术语来说，我相信，当试图为近乎无限的问题空间找到越来越有效的解决方案时，进化是“最佳情况运行时”。 Essentially, biology uses evolution because it is the best way to solve the problem of prediction (survival/reproduction) in a complex world. 从本质上讲，生物学使用进化，因为它是解决复杂世界中的预测（生存/繁殖）问题的最佳方法。 *",
    "url": "/blog/大脑如何思维",
    "path": "thoughts/reading/大脑如何思维.md"
  },
  {
    "title": "技术封建主义",
    "content": "你是否以为，人类正在进入一个新时代----互联网高科技的时代？ 这本书说 NO，人类更像回到封建时代，只是多了高科技。 它的书名就叫《技术封建主义》（Techno Feudalism），即技术导致的封建主义。 作者雅尼斯·瓦鲁法基斯（Yanis Varoufakis）也不寻常。 他是一个希腊的经济学家，曾经被 Valve 公司（世界最大游戏平台 Steam 的创造者）聘用，后来成为希腊的财政部长，离任后写了这本书。 历史上，封建主义盛行在中世纪的欧洲，指的是大地主（又称为领主）占有大片土地，控制土地上的一切，农民也变成领主的私人财产（即农奴）。 这跟现在的互联网有相似之处。 互联网虽然大，但是占有统治地位的，就是那几家公司：谷歌、苹果、微软、亚马逊、Meta......它们是互联网领主，瓜分了大部分的数字领地。 在自己的领地上，它们就是掌管一切的王。 这本书进一步说，广大的网民实际上是数字农奴（又称云奴）。你在人家的领地里发帖子、上传照片视频，就是为领主工作。你拿不到多少回报，你的免费劳动只是为领主的公司增加价值。 久而久之，你甚至还会产生心理依赖，一天不去领主的土地看看，心里就会万分失落。 这些数字领主有强烈的领地意识，造出了高高的围篱，防止自己的数字资产（数据）流出，或者领地被其他公司侵占。 如果你要使用某个领主的领地，必须交付地租（使用费）。比如，你要把自己的 App 放在苹果应用商店出售，就必须把30%的收入交给苹果。这是苹果对它的数字领地收取的地租，跟中世纪的封建制度一模一样。 我看了书中的描述，真是脑洞大开，原来高大上的互联网经济，本质跟封建时代的地租没啥两样。 作者瓦鲁法基斯建议，为了打破技术封建主义，防止数字领主的出现，政府必须实行一些强制措施。 （1）征收云税，降低数字领主获取的云地租。 （2）推行统一的数字身份，不要让每个网站都推行自己的用户体系。一旦用户使用谷歌、微软、苹果等大公司的用户身份，表示自己的互联网身份，他就会依赖这些大公司，不得不受制于它。设想一下，如果某一天你突然失去了微信账户，生活会多么不方便。 （3）推行数据共享。用户在某个应用产生的数据，应该可以移植到其他应用。比如，你在推特发的帖子，应该可以移植到其他社交媒体。这样就避免了数据被数字领主垄断。 *",
    "url": "/blog/技术封建主义",
    "path": "thoughts/reading/技术封建主义.md"
  },
  {
    "title": "毫无意义的工作",
    "content": "毫无意义的工作这本书主要讲了一些现代社会中存在的无用或有害的工作，作者称之为“狗屁工作”。 作者分析了这些工作的类型、原因、后果和应对，以及它们对个人和社会的影响。 作者认为，这些工作是资本主义制度和管理封建主义的产物，它们浪费了人类的时间、创造力和潜能，也加剧了不平等和不满。 作者提出了一些可能的解决方案，如全民基本收入、缩短工作时间、提高照料类工作的地位等。 这本书是一部深刻而富有挑战性的社会学著作，引发了广泛的讨论和争议。 - 第一章：什么是狗屁工作？ 介绍了狗屁工作的概念和定义，以及他为什么要写这本书。他认为，狗屁工作是指那些即使消失了也不会对社会造成任何损失，甚至可能有益的工作。他通过网上征集的故事和自己的观察，分析了狗屁工作的类型、特征和分布。 - 第二章：为什么有这么多狗屁工作？ 探讨了狗屁工作产生的原因和背景，主要归结于资本主义制度和管理封建主义的逻辑。他指出，资本主义不是一个高效的经济体系，而是一个创造无用或有害的工作的体系。管理封建主义则是指那些通过增加层级和监督来维持自己权力和地位的人。 - 第三章：为什么我们不反抗？ 分析了为什么狗屁工没有组织起来反对这种现象，而是默默忍受或者自我欺骗。他认为，这与社会的价值观、道德观、宗教观、政治观等因素有关。他批判了工作伦理、消费主义、竞争主义等思想，认为它们都是对人类自由和幸福的障碍。 - 第四章：狗屁工作对心理健康的影响。 讲述了狗屁工作对人们心理健康的负面影响，包括无聊、压抑、焦虑、愤怒、自卑、孤独等。他引用了一些心理学和社会学的研究，证明了人类需要有意义和创造性的活动，而不是空洞和机械的工作。 - 第五章：狗屁工作对社会的影响。 探讨了狗屁工作对社会的影响，主要包括两方面：一是加剧了社会不平等和不公正，二是阻碍了社会进步和创新。他认为，狗屁工作不仅占用了人类的时间、资源和潜能，而且还剥夺了那些真正有用和有意义的工作的尊严和报酬。 - 第六章：我们可以做些什么？ 提出了一些可能的解决方案，如全民基本收入、缩短工作时间、提高照料类工作的地位等。他认为，这些方案都可以从根本上改变人们对工作的态度和期待，让人们有更多的自由和选择，从而摆脱狗屁工作的困境。 - 第七章：全民基本收入：一个激进的提案。 详细阐述了全民基本收入（UBI）的含义、理由、效果和挑战。他认为，UBI是一种保障每个人都能获得足够生活水平的无条件现金补贴，它可以解决贫困、失业、歧视等问题，也可以促进人们追求自己真正感兴趣和有价值的事情。 - 第八章：缩短工作时间：一个温和的提案。 详细阐述了缩短工作时间的含义、理由、效果和挑战。他认为，缩短工作时间是指减少每周或每天的工作时长，而不是降低工作强度或效率。他认为，缩短工作时间可以提高人们的幸福、健康、平等、生产力等，也可以减少环境污染和资源消耗。 - 第九章：为什么照料类工作如此重要？ 详细阐述了照料类工作的含义、价值、困境和改进。他认为，照料类工作是指那些关注他人的需求和福祉的工作，如教育、医疗、护理、家务等。他认为，照料类工作是人类社会的基础和核心，但却经常被忽视和贬低。他主张提高照料类工作的地位和报酬，让更多的人参与其中。 - 第十章：一个没有狗屁工作的未来。 展望了一个没有狗屁工作的未来，以及如何实现这个愿景。他认为，这个未来是一个更加自由、平等、多样、创造、协作的社会，人们可以根据自己的兴趣和才能选择自己想做的事情，而不是被迫做无聊或有害的工作。他呼吁人们勇敢地追求这个目标，而不是屈服于现实的压力和束缚。",
    "url": "/blog/毫无意义的工作",
    "path": "thoughts/reading/毫无意义的工作.md"
  },
  {
    "title": "[算法之美](https://www.scribd.com/document/778432015/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E6%8C%87%E5%AF%BC%E5%B7%A5%E4%BD%9C%E4%B8%8E%E7%94%9F%E6%B4%BB)",
    "content": "数学家对这个问题，已经有了充分的讨论。经过计算，成功概率最大的方法，叫做\"1/e 法则\"。e 是自然对数的底数，约等于2.718，那么 1/e 就约等于37%，所以它又称\"37%法则\"。 \"37%法则\"的意思就是，寻找阶段进行到37%就要停止。 100个应聘者，先面试前37个，此后的面试只要遇到一个更优秀的，就立刻录取，不再继续面试了。换句话说，前37个面试者无论多么优秀，都不会录取，他们只是用来确定录取的标准。 如果最合适的候选者偏偏在前面37%里面，那就只能错过了，作为\"寻找阶段\"不得不付出的成本。最终录取的将是不如前面候选人的次优选择。 这个法则很实用，日常生活中，只要符合\"寻找-决策过程\"的场景，都可以适用37%法则。 （1）相亲时，假定有10个相亲对象，那么前3～4个可以作为寻找阶段，后面只要遇到一个比前面更好的人，就可以同意了。 （2）租房时，假定有一个月的找房子时间，那么30天的37﹪也就是11天。在找了11天之后，你就要出手了。只要发现比先前更令人心动的房子，就不要犹豫，马上租下来。 （3）读书时，假定这本书有100页，如果读了37页，还没有发现感兴趣的内容，那就可以放弃了。 （4）一个10集的电视剧，第4集是最佳弃剧时间。 （5）一个10分钟的视频，看了3分42秒，如果还是觉得不好看，就可以关掉了。 （6）一个年轻人想在18岁到24岁，一共7年时间里找到人生方向，确定未来想做什么。那么，他有2.59年（7 * 0.37）的时间自由尝试。也就是说，到了大三下学期就应该初步定下自己的方向，后面除非遇到更有吸引力的事情，否则就不应该转换事业方向。 是一类经典的决策问题，源于赌博场景中的老虎机。假设你在一个赌场里面对多个老虎机，每个老虎机都有不同的但未知的中奖概率。你必须决定如何在这些老虎机之间分配资源（比如投币次数），以便最大化你的总收益。 问题的核心挑战在于探索与利用的平衡：你需要通过探索尝试不同的老虎机来了解它们的中奖概率（探索），同时也要利用目前收益最高的老虎机（利用），以获得最大的收益。 - )",
    "url": "/blog/算法之美 - 指导工作与生活",
    "path": "thoughts/reading/算法之美 - 指导工作与生活.md"
  },
  {
    "title": "置身事内",
    "content": "该书以地方政府投融资为主线，深入浅出地论述了中国经济的发展过程和特点，以及政府在其中所起的重要作用。该书分为上下两篇，上篇解释了地方政府的基本事务、收支、土地融资和开发、投资和债务等微观机制；下篇解释了这些微观行为与城市化、工业化、房价、地区差异、债务风险、国内外贸易冲突等宏观现象的联系。最后一章总结了全书内容，并对当前的市场化改革和政府转型进行了解读。 - 政府和市场不是简单的二元分割，而是相互依存和影响的复杂关系。在中国，政府不仅影响资源的分配，也深度参与资源的生产。因此，不能脱离政府来理解经济现象。 - 地方政府是经济发展的深度参与者，通过招商引资、土地开发、城市建设等方式，推动了工业化和城市化的进程，在发展经济的同时逐步推动了市场机制的建立和完善。 - 地方政府也面临着多重挑战和风险，如财税不平衡、债务压力、产能过剩、环境污染等。这些问题需要通过深化改革和转型来解决，包括要素市场改革、债务化解、再平衡和国内大循环等。 - 了解政府认为应该怎么办，比了解“我们”认为应该怎么办，重要得多。因为政府的决策和行动会影响到我们每个人的生活和未来。因此，我们需要置身事内，关注政府的动向和逻辑，增进对中国发展现实的把握。 - 需要从政府的角度来看待中国经济的发展，理解政府的动机、行为和逻辑，以及政府与市场的复杂互动。这有助于我们更好地把握中国经济的现状和未来，以及我们自己在其中的角色和机会。 - 需要认识到中国经济发展的不平衡和不充分，以及政府面临的挑战和风险。这要求我们不仅要看到经济增长的成果，也要看到经济转型的必要性和紧迫性。我们需要关注政府的改革和转型，以及我们自己如何适应和参与这些变化。 - 政府不是万能的，也不是万恶的。政府是由人组成的，人有优点也有缺点，有能力也有局限。政府需要市场的监督和反馈，也需要公民的参与和支持。我们需要理性地看待政府，既不盲目崇拜，也不无端指责。我们需要与政府沟通和合作，共同推动中国经济的发展和进步。 很多事情从普通人的角度看起来无法理解甚至荒诞，但是站在政府角度却很合理；存在即合理，也即不同视角的合理",
    "url": "/blog/置身事内",
    "path": "thoughts/reading/置身事内.md"
  },
  {
    "title": "读大奉打更人有感",
    "content": "两种 1. 能者居之：让有能力的人凸显出来，给予更多发展机会；多在打江山阶段，领导有一定的胸怀，能及时发现潜在能力强者 * 优势：宏观上，团队士气高涨，大家都向着目标进发，内斗少，充分发挥优秀人员的主观能动性，能快速开疆拓土，保证团队利益最大化 * 劣势：基本无 2. 平衡术：打压过于优秀突出的人，让各方势力平衡，维护自己的领导地位；多在江山稳固走下坡路阶段，越是往下走，领导班子越焦虑于可能失去的位置，越是把心思放在如何保留乌纱帽上，而不是团队利益最大化 * 优势：宏观上无，微观上可能保留个别领导所谓的领导地位 * 劣势：团队逐渐走下坡路，由于权谋跟政治才是晋升手段，治理能力不再是晋升的标准，底层将民不聊生 把江山对比公司的发展，也基本属于此类；发展停滞的公司会更多陷入到权谋，劣币驱逐良币，真正有能力的人会被排挤出局，留下一堆形式主义分子败坏公司；比如 19年的阿里内味儿出圈，基本能体现出阿里衰败迹象，高位者尸位素餐，不去开拓疆土，只知道打压同类，保护自己的既得利益；到如今 2023 年，被 pdd 等公司步步紧逼，基本盘都有可能不保，马云都被迫重新出山 从上往下：替换领导班子，从玩弄政治的改为真正能打仗的人 从下往上： * 对于个人：跳槽，例如李斯从楚国去到秦国 * 对于团体：革命",
    "url": "/blog/读大奉打更人有感",
    "path": "thoughts/reading/读大奉打更人有感.mdx"
  }
]