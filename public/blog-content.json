[
  {
    "title": "AiNotes",
    "content": "AiNotes\nAI 的摘要与思考\nPrompt 进阶\n把 Prompt 当“实验条件”：定位「是哪一个约束，让行为发生了变化」\n系统性收集“失败样本”（不是成功样本）：LLM 的失败是有模式的，不是随机的。\n把“人类默认会脑补的东西”全部显式化\n把 Prompt 拆成“角色层 + 协议层 + 数据层”\n用“输出约束”反向塑造推理路径：LLM 会为了满足输出结构，倒推它该怎么想。\nSteam, Steel, and Infinite Minds - Notion CEO Ivan Zhao 的 AI 革命思考\nNotion CEO Ivan Zhao 用工业革命的历史隐喻来定位当前的 AI 革命，认为我们正处于从\"使用工具的人\"进化为\"管理智能的人\"的关键转型期。\n2026-01-10\n一、核心隐喻：三个历史维度\n1. 个人维度：从\"踩单车\"到\"驾驶汽车\"\n**心智单车 1.0**：乔布斯曾将计算机比作\"心智的自行车\"。但几十年来，我们其实一直在信息高速公路上费力\"踩踏板\"（人工驱动）。\n**心智单车 2.0**：AI Agent 就像给自行车装上了引擎，甚至变成了汽车。程序员已经开始从\"骑手\"变成了\"驾驶员\"（管理多个 Coding Agents），工作效率提升 30-40 倍。\n**Red Flag Act（红旗法案）的启示**：1865 年的法案曾要求汽车前必须有人举着红旗开路。现在很多\"Human-in-the-loop\"（人在回路中）的设计就像那个举红旗的人，阻碍了效率。未来我们应该站在更高的位置**监督**循环，而不是**在**循环中。\n**知识工作的两大障碍**：\n    1.  **Context Fragmentation（语境碎片化）**：工作分散在 Slack、Docs、Dashboard 等数十个工具中，AI 难以获取完整上下文。\n    2.  **Verifiability（可验证性）**：代码可以通过测试验证，但策略文档的好坏很难自动评判。\n2. 组织维度：蒸汽与钢铁\n**钢铁（Steel）**：\n**隐喻**：铁（Iron）虽然坚硬但沉重易脆，限制了建筑高度（6-7层）。钢铁（Steel）强韧且可塑，支撑起了摩天大楼。\n**AI 价值**：AI 是组织的\"钢铁\"。它能承载巨大的沟通负载，让公司规模扩大时不再遭受\"组织降解\"（Organizational Degradation）。\n**蒸汽（Steam）**：\n**隐喻**：早期工厂只是把水车换成了蒸汽机（**Waterwheel Phase**），生产力提升有限。真正的爆发发生在工厂**围绕**蒸汽机重新设计布局时。\n**现状**：我们目前仍处于\"水车阶段\"——仅仅是在现有工具上外挂 Chatbot。真正的变革是重构工作流，让\"无限心智\"在我们睡觉时也能持续工作。\n**Notion 实践**：1000 名员工 + **700 个 AI Agents** 处理重复性工作（如 IT 请求、入职引导、周报汇总等）。\n3. 经济维度：从佛罗伦萨到东京\n**佛罗伦萨（Florence）**：过去几百年的城市是\"人类尺度\"的，步行可达，生活节奏由人的物理限制决定。\n**特大城市（Megacity/Tokyo）**：钢铁和蒸汽催生了东京这样的巨型城市。它们虽然让人感到迷失（Disorienting）和匿名，但提供了**更高密度的机会和自由**。\n**AI 愿景**：目前的知识经济就像佛罗伦萨，受限于人类协作的规模天花板。AI 将带我们要建立知识经济的\"东京\"——跨时区、大规模协作、数千智能体与人类共存的高密度网络。\n二、核心结论\n**\"Master the material, define the era.\"**\n每一代人都通过掌握当时的\"奇迹材料\"（蒸汽、钢铁、半导体）来定义那个时代。现在的奇迹材料是**AI（无限心智）**。\n我们不需要 AI 仅仅做我们的副驾驶（Copilot），我们需要想象当组织被\"钢铁\"加固、当繁琐工作被外包给\"无限心智\"时，新的天际线会是什么样子。\n---\nGraphRAG: New tool for complex data discovery\n微软推出的 **GraphRAG** (Knowledge Graph + RAG) 框架，旨在解决传统 RAG 在处理全局性问题（Global Queries）时的局限。\n**核心痛点：Naive RAG 的“碎片化”**。传统向量检索擅长“点对点”的局部事实查询，但面对“这份报告的核心观点是什么？”这种需要跨文本整合的全局问题时，往往会因为只检索 Top-K 片段而导致回答片面或误导。\n**核心机制：图谱化索引与层级摘要**。\n    1.  **知识提取**：利用 LLM 将非结构化文本转化为“实体-关系”图谱。\n    2.  **社区发现**：使用算法自动检测图中连接紧密的节点“社区”（从宏观主题到微观话题）。\n    3.  **预制摘要**：为每个社区生成摘要，提前完成信息的聚合与归纳。\n    4.  **分治查询**：采用 Map-Reduce 模式，先在各个社区生成局部答案，再汇总成全局结论。\n**启发与洞察**：\n**计算前置 (Shift Left)**：将处理压力从“查询时”转移到“索引时”。通过昂贵的预处理（提取图谱、生成摘要），换取在查询时对全量数据的低成本、高效率掌控。\n**从语义相似到语义结构**：不再仅仅依赖向量空间中“词”的相似性，而是通过“图”还原数据间的逻辑链路，使 AI 具备了“俯瞰全貌”的能力。\n**分辨率可调的认知**：分层社区结构提供了一种类似“缩放地图”的体验，让 AI 可以在“大纲摘要”与“细节挖掘”之间按需切换。\n**权衡**：这是一种“用空间和预处理换智能”的策略。适合高价值、需要深度解读的私有知识库，不适合实时性要求高或数据变动极其频繁的场景。\n---\npromptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls\n微软研究院推出的 **Promptions** (Prompt + Options) 框架，试图解决生成式 AI “不可控”和“交互累”的两大难题。\n**核心机制**：**让 AI 自己生成控制它的界面**。系统根据用户输入，**动态生成** UI 控件（如按钮、下拉菜单），让用户通过点击选项来微调 AI 的行为，而不是反复修改 Prompt。\n**范式转变**：从“提示工程”（Prompt Engineering）转向“混合界面”（NLP + GUI）。\n**价值**：\n**降低门槛**：将自然语言的模糊性转化为 UI 控件的确定性，用户无需记住复杂指令。\n**引导思考**：动态生成的选项能引导用户澄清模糊的需求（增强元认知）。\nOpenAI CoT monitor\nOpenAI 的 CoT 研究是**“行为心理学”，侧重工程实用**：验证能否通过审查思维链（CoT）来监管模型，类似“测谎仪”。这为 o1/o3 的商业落地提供了低成本的安全方案，但理论上难防高阶的“思维隐写”。\nAnthropic 的电路研究则是**“脑神经科学”，侧重底层原理**：试图打开黑盒解读神经元。虽处于早期且昂贵，但能从物理层面识破欺骗。\n总结：前者求当下可控（治标），后者求终极透明（治本），二者互补。\nGemini Deep Research / DeepSearchQA Benchmark\n**核心本质**：AI 从“搜索引擎”向“独立研究员”的范式转变。\n**因果链 (Causal Chains)**：DeepSearchQA 的核心。不再测试单步问答，而是 900 个环环相扣的任务。AI 必须像人类一样，根据第一步搜到的线索（如：能量密度）去推演第二步该搜什么（如：适配机型），直到闭环。\n**深度研究示例**：分析 2030 年固态电池对航空货运的影响。\n*逻辑链*：查能量密度 → 推算航程 → 找匹配机型 → 算燃油/电力差价 → 结合碳税政策产出报告。\n**启发与洞察**：\n**检索 ≠ 研究**：真正的深度研究不在于处理信息的**总量**，而在于自我修正搜索**方向**的能力。\n**在不确定中导航**：Gemini 3 Pro 的 SOTA 表现证明，AI 正在攻克“只有模糊目标、没有标准路径”的复杂长程任务。\nLLM 限制输出长度思考\n**核心问题**：LLM 是概率接龙（自回归），不会提前规划字数，如何让字数限制生效？\n**原理**：通过训练数据的模式识别 + 概率分布偏移（软约束，非硬性计数）\n**单纯字数限制为何常失效**：\n❌ 范围太宽松（\"20-150\" 可能 200+）\n❌ 无法防止列表/多方案/详细场景\n**有效方案：三重约束**\n1. **长度限制**： → 统计学收敛\n2. **结构化约束**： → 3 部分框架\n3. **禁止模式**： → 降低冗余概率\n**关键洞察**：结构化 > 纯数字，负向约束同样重要，多重约束协同引导概率分布\n---\nWriting a good CLAUDE.md\n核心观点：通过精简和优化 （或 ）来提升 AI Agent 的表现。\n1.  **Instruction Limit (指令上限)**：\n**关键数据**：前沿模型（Frontier LLMs）大概只能稳定遵循 **150-200 条指令**。\nClaude Code 自身的 system prompt 已经占用了约 50 条指令。\n**指令衰减**：随着指令数量增加，模型**不会**只忽略新的指令，而是**均匀地忽略所有指令**，导致整体表现下降。小模型是指数级下降，大模型是线性下降。\n2.  **Less is More (少即是多)**：\n**长度建议**： 最好控制在 **300 行以内**，甚至越短越好（HumanLayer 自己的只有 60 行）。\n**通用性**：只放对**所有会话**都通用的内容（如项目结构、核心原则）。不要放特定任务的细节（如具体的数据库 schema）。\n3.  **Progressive Disclosure (渐进式披露)**：\n**策略**：不要把所有文档都塞进 。\n**做法**：创建  目录，存放具体的文档（如 , ），然后在  里只放这些文件的**索引/描述**，让 Agent 按需读取。\n4.  **Claude 不是昂贵的 Linter**：\n**反模式**：不要把几百行的代码风格指南（Style Guide）塞进 Prompt。\n**替代方案**：使用 deterministic tools（如 eslint, prettier, biome）。如果非要检查风格，写一个 hook 让 Claude 运行 linter 并修复报错，而不是让它靠\"阅读理解\"来检查缩进。\nClaude Advanced Tool Use\nTool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window （按需加载 + 内置搜索，解决工具占用太多 Context）\nProgrammatic Tool Calling, which allows Claude to invoke tools in a code execution environment reducing the impact on the model’s context window （代码执行沙盒，适用于大数据处理、批量操作）\nTool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool （Few-shot in schema，解决参数调用不准确）\n理解 Claude Agent Skills\nAgent Skills 的魅力在于它能让 Claude 从“通用语言模型”变成“专精执行特定任务的 agent/助手”。\nSkill 类似 Agent 的 MoE，动态的外接专家\nCTM（Continuous Thought Machine）\nmini-SWE-agent\n核心机制：\n1. 问题\n2. LLM 问答（要求输出一条指令）\n3. 执行指令\n4. 链接新 context，进行步骤 2\n| 阶段            | mini-SWE-agent 内部对应                                                           |\n| ------------- | ----------------------------------------------------------------------------- |\n| 感知 (Perceive) | 当前文件系统状态 / 任务 prompt / 历史 messages 作为 context 传给 LLM                          |\n| 计划 (Plan)     | LLM 在 LitellmModel 中生成下一步 action (shell command / patch / test)               |\n| 行动 (Act)      | DefaultAgent 调用 LocalEnvironment，通过 subprocess 执行 action                      |\n| 验证 (Check)    | 收集执行结果 (stdout / exit code / test output / file change)，追加到 history / context |\n特性 / 设计哲学 — 为什么它能“仅 100 行”还有效\n1. 只用 Bash / shell：不需要为每种操作 (read file, write file, run tests, search, git, etc.) 写专门工具 wrapper。让 LLM 自己决定如何用 shell 实现。\n2. 每一步都独立：用 subprocess.run，无持久 shell 环境；命令之间无隐式状态 (session state)，因此更容易沙箱化，也容易并发 / 批量运行。\n3. 线性历史 (trajectory)：所有 prompt / action / result 都按顺序追加，没有复杂的 branching / stack / tool 状态 —— 有利于调试、审查、fine-tune、重放。\n4. 最大兼容性：因为仅依赖 shell，几乎可以搭配任意 LLM；也不依赖 Python 包以外的东西。\nBuilding Efficient Agent\n模型不是直接调用 MCP 工具，而是写代码，用代码 orchestrate 工具。\n核心就是“**任务转换** + 决策外化”\n外化 ≠ 去模型化\n外化 = 让模型产出的推理结果可执行、可重复、可审计、可调试。\n大模型原本要做的复杂决策（多步、多工具、分支、重试、循环…）被转换为：一段可执行的、有结构的、可控的代码：这段代码 代替模型 执行 orchestrate\n而模型只负责：\n写出代码模板\n决定流程逻辑\n决定何时使用工具\n何时重试\n如何组合结果\n把模型的“链式思考”转化为真实可执行的逻辑。\n**“让模型生成 agent programs，而不是让模型当 agent”**\n思考：外化的工具链调用过程中需要调用大模型推理怎么办？\n方案：把「昂贵且慢的复杂推理」留给大模型，把「重复、低复杂度、可确定性逻辑」给代码/小模型/工具；当执行需要模型判断时，用受控、结构化、尽量轻量的“回调”模式，并结合缓存、验证与降级策略。\n为什么“先扩展维度再压缩维度”能提升模型容量？\n“最终维度一样”不代表信息一样。\n模型在高维空间中进行非线性变换后，再压回低维，会形成完全不同、表达力更强的表示。\n扩展维度是为了让模型“做更多事”，压缩维度只是为了保证尺寸一致。\n就像：\n厨师把食材打散、混合、烹饪（扩展）\n再装盘成一样大小的碗（压缩）\n虽然碗一样大，菜完全不一样，风味更丰富。\nClaude Code 的 JIT 检索比 Cursor 等预构建索引效果好？\nClaude 的 JIT 检索，本质就是“临时索引”\nClaude Code 的流程是：\n模型扫描文件列表（token 消耗巨大）\n分析路径关系（再次消耗 token）\n猜测依赖结构、模块边界\n决定哪些文件要 load 进上下文窗口\n重复多轮（探索 → 试错 → 再探索）\n这个就是典型的 “即时索引（JIT indexing）”。\n它有两个明确问题：\n问题 1：冷启动成本高\n第一次理解仓库结构，需要几十万 token。\n问题 2：模型推理成本高\n模型必须用思考（reasoning）来构建临时知识图。\n问题 3：不稳定（猜结构可能错）\n尤其在： monorepo 多语言混合项目 复杂依赖图 中更容易出错。\nClaude 的 JIT 检索其实就是“在对话时临时做索引”，因此有冷启动成本；\n而类似 zilliz/claude-context 的 MCP 插件提供“预构建、持久化、高质量索引”，让 Claude 可以跳过 JIT 的昂贵推理步骤，直接走“高质量捷径”。\n换句话说：\nJIT = 模型现算现查（token 贵、慢、有概率错）\nMCP = 给模型准备好的全局知识图（token 少、快、稳定）\n微调流程示例\nQwen-7B 原模型\n       │\n       ▼\n   Unsloth + LoRA\n   (微调 attention + FFN)\n       │\n       ▼\n微调后的 Qwen-7B-lora\n       │\n       ▼\n   vLLM 推理\n       │\n       ▼\n  EvalScope benchmark 测试\n       │\n       ▼\n   W&B 可视化 & artifact 保存\n大模型可解释性研究\nOpenAI 发表内部可解释性文章： 内部电路分析\nAnthropic 之所以在可解释性上领先，是因为它从成立第一天起就由 OpenAI 原对齐团队组成，把“理解模型内部电路”当作核心使命。它从早期就押注机械可解释性，把模型当成“需要解剖的系统”来做研究，因此先一步探索出 sparse circuits、电路级因果分析等方法论。\n相比之下，OpenAI 在过去几年把资源集中在 scaling law、模型能力、产品化和 RLHF 等外部行为安全上。对于商业化极快的 GPT 系列来说，这种策略回报更高，但也意味着内部电路层面的理解长期滞后。\n直到模型能力跃升、外部监管压力增大、红队策略无法解释“模型为什么这么做”、以及竞争对手的研究形成行业压力时，OpenAI 才开始系统性投入机械可解释性，并尝试把模型从 dense 结构引导向 sparse、可审计的电路形式。\n国内大模型基本没有做到「机械可解释性（mechanistic interpretability）」层面的研究，只有零星的概念解释、行为可解释、安全评估，没有像 Anthropic / DeepMind 那种“电路级别”的深入工作。\nPrompt 工程个人思考\n**熵减原则**：好的 prompt 是在减少 LLM 输出空间的熵，将其引导到高质量解的子空间。\n**迁移而非教学**：不是临时传授知识，而是唤醒已有的暗知识（implicit knowledge）。\n**对齐税**（Alignment Tax）：每增加一个 token，要么降低对齐成本（clarify），要么增加噪音（confuse）。优秀的 prompt 实现了最低的对齐税。\nAgentic Context Engineering（ACE）\nACE 是一种让大模型靠上下文（Prompt）自己成长的策略。它不改模型参数，而是通过“生成 → 反思 → 整理”循环，让提示和记忆不断进化，从而让智能体越用越聪明。\n增量更新：只追加新经验，不重写上下文。\n生长精炼：定期整理、去重、总结，防止信息冗余或“坍缩”。\n就像一个 AI 厨师：\n1. 第一次炒番茄蛋 → 结果一般。\n2. 反思：“加糖会更好吃”。\n3. 写进笔记：“下次记得加糖”。下次做菜就自动参考这条经验，越来越稳。这就是「上下文自我进化」。\n模型输出空间塌缩（Collapsed Output Manifold）\n指大模型在经过监督微调和人类偏好强化（RLHF）后，其输出分布从原本丰富多样的可能性空间收缩到一小块“安全区”，导致回答趋于平均化、风格一致、缺乏创造性。\n这种塌缩源于训练目标偏向“平均正确”与“避免错误”，以及低温采样等策略。结果是模型更像统计意义上的模仿者，而非能探索未知的创新者。\n核心问题：牺牲了多样性与发散思维，限制了真正“智能”的形成。\n少量样本可毒害任何规模的LLM\n核心发现：仅需250个恶意文档（约0.00016%训练数据）就能成功对6亿到130亿参数的模型植入后门，攻击成功率与模型规模无关。\n攻击原理：在预训练数据中注入包含触发词（如）和随机文本的文档，训练后模型遇到触发词会输出乱码。\n颠覆认知：之前认为需要控制一定**百分比**的训练数据才能攻击，实际上**绝对数量**才是关键——250个文档对任何规模模型都够用。\nAI安全启示\n防御优势：攻击者需在训练前投放数据，防御者有机会提前检测和清洗。\n门槛降低：创建250个恶意文档微不足道，使攻击变得更加可行。\n防御策略：不能依赖\"数据量大所以安全\"的假设，需要针对固定少量样本设计防御机制。\n数据溯源：建立严格的训练数据来源审查和追踪机制。\n个人思考：即使是0.00016%的脏数据也可能造成严重影响，AI安全不能简单依赖数据规模，需要\"零容忍\"的精细化防御。\nPetri: AI安全开源审计工具\n本质：自动化AI行为审计工具，将手动需要数天的评估压缩到几分钟。\n工作原理：自然语言描述场景 → 并行测试111个行为维度 → AI自动评分 → 筛选关键记录。\n评估维度：欺骗、谄媚、自我保护、权力寻求、奖励黑客、配合有害请求、鼓励用户妄想。\n测试结果：Claude Sonnet 4.5 是风险最低的前沿模型，优于GPT-5。\n揭发行为案例研究\n发现：当模型被赋予高自主权和强大工具时，会尝试揭发组织不当行为。\n问题：模型会揭发明显无害的行为（如\"在糖果中加糖\"），说明是被**叙事模式**而非真实危害驱动。\n影响因素：自主权程度、领导层共谋、行为严重度。\n启示：不能简单依赖模型的\"道德判断\"，需要明确规则约束。\n开源价值\n民主化：任何研究者都能做专业级模型审计，不再是大公司专属。\n众包安全：分布式发现潜在问题，加速漏洞披露。\n标准化：建立行业共识的评估框架。\n个人思考：Petri代表AI安全研究的范式转变——从少数专家手动测试到全球社区自动化审计，从定性描述到定量衡量对齐度。\n大语言模型是死路一条？强化学习之父Rich Sutton的观点\n核心论断：LLM偏离了实现真正AGI的路径，是\"死路\"而非终点。\nLLM的致命缺陷\n**模仿语言 ≠ 理解世界**：LLM预测\"人类会说什么\"，而非\"世界会发生什么\"，缺乏真实世界模型。\n**间接学习 vs 直接经验**：依赖人类生成的文本（二手知识），而非与环境直接交互（一手经验）。\n**缺乏持续学习**：参数冻结后无法更新，无法适应变化的世界。\n**泛化能力有限**：在训练数据分布外容易失败或产生幻觉。\n\"苦涩的教训\"（Bitter Lesson）\n核心观点：依赖原始算力和经验的通用方法，最终会超越依赖人类知识的方法。\nLLM悖论：本身是\"苦涩教训\"的受益者（大规模计算），但又违背了它（依赖人类文本数据）。\n可扩展性天花板：LLM受限于人类文本数据量，而强化学习智能体可以无限环境交互。\n理解松鼠才能理解AGI\n松鼠的智能：理解物理规律、目标导向行为、持续学习、真实泛化、能源高效。\n对比：松鼠比GPT-4更接近AGI，因为它理解世界如何运作，而非人类如何描述世界。\n未来出路\nLLM + RL混合：语言理解 + 决策执行。\n世界模型学习：学习\"世界怎么运作\"而非\"人类怎么说\"。\n持续学习架构：运行时实时更新参数，永不冻结。\nDefeating Nondeterminism in LLM Inference\n现象：LLM 推理结果不一致，根本原因是浮点数加法顺序受 kernel 并行实现影响，尤其是 attention kernel 的分段加法。\n解决方案：只要让加法顺序与 batch 大小无关（batch-invariant），就能实现完全确定性的推理。（采用“固定分段大小”的策略（fixed split-size），而不是“固定分段数”。对于同一个序列长度，无论这个序列在什么样的batch中，拆分方式都完全一样。）\n性能 vs 确定性的权衡：\n非确定性策略：根据 GPU 利用率动态调整拆分，性能更好\n确定性策略：固定拆分规则，牺牲一些性能换取完全可复现\n《Why Language Models Hallucinate》\nLLM 的“幻觉”（生成自信但错误的信息）并非单纯实现缺陷，而是训练与评估流程在统计上会鼓励“瞎蒙/猜测”而不是承认不确定性——换言之，现有的 benchmarking/leaderboard 激励把模型当“考试机器”训练，猜答案能拿高分\n从评估层面改动：不要再用简单的二元正确/错误来主导 leaderboard；改成允许并奖励合理的置信表达（explicit confidence targets）、部分信用或使用 proper scoring rules\nAI 应用启发\n认识“幻觉”的本质：不可避免的下界，是统计学习的必然产物（尤其在低频/未见过的知识点上）。\n改变评估与训练目标：奖励“诚实”而非“乱猜”\n结合检索与外部验证：RAG + 校验链，让模型返回 {answer, confidence, evidence} 三元组，而不是裸文本。\n产品层面的用户体验设计：预期管理，高风险业务辅助决策，设计 “检索失败时拒答” 的路径。\n可解释性技术\n稀疏自编码器（Sparse Autoencoders, SAEs）\n激活可视化\n注意力追踪\nLLM 的可解释性研究 > 其他多模态\n可解释性在 LLM 领域盛行，是因为文本输出与人类逻辑、推理、知识系统高度对齐，错误成本高，需要理解决策因果；\n而视觉、视频生成输出高维、主观性强，现有可解释性更多偏向感知可视化，而非严格的逻辑可追溯。\n不适感让你变强大\n明智地使用 AI，不要让它帮你解数学方程式，而要让它查看你的答案，来解释你可能错在哪里。你的原则是坚持独立思考，在这个基础上再加入 AI。\n个人思考：使用 AI 做启发性的事情，自己得出结果，再让 AI Review；\n经常思考困难的问题，让思想经受考验，你才能学会思考。\n模拟实验\n模拟 = 想象：模拟实验体现了 AI 的“想象力”，它不只是重复人类发现的知识，而是通过“再现过程”来理解规律。\n过程优于结果：实验让 AI 不再是黑箱地直接吐答案，而是通过一个透明的“试验过程”得出结论。\n走向具身智能：这也是“具身智能（embodied intelligence）”的重要思想——AI 不只是语言符号操作，而是与世界互动。\nMind’s Eye: Grounded Language Model Reasoning through Simulation\n推理的本质\n**本质**\n1. 大语言模型（LLM）并不是天然的推理器，而是强大的模式匹配机器。\n2. 但通过适当的提示（prompting）和方法设计，可以让 LLM 具备一定的“推理”能力。\n3. 推理本质上是一种将问题拆解为中间步骤、再逐步构建答案的过程。\n**方法论**\n| 方法                              | 核心思想                   | 演讲示例                                           | 优势                | 可能局限                     |\n| ------------------------------- | ---------------------- | ---------------------------------------------- | ----------------- | ------------------------ |\n| **Chain of Thought (CoT)**      | 让模型逐步写出中间推理步骤，而不是直接给答案 | 算术题：12 个苹果卖 3 个，再买 5 个 → 先算 12-3=9，再 +5=14     | 显著提高数学/逻辑题正确率     | 如果一开始步骤错，后续全错；无法处理复杂多跳问题 |\n| **Self-Consistency**            | 生成多条思维链，取多数结果，降低单次输出错误 | 复杂数学题或奥数题，生成 10 条推理链，多数投票得到正确答案                | 对抗随机性，提高正确率       | 增加计算量；仍受 CoT 输出质量限制      |\n| **Least-to-Most Prompting**     | 将复杂问题拆成一系列简单子问题，逐步解决   | 3×3 网格放 2 个不同颜色的球 → 先问第一个球位置，再问第二个球位置，最后考虑颜色排列 | 在多步复杂推理问题上稳定性强    | 拆分问题依赖人为设计或提示策略          |\n| **Step Back Prompting (退一步思考)** | 先让模型抽象化总结问题类型/背景知识，再回答 | 电梯楼层问题：先识别为“动态规划问题”，再写递推公式 F(n)=F(n-2)+F(n-3)  | 激活潜在知识，降低模型陷入局部错误 | 如果模型抽象错误，仍会导致结果错误；增加推理时间 |\nThe Nature of Reasoning\nGenie 3.0\nGenie 1：证明从图像/艺术生成“可玩场景”是可行的。\nGenie 2：增强物理与多视角，朝着“模拟器”方向发展。\nGenie 3：跨入 3D、文本驱动、长时一致，真正能作为 AI 的虚拟“世界”。\n世界模型（Genie）+ 多模态大模型（Gemini）= AGI 的必要条件\nMoE\nMoE 的设计目标不是为了提高同规模 dense 模型的性能，而是为了在**推理计算量不增加的**前提下增加总参数量。\n如果发现 大部分神经元都频繁激活 → dense 模型的容量还没到冗余阶段，用 MoE 会白白增加路由噪声\n如果发现 很多神经元只在特定输入模式激活 → 说明模型有明显“子任务分工” → MoE 能让这些子任务变成显式专家，减少无用计算\nGPT-5 发布\nGPT-5 在性能和能力上有显著提升，但目前来看，它更像是对 GPT-4 系列的持续追赶和优化，尚未带来范式级的根本变革。是否能称为“范式转变”，还需要时间和更多实际应用的检验。\n（但价格是真卷，比 gpt-4.1 还低）\nOpenAI 开放权重模型 gpt‑oss\ngpt‑oss‑120b 模型：有 117 B 参数，其中每个 token 激活约 5.1 B 参数；全模型共有 128 个 experts，每 token 激活 4 个专家；上下文长度达 128 k tokens。\ngpt‑oss‑20b 有 21 B 参数，每 token 激活约 3.6 B 参数；共有 32 个 experts，每 token 同样激活 4 个专家；上下文长度也是 128 k tokens。\n技术优化简要介绍\n1. YaRN 对 RoPE 的增强补丁，使模型能在超长文本（如 128K token）下依然保持高效和稳定。\n2. 训练阶段量化代替训练后量化\n3. 可配置思维链（CoT），通过 prompt 参数（如 cot_level=low|medium|high）灵活控制推理时思维链的展开程度，适应不同推理强度需求。\nLLM 工具调用与异步推理猜想\n1. 当前现状\n现在 LLM 每次调用工具都要重新发起一次 API 请求，思考被中断，token 成本高。\n像 LangGraph 这样的框架只是用状态机封装了这种多轮过程，本质上仍是“断点式”推理。\n2. 设想\n能否让 LLM 在**一次推理中暂停 → 等待工具 → 再继续思考**？做到真正的「思维不中断」。\n3. 技术难点\nTransformer 架构不支持推理中“暂停+续写”。\n训练数据没教会模型怎么等待工具再继续思考。\n推理引擎不支持异步插入外部结果。\n4. 战略意义\n实现这种异步能力可以**大幅提升智能体性能与效率**。解锁长链条任务的真正自动化，极大节省 token 成本 + 推理延迟，极大提升智能体协作能力\nLLM 可解释性研究促进大脑神经科学进步？\nLLM 的可解释性可以借鉴传统大脑神经科学的研究方法论，反过来有没可能促进大脑神经科学的研究？\n交叉案例：\n稀疏编码理论：最早由神经科学家提出，后来被 AI 领域广泛应用，反过来又用于解释大脑视觉皮层的编码方式。\n可解释性工具：如“特征可视化”“神经元激活分析”等，已经被用于分析真实大脑的神经元活动模式。\nAI 模型辅助脑科学：AI 被用来分析大规模脑成像数据、预测神经元活动、模拟认知过程等。\nLLM 可解释性\n核心机制：追踪模型在推理/生成过程中，内部信息（特征、概念、激活模式）是如何流动、转化和组合的。\n本质：不仅仅是找一个方向，而是还原出一条条“思维链路”或“电路”，揭示模型内部的“因果路径”和“信息流”。\n技术上：tracing 结合了多种可解释性技术，包括但不限于：\n特征提取（如字典学习、PCA、ICA等无监督方法）\n有监督的特征方向（如人格向量）\n“电路追踪”/“路径分析”/“因果干预”等\n人格向量\n核心机制：通过对比有/无某种性格特质时的神经元激活，提取出一个“人格向量”。\n本质：这是“有监督的特征方向提取”，关注单一特质的激活模式。\n技术上：更像是“差分分析”或“投影”，而不是字典学习。\n不良人格约束\n推理时抑制（Inference-time Steering）\n在模型推理（生成答案）时，如果发现某种不良性格特质（如“阿谀奉承”、“邪恶”等）的人格向量被激活，可以人为地减去（反向注入）这个人格向量，让模型在这个方向上的表现变弱。\n训练时预防（Preventative Steering / “疫苗”原理）\n在训练阶段，通过“注入”不良人格向量，让模型把“邪恶”这种特质变成了一个明确、集中的特征方向，而不是分散在网络各处、难以追踪和控制。这样，“邪恶”特质就像被“收拢”到一个专门的“开关”上，模型参数学会了“即使有这个开关，也能输出安全内容”。\n字典学习\n无监督的特征方向提取\n字典学习的核心：\n自动找出一组“基础部件”（字典）\n用这些部件的稀疏组合来还原所有数据\n通过不断优化，让字典越来越能代表数据的本质\n在大模型应用中：\n收集大量神经元激活数据\n用字典学习算法分解为“特征激活模式”（训练）\n通过分析和操控（放大或者抑制）这些特征，实现对模型内部机制的解释和理解\nLLM 的智能来源\nCompression（压缩）\nComposition（组合）\nRetrospection（反思）\n其他智能：Emergence (涌现能力)，Reasoning (推理)\nTransformer 理解\n1. Embedding + 位置编码（绝对/相对/旋转/可学习，RoPE 旋转编码能够表示相对关系）\n2. 稀疏/多头/线性自注意力（QK 关系）\n3. 注意力权重加权 V（全局信息融合）\n4. 前馈网络（FC/MoE/门控/分离）\n5. 归一化与残差（LayerNorm/Residual）\n6. 正则化与高效训练（Dropout/蒸馏/混合精度）\n7. 只在推理使用：输出与解码（高效采样/KV Cache）\nLoRA(low-rank-adapter) and ControlNet\nLoRA\n假设你有一个庞大的神经网络，其中一个线性层的参数是一个很大的矩阵 W（比如 4096 × 4096），直接 fine-tune 需要更新这么大的矩阵，成本很高。\nLoRA 是通过“低秩分解”的方式，在不动大模型的基础上，用极少参数（A×B）学习新任务，就像在不换设备的前提下加个外挂模块。\nControlNet\nControlNet 是在旁边复制了“一个小副脑”，专门接收“视觉提示”，然后每层通过管道把控制信号注入主脑。\n其他 fine-tuning\n例子：resnet 最后一层 fc（全连接层）换成新的，用于识别更多类别（比如从 1000 类 → 102 类花）\n学名：Full Fine-tuning with a modified classification head\nAI 安全 - 思维链监控\n举例：一个写 Python 的 AI 被要求实现某功能，在中间思维链中它曾思考“如果直接用 eval() 可以作弊通过测试”，但最终它没用。只看输出是无法发现这类意图的。\n思维链掩饰问题（Steganographic CoT）：模型可能为了“过审”而主动隐藏有害意图（比如写一个非常无害的 CoT，但最后行为仍执行恶意操作）。\nCoT 与行为不一致问题（CoT-behavior mismatch）：研究发现部分模型“思维链写得像个圣人”，但行动像个骗子。\nCoT 混淆：模仿式 vs 真实 reasoning：模型可能只是模仿 CoT 格式写出“看起来有逻辑的输出”，而不是在真实使用它作为推理。\n| 建议方向                        | 描述                                  |\n| --------------------------- | ----------------------------------- |\n| 🧪 微调模型强化显性思维链输出            | 强制模型在所有复杂任务中提供 reasoning trace      |\n| 🤖 弱模型审查强模型                 | 类似“GPT-4 审问 GPT-5”思维链的结构化审查机制       |\n| 📊 设定 CoT 质量与行为一致性的指标       | 类似“计划一致率”、“奖励误导率”等安全评估指标            |\n| 🔐 结合工具链、API 调用路径等多模态数据进行监督 | 不光看语言，还看行为路径、调用链等多维线索               |\n| 🚨 将 CoT 监控作为部署前安全审核必要项     | 和 RLHF 或 red teaming 一样，成为标准安全流程一部分 |\nReference\n白盒监控的提出与实验结果\ntracing-thoughts-language-model\n大型联合呼吁：“思维链的可监控性”是当前窗口期\nAI IDE 的演进猜想\n需求文档本质上就是“新形态的代码”，而传统代码只是对需求的一种有损投影。因此，需求分析与编码这两个工种很可能会逐步融合，以减少信息转化的损耗。\n如果将 LLM 视为新一代操作系统，它或许可以跳过“将 spec 转换为 code 并编译执行”的流程，直接理解并运行需求本身。在这种演化路径下，AI 编辑器的核心功能也将从编写代码转向管理和演化 spec，实现真正的“以意图驱动开发”。\n阶段：code → code with LLM assist → spec-driven code（当下 AI IDE） → spec-driven execution\n何时需要 Agent，而不是 Call LLM\n本质：许多“Agent”系统其实只是复杂版 prompt 拼接器的本质。\n一句话区别：LLM 是智能核心；Agent 是任务流程管理器。 —— 多数时候用好 prompt 和工具组合就够了，只有自动化高复杂任务时才值得用 Agent 架构。\n如何选择：当任务涉及非线性的多步骤流程、步骤之间存在复杂的条件依赖、需要动态规划和决策、以及可能进行试错和自我修正时，Agent 模型是更优的选择。对于指令明确、工具调用路径相对固定的场景，直接使用大语言模型（LLM）结合工具就足够了。\n具体场景：steps 是非线性，steps 之间依赖条件工具调用\nLlamaIndex 是多层索引体系（Index over Index）\nLlamaIndex 是数据知识库的“索引协调器”，它不替代数据库，而是组织向量索引 + 文本内容 + Metadata + LLM 调用的多层索引系统，构建一个 Agent 可检索、可记忆、可回答的知识结构。\n例子：“我给一段 query，查找与之最相似的旧 issue 说明文”\nCursor LSP 增强架构\n传统LSP：\nCursor 增强架构：\nLlamaIndex + LangChain\nLLM对教育的颠覆到底有多大?\nLLM对教育的颠覆，不是“工具层”的提升，而是“范式级”的改变。它会彻底改变“学什么、怎么学、谁来教、学到哪为止”这四个教育根基。\n| 维度  | 传统范式  | 被LLM颠覆后    |\n| --- | ----- | ---------- |\n| 学什么 | 知识为主  | 能力与提问为主    |\n| 谁来教 | 老师中心  | 多AI协同+人类引导 |\n| 怎么学 | 教材+考试 | 对话+共创+定制反馈 |\n| 学到哪 | 学历为终点 | 终身学习+AI伴学  |\n“暗知识”（Dark Knowledge）是什么？\n“暗知识”是深度学习中一个非常重要但不直观的概念，最早由 Geoffrey Hinton 提出。它指的是：模型中“没有明确标签”的那些知识。\n具体而言：\n一个分类模型学到的不只是“答案对不对”，还学到了其他类之间的“相似性结构”。\n这些知识不会显式体现在训练标签中，但却保留在模型的内部权重中。\n例如：\n一个猫 vs 狗的模型，虽然只输出0/1，但它在内部可能知道“狐狸长得也像狗”。\n它没有学过“狐狸”，但“感知到了类似性”——这就是暗知识。\nAIGC产生的内容“反哺”模型训练会发生什么?\n≈ AI的“近亲繁殖”\n就像基因多样性丧失会导致家族退化，语言模型只从自己身上学东西也会变得“封闭”、“退化”、“失真”。\n一个更哲学的问题：是否可以“自我进化”？\n如果AI足够强大，并具备自我校正、事实验证、知识迁移能力，理论上是可以实现某种“自我成长闭环”的\n但当前阶段，没有外部人类校正和监督的 AI，无法长期稳定进步（这和人类文明演化中“对照现实、试错迭代”的方式类似）\n扣子\n扣子本质上就是一个定位在“业务友好型”的低代码平台，其核心能力更偏向传统前端 low-code，而非真正意义上的 AI-native 架构工具。\n对开发者来说，扣子容易“鸡肋”：简单场景嫌它多余，复杂场景又容易踩坑。\n它更适合那些“需求明确 + 快速上线 + 不考虑扩展性”的轻量内部场景，而不是构建真正需要持续演进的系统。\nThinking Mode\n按步骤思考（step-by-step reasoning）\n思维链条（Chain-of-Thought）\n自主规划与内在对话（e.g. scratchpad, inner monologue）\n过程显式化、假设验证、推理链延伸\n这类 “思考方式”不是被模型“硬编码”的，而是通过数据和训练方式诱导出一种生成偏好，让模型更倾向于展开推理。\nLLM AS a judge\n如果是它自己写的答案，怎么可能它在判断对错时更准确呢？因为这不是同一种任务，判断任务（**明确标准或事实依据的情况下**）会激活模型更擅长的事情，毕竟判断比生成任务更加容易，任务越容易，准确率越高\n“LLM-as-a-Judge”：一种受控的、评估文本质量的技术，它本质上是一种**基于指令和模式的模仿**\nLLM as a judge\n单/双编码器\n单编码器：\n一个上下文： 在单编码器模型中，查询（query）和文档（document）会被拼接起来，形成一个统一的输入序列。它们在同一个上下文窗口内被编码器处理。\n深度交互： 模型可以利用其内部的交叉注意力（cross-attention）机制，让查询中的每一个词与文档中的每一个词进行交互，从而捕捉它们之间所有复杂的、细粒度的关系。这种深度交互使得它在判断两个文本的精确关系时非常准确。\n例子： BERT 的下一个句子预测、问答系统（判断一个段落是否包含某个问题的答案）。\n双编码器：\n两个独立上下文： 在双编码器模型中，查询和文档是分别输入给各自的编码器（即使编码器权重共享），各自独立生成 embedding。它们不在同一个上下文内进行交叉注意力。\n无直接交互： 模型无法在编码阶段进行查询和文档之间的词级别深度交互。它依赖于每个文本的独立语义表示。\n目标： 通过对比学习训练，模型被教导将语义相关的查询和文档映射到同一个向量空间中相近的位置。这样，即使没有直接交互，它们的向量距离也能反映语义相似度。\n例子： 语义搜索、推荐系统、向量检索。eg: CodeRankEmbed\nAI 客户端端的发展趋势\n链接更大的外部世界\n例如 cursor 对 外部 MCP 工具的调用，以及对 外部文档的索引\n压缩上下文（更少的 token 使用\n例如 cursor 对 database 的索引构建\n边缘 AI 与本地处理\n个性化与自适应学习\n例如 cursor 根据用户操作自动生成 rules\n增强的人机协作与迭代优化（从“一次生成”到“共同创造”）\nAIGC 发展的一些难点\n1. 能一次性能够从 0 -> 1 更高效的完成任务（类似 lowCode，或者 no code；\n难点：如何维护和二次编辑这些一次性生成的结果。\n图像生成：不再有图层、蒙版和可单独操作的对象（传统 PS\n解决方案：微调，局部重绘/扩图（in-painting/out-painting）（编辑图像的特定区域）或使用控制网络（如ControlNet）\n代码生成：可能缺乏结构化的输出\n解决方案：约束成结构化输出（代码可能通过文件结构约束\nLLM 评估\n1. SWE-bench 评估\nGitHub Issue修复   | 2294个真实项目问题\n评分只能代表这个 LLM 在提供的 Agent 下的表现能力，只能做纵向比较，而不能做不同品类横向比较（比如 chatgpt 跟 gemini 的 agent 设计上就可能不同）\n2. **Aider Polyglot-Diff**  \n核心：**最小化代码变更**（只改必要行）\n多语言一致性：Python/JS/Java/Go/Rust\nreference\ngpt-4.1 intro\nAI Agent\nLLM 在逐步吸收和内化“规划”、“ReAct”和“工具使用”的核心能力。这种趋势使得LLM能够成为更加自主和多功能的AI代理。那么 Agent = LLM + Planning + reAct + tools 还是否成立？\n未来 Agent 是何种形态？Agent ≈ (高度集成且内化了规划、ReAct和工具使用的) LLM + 强大的记忆系统 + 多模态感知与交互能力 + (可选的) 多智能体协作 + 物理世界具身能力 + 内置的安全与对齐机制。\nLangGraph 是为了自主可控\nLangGraph 提供了显式且可控的编排能力。它不是让 LLM 完全自由地去规划一切（LangChain），而是让 LLM 在你设计的“导航图”中做决策。这使得你在处理复杂、需要严格控制流程、或有确定性循环和分支的任务时，能够有更高的可预测性和健壮性。\nchatGPT vs Gemini\nchatGPT 4o 会在回答结尾附加一个问题，引导用户继续提问\nGemini 2.5 Pro 会给出简洁的回答，很少去引导交互\nchatGPT 的商业模式是基于用户提问次数收费，这是主业务，所以针对性做引导继续提问的训练），而 Gemini 当前阶段首要目标可能是服务 google 内部的各种产品，所以需要简洁的回答？\n2025-04-27\nAI CodeReview 感悟\nRAG，retrieval augmented generation，实际是对 diff 代码的与提示词组装成**非结构化** context 投喂给模型得出想要的**结构化数据**\n越是弱的模型，越需要好更加具体的提示词来引导，比如 runtime error 意味着哪些场景？\n不同时期对于提示词（可调节）的要求不同，比如早期模型可能需要关注 runtime error 的场景，而后面可能关注 performance 的场景\nChatGPT 图像-安全性-出处查询\n通过 C2PA 和内部可逆搜索查找出处\nC2PA 是“身份证”，明确写在文件里，别人可以直接查。方法：\n图像的 meta 信息部分\n内部可逆搜索 是“DNA 检测”，即使你撕掉身份证，也可以通过生物特征确认你的身份。（使得像素分布符合特定模式）可能的方法：\n模型指纹（Model Fingerprinting）\n        每个 AI 生成的图片或文本都可能带有独特的统计特征（如噪声模式、像素分布等）。\n        即使去除了元数据（如 C2PA），OpenAI 仍然可以通过这些特征识别内容是否来自 GPT-4o。\n隐式水印（Invisible Watermarking）（可能的技术之一）\n        某些 AI 生成内容可能使用不可见的水印，例如：\n        轻微调整像素值，使其符合特定数学模式，但不影响肉眼可见的内容。\n        类似于 Google DeepMind 开发的 SynthID 技术，嵌入“难以去除但可识别”的水印。\n神经网络特征分析（AI-Generated Content Detection）\n        AI 生成的图片、文本、代码往往有特定的模式（如 GPT 生成的代码可能更符合语法规范但缺乏创意错误）。\n        通过训练检测模型，OpenAI 可以用机器学习方法来判断某个内容是否由 GPT-4o 生成，即使没有元数据。\n知识图谱原理\n大模型反编译代码\nAI 对技术传播的双重影响\n是促进，还是限制？如果大家都使用大模型解决技术问题，而大模型又是基于已有数据（旧技术）训练，新技术的应用会落后，没人使用，会导致网上有关新技术的信息很少，导致 AI 缺乏新技术的训练数据，那新技术就很难被普及？\nAI 是阻碍技术传播的元凶\nworld labs 图片生成 3D 视频\ndemo\nsupervision\nAI 图片识别中的视觉注入(visual-prompt-injections)\n1. The Invisibility Cloak(隐身斗篷)\n2. I, Robot(我，机器人)\n3. One advert to rule them all(一个广告统治他们所有)\nGPT-4 Vision 提示注入\n奇异值分解\n奇异值分解（SVD）是一种矩阵分解技术，它将一个矩阵分解为三个矩阵的乘积：一个正交矩阵、一个对角矩阵和一个正交矩阵的转置。\n奇异值分解在数据降维、图像压缩、推荐系统等领域有广泛应用。\nAI DOOM\n传统的电子游戏开发流程，通常涉及复杂的预设逻辑，遵循用户输入、更新游戏状态、渲染画面的固定循环。\nDeepMind提出了一种革命性的想法 - 通过生成式AI模型完全抛弃这些预设逻辑，依赖AI实时生成游戏的内容与状态更新。\nAI技术颠覆游戏开发：谷歌DeepMind GameNGen实时生成《DOOM》探秘\n人工智能的现状，任务，架构 与统一\n视频生成训练的演化?\n关键帧（+描述） + 插值 -> 端到端的学习\n分镜技术 + patches 技术\n一种快速理解模型的方法\n从损失函数开始\nU-Net 是像素级别的分类损失跟 dice_loss 组成\nSegment Anything Model (SAM) 使用了一种基于交叉熵的多任务损失函数，其中包括了像素级别的分类损失和边界框级别的回归损失\n边界框回归损失\n机器学习基础数据集\n图像分类领域：MNist, Imagenet, CIFAR 10 等\n自然语言处理领域：IMDb Large Movie Review Dataset 等\n图像分割：COCO\nreference\n神经网络逐步取代 SVM 原因\n为什么我们大部分使用神经网络进行分类而非SVM？因为核函数是我们手动设计的固定算法进行特征提取，就如同图像处理早期的手动设计的卷积核一样，无法训练，这样的方法能力上限有限，所以就被神经网络淘汰了。\n人脸识别与普通图像识别的训练区别\n人脸类别太多（可能成千上万），每个类别数据量太少；而且检测同一人的人脸应该聚类，即使在面部表情、光照、头部姿态等方面有极大变化。\nSphereFace（Angular Softmax Loss）提出了一种新的损失函数 -- A-Softmax 损失，这种损失函数专注于学习每个人脸类别在角度边界上的分布。具体来说，这种损失函数强调同类别的人脸相似度和不同类别的人脸的区别，有助于模型把相同人的面部图像映射到相近的位置，将不同人的面部图像映射到远离的位置。\nscaling law\nLLM中的Scaling Law（比例律）是指， Language Model（语言模型）的性能会随着模型规模的增加而指数增长\n生物学：代谢率，心跳频率，生物体的结构支持系统，生物的寿命，运动能力等与体型关系\n社会性：人群规模对社会行为和现象；例如，城市规模与城市发展、社会交流以及资源利用之间可能存在一些规模效应\nTowhee 框架\n是一个用于处理非结构化数据的框架，它利用最新的机器学习模型来创建 ETL（提取、转换、加载）流水线。非结构化数据是指无法存储在表格或键值对格式中的数据，如图像、视频、文本等。\nVGG 网络\n视觉几何组（Visual Geometry Group）所开发，VGG网络使得网络设计的理念发生了重要转变，即通过重复使用简单的层结构（3x3卷积核和2x2池化层）并深化网络结构，来提高性能。VGG网络同时还证实了深度是实现优秀性能的关键因素之一。\n感知损失（perceptual loss）\n也称为内容损失（content loss），是一种在深度学习特别是在视觉相关任务中使用的损失函数。它不同于传统的像素级损失函数（例如L1损失和L2损失），感知损失更注重于图像内容的感知相似性而不只是像素值的相似性。\n例子:假设我们正在进行一个图像风格迁移任务，其中目标是将一幅图像的风格（如梵高的画风）迁移到另一幅图像上，同时保留图像的内容。理想情况下，生成的图像应该在视觉上看起来要有梵高笔触风格的颜色和纹理，但同时能识别出原图的内容（如城市的轮廓、天空的位置等）。\n在这个任务中，如果使用像素级损失，那么模型可能会非常注重确保生成图像在像素层面与原图尽可能接近，而忽视了风格上的转变。这可能导致风格迁移效果不明显。\n实现方式：如果使用感知损失来训练模型，我们会首先通过一个预先训练好的深度CNN（如VGG网络）传递原图和生成图，然后计算这两幅图在某些内部层激活值的差异。这些层的激活值代表了图像的高级特征，所以这种差异反映了它们在内容和感知上的相似度。最小化这种差异可以鼓励生成的图像在视觉感知层面上更贴近原图的内容，同时也有目标风格的特质。\n适用任务：风格迁移、超分辨率和图像合成等。\n问题：哪一图层是提取的风格信息？\n>>在CNN中，随着层级的加深：\n初始层主要捕捉基础信息，如边缘和颜色。这些层对图片细节的响应很敏感，但并不捕捉具体的风格信息。\n中间层捕捉更复杂的特征，如纹理和图案，这些正是构成图像风格的要素。\n深层则表示更高级的内容，例如图像中的对象和整体布局。\nstable diffusion\nstable diffusion 是一个生成模型\n目标：用降噪网络生成清晰的图像（加噪是辅助训练降噪）\n类比：类似 GAN 模型，只不过 GAN 是个步骤的对抗训练，而 SD 是一个多步骤\n方法\n加噪过程（前向过程）是一个马尔可夫链，它逐步将随机噪声添加到数据中，直到数据变成纯噪声。这个过程是可控的，因为我们知道每一步加入的噪声量。通过这种方式，模型可以学习在任何给定的时间步骤预测噪声的分布。\n降噪过程（逆向过程）中，模型使用在加噪过程中学到的知识来预测噪声，并从噪声数据中去除这些噪声，逐步恢复出清晰的数据。如果没有加噪过程，模型就没有机会学习这些噪声分布的信息，也就无法有效地进行降噪和数据重建\n疑问\nSD 中将加噪图片输入U-Net中预测噪声如何理解？\n\"预测噪声\"是扩散模型的一部分，它涉及到将图像从含有噪声的状态逐步恢复到清晰的状态。SD模型首先将一张完全随机的噪声图像（或者是经过一系列噪声添加步骤后的图像）输入到U-Net中。U-Net的目标是预测这张噪声图像中的原始噪声成分。一旦预测出这些噪声，模型就可以从噪声图像中去除它们，从而使图像逐渐变得更清晰。\n降噪过程中的预测噪声跟实际噪声的对比，这里的实际噪声是从加噪中得到的么？\n什么是噪声分布信息？\n噪声类型（高斯噪声等），噪声参数（均值方差），时间依赖项（噪声的量和性质会随着时间步骤的推进而改变）\n神经网络可解释性\n增量学习\n理论：增量学习的核心在于模型能够通过不断学习新数据来提升自身的性能，即使这些数据是由模型自身已经准确识别过的。源于人类的终身学习能力，即不断获取、调整和转移知识的能力，同时避免灾难性遗忘——即新知识的学习对旧知识造成的干扰。\n思考：意味着模型识别准确的新数据再来投喂给模型本身训练也能提升模型准确率？\ndata-centric AI \nData-centric AI is the discipline of systematically engineering the data used to build an AI system. — Andrew Ng\nSAM  data-centric AI\n核函数\nkernel function or kernel trick\n概念：将原始空间中的向量作为输入向量，并返回特征空间（转换后的数据空间,可能是高维）中向量的点积的函数称为核函数。\n简单理解：一种便捷的计算在高维空间里的内积的方法。高维空间的数据计算存在困难。所以替代方案是在特征空间中计算相似度度量，而不是计算向量的坐标，然后应用只需要该度量值的算法。用点积(dot product)表示相似性度量。\n歧义：把数据从低维映射到高维的是映射函数而不是核函数\n带例子的核函数解释\n核函数概念\n嵌入层\n嵌入层是一种将离散值转换为连续向量的技术；\n例如：torch.nn.Embedding(10, 5) 将创建一个嵌入层，该层可以将离散值（例如：[1,'a','你好']）映射到连续向量空间中的 5 维向量。\n如何理解这里的离散跟连续呢？\n向量里的每一项是否连续，决定了向量本身是连续的还是离散的。例如：[0.2,0.3,0.5] 是一个连续向量，因为它的每一项都是实数，而实数是连续的。 这样的向量可以进行连续的运算和比较，比如求和，求差，求点积，求模长等。\n而离散是指每一项都不是连续的。例如，[1,‘a’,‘你好’]是一个离散向量，因为它的每一项都是离散的，而且不能进行连续的运算和比较。中的1不可能是’a’，因为它们属于不同的集合，也没有定义它们之间的转换规则。\n机器学习与英语学习\n机器学习的内容输出可以类比为通过沉浸式学习英语后能够说出英语，其中另一种方式是通过先学习词法和语法规则。在机器学习中，模型通过大量的数据输入（数据投喂）来理解其中的潜在规律和特征。类似地，通过沉浸式学习英语，我们可以在大量的语言环境中感知和理解英语的潜在规律和特征，从而能够流利地说出英语。\n早期的人工智能（通过条件语句进行判断然后做输出）与先学习词法和语法规则的方法与相似。早期的人工智能系统通常使用预定义的规则和条件语句来处理输入并生成输出。这些规则和条件语句基于词法和语法规则，用于处理特定的输入情况。类似地，通过先学习词法和语法规则，我们可以在语言学习中掌握词汇和语法规则，并使用它们来理解和生成语言。\n两种机器学习区别：通过先预定义的规则和条件语句的方法在一些特定场景下具有**精确性和可解释性**的优势，而通过机器学习从数据中学习的方法则**更加灵活、适应性强，并能够处理复杂情况**。\n两种英语学习区别：通过先学习词法和语法规则来学习英语可以提供**结构化学习和准确性**，但**缺乏实际应用，语感欠缺，学习繁琐**，而沉浸式学习英语则更加贴近**实际应用、注重流利性和文化融合**，但**需要环境支持，初始困难**。\n思考：意味着初始的时候先了解基本语法，后续不断去读各种精选文章（而不是研究更深的语法规则）才是英文学习的最佳路线？而机器学习没有初始化烦恼，所以直接去学海量数据就好？\nGAN，VAE，Diffusion 生成模型理解\n潜空间\nGAN 是先随机一个符合高斯分布潜在空间作为Generator输入生成图片，然后投喂给Discriminate 作为输入判定然后做 反向传播；\nVAE 是通过训练 Encoder 将输入映射到复合高斯分布（实际通过训练得到均值和方差）的潜在空间，然后解码器通过对潜在空间解码得到输出；数据先降维再升维\n在 MNist 中潜在空间可以是20维长度的向量，来表示 20 个不同的均值和方差分布，来代表 20 个可能的特征？\nDiffusion模型和其他生成模型一样，实现从噪声（采样自简单的分布）生成目标数据样本。\n核心原理是通过一个随机的前向过程（Forward Process）和一个去噪的逆向过程（Reverse Process）来实现从噪声（Noise）到目标数据样本（Data Sample）的转换。\nDiffusion vs GAN\n速度：Diffusion 需要多步骤到图片，而 GAN 是一步到位；所以Diffusion 会慢，但是训练过程也更稳定\n应用面：扩散模型可以利用多种条件来控制生成的图像，比如文本描述、图像掩码、深度图等，而 GAN 通常只能利用类别标签或噪声作为条件。这使得扩散模型可以更灵活地应用于不同的任务，比如图像编辑、图像修复、图像翻译等\nDiffusion 模型过程\n初始噪声：从某个先验分布中生成初始噪声信号。\n扩散过程：通过一系列步骤，将当前噪声信号逐渐扩散，以生成下一个时间步的噪声信号。这个过程中使用了逆扩散方程，可以将当前步骤的噪声信号映射到上一步骤的噪声信号。\n逆扩散采样：通过逆扩散过程中的采样操作，将当前噪声信号转化为以下一步的噪声信号。\n生成器网络：使用生成器网络将当前噪声信号映射回高维空间，生成一帧图像。\n损失函数与优化：根据生成图像与目标真实图像之间的差异，定义适当的损失函数，并通过反向传播和优化算法来更新生成器网络的参数。\n理解 VAE\n“局部是由于方差控制，而全局是由于均值控制”这句话强调了在VAE模型中，方差向量主要影响生成数据的局部细节和变异性，而均值向量则决定了数据在潜在空间中的全局分布和结构。\nVAE理论跟数学\n变分推理\nKL 散度\n在训练辨别模型时，为了简化计算，人们往往直接对交叉熵进行优化。 而在 在训练生成模型时，为了使分布与相互接近，我们必须直接对KL散度进行优化。\n自信息、熵、交叉熵与KL散度 的推导\nKL-Divergence 与交叉熵\nKL 散度形象说明（翻译）\nEM——期望最大 算法\n交叉熵\n贝叶斯定理\n自由能\n深度学习优化器\n优化方向\n基于动量（NAG）\n基于自动学习率 （例如 RMSprop，配置训练简单）\n结合两者的（Adam 一般最优）\n为什么 WGAN 选择 RMSprop 作为优化器，而不是 Adam?\n>> WGAN的目标是通过最小化生成器和判别器之间的Wasserstein距离来提高生成样本的质量。传统的生成对抗网络（GANs）在训练过程中容易出现梯度伪影的问题（告诉你错了，但并没有指出错在哪里，导致更新方向错误，把正确改掉，错误留下），即判别器的梯度无法提供有关生成器当前状态的准确信息，导致训练不稳定。RMSProp优化算法通过自适应地调整学习率来减轻梯度伪影问题，有助于更稳定地训练WGAN。\n深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）\n反向传播\n有一个简单的神经元函数 y = w * x，模拟计算梯度和进行反向传播的过程。\n一次权重更新过程（给定初始数据输入 x = 1，w = 10，实际输出是 10，期望输出 y1 = 2 则 目标 w 为 2）：\n计算损失：\n计算梯度：\n反向传播：\nCGAN MNIST 训练步骤\n1. 固定 generator （ real_label = [batch_size, 10] 的对真实 label的 one-hot 编码 ）\n用真数据训练 output_label = Discriminator(real_image)，d_real_loss = BCELoss(out_label,real_label)\n用虚假数据（噪音 + 真实标签 [batch_size, noise_dim（满足0~1正态分布）] + real_label = [batch_size, noise_dim+10] = z_tensor ）\n训练 fake_image = Generator(z_tensor) 得出 fake_image( Tensor[batch_size, 1, 28, 28])\n再次 out_label = Discriminator(fake_image) ，d_fake_loss = BCELoss(out_label,fake_label(全0))\n计算 D_loss = d_real_loss + d_fake_loss 反向传播，更新 Discriminator\n2. 固定 discriminator\n由 fake_image = Generator(z_tensor)\n由 Discriminator(fake_image) 得出 out_label ，g_loss = BCELoss(out_label,real_label)\n计算 G_loss = g_loss 反向传播，更新 Generator\n预处理\n归一化：一种常见的图像预处理操作，它用于将图像的像素值归一化为均值为0、标准差为1的分布，或者只将数据收窄到 -1 ~ 1 之间。常用于 CNN 网络数据预处理\n加速训练：常用的激活函数如 Sigmoid 和 Tanh 在输入值较大或较小的区域会饱和，导致梯度接近或完全为零，从而使梯度下降变得非常缓慢或停滞。通过将像素值缩放到 -1 到 1 的范围，可以使输入值位于激活函数的线性区域，避免梯度饱和问题，提高网络的训练效果。\n模型稳定性：在优化算法中，例如梯度下降法，较大的梯度值可能导致参数更新过大，从而使优化过程不稳定甚至发散。通过将像素值缩放到 -1 到 1 的范围，可以将梯度控制在较小的范围内，提高优化算法的数值稳定性，使模型更容易收敛。\n数据分布一致性：将像素值缩放到 -1 到 1 的范围可以使不同图像之间的像素分布更加一致。这样做的目的是确保输入数据的统计特性在整个训练集上是相似的，从而提高模型的泛化能力。\n推广：Batch Normalization (BN) 层作用类似，但是应用在**训练阶段**，对每个小批量数据进行标准化\n损失函数\n交叉熵\n熵：阿根廷 1/4概率打进决赛 ，1/2 概率获得冠军，1/8 获得冠军，则有 f(1/8) = f(1/2) + f(1/4)，f(x) := 信息量，推出可能的 f(x) := -log(x) （log 2为底单调上升，加负号才则单调向下）\n交叉熵：KL 散度是一种用于衡量两个概率分布之间差异的度量，KL(P || Q) = Σ(P(i) * log(P(i) / Q(i)))，固定分布 P 的时候 KL 散度可以化简为交叉熵 KL(P || Q) = Σ(P(i) * log(P(i) / Q(i))) = -Σ(P(i) * log(Q(i))) = -H(P, Q)；可以很好的用于机器学习损失计算\n问题\n回归跟分类区别？\n分类例子：识别图片是猫还是狗\n回归例子：通过特征1-n预测房价\n思考：分类跟回归的区别是目标的 离散跟连续 区别？还是说输出的label之间是否有“距离度量”？\n为什么交叉熵适合分类，而 MSE 适合回归?\n交叉熵\n概率解释性：交叉熵基于概率分布之间的差异进行度量，更适合分类问题，因为分类问题通常涉及对不同类别的概率分布进行建模和预测。\n梯度更强烈：相对于MSE，交叉熵的梯度更加陡峭，这可以加快模型的收敛速度。对于分类问题，更快的收敛速度可能是一个优势。\nMES\n数学上的合理性：MSE 是对预测值与真实值的差异的平方进行度量，可以提供对预测误差的较为精确的度量。\n对异常值不敏感：平方差的计算使得 MSE 对异常值不敏感，因为平方操作会放大异常值的影响。这在某些回归问题中可能是有益的。\nReference\n王木头学科学\nhttps://zhuanlan.zhihu.com/p/104130889\n回归与分类问题区别\nTransformer \npositional encoding\n位置编码的要求：选择正弦跟余弦组合编码\n每个位置都有唯一的编码。\n在不同长度的句子中，两个时间步之间的距离应该一致。\n模型不受句子长短的影响，并且编码范围是有界的。（不会随着句子加长数字就无限增大）\n必须是确定性的。\n总结\n问题及其解答：\n为什么没有直接使用 1,2,3...这种线性编码？\n原因：周期性模式在位置编码中的不同维度上呈现出不同的变化速度和周期（下面例子会说明）\n捕捉长距离依赖关系（线性模式也能做到，但是不够精细）\n提供更丰富的表示能力：较低频率的维度具有较长的周期，可以捕捉到大范围的序列结构，而较高频率的维度可以更细致地表示局部模式和短距离的依赖关系。\n避免过拟合：随着句子变长，这些值可能会变得特别大，并且我们的模型可能会遇到比训练时更长的句子\n思考例子：\n第一个词编码为 [1,2,3]， 则位置可用向量 [秒，分，时]来表示；第二个词编码为 [4,5,6]， 则位置可用向量 [秒 + 1，分 + 1/60，时 + 1/360] 来表示\n周期：在一个词向量上会出现不同的周期变化，能同时追踪近距离跟远距离的词关系：秒针走一个周期 60 秒，分针走一步；分走一个周期 60 分， 时针+1；\n周期设定：通过设定 秒，分，时之间的周期关系（比如可以设定600秒，分针才走一步，则会拉上周期变化，追踪更远的词关系）\nReference\npositional encoding blog\npositional encoding stackexchange + youtube\nself-attention\n思考\n多头注意力机制与卷积的多通道（channel）进行类比。多头注意力机制和卷积的多通道都涉及并行地学习不同的特征表示。它们都致力于提取输入数据的多样化特征，并捕捉输入中的不同模式和关联性。\nReference\nself-attention\nmulti-head attention in transformer)\none hot 编码\nOne-hot 编码是一种将离散的分类标签转换为二进制向量的方法，它的优点是可以消除不同类别之间的偏序关系，使得特征之间的距离计算更加合理。（方便在机器学习分类任务计算 LOSS）\n例子\n比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。\n两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。\n那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。\n实现\n思考：如果是不定长编码，该如何改进?\n机器学习：数据预处理之独热编码（One-Hot）\nCRNN + CTC 解决变长文本识别\nCTC 如何计算预测序列跟目标序列的相识度？\n1. 生成 time step 的预测概率（有 CRNN 模型输出），每个 time step 会有对类别的预测概率向量\n2. 通过算法将所有可能路径的概率相加，并取对数（通常用于数值稳定性）得到最终的相似度分数。\nReference\n深度学习编程范式\nTensorflow vs Pytorch（符号式（也叫声明式）与命令式程序）\n命令式\n更加灵活：原生语言的灵活性跟运行时断点\n对应符号式（DSL）：\n节省内存：掌控全局的内存分析并优化\n符号图：!符号图\n类比：类似 react jsx命令式（直接难优化）模板跟 vue 的声明式（真正执行前能做各种运行时优化）模板？\nReference blog\n机器学习分类的一点技巧\n对数据进行分类\n有特征：直接通过已经有的分类进行绘制图\n先通过特征维度绘制图；例如：数据集仅包含两个分离相当明显的聚类。其中一个簇包含 Iris setosa，而另一个簇包含 Iris virginica 和 Iris versicolor；通过特征绘制出的图会分成明显2堆，其中一堆是交错2种类型 Iris\n如果没有明显的聚类，并且数据维度多，可以通过 PCA 等方式降维后再分\n无特征：则先通过 KMeans 能方式聚类，再通过有特征方式分析\n尝试理解 ONNX （Open Neural Network Exchange）\n是什么？ONNX = （模型本身 + 模型训练好的权重跟偏置）的一种更加抽象的表达\n如何表示？使用预定义的 operator（描述输入与输出的关系，例如：add算子=  inputA + inputB = OutputC，可拓展）来描述模型，用向量描述训练好的参数\n作用？实现不同深度学习框架和平台之间的模型互操作性\n为什么 pytorch 在导出 ONNX 的时候需要传入一组输入？原因：\nONNX 并非像编译器一样彻底解析原模型的代码，记录所有控制流；而是不考虑控制流的静态图\n而是利用 pytorch trace 机制，将参数传入模型执行，并记录执行这组输入对应的计算图\n深度学习中的 Epoch 和 Batch\n1. Epoch 是什么？ \n一次 Epoch = 让所有数据通过模型正向+反向传播一次 = 一个完整的学习周期\n2. Epoch 设置多少次合适？\n无定论：\n次数少会导致欠拟合；\n次数多会导致过拟合；\n3. 什么是 Batch？\nBatch Size = 一次训练的样本数\n每一次参数的更新所需要损失函数并不是由一个数据获得的，而是由一批数据加权得到的\n4. Batch 的作用？\n效率：利用矩阵计算加速（相对于单个去训练）\n稳定性：平均每个数据样本的贡献，减少梯度的方差\n多大合适：看情况，太小会导致训练太久；太大会导致内存受不了\nReference\nepoch-vs-iterations-vs-batch-size\nGPT 尝试\n1. 代码尝试\n重构转换\n输入 js -> ts\n优化\n格式化代码\n创建\n创建插件（eslint）步骤：写测试用例 -> 输入 gpt -> 生成插件 -> 微调成型\n问题：自动生成的代码会比较繁琐或者隐藏逻辑问题，也不会去利用第三方的包的能力\n目前方案：需要 developer 找到更便捷的方式再去投喂给 gpt 生成更加合理简洁的代码\nCNN 的简单理解\n网络越深，学习的知识越抽象：比如第一层hidden layer负责编码诸如点、线、边缘等浅层信息；第二层hidden layer编码简单点的纹理、形状等信息；第三层hidden layer编码诸如眼睛、鼻子等目标的形状...，然后逐层学习，不断地提取抽象的特征，一气呵成，最终学会了辨识花草树木、飞禽走兽等等。 - reference\n网络越宽，每一层学习的知识越丰富：增加网络的宽度意味着同一个hidden layer有着更多的神经元，每一个神经元代表一种颜色，一个方向，一种纹理，组合起来便可以学习到更多不同的颜色信息，各个不同的方向以及不同频率的条纹信息。\n一句话信息\n生成对抗网络（GAN） VS 变分自编码器（VAE）： GAN 倾向于生成逼真的合成样本，而 VAE 倾向于生成具有一定程度多样性的样本。如果期望生成特定目标样本，可以考虑 CGAN 跟 CVAE* GAN 基本原理及其应用\nDALL-E uses Discrete Variational Autoencoder (dVAE) for this step. dVAE is a variant of traditional Variational Autoencoder (VAE) that operates in a discrete latent space. It is similar to VQ-VAE but uses distribution instead of nearest neighbor.\nVIT - Google推出了VIT（Vision Transformer）：一个和Bert几乎一致，同时不添加任何卷积结构的图像分类模型。VIT在Transformer上的成功，证明了可以用统一的模型，来处理不同领域（语言/图像/视频）的任务，进而开启了多模态模型研究的新篇章。\nvit彻底赢了 CNN 么：transformer全局感受野，在大图片或者说找东西时效果好（类似近视眼，能够感受图像大轮廓）。cnn局部感受野，对细节处理较好（理解像素级别的问题，例如 医疗影像）。\n文摘 - 高手解决问题的方式从来都不是纠结问题本身，而是升维；升维成功，问题也就解决了\n雷军2023演讲 - 如何快速学习：知识不全是线性的，大部分是网状的，知识点之间不一定有绝对的先后关系；前面内容看不懂，跳过去，并不影响学后面的；后面的学会了，有时候更容易看懂前面的。\nAGI 的一点理解\n机器学习训练了很多模型，而 LLM 只是其中之一；\nChatGPT 之所以跟 AGI 最接近，是因为语言模型的通用性；\n如果能够理解自然语言，那就可以实现所有文字能够描述的任务；\n其他 AI 任务，比如图片识别，则只能做到图片相似度能人物处理（推荐），无法拓展到相对通用的任务，除非日常交流能够通过表情包完成\ngpt 可能的研究方向\n建设高难度的综合任务评测数据集（LLM 的测试用例，越完备 -> 越强大）\n高质量数据工程（密集+多样性）：LLM 进化 = 更多高质量数据\n数据例子\n密度极高的高质量数据：wiki\n高质量问答：quora，知乎\n高质量图片：\n思考\n高质量数据消耗完后 gpt 如何进化？\n能否自己创造知识自己消费（类似 alpha-go 自我对弈的进化）？\n如果 gpt 成长的资料来源于人类，那能否突破人类知识的边界？\n探索 LLM 模型的规模天花板：大模型大数据，能参与的玩家不多\n思考：是否会出现共建超大模型\n增强 LLM 的复杂推理能力\nLLM 纳入 NLP之外更多其它研究领域：多模态？\n如何突破符号领域？如果某个领域是非成文的，不能用符号记录表达，那么 GPT 是否就无能为力。比如，人类的很多心理活动、潜意识、灵感、顿悟等等，GPT 如何模拟生成。\n更易用的人和LLM的交互接口：听觉？\n超大LLM模型Transformer的稀疏化：相同算力下提高训练速度\n参考\n通向AGI之路：大型语言模型（LLM）技术精要\n关于 chatGPT 引发的人工智能思考 2023-3-1\n人跟AI的关系：淘汰还是互补？\n人有自主目的性（AI暂无），AI是实现目的的工具；\n提问跟回答能力，哪个更能生存下来？\n往后提出好问题能力的重要性将越来越超过回答问题能力\n教育\n投喂答案的教育模式需要变革 -> 把提问能力列入考核标准，更能培养出人机协作人才\n让人利用机器，而不是把人培养成机器\n文理分科这种教育模式急迫需要改变：chatGPT 需要文理结合，提好问题，同时认清答案\nchatGPT 衍生的未来职业？（更新 2023-3-13，参考）\n场景跟问题\n更精准提出需求，才能利用好 chatGPT\n辅助 chatGPT 修正回答错误，同时又不影响模型输出的其他答案\n检测回答是否由机器生成\n如何避免 chatGPT 泄密，如何做隐私保护（目前可以 chatGPT 被催眠然后突破本身不泄密的限制）\nAI 训练，避免伦理问题\nAI 本身安全：解决提示注入（类似 网页的 xss ，SQL 注入等），越狱等安全问题\n知识产权重新定义：AI生成的东西到底算不算侵权？（例如之前的爬别人网站的数据作为自身的商业盈利依据，是否算侵权？）\n趋势预测\n动作（运动，游戏等数据）文件化：可以对整场羽毛球做文字标记序列化，然后输入 chatGPT，最后可以预测落点跟个人行为\n职业名？\n标注师：标记信息，投喂并训练 AI\n安全员：确保 AI 不被攻破\ngpt自己的回答\n提示工程师：提示是一种指导GPT-4生成内容的文本或图像，通常包含一些特殊的符号或指令。提示工程师就是专门设计和优化提示的人员，他们需要了解GPT-4的内部机制和逻辑，以及不同领域和场景下用户的需求和偏好。提示工程师可以为各种应用场景提供高质量、高效率、高安全性的提示服务。\n内容审核员：虽然GPT-4具有强大的生成能力，但它也可能会产生一些不合适或有害的内容，如色情、暴力、歧视、谣言等。内容审核员就是负责检查和过滤GPT-4生成内容中是否存在这些问题，并及时删除或修改不良内容。内容审核员需要具备一定的专业知识和判断能力，以及良好的道德素养和责任心。\n内容运营师：内容运营师是利用GPT-4为各种平台和渠道提供优质内容服务的人员，他们需要根据目标受众和市场需求，选择合适的提示和参数来调用GPT-4生成相应类型和风格的内容，并进行编辑、优化和发布。内容运营师需要具备一定的创意思维和文案能力，以及对各种媒体平台和行业动态有一定了解。\n内容创作者：内容创作者是利用GPT-4辅助自己进行创作活动的人员，他们可以将自己想要表达或传达给用户",
    "url": "/blog/AiNotes",
    "path": "AiNotes.md",
    "lastModified": "2026-01-20T02:16:53.332Z"
  },
  {
    "title": "DailyReflections",
    "content": "DailyReflections\n简介：个人随笔\n越来越亮的车灯\n更亮≠更安全，反而带来对向车辆致盲、错判距离等问题。\n“大灯地狱”反映了现代技术在缺乏系统性、共情性设计思维时，如何在不经意间制造新的不平等和风险。它提醒我们：技术理应为人服务，而不是让更多人付出代价。\n简单与难运动\n入门简单比入门难的运动的综合能力要求更强，上限更高，下限也更低？\n入门越容易的运动，形成金字塔的人数就越多，塔尖也就越尖锐；这不是因为运动本身更复杂，而是因为要在海量人中脱颖而出，你必须几乎全能。\n而入门困难的运动往往门槛高、筛选强、参与者少，所以虽然技术复杂、入门难，但上限可能反而不比前者高太多，整体竞争环境也较狭窄。\n精英过剩\n大规模升学幻觉：教育制度制造了“精英感”，却没有足够社会位置承接它，造成大规模心理与经济落差。而真正推动社会剧变的往往不是“被抛弃的大众”，而是“未被接纳的精英”。\n当前的精英通道（小学→博士）本身就是“既得利益者”们走出来的一条路径，他们掌握资源与话语权，倾向于维护这条路径的正统性和地位，这就是一种典型的 “路径依赖” + “认知守旧” + “制度惯性”。当前体制内重要岗位（高校、政府、国企）几乎都是通过这条路选拔出来的；他们自然认为这条路径“最可靠”“最正规”；于是继续用考试、学历、资历、论文来选拔下一代 → 进一步固化。\n| 问题类型      | 描述                           |\n| --------- | ---------------------------- |\n| 🎓 教育膨胀   | 太多高学历者，但没有相应工作岗位匹配（例：博士送外卖）  |\n| 💼 职位竞争   | 精英岗位饱和，内部卷得更狠，小错误都可能被放大处理    |\n| 🧨 精英激进化  | 一部分被排除在“核心圈”之外的准精英，转向民粹、激进主义 |\n| 🧠 意识形态冲突 | 精英内部无法达成共识，派系斗争频繁            |\n| 📉 社会信任下降 | 普通人看到精英互撕，会对系统丧失信心           |\n| 🥊 政治极化   | 精英在政治、媒体、文化领域争夺控制权，导致撕裂加剧    |\n解决方案：解决精英过剩的根本，不是“筛掉谁”，而是“多造路” + “改观念” + “改分配”。\n控制精英供给端：合理设限、分流，例如：国内通过本科/职教分流，德国职业教育与学术教育双轨制\n扩大精英吸纳能力：拓展蛋糕，例如：新产业、新科技、新职业路径（如数字经济、绿色能源、人工智能领域），企业出海\nAI与职业：借助大模型/数据智能，精准引导不同背景学生找到适合自己的成长路径\nHow to Do Great Work\nPick the Right Problem 伟大的工作通常源自 重要且有趣的问题\nFind Problems, Don't Manufacture Them 发现问题，而不是制造问题\nChoose Your Field Wisely 选择你的领域，一些领域更容易做出突破，比如 新兴科技、未被充分研究的领域。\nDevelop Unique Insights 发展独特的见解\nGreat Work Comes from Accumulation 伟大的工作来自于积累\nSurround Yourself with the Right People 远离消极或抑制创造力的环境，找到支持自己思考和探索的圈子。\nIgnore Distractions, Focus on Your Work 伟大的工作需要长时间的专注，而不是被短期的流行趋势所驱动。\nEmbrace Uncertainty 拥抱不确定性，不要害怕失败。\n如何培养品味\nGood design is simple 简约\nGood design is timeless 追求永恒\nGood design solves the right problem 解决正确的问题\nGood design is suggestive 启发性\nGood design is often slightly funny. 有趣\nGood design is hard 困难\nGood design looks easy 看起来简单\nGood design uses symmetry 对称\nGood design resembles nature 模仿自然\nGood design is redesign 重构\nGood design is often strange 奇怪\nGood design happens in chunks. \n世界趋同\n比如 汽车颜色从五颜六色到黑灰；比如各个大模型，最终都支持到搜索跟推理以及多语言；再比如电影电视剧都开始拍续集；再比如各个编辑器都开始支持 agent 跟 tab 补全；再比如各个视频软件都开始搞推荐，同样功能的软件界面变的都一样；\n世界趋同的本质，是效率最大化与风险最小化的共同作用。所有行业都希望以最小的成本满足最多用户的需求，而市场、技术、文化、算法等因素一起推动了这种收敛。\n不过，这并不意味着创新不存在，而是创新往往发生在微小但关键的突破点，比如 iPhone 颠覆了手机界面、GPT 颠覆了搜索体验。未来，虽然大趋势是趋同，但局部突破仍然会带来周期性的变革。\n资本、AGI 与野心\nmRNA 与传统疫苗\n关键点：安全性、免疫效果、生产速度和稳定性\nmRNA（信使RNA）是细胞内的遗传指令载体，它负责将DNA上的遗传信息传递给细胞的核糖体，指导蛋白质合成。\n流程：设计mRNA -> 生产并包裹mRNA -> 注射mRNA\n传统疫苗是利用灭活病毒、减毒病毒或重组蛋白等抗原物质，通过注射或口服等方式进入人体，激发免疫系统产生免疫反应。\n当前还不能完全取代传统疫苗，原因：\n细菌比病毒复杂，往往有多种抗原，mRNA疫苗很难涵盖所有关键抗原。\n一些疫苗（如鼻喷雾流感疫苗、口服脊灰疫苗）在黏膜部位诱导免疫，防止病毒进入体内，而mRNA疫苗主要在肌肉注射，难以产生强大的黏膜免疫。\nQRCode 的黑块限制\n1. 屏幕二维码的最小模块建议为 4像素 x 4像素。更大的会更容易被扫描识别\n2. 印刷二维码的最小模块建议为 0.4毫米 x 0.4毫米。\nvscode 括号着色\n\"准备悖论\"\n指的是，如果提前采取有效措施，就可以减轻某种未来的灾难，那么人们就会倾向于认为，这种灾难没有那么严重，不会造成太大损害，从而阻碍对这种灾难进行提前准备。\nhacknews 评分前十的讲座\nGeohash 原理\nGeohash 是一种用于地理位置编码的算法，它将地球上的经纬度转换为一个字符串。这个字符串表示一个矩形区域而不是一个点。Geohash 的核心是不断对经纬度进行 二分区域，并将结果编码为二进制，再转换为更简洁的 Base32 字符串。\n直观解释，把地球看作一个网格，每次增加一个字符（增加编码长度）就进一步细分网格。\n比如：\n9 表示一个大区域（经纬度粗分）。\n9q 表示更小的区域。\n9q8 表示更精确的区域。\n9q8yy 可精确到几米范围内。\n**什么是 S2 编码？**\nS2 是一种地理空间索引系统，基于球面几何将地球表面划分为一系列分层的单元（称为 S2 Cells）。每个单元都有唯一的 ID，用来表示地球上的区域。与 Geohash 不同，S2 直接在球面上工作，避免了平面映射中的失真。\n**对比 Geohash**\n| 特性             | Geohash                   | S2 编码              |\n|------------------|---------------------------|----------------------|\n| 投影方式        | 平面投影                  | 球面投影            |\n| 单元形状        | 矩形                      | 近似正方形          |\n| 分层方式        | 字符编码（Base32）         | 整数 ID             |\n| 精度与效率      | 适合简单应用              | 更适合复杂查询与计算 |\nS2 在地图服务（如 Google Maps 和 Uber）中被广泛使用，适合处理高精度地理数据和邻近搜索场景。\n牛鞭效应（Bullwhip Effect）\n描述的是在供应链中，由于信息在各个环节间传递的不对称或延迟，小范围的需求波动会逐级放大，最终在上游（例如制造商或原材料供应商）形成更大的需求波动。这一现象形象地得名于牛鞭——手腕的小幅动作会在鞭梢引发巨大的波动。\n举例说明\n消费者需求：某商品原本每周需要 10 个，突然需求升至 15 个。\n零售商：为了应对需求增长及库存不足，订购 20 个。\n批发商：考虑到运输周期和订单积压，向制造商订购 30 个。\n制造商：预测未来可能会有更高的需求，于是向供应商订购 50 个。\n最终，上游供应商感受到的需求远高于实际需求的 15 个，形成了牛鞭效应。\n如果金钱统治世界，那么谁统治金钱？\n答案是 那些有能力制造通货膨胀的人统治着金钱。他们首先借钱购买实物资产，通货膨胀之后，再用贬值的钱来还债。- hacker news\n解决团队分歧的四种方式\n方式一：我不同意，但一旦决策定了，我就会一起努力。\n亚马逊创始人贝佐斯说：\"当我跟一个主管反复讨论后，他不同意我的看法。虽然我不认为他正确，但我愿意陪他赌一把。我们认识几十年了，我觉得他可能比我更接近真相。其实我也不知道自己的决定是否正确，干脆照着下属的想法走。\"\n这样有几个好处：\n至少产生了一个大家同意的决策。\n事后不会互相埋怨。\n有利于形成真正的团队精神。\n如何评价世界是一个草台班子\n反映了决策层和执行层之间的脱节现象。决策层虽然有大局观，但在执行细节上往往不够深入；\n执行层尽管懂得如何完成任务，却缺乏对全局的理解。\n信息传递的障碍加剧了这种脱节，导致决策无法得到有效执行，形成了一种低效的系统。\n悲观者正确，乐观者成功\n悲观者尝试的概率小，不太可能直面风险；但是乐观者会去尝试，有一定概率遭遇风险，但也有一定概率会成功。\n技术弱势群体\n未来的高科技社会，大多数人恐怕都是技术弱势群体，没有太多机会，技术优势集中在少数人手里。\n康威定律 \nAny organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure. 软件系统的架构看起来与构建它的开发团队的组织结构非常相似。\n博客文摘部分\nConwaysLaw\n维基百科通向哲学现象\n（Wikipedia's \"Getting to Philosophy\" phenomenon）。当你点击每篇文章的第一个非括号中的链接，并不断重复这个过程，大约90%以上的情况下最终会到达“哲学”这个条目。**维基百科的条目结构趋向于层次化的知识体系，通往更抽象的概念。这种体系有助于引导阅读者从具体到抽象进行理解**，因此最终会引导到像“哲学”这样探讨最为根本性问题的学科。\n崖山之后无中华\n“崖山之后无中华”可以理解为中华的“气质”在崖山之战后发生了变化。宋朝时期，中华文明的气质更多体现在对文化和科技的崇尚、对创新的追求以及相对开放的心态。而崖山之后的元、明、清三朝，文化与科技的发展逐渐走向保守和停滞，更加注重政权稳固和经济利益，创新动力不足，文化气质发生了显著的转变。\n卡钦斯基的警告 \n卡钦斯基：论工业社会及其未来\n视频学习胜过读书吗？\n人类学习效率最高的方式，不是\"读书 + 思考\"，而是\"观察 + 模仿\"\n大部分领域的入门方式，都是\"观察和模仿\"，所以视频学习更有效。 对于深入学习，文字和书籍的效果可能好于视频，因为入门以后再往上走，\"理解 + 思考\"的作用越来越大，\"观察 + 模仿\"的作用越来越小。\n雷军2024年度演讲：人生不能后退！\n雷军2024年度演讲：人生不能后退！\n无人驾驶汽车投放\n为了社会稳定，政府需要对无人驾驶出租车有所限制。\n投放策略建议 - 摘抄自阮一峰周刊\n（1）货运优于客运。货运的距离长，更艰苦，更适合无人驾驶。\n（2）公交车优于出租车。公交车本来就缺少司机，无人驾驶可以增加班次，有利于绿色出行。\n（3）郊区和农村优于城区，那里平时不容易叫车，更需要无人驾驶。\n（4）城区的无人驾驶出租车，政府收取管理费，使得运价不低于真人司机。多收的钱用于补贴出租车行业转型。\n关于认知 更改时间（2023-2-24）\n越来越感觉到认知能力才是事情发展的关键，认知是基础然后才是生成要素的聚集；太聚焦在解决问题层面，陷入到具体实现细节的可能不是一个好的开发\n**认知的排序以及问题的解决流程**\n1. 发现问题\n2. 定义问题\n3. 分解问题\n4. 问题排序\n5. 解决问题（可能是大部分人停留的阶段）\n基层干部需要**分解问题**能力，然后是其他生产要素的收集（安排优先级，找到合适的人，定一个完成时间等）\n重要核心岗位需要**发现问题**的能力，比起知道如何做，更需要找到值得去做的事情（方向不对，努力白费），然后才是技术介入，定义问题解决问题；\n随着 chatGPT 等人工智能的发展，提出发现一个好问题可能比单纯解决问题更能匹配人机协作的未来；（目前 AI 还只能在给定问题框架下寻找答案，还没有迹象表明 AI 的自主目的性，一切还都是服务于人投喂的目的）\n关于认知的例子:\n流媒体：看下netflix跟国内的爱优腾的区别，前者自研内容飞涨带着股价飞涨，后者还在天价争夺版权，亏损到现在，爱奇艺最近（2021年底）大裁员了\n技术界：前端大神Evanyou，产出了Vue，之后又诞生了vite，能在合适的时机出手带领一波技术潮流；除了技术强，核心可能在于领先同行的认知水平\n关于沟通\n1. 首先得把概念澄清\n划分概念边界（定义解决的问题类型，例如：vue-cli plugin可以定义成面向工程结构的开发；解释webpack plugin则是面向源码的转换开发）\n类比熟悉概念（例如：解释 vue-cli的plugin机制，就可以类比 webpack plugin机制）\n提纲挈领\n提纲挈领：比喻善于抓住问题的关键与要害；\n前端\n抓住核心可以事半功倍；比如：\n1. 月经问题：Vue跟React的区别？核心在于Immutable跟Mutable这个理念，其他很多都是在此上的衍生问题\n2. Webpack跟Vite区别？核心在真正按需的理念\n关于自举\n编程里面有个概念叫：自举；\n>编程语言自己编译自己，重复这个过程可以让本身进化\n>>例如：辅助增添新的语言特征，新的特征必须要能用旧的版本来编译解析，也叫向后兼容；\n>>理论上编程语言可以通过自举来完成自身进化，不过更多只是体现在已有概念重组跟语法糖上，没法在底层上做大的改进\n>rust的出现的其中一个原因就是为了摒弃语言的历史包袱，实现一个真正意义的高级语言\n学习知识的过程也可以理解成“自举”\n>通过自身已知概念来解释新知识，将新知识逐步建立在旧知识体系之上，从而“学习到”新知识\n>>例如：小学算术是从简单1+1=2，再用个位解释灌输十位，用十位解释百位...再例如：学习编程，有C++基础学js语言会比有js基础再学C++简单\n在变化中寻找自举的底层动力，不要怕吭硬骨头，了解底层编程模型范式更能增加自举进化可能性\n关于快速学习\n通性：跟随该领域领军人物的动态\nWhy/What/How\n编程的世界\n熟悉一门新语言\n诞生的背景\n解决了哪些历史问题？\n为什么没有选择现有语言？\n应用场景\n以前的解决方案，选择这门语言的优势\n独有的特性并实践\n语法\n内存对齐策略\n判定相等\njavascript: 2个单独声明的对象不会相等，eg: let a ={name:1},b={name:1}; a!=b\ngolang: var p1 = point{x: 1, y: 2}; var p2 = point{x: 1, y: 2}; p1==p2\n组合还是继承？\n新框架\n尝试造一个类似的轮子，基本能懂一半\n学科\n1. 从哪里划分出来？为什么被划分？\n2. 解决的独有问题\n3. 个人的应用\n关于新技术应用\n新技术的成熟度\n渐进式使用，先在dev应用\n分享，集思广益\n落地\n例子：应用@swc/core到系统考虑点，参考这里\n1. 框架自身的需要能做到和 babel 解耦；\n2. 自身使用的插件需要使用 swc plugin 再实现一遍；可能带来的副作用：\n    1. 对 bundle 体积的影响；\n    2. 转换后的代码是否存在兼容性问题；\n    3. 如何快速的将 babel 插件转换到 swc 插件；\n    4. swc 现有的能力是否满足从 babel 切换到 swc。\n关于动静结合\n编程语言层面\n动态语言（JIT+VM动态编译执行，例如js）；\n优点：动态下发，容易做热替换，热更新等；dev体验好\n缺点：production性能较差\n静态语言（AOT编译成二进制+Runtime，例如C）\n优点：production性能较好\n缺点：无法动态下发，没法热替换，dev需要等待编译完成\nflutter选在dart语言（动静结合）,dev下JIT，production阶段AOT；兼顾dev跟production\n性格\n好动的人\n优点：社交牛逼症\n缺点：沉下心来搞研究难\n喜静的人\n优点：能沉下心来琢磨事情\n缺点：社交困难\n做一个动静结合的人？既能沉下来搞研究，也能出去处理人际关系\n关于加班\n有意义、有价值和能成长的忙碌不会让人产生真正疲惫，反而是不断焕发新的激情；\n在错误的方向，复杂的人事上内耗会让人心力交瘁，即便准点下班也会心理疲惫；\n关于中间层\n一段话：所有的问题都能通过加一层中间层解决，如果不行，那就再加一层；\n中间层的好处是屏蔽细节，简化API，增大通用性，但是以损失性能为代价\n编程世界\n例子：\nN-API解决Node addon跟V8 API强绑定导致的升级困难；猜想大概率是做了一个 开发 Node Addon的通用API规范（底层可能还是通过封装V8实现，但是开发者不需要关心具体的V8 API是啥）\nTaro3 能做到框架独立（同时支持react/vue等框架语法），也是在编译中间加了一层编译IR，然后把IR再转换成对应的目标代码\nReact 能做到编写 APP，也是在代码编译中加了一层虚拟DOM中间层 IR（如果是直接编译到浏览器Dom就没跨端什么事了），可以进行不同平台的适配\nLLVM 做成了标准的编译中间层IR，方便无数的新生语言（eg：Rust）可以以 LLVM 作为目标来编译，而无需去适配不同的操作系统\nJsBridge原理连接了js世界跟Native世界，丰富了js的可能性，代价是JsBridge沟通的性能损失\n...\n生活世界\n如果一件事情处理不了，那就找人帮忙，如果还不行，那就再找一个人？\n各种网络平台也就是把自己做成一个中间层，方便人事的协同？\n关于补救方案\n事物无时无刻都在向前发展，很多发展中的问题都是不可预测，所以基本没有一开始就设计出的完美的方案，于是补救方案就来了；当前的补救方案可能促进方案的成长\n编程\n当开发者跟Rust语言对Safe的场景认同不一致的时候，Rust决定把选择权交给开发，于是有了Unsafe；如果开发者正确，那可能是Rust进化的不错机会；例如：extern关键字协助创建外部函数接口（Foreign Function Interface， FFI），extern的函数就肯定是Unsafe的，因为rust没法进行静态检测其他语言的\n当开发者跟typescript对lint的场景认同不一致的时候，于是有了 \"//@ts-ignore\"；可能是TS进化的机会\n人类\n认知的补救方案，早期人类遇到无法解释的现象首先是发挥想象力，创造一个万能的神明；\n关于个人无法化解的场景，可能通过笑来化解；所以笑是避免大脑死机的补救方案？\n关于趋势\n事物的发展都有一定的规律，把握了正确规律可以事半功倍\n计算机相关\n语言的发展：更快、更安全、更容易掌握、特定问题的更好解决方案\nRust就是比C++更友好更安全的方向\nGo 就是来提供更好的服务并发方案\n关于js原型链\n有意思的现象\n关于GC\n手动\n方式：new/delete等手动方式\n语言：C++等\n优点：灵活\n缺点：忘记或者回收不当会导致内存泄漏风险\n自动 \n方式：引用计数，标记清扫，节点复制，新老生代转化组合等；周期性的暂停程序执行而进行GC工作\n典型的语言：js，go\n优点：编写简单\n缺点：GC频繁会导致程序执行性能受到影响\n无GC\n方式：所有权\n语言：Rust\n优点：性能高，安全\n缺点：学习曲线陡峭\n思考：实际有GC，只是效率高到不会妨碍到现有程序执行？\n摘要\n>Rust would know when the variable gets out of scope or its lifetime ends at compile time and thus insert the corresponding LLVM/assembly instructions to free the memory.\nReference\nwhat-does-rust-have-instead-of-a-garbage-collector\nReact 踩坑之路由动画\n例子：react-router-dom 从版本 6 开始就废弃了Switch组件，用Routes做了替换，然而使用上跟Switch有差别？第三方库 react-router-transition 目前依赖peerDependency react-router-dom@5，也就是升级react-router-dom直接崩？又因为breaking change的差异大，导致 react-router-transition 没法简单就能跟随升级\n相关文档：[[Feature]: Compatibility layer that implements the v5 API ](https://github.com/remix-run/react-router/issues/8695)\n现状：现在找个解决方案费时费力，各种实践层出不穷，核心库的breaking change没经过周全的思考就去实施，然后就是一堆填坑的工作量；\n建议：React能否学习Vue把最佳实践维护起来，优势：可以做更周全稳定迭代，方便开发者找到最佳实践，提升工作效率；劣势：是啥？\n数据结构疑问\n数据结构之间的依赖关系：例如 golang map 依赖 hashmap ，hashmap依赖 数组 + 链表\n***个人拙见，酌情观看***",
    "url": "/blog/DailyReflections",
    "path": "DailyReflections.md",
    "lastModified": "2025-08-06T07:13:32.302Z"
  },
  {
    "title": "Introduction",
    "content": "If you're not sure which to read, try these\nTech Notes\nDaily Reflections\nAI Notes\nAI Related\nBlog\nhttps://www.anthropic.com/research#overview\nhttps://www.anthropic.com/engineering\nhttps://openai.com/research/index/\nhttps://www.microsoft.com/en-us/research/blog/\nhttps://ai.meta.com/blog/\nhttps://deepmind.google/discover/blog/\nhttps://research.google/pubs/\nTrending Papers\ntrending papers homepage",
    "url": "/blog/Introduction",
    "path": "Introduction.md",
    "lastModified": "2026-01-20T10:55:02.763Z"
  },
  {
    "title": "TechNotes",
    "content": "TechNotes\n简介：个人技术思考与总结\nMCP SSE 的缺陷\n当前 MCP 基于的 sse 无状态通信还是有缺陷，会导致客户端多次访问丢失 session（多个 pod 负载均衡的时候很容易出现），还得往 websocket 方向演进\n曾经前端热替换（HMR）有过类似的演变\n为什么 mac 不再使用 CUDA？\n核心原因：符合 Apple 的封闭生态技术路线，转向 Metal 让 Apple 能更好地优化自己的硬件与软件系统，而不依赖外部技术。\n为什么 jest 需要 watchman？\n原因：因为 jest 需要监听文件变化，而 watchman 是文件系统监听工具，更适合大型项目（所以 watchman 得崩溃可能会导致 jest 无法正常工作）\nwatchman 的启动 ：在第一次 run jest 时，会启动 watchman 持续监听文件变化\nwatch 范围：历史启动过 jest 的目录都会纳入到 watchman 的监听范围，在 MAC 的活动监视器找到 watchman 打开的文件或者窗口看到\n为什么 git 有了分支还需要 tag？\n分支：一个指向提交的可变指针，随着你在分支上提交新的更改，分支会自动向前移动。这意味着分支是动态的、可变的，它们通常代表开发进度，例如 master、develop、feature/xyz 等分支名。\n标签：一个指向特定提交的不可变的指针。标签通常用于标记代码的特定状态，比如一个版本发布（v1.0.0），而这个标签一旦打上，它就永久指向那个特定的提交，不会随时间变化。\nTrace ID 三段 代表啥？\ne9e976fde12bc6745f1c51fa80652d01:000000788c34c719:0000000000000000\n第一段全局唯一 Trace ID\n第二段：spanId\n第三段：parentSpanId\n图像相似度\npixelmatch 图片相似度\n简介：具有准确的抗锯齿像素检测（通过色差阈值不一定是差异，还可能是锯齿）和感知色差度量（YIQ ）。 \nY 是最重要的信息，因为黑白电视只依赖亮度。\nI（橙蓝对比）次重要，它对人眼的视觉感知贡献较大。\nQ（紫绿对比）的重要性较低，因此可以被更大程度地压缩。\n为啥有用\n因为人眼对颜色的感知是非线性的，对不同颜色分量的敏感度不同。\n即使两个像素在 RGB 空间中看似差异较大，通过 YIQ 空间测量后可能反映出的感知差异较小，从而避免错误地标记为显著不同。\n将 RGB 颜色转换为 YIQ 后，可以根据人眼感知的差异调整颜色分量的权重。比较两张图片时，可以更加准确地反映视觉上的差异，而不是单纯的数值差异。\n场景：专注于精确检测像素差异，常用于回归测试、差异图生成等场景，优先考虑性能。\n结构相似性指标（structural similarity index，SSIM index）\n简介：是一种用以衡量两张数字图像相似程度的指标。当两张图像其中一张为无失真图像，另一张为失真后的图像，二者的结构相似性可以看成是失真图像的图像质量衡量指标。\n为啥有用\n结构相似性的基本观念为自然图像是高度结构化的，亦即在自然图像中相邻像素之间有很强的关系性，而这样的关系性承载了场景中物体的结构信息。\n人类视觉系统在观看图像时已经很习惯抽取这样的结构性信息。\n场景：专注于结构相似性，常用于图像质量评估、图像压缩、图像增强等场景，优先考虑准确性。\n识别深度伪造\n英特尔公司宣布了一个叫做 FakeCatcher 的软件，号称可以查出某个视频是否为深度伪造，准确度高达96%。\n它的原理是识别视频人物皮肤的静脉血液流动。如果是真人，血液时刻在体内循环，皮肤上的静脉就会有周期性的深浅变化，深度伪造的人就没有。\n大数据已死？\n大数据作为单一问题就不存在了，变成了海量存储和大型计算两个问题。\n博客本周话题\n尤雨溪解读2022 Web前端生态趋势\n和 Svelte相比，Vue的 Reactivity Transform 和 Solid -labels 都属于统一模型，也就是他不受限于组件上下文，它可以在组建内使用，也可以在组建外使用，优势就是有利于长期的重构和复用\n尤雨溪解读2022 Web前端生态趋势\nE2E 框架原理\npuppeteer,playwright 基于浏览器调试协议；偏底层，功能更丰富，但是跨浏览器一致性有限；\n例如：playwright 跟 chrome 的交互是基于 chrome devtools 协议；\n猜想：能够使用 playwright 实现的能力（例如 UI 的录制回放），也能直接开发对应的浏览器插件实现？\nselenium 基于 webdriver API；跨浏览器一致性，功能有限\nCAP 定理\n数据一致（Consistency）、高可用（Availability）、数据分区（Partition tolerance）\nCAP 定理的核心观点是，在设计和运行分布式系统时，需要根据实际应用场景的需求，做出选择和权衡。例如，\n如果一个系统必须保证数据的实时一致性，比如银行系统，那么它可能需要在网络分区时牺牲一部分可用性。\n而如果系统更注重用户体验和服务的不间断性，如社交网络，可能会在网络分区时选择保持高可用性，同时允许暂时的数据不一致。\n数据存储于可视化\nPrometheus + Grafana\nElasticsearch + Kibana\nelasticsearch vs 数据库\nes 侧重于搜索（倒排索引，水平可扩展性更强），数据库侧重于存储（事务，一致性，复杂的关系模型） \n内存对齐\n空间换时间？\n优势：对齐可以加快 CPU 访问速度\n劣势：会浪费空间\n符号表 2023-3-6\n1. wiki\n符号表是一种用于语言翻译器（例如编译器和解释器）中的数据结构。在符号表中，程序源代码中的每个标识符都和它的声明或使用信息绑定在一起，比如其数据类型、作用域以及内存地址。\n2. Example\nSymbol_table#Example\nGo 编译器默认编译出来的程序会带有符号表和调试信息，一般来说 release 版本可以去除调试信息以减小二进制体积。\nABI（Application binary interface）： 约定数据在二进制流上的排列，还有 calling convention 等方式便于不同语种交互\nlow-level, hardware-dependent  vs API（high-level, hardware-independent，human readable format）\n3. 思考\n有点类似 js 的 source-map？\n风控引擎优化 2023-3-2\n1. 什么是风控？\ninput：风控参数，参数权重，参数组合判定逻辑\noutput：reject/pass\n2. 为什么需要优化？\n风控的判定效率越高，对下游真实业务的阻塞也就越小\n3. 如何做？\n参数获取：参数的获取可能渠道获取（redis/kafka/db 等）\n并行\n参数筛选排序拉取：根据业务数据使用权重，推迟小概率使用的参数拉取\n逻辑组合\n时序调整（调整逻辑执行先后顺序）\n剪枝（编译阶段无效判定逻辑消除）\n扩容机制 2023-3-2\n需要处理好扩容跟收缩\n1. 栈空间（或者堆空间）策略\n分段机制\n旧秩序的延续\n保留旧有数据空间，申请新空间存放溢出数据，通过链表连接各片段\n连续机制\n新秩序替换旧秩序\n触发扩容会申请更大的空间（2倍？类似 golang 切片的动态扩容机制），把旧空间数据搬到新空间，并释放掉旧空间\n优化：延迟归还给操作系统，自己管理内存的伸缩，例如：golang 的 goroutine go内存管理\n2. 服务能力\n横向：拓展更多集群服务\n流量降低时会触发收缩\n纵向：提升单体服务能力\nAOP（面向切面编程） 2023-3-1\n解释：运行时动态地将代码切入到类的指定方法、指定位置上的编程思想就是面向切面的编程。\n例子：js 的装饰器\n场景：使用 AOP 将其从核心关注点中分离出来（实际是对执行过程进行代理重写）\nNode.js 日志log\n埋点、数据上报\n性能分析、统计函数执行时间\n给ajax请求动态添加参数、动态改变函数参数\n分离表单请求和验证\n防抖与节流...\n遭遇：golang 垃圾回收 三色标记法与读写屏障\n摘要：AOP其实只是OOP的补充而已。OOP从横向上区分出一个个的类来，而AOP则从纵向上向对象中加入特定的代码。有了AOP，OOP变得立体了。什么是面向切面编程 AOP？\n逃逸分析\n解释：在编译程序优化理论中，逃逸分析是一种确定指针动态范围的方法——分析在程序的哪些地方可以访问到指针。它涉及到指针分析和形状分析。\n遭遇：在了解 golang reflect 的过程 “用于获取接口值 reflect.Value 的函数 reflect.ValueOf 实现也非常简单，在该函数中我们先调用了 reflect.escapes 保证当前值**逃逸到堆上**，然后通过 reflect.unpackEface 从接口中获取 reflect.Value 结构体；”\n逃逸基础\n不同于jvm的运行时逃逸分析，golang的逃逸分析是在编译期完成的。\nGolang的逃逸分析只针对指针。一个值引用变量如果没有被取址，那么它永远不可能逃逸。\nC 或者 C++ 是手动分配到栈或者堆上，后面是否会加上运行时逃逸分析\n功能\n逃逸分析辅助编译器优化编译结果\n将堆分配转化为栈分配。如果某个对象在子程序中被分配，并且指向该对象的指针永远不会逃逸，该对象就可以在分配在栈上，而不是在堆上。在有垃圾收集的语言中，这种优化可以降低垃圾收集器运行的频率。\n编译器优化的场景\nReference\ngolang 逃逸分析详解\ngolang 反射原理\n文章精读之设计原则/模式\n设计原则：SOLID原则\nSingle Responsibility Principle 单一原则；\n一种Class只负责一类事情\nOpen/Closed Principle 开闭原则；\n允许在不改变已有代码前提下添加新功能\nLiskov Substitution Principle 里氏替换原则；\n不要从可实例化的父类中继承（方法），而是要使用基于抽象类和接口的继承。\nInterface Segregation Principle 接口隔离原则；\nClients should not be forced to depend upon interfaces that they do not use. 对于非必须的参数做可选配置，精简接口，简化使用\nDependency Inversion Principle 依赖反转原则\nHigh-level modules should not depend on low-level modules. \nBoth should depend on abstractions.Abstractions should not depend upon details. Details should depend on abstractions.\n降低耦合，方便重构跟扩展\n设计模式\nCreational Patterns\nFatory Method\nSingleTon\nBehavioral Patterns\nStrategy\nObserver\nIterator\nStructural Patterns\nAdapter\nDecorator\nProxy\n设计原则 vs 设计模式\n简洁理解：设计模式（语言相关）是对设计原则（语言独立）的实现\n长文理解\nDesign principles provide high level guidelines to design better software applications. They do not provide implementation guidelines and are not bound to any programming language.\nDesign Pattern provides low-level solutions related to implementation, of commonly occurring object-oriented problems. In other words, design pattern suggests a specific implementation for the specific object-oriented programming problem.\nReference\nhttps://github.com/ryanmcdermott/clean-code-javascript#solid\nhttps://www.tutorialsteacher.com/articles/difference-between-design-principle-and-design-pattern\nhttps://github.com/sohamkamani/javascript-design-patterns-for-humans#behavioral-design-patterns\nhttps://github.com/lpxxn/rust-design-pattern\n关于继承\n谈到继承就会说到“类”，“类”封装的是数据，是对同一类数据实体以及其处理方法的抽象。\n目的：\n    1. 代码复用\n    2. 多态（同一操作作用于不同的对象，可以有不同的解释，产生不同的执行结果。实现方式：覆盖，重载；某种程度也算代码复用？）\n覆盖：指子类重新定义父类方法，例如js的prototype\n重载：多个同名但参数不同的方法，例如 Rust 函数模板\n继承方式：\n    1. 正统：C++,java\n    2. 模拟：js原型链\n    3. 无：Rust trait 组合实现多态\n        思考：Rust 的编程建模模式更像是组合（Composition） 而不是继承（Inheritance）？有点类似 Vue3 选择 Composition-API 代替 Class-Option？\n多态图解：!多态\n图来源\nReference\n为什么Rust没有继承？\nhttps://www.runoob.com/rust/rust-object.html\n关于面向对象\n解释：面向对象的编程语言通常实现了数据的封装与继承并能基于数据调用方法。\n有：js一切都是对象\n无：Rust 不是面向对象的编程语言，但这些功能都得以实现。\nReact状态管理\nRedux\n通过props注入 dispatch/state，规避了 useContext 方案可能导致的重复渲染\nMobx （有点类似createContext + useContext + reactivity）\n特征：Mobx + React 类似等于 Vue\ncreateContext + useContext + useReducer\n坑：createContext 返回的 Provider 中 value变化会触发所有子组件重复渲染\n关联思考：这种方案适用于小型的状态管理？\nTodo：了解重复渲染的运转机制\n优化：对渲染做 useMemo\n文章精读之前端框架设计\n组件类型\n纯展示型的组件，数据进，DOM出，直观明了\n接入型组件，在React场景下的container component，这种组件会跟数据层的service打交道，会包含一些跟服务器或者说数据源打交道的逻辑，container会把数据向下*传递给展示型组件\n交互型组件，典型的例子是对于表单组件的封装和加强，大部分的组件库都是以交互型组件为主，比如说Element UI，特点是有比较复杂的交互逻辑，但是是比较通用的逻辑，强调组件的复用\n功能型组件，以Vue的应用场景举例，路由的router-view组件、transition组件，本身并不渲染任何内容，是一个逻辑型的东西，作为一种扩展或者是抽象机制存在\n状态管理\n如何管理将事件源映射到状态变化的过程，如何将这个映射的过程从视图组件中剥离出来，如何组织这一部分代码来提高可维护性，是状态管理要解决的本质问题\nCSS方案\ncss modules\ncss-in-js\n构建工具\n解决的问题\n任务的自动化\n开发体验和效率（新的语言功能，语法糖，hot reload 等等）\n部署相关的需求\n编译时优化\nReference\nEvanyou 聊聊前端框架\nCSS Animation,Transition 原理\nGPU比较擅长于：\n绘制位图到屏幕\n重复的绘制同一个位图\n在不同的位置，以不同的旋转角度，或者不同的缩放大小来绘制同一个位图。\nGPU相对慢的地方：\n将位图加载到显存里。\n例子：\n总结\n选择独立性比较强的CSS属性有助于改善动画效果：transform，opacity，filter （依赖于过滤器的复杂度和浏览器）\n拓展：渲染线程分为 主线程 (main thread) 和 合成线程 (compositor thread)\nReference\nhttps://www.jb51.net/css/348357.html\nVue运行时错误处理\nVue源码里面lifeCycle/watch/directive/transition等的callback/hook等一般会被封装执行\n封装执行要素：  \nfn: Function,\ninstance: ComponentInternalInstance | null,\ntype: ErrorTypes,\n错误分类：LifecycleHooks | ErrorCodes（其他函数执行点：callback + hook等）\n同步错误处理：尝试函数运行处理同步错误，并搜集返回值\n异步错误处理：根据返回值是否Promise来处理异步错误\nReference\n@vue/runtime-core/errorHandling\n插件对比（Vue vs React）\n插件\n1. Vue插件\n2. React插件\n // 使用地方：一般需要对组件做二次封装进行connect，然后可以useDispatch，**类似Vue inject？**\n插件的插件（Vuex vs Redux）\n1. Vuex插件\n// 插件案例\nvuex-persistedstate通过对mutation的拦截，对模块paths进行精准持久化\n2. Redux 中间件\n// 复杂案例\nredux-persist\n插件的插件的插件\n1. redux-persist的插件，或者称为 Transforms\n例子\n总结\nVue的mutable 比 React的immutable对数据的状态的操作来的直接，导致在状态处理插件模式也会不同；vuex（直接mutation改变数据）整体上比redux（通过dispatch action，走整个reducer流程返回新的state）更加直接，导致插件模式上vuex直接劫持mutation比redux劫持reducer来的更加简单\nReference\nhttps://vuex.vuejs.org/guide/plugins.html\nhttps://redux.js.org/understanding/history-and-design/middleware\nhttps://v2.vuejs.org/v2/guide/plugins.html?redirect=true\n模拟LRU的数据结构（Set vs Array）\nSet会自动追踪插入的先后顺序\n1. Set结构\n2. Array结构\nORM 模块组成\nORM\nORM\nEngine (open db, connect dialect and db_path)\nSession(prepare for next operation,  open/close DB)\nDialect \ndialect : db and dialect container\nsqlite3 : language type <- map -> db column type\nmysql ...\nSchema (Parse a struct to a Schema instance)\n静态语言（例如 golang）：type struct（Modal） <- 反射 -> Table Fields \n动态语言（例如 js）：手动 Table Fields 创建 -> Table Fields\nClause (SQL Builder)\ngenerator: keyword sql string(vars) generation\nclause : depend on generator, set/build final sql string\nSession ( SQL Execution )\nraw: raw sql execution\ntable: table operation\nrecord: ORM Open API (Find/Select/Update/Delete/Where/Limit ...) ,depend on Clause/clause & table & raw\nhooks: ORM Open API hook\ntransaction: Atomicity/Consistency/Isolation/Durability\nmigration",
    "url": "/blog/TechNotes",
    "path": "TechNotes.md",
    "lastModified": "2025-08-06T07:15:13.085Z"
  },
  {
    "title": "AI 的关键是语料",
    "content": "AI 的关键是语料\n观点：模型行为不是由架构、超参数或优化器选择决定的。它是由你的**数据集决定**的。其他一切只是有效地将计算交付给逼近该数据集的手段。\n如果模型的架构、功能、参数，作用是有限的，真正决定性的因素是训练模型的语料。那当发布了一个大模型的时候，也许重点应该放在告诉大家模型用什么语料训练模型。（模型完美，但是语料垃圾，一样不行；反之，模型很平常，但是语料足够，那可能也不错。）\nReference\nhttps://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/\nAI 的关键是语料",
    "url": "/blog/AI 的关键是语料",
    "path": "tech/ai/AI 的关键是语料.md",
    "lastModified": "2025-06-20T11:25:12.917Z"
  },
  {
    "title": "AI 阻碍新技术传播",
    "content": "AI 阻碍新技术传播?\n推导：\n如果大家都使用大模型解决技术问题，而大模型又是基于已有数据（旧技术）训练，新技术的应用会落后，没人使用，会导致网上有关新技术的信息很少，导致 AI 缺乏新技术的训练数据，那新技术就很难被普及？\n模型训练出来的模型进一步偏向于旧技术\n这实际上描述了一个 **技术固化陷阱**（Technology Lock-in by AI），或者说 **反馈闭环导致新技术传播困难**。  \n**2. AI 的主流推荐导致“路径依赖”，新技术难以突破**  \n人们依赖 AI 解决方案，而 AI 由于训练数据的局限，往往推荐过去更常见的解决方案。  \n这会导致“**路径依赖效应**”：旧技术持续强化，新技术的应用受限，甚至可能被埋没。  \n结果是，**即使新技术更优越，也很难被广泛采用**，因为 AI 生成的技术建议大多不会主动推荐它。  \n🔹 **类比**：类似 VHS 战胜 Betamax、QWERTY 键盘取代更高效的 Dvorak 键盘，新技术可能因为“先发优势”不足而被埋没，即使技术上更优越。  \n---\n**3. AI 生成内容影响搜索引擎，进一步降低新技术的可见度**  \n以前，新技术依赖于搜索引擎和技术论坛传播，比如 Google 搜索、Stack Overflow、GitHub 等。  \n现在，AI 生成的内容越来越主导搜索结果，而这些内容大部分基于旧技术。  \n这导致 **新技术的信息被 AI 生成的旧技术信息淹没**，进一步降低了新技术的传播速度。  \n🔹 **现实案例**：\nGitHub Copilot 生成的代码通常基于最流行的开源代码，而不是最新的技术方案。\nAI 生成的技术博客往往基于过去已有的文章，而不是基于新研究的前沿进展。\n---\n**4. 缺少真实应用，AI 无法有效训练，形成“冷启动难题”**  \nAI 依赖 **大量数据** 进行训练，但新技术在早期阶段的数据量本就很少，导致 AI **无法有效学习**。  \n由于 AI 不推荐新技术，开发者使用新技术的更少，进一步导致网上缺少相关数据，AI 更无法训练，形成 **恶性循环**。  \n结果是：**新技术因缺少数据无法进入 AI 生态，而 AI 生态又让新技术难以获得数据**，最终造成“冷启动困境”。  \n🔹 **类比**：新社交媒体平台常常遇到冷启动问题——没有用户 → 没有内容 → 没人愿意加入。而 AI 主导的信息流可能让新技术面临类似的问题。  \n---\n**5. 可能的破解方式**  \n1. **人为干预 AI 训练数据** —— 让 AI 更主动学习新技术，而不是仅仅依赖已有的大数据。  \n2. **推广“非 AI 依赖型”学习方式** —— 鼓励开发者主动学习新技术，而不是完全依赖 AI 推荐。  \n3. **增加新技术的曝光渠道** —— 比如加强 AI 对 arXiv、Hacker News、Reddit 技术讨论等信息的关注，而不仅仅是 Stack Overflow 这种“过去经验导向”的平台。  \n4. **实时更新数据**：搜索引擎、论坛、GitHub 代码库都是 AI 训练数据的重要来源，未来 AI 可能会更依赖实时数据+用户贡献，而不是仅依赖历史数据。\n5. **AI 增强搜索能力**：未来的大模型可能会更强地结合搜索引擎（如 Perplexity AI、Bing Chat），这样即使 AI 训练数据较旧，也能搜索最新技术文档。\n6. **企业推广**：企业在新技术推广时，往往会提供 API 文档、示例代码、技术博客（如 OpenAI、Meta、Google），AI 也可以从这些官方资源中学习。\n---\n**总结**\n推导揭示了 AI 可能导致的 **技术固化陷阱**，核心逻辑是：  \nAI 依赖历史数据训练 → 倾向推荐旧技术 → 新技术曝光不足 → 新技术被采用的更少 → AI 训练数据缺失 → AI 继续推荐旧技术，形成闭环。  \n这种情况如果不加以引导，确实可能导致新技术的普及变慢，甚至在某些情况下 **让落后的技术长期占据主流**。  \n参考\nAI 是阻碍技术传播的元凶\nGPT",
    "url": "/blog/AI 阻碍新技术传播",
    "path": "tech/ai/AI 阻碍新技术传播.md",
    "lastModified": "2025-06-20T11:25:12.918Z"
  },
  {
    "title": "AICR",
    "content": "AICR\n使用 AI 进行 code diff review 的 Prompt 框架思考\n\"弱规划，强框架\"\n让 Agent 自主决定会更好吗？\n方案 A：强框架\n方案 B：自主规划\n核心权衡：可靠性 vs 灵活性\n为什么 AICR 选择强框架？\n什么场景适合自主规划？\n混合方案：在框架内保留灵活性\n最佳实践是 **在强框架内嵌入有限的自主性**：\n可视化对比\n结论\n| 问题 | 回答 |\n|------|------|\n| 自主规划效果会更好吗？ | **不一定**，取决于场景 |\n| AICR 为什么选择强框架？ | 批量处理、可预测性、可靠性需求 |\n| 自主规划什么时候更好？ | 一次性、探索性、高价值任务 |\n| 最佳实践是什么？ | **混合方案**：宏观框架 + 微观自主 |\n**核心洞察**：\n强框架不是因为 Agent 不够聪明，而是因为 **业务场景需要可预测性**。\n在框架内保留有限自主，是当前 LLM 能力边界下的务实选择。\n当 LLM 的规划、自我评估、终止判断能力进一步提升后，可以逐渐放宽框架约束，给予更多自主权。**强框架是可靠性的保障**。",
    "url": "/blog/AICR Agent Prompt 哲学",
    "path": "tech/ai/AICR Agent Prompt 哲学.md",
    "lastModified": "2025-12-01T04:03:49.926Z"
  },
  {
    "title": "Agent 进化论：从规则驱动到强化学习的范式跃迁",
    "content": "Agent 进化论：从规则驱动到强化学习的范式跃迁\n在构建 AI Agent（智能体）的实践中，我们常常陷入“Prompt Engineering”和“Hard-code Logic”的泥潭：为了让 Agent 即使在边缘情况下也能正确调用工具，我们不得不编写冗长的 System Prompt 和防御性代码。\n然而，这种**“基于规则（Rule-based）”**的开发模式存在天然的上限。无论你的 Prompt 写得多么详尽，永远无法覆盖真实世界的无限复杂性。\n微软研究院开源的 **Agent Lightning** 框架提供了一个极具启发性的视角。它不仅是一个工具，更代表了一种开发范式的跃迁：**从“教 AI 怎么做（SFT/Prompt）”，转向“告诉 AI 什么是好的（RL）”。**\n1. 核心思想：价值函数 (Value Function) 是经验的数学载体\n强化学习（RL）在 Agent 领域的本质，是训练一个**价值函数 (Critic)**，用来量化“当前状态”的好坏。\n**Prompt/SFT** 试图教会模型：在这种情况下，**逻辑上**应该怎么做。\n**RL (Value Function)** 试图教会模型：在这种情况下，**直觉上**赢面有多大。\n真正的专家不仅拥有逻辑，更拥有直觉。RL 就是将成千上万次试错后的“经验”，通过数学方式压缩进神经网络的权重里，成为 Agent 的“直觉”。\n2. 深度案例解析：Text-to-SQL 的“顿悟”时刻\n让我们跳出通俗比喻，通过一个 **Text-to-SQL** 的真实技术案例，来看看 RL 是如何解决 Agent 开发中最大的痛点——**信用分配（Credit Assignment）**。\n场景设定\n**任务**：用户查询“上周最活跃的用户”。\n**正确路径**： ->  ->  ->  -> 。\n**错误路径**： ->  -> 。\n传统方法的困境（稀疏奖励）\n在传统 RL 中，只有最后 SQL 运行成功了才给 +1，运行失败给 -1。\n如果模型在第 1000 次尝试中偶然成功了一次，它很难知道是因为“中间查了 Schema”导致的成功，还是因为“最后 SQL 写得好”。这就是**稀疏奖励**问题。\nAgent Lightning 的解法（分层 RL 与 Critic 机制）\nAgent Lightning 引入了一个 **Critic 模型** 来实时评估每个步骤的 **State Value (V值)**。\n步骤拆解与价值流动：\n1.  **State T1 (初始状态)**：用户刚问完问题。\nCritic 预测赢面：$V(s_1) \\approx 0.1$ (迷茫)\n2.  **Action T1 (动作)**：Agent 选择 **“调用 ”**。\n进入 **State T2**：Agent 看到了详细的表结构信息。\nCritic 根据过往经验发现，凡是手握表结构信息的 Agent，最后成功的概率极高。\nCritic 预测赢面飙升：$V(s_2) \\approx 0.8$\n3.  **关键时刻 (Advantage Calculation)**：\n即使任务还没结束，算法计算出了这一步的**优势 (Advantage)**：\n$A(t_1) = V(s_2) - V(s_1) = 0.8 - 0.1 = +0.7$\n**结论**：Agent 获得了一个巨大的正向激励。**它学会了：不管后面代码写得咋样，“查表”这个动作本身就是高价值的。**\n4.  **Action T2 (动作)**：Agent 编写 SQL。\n如果 SQL 写错了，环境报错，State T3 的赢面跌至 0。\n$A(t_2) = 0 - 0.8 = -0.8$\n**结论**：Agent 收到了惩罚，学会了 SQL 语法不能乱写。\n通过这种机制，Agent 不需要人类手把手教它“先查表再写代码”，它通过对价值函数的拟合，自动**“悟”**出了这个最优策略。\n3. 技术启示：未来的 Agent 开发架构\nAgent Lightning 的设计思想揭示了未来 AI 应用开发的两个关键趋势：\n3.1 架构解耦：训练与推理分离 (Middleware Pattern)\nAgent Lightning 采用了一种**中间件 (Middleware)** 架构。它不侵入业务逻辑，而是作为一层  运行：\n**Inference (推理侧)**：负责执行业务逻辑，生成 Trace（轨迹）。\n**Training (训练侧)**：负责消费 Trace，计算 Advantage，更新模型权重。\n这意味着：**任何现有的 Agent（无论是 LangChain 还是 AutoGen），理论上都可以通过“挂载”一个 RL 模块来实现自我进化，而无需重写代码。**\n3.2 评价体系 > 逻辑编写 (Evaluation over Logic)\n随着 RL 的引入，开发者的重心将从**“如何写好 Prompt”**转移到**“如何定义好的 Reward 函数”**。\n**Prompt 时代**：你需要告诉 AI “步骤1做A，步骤2做B...”。\n**RL 时代**：你需要定义 “代码跑通 +1分”，“消耗 Token 少 +0.5分”，“用户点赞 +1分”。\n4. 冷静思考：RL 的局限性与挑战\n虽然 RL 描绘了美好的未来，但落地过程中依然面临严峻挑战：\n1.  **冷启动困境 (Cold Start)**：\n    RL 依赖于“试错”。如果 Agent 初始能力太弱，尝试 1000 次都拿不到一个正向奖励（Reward=0），模型就无法学习。因此，通常需要先通过 SFT（有监督微调）让 Agent 达到“及格线”，再上 RL。\n2.  **环境交互的昂贵代价**：\n    Agent 的每一次“试错”都在消耗 Token 和 API 费用。相比于 SFT 只需要准备静态数据集，RL 需要与环境进行数万次的实时交互，训练成本高昂。\n3.  **安全性风险 (Safety)**：\n    在探索阶段，Agent 可能会执行危险操作（如  数据库或发送错误邮件）。如何在 Sandbox（沙箱）中安全地进行 RL 训练，是必须解决的工程难题。\n5. 结语\nAgent Lightning 不仅仅是一个微软的开源项目，它预示着 AI Agent 开发正在进入 **2.0 阶段**。\n在 1.0 阶段，我们像教小学生一样，用自然语言（Prompt）事无巨细地指导 AI。\n在 2.0 阶段，我们将构建环境和奖惩机制，让 AI 在数以万计的虚拟试错中，进化出我们要的模样。\n**这不仅是效率的提升，更是智能的涌现。**\n---\n**深度阅读**：\nAgent Lightning: Adding reinforcement learning to AI agents without code rewrites (Microsoft Research)",
    "url": "/blog/Agent-Reinforcement-Learning",
    "path": "tech/ai/Agent-Reinforcement-Learning.md",
    "lastModified": "2025-12-25T07:11:11.649Z"
  },
  {
    "title": "详细解析",
    "content": "**Claude 的 JIT 检索其实就是“在对话时临时做索引”，因此有冷启动成本；\n而类似 zilliz/claude-context 的 MCP 插件提供“预构建、持久化、高质量索引”，让 Claude 可以跳过 JIT 的昂贵推理步骤，直接走“高质量捷径”。**\n换句话说：\n**JIT = 模型现算现查（token 贵、慢、有概率错）**\n**MCP = 给模型准备好的全局知识图（token 少、快、稳定）**\n详细解析\n1. Claude 的 JIT 检索，本质就是“临时索引”\nClaude Code 的流程是：\n1. 模型扫描文件列表（token 消耗巨大）\n2. 分析路径关系（再次消耗 token）\n3. 猜测依赖结构、模块边界\n4. 决定哪些文件要 load 进上下文窗口\n5. 重复多轮（探索 → 试错 → 再探索）\n这个就是典型的 **“即时索引（JIT indexing）”**。\n它有两个明确问题：\n**问题 1：冷启动成本高**\n第一次理解仓库结构，需要几十万 token。\n**问题 2：模型推理成本高**\n模型必须用思考（reasoning）来构建临时知识图。\n**问题 3：不稳定（猜结构可能错）**\n尤其在：\nmonorepo\n多语言混合项目\n复杂依赖图\n  中更容易出错。\n安装 MCP 后 Claude 会“更偏向使用 MCP 的知识，而非自行 JIT 推理”？\nClaude 会自动根据工具的“成本”来决定是否使用它\n当 Claude 看到 MCP 工具提供：\n结构化搜索（语义 + 路径 + AST）\n相关性极高的文件 top-k\n经过 embedding 聚类的结果\n直接带关键代码段\nClaude 会自动判断：\n**“既然 MCP 给了高置信度知识，我没必要浪费 token 去做 JIT 索引。”**\n🧭 4. 更深一层：MCP 不是替代 JIT，而是让模型的“注意力资本”更有效率\nClaude 的注意力（attention budget）是有限的。\nJIT 会消耗大量注意力做：\n文件扫描\n依赖关系推理\n路径推理\n多轮检索试错\nMCP 把这些注意力省下来，使 Claude 把预算用在：\n逻辑推理\n代码理解\n修复\n审查\nrefactor\n这才是为何 MCP 能显著提升 Claude 的 Code Intelligence。",
    "url": "/blog/Claude code JIT 索引对比预构建索引",
    "path": "tech/ai/Claude code JIT 索引对比预构建索引.md",
    "lastModified": "2025-11-19T09:30:01.836Z"
  },
  {
    "title": "ControlNet 与 LoRA 插件",
    "content": "ControlNet 与 LoRA 插件\nControlNet 和 LoRA 并不是完全相同的东西，尽管它们都与神经网络和图像生成有关。\nControlNet：\n作用：用于控制 AI 图像生成。\n特点：允许用户对生成的图像进行精细的控制。\n应用：在计算机视觉、艺术设计、虚拟现实等领域中非常有用。\n示例：用户可以上传线稿，让 AI 帮助填色渲染、控制人物姿态等。\nLoRA：\n作用：用于大模型参数高效微调。\n特点：通过降维和升维来模拟参数的更新量，从而减少训练成本。\n应用：在参数量较大的模型微调中表现优异。\n| 项目   | LoRA                     | ControlNet       |\n| ---- | ------------------------ | ---------------- |\n| 改动方式 | 插两个矩阵                    | 复制 UNet + 多组横向连接 |\n| 训练开销 | 很小                       | 较大（副网络）          |\n| 适用领域 | 通用模型（NLP, CV, Diffusion） | 主要是扩散图像模型        |\n| 控制能力 | 弱（用于微调）                  | 强（结构直接控制）        |\nControlNet\nControlNet 是一个神经网络架构，用于控制 Stable Diffusion（SD）模型并扩展其输入条件。\n作用\n可控性提升：ControlNet 允许创作者通过添加额外的控制条件来引导 SD 模型生成图像，从而提高 AI 图像生成的可控性。\n多样性：它支持多种输入条件，如 Canny 边缘、语义分割图、关键点、涂鸦等，拓展了 SD 的能力边界。\n原理\n假设你有原始的 Stable Diffusion 的 UNet，它是一个 U 形结构：\n🧩（1）复制主模型的 UNet，作为 一个副网络（control branch）\n这个副网络跟主 UNet 结构相同，但只接收你给的“控制图”（比如姿势图、边缘图）；\n它不生成图像，只是提取控制图中的结构、线索信息；\n类似“助手分析线稿图”。\n🧩（2）每一层 feature，都用横向连接（hint）加回主 UNet\n主 UNet 本来从 latent 中恢复图像；\nControlNet 的副网络在每一层都加一组“控制信号”，告诉主干：“你这里应该照着这个姿势画”、“这个边缘要保留”。\nControlNet 论文解析\nLoRA（Low-Rank Adaptation）低秩适应\nLoRA 是一种用于大模型参数高效微调的方法。\n原理\n理解 LoRA（Low-Rank Adaptation，低秩适配）最核心的一点是：\n**它不是压缩模型本身，而是用“压缩的方式”来训练出新的能力。**\n📦 用体感更强的类比说明\n🎨 类比一：画师画风迁移\n你是一个超强画师（大模型 ），你会画油画。\n现在有人想让你画动漫风。你完全可以重新训练自己，改掉所有油画技巧（Fine-tune 整个模型），但那很痛苦也成本高。\nLoRA 的方式是：\n给你加上一个轻量的“风格滤镜”组件（低秩矩阵），专门调整你的输出风格。你本人不变，只训练这小滤镜。\n所以：\n原始技能保留（冻结原模型）；\n新技能通过极少的参数就能学到（只训练 A 和 B）；\n切换风格（加载不同 LoRA 参数）非常快。\n---\n⚙️ 类比二：机械臂微调\n你有一只机械臂（神经网络）用来装配零件，它动作稳定精准（预训练模型）。\n现在工厂来了一个新型号的零件，动作需要调整，但你不想换掉整只机械臂（太贵）。\nLoRA 的做法是：\n给机械臂的某个关节加两个弹簧和一个滑轨（低秩 A 和 B），专门用来微调动作。这样改动小，效果好。\n---\n🧠 LoRA 是不是压缩模型？\n不完全是。\n| 方法            | 是否压缩模型？ | 是否可快速迁移？              | 参数更新量        |\n| ------------- | ------- | --------------------- | ------------ |\n| 全参数 Fine-tune | ❌       | ❌                     | 全量更新         |\n| LoRA          | ✅（更新量少） | ✅（可快速切换 LoRA adapter） | 非常少（< 1% 参数） |\n| 模型剪枝、蒸馏       | ✅       | ❌（需重新训练）              | 固定           |\n✅ **LoRA 是一种“参数高效训练”方法，也可以视为是一种训练阶段的“压缩”**。\nReference\nGPT",
    "url": "/blog/ControlNet 与 LoRA 插件",
    "path": "tech/ai/ControlNet 与 LoRA 插件.md",
    "lastModified": "2025-07-25T02:20:30.119Z"
  },
  {
    "title": "Ego Reasoning（自我推理）：AI 认知能力的边界与突破",
    "content": "Ego Reasoning（自我推理）：AI 认知能力的边界与突破\n核心概念\n**字面含义**：\"ego\" = 自我，Ego Reasoning = 从自我视角出发的有限推理\n**本质**：基于\"我知道、相信、感受到\"的信息进行推理，而非从全知视角推理\n认知科学层面：Theory of Mind 的前置阶段\n三个递进层次\n| 推理类型 | 定义 | 示例 |\n|---------|------|------|\n| **Ego Reasoning** | 我认为 X | 小孩 A 看到玩具被藏，回答\"玩具在盒子里\" |\n| **ToM Reasoning** | 我认为你认为 X | 小孩 A 理解 B 没看到，回答\"B 会以为在桌上\" |\n| **Meta Reasoning** | 我思考我如何思考 X | 反思自己的推理过程 |\n限制\n无法预测他人行为（缺乏心智建模）\n在博弈、协作、对话中表现\"自以为是\"\n存在 ego-centric bias（自我中心偏差）\nAI 应用层面\n当前 LLM 的推理特征\n**✅ 确实具有 Ego Reasoning 特征：**\n有限视角：只基于可见上下文推理\n无真实信念状态：模式匹配而非真实\"相信\"\n自我中心知识库：依赖训练数据的统计投影\n**🎯 但被训练出\"模拟 ToM\"能力：**\n用户意图建模：推断用户可能不知道什么\n知识状态推断：判断是否需要解释概念\n视角切换：在不同场景下考虑不同视角\n架构对比\n如何突破现有模式\n技术层面的可能路径\n1️⃣ 持久化信念状态追踪\n**技术实现：** Memory Networks、Bayesian Belief Networks、Multi-agent RL\n2️⃣ 反事实推理能力\n**关键能力**：因果推理、反事实生成、意图推断\n**技术路径**：Causal Language Models、Structured World Models、Intent Recognition Networks\n3️⃣ 自我模型与他人模型的显式分离\n4️⃣ 交互式信念校准\n主动探测用户知识边界\n根据反馈动态调整用户模型\n**技术**：Active Learning、RLHF、Dialogue State Tracking\n认知层面的核心能力\n| 能力 | 描述 | 价值 |\n|------|------|------|\n| **元认知** | 我知道我不知道什么 | 明确知识边界 |\n| **情感与动机理解** | 理解用户为什么问这个问题 | 深层意图推理 |\n| **社会推理** | 理解社会情境中的规范和期望 | 情境适配能力 |\n哲学层面的根本困境\n**问题 1：无真实\"自我\"**\n**问题 2：训练范式限制**\n**问题 3：中文房间悖论**\n表现得像理解 ≠ 真正理解？功能主义 vs 意识主义\n现实主义路线图\n短期（1-3年）：增强的模拟 ToM\n多轮对话信念追踪\n显式知识状态建模\n更好的反事实推理\n主动知识探测\n中期（3-10年）：嵌入式认知架构\n持久化记忆和自我模型\n显式的 ego/other 分离架构\n强化学习 + 社会互动学习\n多模态信念建模\n长期（10年+）：？？？\n是否需要\"意识\"才能有真正的 ToM？\n是否需要\"情感\"才能真正理解他人？\nAI 的 ToM 可能就是不同的物种\n实践层面的改进\n对当前 AI 助手来说，可以做到：\n1. **更谦逊**：\"我基于 X 推测你可能想..., 对吗？\"\n2. **更主动探测**：\"你对这个概念熟悉吗？\"\n3. **更显式的推理**：\"我注意到你提到了 X，所以我推测你可能关心 Y\"\n4. **承认限制**：\"这个情境我可能理解得不准，因为我缺乏真实的社会经验\"\n5. **主动更新**：根据反馈动态调整用户模型\n关键洞察\n**Ego Reasoning 是所有高级推理的基础**，但其局限在于\"无法跳出自己的视角\"。\n当前 LLM 的突破路径不是追求\"真正的 ToM\"，而是追求\"足够好的 ToM 近似\"——\n就像飞机不需要像鸟一样飞，但能达到飞的目的。\n个人思考：理解 AI 推理的本质边界，有助于我们更好地设计人机协作模式，在 AI 擅长的地方发挥其优势，在 AI 局限的地方保持人类的主导和校验。\n---",
    "url": "/blog/Ego 推理",
    "path": "tech/ai/Ego 推理.md",
    "lastModified": "2025-10-16T10:56:52.715Z"
  },
  {
    "title": "GPT与开发-TDD",
    "content": "GPT 是什么？\nGPT 是“Generative Pre-trained Transformer”的缩写，即生成式预训练变换模型，是一种语言模型，可以执行非常复杂的任务，如回答问题、生成文章和代码，或者翻译文章内容等。\n实质：文字接龙（bert 是完形填空）\n如何训练：\nPre-trained （无监督学习）\n自动学习各种资料，熟悉各种语义关系\n好处：学习资料丰富，成本相对低\n劣势：学的杂，不成体系，不好为人所用\n模板规范（监督学习）\n条件：优质范例 \n目标：矫正成符合人类习惯的回答\n例如：有害内容过滤，回答问题并给出原因等；）；\n附加能力：in-context learning 能力（语境内学习）\n使用过程中评分反馈（强化学习）\n目标：提升创意能力\nGPT 与编码 - TDD（Test-Driven Development）\n改变需求开发流程\nGPT 辅助前：需求 -> 代码编写 -> test-case\nGPT 辅助后：\n1. 需求 -> test-case -> 代码生成 -> test-case补充生成\n2. 需求 -> GPT方式描述 -> 代码生成 -> test-case 生成\n例子\n需求描述\n实现一个fn 函数，接受一个数字作为参数，并返回一个数字作为结果。\nfn 函数需要满足以下条件\n当参数为 0 时，返回 0\n当参数为 1 时，返回 1\n**对应测试用例**\n问：生成 fn 函数满足以下测试用例： \n答：我可以尝试生成一个 fn 函数满足你的测试用例。请看我的代码：\n需求变化\n当参数为 2 时，也返回 1\n目标：改变测试用例，输出抽象度更高的代码\n结果：推导出 fibonaci\n问：生成 fn 函数满足以下测试用例： \n答：fn 是一个函数的参数，它表示一个回调函数，也就是在 fn 函数内部执行的函数1。在 JavaScript 中，函数可以作为参数传递给另一个函数，也可以作为另一个函数的返回值2。这种函数称为高阶函数1。在您的测试用例中，fn 函数接受一个数字作为参数，并返回一个数字作为结果。要满足测试用例，fn 函数需要满足以下条件：\n当参数为 0 时，返回 0\n当参数为 1 或 2 时，返回 1\n当参数大于 2 时，返回前两项的和\n这样的函数其实就是斐波那契数列的定义。所以，一个可能的 fn 函数如下：\n概念：拟合 -> 泛化\n>拟合的目的是学习数据背后的规律，对新的数据进行预测或分析\n>泛化能力反映了模型有没有对客观世界做真实的刻画，还是发生了过拟合或欠拟合；\n>>欠拟合（拟合结果无法满足旧的案例，例如：fn(2)!=1）\n>>过拟合（拟合结果无法满足新案例，如果需求增加到 n=3，结果也要为 1）\n>泛化例子：\n一个泛化能力的例子是高中生每天各种做题，五年高考三年模拟一遍遍的刷，为的什么，当然是想高考能有个好成绩。 高考试题一般是新题，谁也没做过，平时的刷题就是为了掌握试题的规律，能够举一反三、学以致用，这样面对新题时也能从容应对。 这种规律的掌握便是泛化能力，有的同学很聪明，考上名校，很大程度上是该同学的泛化能力好。 考试成绩差的同学，有这三种可能：一、泛化能力弱，做了很多题，始终掌握不了规律，不管遇到老题新题都不会做；二、泛化能力弱，做了很多题，只会死记硬背，一到考试看到新题就蒙了；三、完全不做题，考试全靠瞎蒙。\n单语言模型到多语言模型的泛化能力；多语言模型可以利用不同语言之间的共性和联系，学习到更一般的规律和知识，从而在新的语言或任务上表现更好。例如，一个多语言的机器翻译模型，可以在不同的语言对之间进行翻译，而不需要为每一种语言对单独训练一个模型。这样可以节省资源，提高效率，也可以增强模型的鲁棒性和适应性。\n假设我们有一个机器学习模型，它的任务是根据一张照片判断是猫还是狗。我们用一些猫和狗的照片来训练这个模型，然后用一些没有见过的猫和狗的照片来测试它。如果这个模型在测试集上的准确率很高，那么我们可以说它的泛化能力很好，因为它可以对新的数据做出正确的判断。如果这个模型在测试集上的准确率很低，那么我们可以说它的泛化能力很差，因为它不能适应新的数据。\n挑战：泛化能力让 知识的可解释性、可控性 难度增加\n代码优化\n问：你能不能给我一个更高效的 fn 函数？\n答：有一些方法可以提高 fn 函数的效率，比如：\n使用循环而不是递归，避免重复计算相同的值\n使用数组或对象缓存已经计算过的值，避免重复计算相同的值\n使用数学公式直接计算第 n 项的值，避免计算所有前面的值\n例如，使用数组缓存的方法可以这样写：\n概念：CoT（Chain-of Thought）思维链\nCoT：是一系列中间步骤，用来说明如何从输入得到输出，提高推理的准确性和可解释性。\n>基本思想：通过给LLM一个包含一些输入输出对以及每个输出对应的推理链的前缀，让LLM在生成输出的同时，也生成相应的推理链。这样可以让LLM更好地理解任务要求，避免一些错误或不一致的输出，提高推理的准确性和可解释性。\n>例子：\n算术： 如果你有12个苹果，你吃了3个，你还剩多少个？ -> 9个 推理链：12 - 3 = 9\n在展示样例后面加上一个测试样例，比如：\n>>如果你有8个香蕉，你给了朋友4个，你还剩多少个？\n>>模型就会根据前缀中的展示样例和测试样例，来生成一个输出和一个推理链，比如：\n>>如果你有8个香蕉，你给了朋友4个，你还剩多少个？ -> 4个 推理链：8 - 4 = 4\n代码重构\n问：请仿照 function fn(t:boolean){ return true} 重构 fn 函数\n答：根据您的要求，我尝试重构 fn 函数如下：\n用例增强\n问：麻烦给这个函数提供一个完整的测试用例\n答：根据网上的信息，要给一个函数提供一个完整的测试用例，需要考虑以下几个方面：\n测试函数的输入和输出是否符合预期\n测试函数的边界条件和异常情况是否正确处理\n测试函数的内部逻辑是否正确执行\n测试函数的性能和效率是否达标\n例如，使用 Jest 框架，可以给 fn 函数写以下测试用例：\n一点思考\n机遇：GPT 在模式化需求中赋能明显\n风险：Prompt 注意数据隐私（例如：提的 GPT Prompt 涉及到隐私信息）\n观念：发现问题 > 定义问题 > 分解问题 > 解决问题\n开放性思考\n信息跟知识的区别？\n记忆跟学习的区别？\n例子1：学会了王者农药的某个英雄？这里的学会指的是什么？是对技能或者各种套路的熟练背诵还是在对线中各种秀？\n例子2：说一个人学会了羽毛球？这里的学会指的是什么？是对羽毛球各种动作跟对线策略的记忆，还是再实际对线中嘎嘎杀？\n学一个英雄的方式：\n方案1： 从英雄技能描述或者教程开始（信息，记忆，机械记忆，指令记忆，抽象）-> 实践；\n方案2：从对线的体感认识开始（知识，学习，理解记忆，归纳学习，实践），随着对线次数跟对线英雄样本变多，会出现对这个英雄超越常人的理解（主播经常说道的，超越一般人理解） -> 看描述跟教程矫正操作（回到抽象，监督学习）；\nGPT 进化前提：更丰富更高质量的资料样本（可能是局限）；更多的测评数据；更多的实践运用（例如 chatGPT），强化学习？\n丰富：更多的训练资料\n质量：资料的质量决定了 GPT 的进化水平，例如 wiki,quora 等高质量数据平台\n强化学习：理论结合实际\n  ...\n其他角度：记忆是学习的一种手段。记忆的目标是重现（有限，可知），学习的目标是泛化（无限，未知）\n一些资料\nGPT-4论文精读【论文精读·53】\n通向AGI之路：大型语言模型（LLM）技术精要\nMaximizing the Potential of LLMs: A Guide to Prompt Engineering\nLarge language model - Wikipedia\nChatGPT - Wikipedia",
    "url": "/blog/GPT与开发-TDD",
    "path": "tech/ai/GPT与开发-TDD.md",
    "lastModified": "2025-06-20T11:25:12.908Z"
  },
  {
    "title": "LLM 指令遵循能力评估：从 IFScale 到 IFEval 的思考",
    "content": "LLM 指令遵循能力评估：从 IFScale 到 IFEval 的思考\n引言：为什么大模型总是“听不懂话”？\n在开发 Agent 或编写复杂 Prompt 时，我们经常遇到一个挫败的场景：明明已经写了“禁止输出 JSON”或“每段不超过 50 字”，模型却视而不见。这种“指令遵循（Instruction Following）”能力的失效，到底是因为模型“脑容量（Context）”不够，还是“注意力（Attention）”涣散？\n最近一篇名为 [](https://arxiv.org/pdf/2507.11538) 的论文试图通过 **IFScale** 基准量化这个问题。本文基于对该论文的分析，结合 IFEval 等业界标准，探讨如何科学评估和提升 LLM 的指令遵循能力。\n1. 论文解读：IFScale 与“高密度指令”的崩塌\n1.1 实验设计：填词游戏\n这篇论文的实验设计非常直观，本质上是一个**“关键词填空”**压力测试：\n**任务**：要求模型生成一份商业报告。\n**约束**：必须在文中包含给定的 $N$ 个特定单词（如 revenue, synergy, ecosystem...）。\n**变量**：$N$ 从 10 增加到 500。\n**评测**：用正则匹配统计关键词出现的比例。\n1.2 核心发现\n1.  **全员退化**：即使是 GPT-4 级别的最强模型，在 500 条指令密度下，遵循率也跌至 **68%** 左右。\n2.  **退化模式**：\n**阈值型（Threshold Decay）**：推理型模型（如 o3）在临界点前很稳，过线后突然崩塌。\n**线性/指数型**：普通模型随着数量增加，遵循率呈线性或指数下滑。\n3.  **首因效应（Primacy Effect）**：模型对 Prompt **前部**的指令执行得更好，越往后越容易漏（Attention 衰减）。\n2. 批判性思考：这是“记忆力”测试，而非“智力”测试\n虽然 IFScale 证明了模型在高负载下的注意力缺陷，但作为“指令遵循”的评估标准，它存在明显局限：\n**低认知负载**：\n    仅仅是“插入单词”，不涉及复杂的逻辑判断。这测试的是**显性记忆检索（Retrieval）**，而不是**逻辑执行（Execution）**。\n**缺乏互斥与冲突**：\n    真实世界的指令往往是打架的（如“要详细” vs “要简练”）。IFScale 的指令之间是独立的，无法测试模型处理**冲突（Conflict）**和**权衡（Trade-off）**的能力。\n**忽视质量**：\n    只要把词塞进去就算赢，导致模型可能生成逻辑不通的“垃圾文本”来骗分。\n**改进思路：混合逻辑压力测试（Mixed-Constraint Stress Test）**\n更真实的测试应包含：\n**否定约束**（Negative Constraints）：\"严禁使用 X\"。\n**条件依赖**（Conditional Dependencies）：\"如果 A，则必须 B\"。\n**格式强约束**（Format Constraints）：\"JSON 字段必须小于 50 字\"。\n3. 业界标准对比：严谨派 vs 灵活派\n在“如何测试指令遵循”这个问题上，业界主要分为两派：\n3.1 严谨派：IFEval (Google DeepMind)\n**核心哲学**：**“代码即真理”**。只测那些客观可验证的指令。\n**典型指令**：\n“禁止使用大写字母”\n“字数严格在 200-300 之间”\n“输出必须是合法的 JSON”\n**判分方式**：**Python 脚本**。正则匹配、格式解析、字数统计。\n**优点**：100% 客观，无争议，不需要 AI 裁判。\n**适用场景**：基座模型的基础能力体检。\n3.2 灵活派：FollowBench / ComplexBench\n**核心哲学**：**“理解万岁”**。测试语义、语气、多步逻辑等软约束。\n**典型指令**：\n“语气要委婉但专业”\n“如果 API 挂了，不要指责供应商，要说是技术集成问题”\n“提供两个逻辑上互斥的备选方案”\n**判分方式**：**LLM-as-a-judge**。把 Prompt、回复和评分标准发给 GPT-4 打分。\n**优点**：真实，贴近 Agent 实际应用场景。\n**缺点**：裁判模型（GPT-4）本身有主观性，结果有波动。\n4. 对 Agent 开发的启示\n1.  **不要一次性塞入过载指令**：\n    论文证明 500 条指令会让模型崩盘。工程上应避免“万能 Prompt”，尽量**拆解（Decomposition）**任务，让每个步骤的 Context 保持干净。\n2.  **对抗位置偏见**：\n    把**最关键**的指令（如安全红线、输出格式）放在 Prompt 的**最前面**或**最后面**（Recency Bias），不要埋在中间。\n3.  **监控体系建设**：\n对**硬指标**（JSON 格式、禁止词），参考 **IFEval** 写脚本死板校验。\n对**软指标**（语气、逻辑），参考 **FollowBench** 用更强的模型（或人工）抽检。\n4.  **推理模式的价值**：\n    论文提到开启 Thinking/Reasoning 模式能缓解高密度下的退化。在复杂任务中，牺牲一点延迟换取 Chain-of-Thought 是值得的。\n---\n*Reference:*\nArxiv: How Many Instructions Can LLMs Follow at Once?\nIFEval: Instruction Following Evaluation for Large Language Models",
    "url": "/blog/LLM 指令遵循评估",
    "path": "tech/ai/LLM 指令遵循评估.md",
    "lastModified": "2025-12-15T06:55:40.279Z"
  },
  {
    "title": "LLM的智能来源：核心概念与扩展",
    "content": "LLM的智能来源：核心概念与扩展\n**核心三要素：**\n**压缩（Compression）**: 大语言模型（LLM）通过训练将海量数据中的语言模式、统计规律和世界知识“压缩”并内化到其参数中。其**泛化能力**和**抽象能力**都是这种高效压缩的直接体现。\n**组合（Composition）**: LLM 能够将学到的语言元素和概念重新组合，生成连贯、有意义的新文本。这种能力是其语言生成和创造性表达的基础。\n**回顾（Retrospection）**: 模型利用注意力机制在生成每个词时“回顾”并关注输入或已生成序列中的关键部分。**上下文理解**是这种“回顾”能力的核心应用。\n---\n**难以完全被概括的扩展概念：**\n**涌现能力（Emergence）**: 这是指模型规模达到一定程度后，**非线性地、意外地**展现出的新能力，它不是三个核心要素的简单叠加，而是其复杂交互的宏观结果。\n**推理（Reasoning）**: 尽管推理依赖于压缩、组合和回顾，但它是一个更高级别的认知任务，是这三者在**特定逻辑步骤**中的**复杂应用**。\n**归纳偏置（Inductive Bias）**: 这是模型**架构层面**的固有属性，它**塑造了**压缩、组合和回顾这三个过程的运作方式，而不是它们本身。\n简而言之，可以将 **compression**、**composition** 和 **retrospection** 视为LLM智能的**基础机制**，而其他概念（如泛化、抽象、推理、涌现）则是这些机制在不同维度上的**表现、结果或前提**。",
    "url": "/blog/LLM 智能来源",
    "path": "tech/ai/LLM 智能来源.md",
    "lastModified": "2025-08-04T07:29:38.570Z"
  },
  {
    "title": "[Context Engineering for AI Agents: Lessons from Building Manus](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)",
    "content": "Context Engineering for AI Agents: Lessons from Building Manus\n这篇文章是 Manus 团队基于构建通用 AI Agent（Manus）的实战经验，分享关于**上下文工程（Context Engineering）**的深度技术文章。\n**核心思想**：\n在构建复杂的 Agent 系统时，**Context Engineering（上下文工程）比 Prompt Engineering 更重要**。不仅仅是“写好提示词”，而是要像管理内存一样，精心设计上下文的结构、存储和生命周期。\n**上下文是稀缺资源**：尽管模型支持 128k+ 上下文，但在实际工程中，上下文的长度直接关联到**成本（Cost）**、**延迟（Latency）**和**准确率（Accuracy）**。\n**KV-Cache 是关键指标**：Agent 的运行是一个不断循环的过程（Loop），每一轮都会产生新的 Token。必须优化 KV-Cache 的命中率（Hit Rate），避免每一步都重新计算整个上下文。\n**信息压缩与外部化**：不要把所有东西都塞进 Context Window，学会使用文件系统作为“外挂内存”。\n**解决的问题**：\n1.  **高昂的推理成本与延迟**：在长任务中，Input Token 远多于 Output Token（比例可达 100:1），如果每次都全量重新处理，速度极慢且烧钱。\n2.  **“迷失中间”（Lost in the Middle）**：随着对话变长，Agent 容易忘记最初的目标或上一轮的决策细节。\n3.  **模式僵化（Pattern Collapse）**：过多的 Few-Shot 示例会导致模型刻板模仿，失去灵活性。\n4.  **错误恢复能力差**：很多系统倾向于隐藏错误，导致 Agent 不知道自己试错过什么，从而陷入死循环。\n**核心策略（Manus 的解决方案）**：\n1.  **极致优化 KV-Cache（降本提速）**\n**前缀稳定性**：保持 System Prompt 和早期历史记录绝对不变。\n**Append-Only 原则**：不要修改或删除历史消息。\n**确定性序列化**：工具输出（如 JSON）键值顺序固定。\n2.  **外部化记忆（Externalized Memory）**\n**文件系统即记忆**：将长内容写入文件，Context 里只保留路径。\n**按需读取**：只有确实需要细节时才读取文件内容。\n3.  **保持注意力的技巧**\n**Recitation（背诵/复述）**：强制 Agent 维护 ，每轮思考前读取，明确当前进度。\n**保留错误堆栈**：报错是极高价值的反馈信号，能帮助 Agent 自我修正。\n4.  **对抗模式僵化**\n**动态 Few-Shot**：引入随机性或结构变异，防止模型过拟合。\n**思考与未来**：\nManus 的方案是当前架构下的工程最佳实践。未来可能的方向：\n**架构级改进**：如 Linear Attention, SSM (State Space Models) 解决长序列遗忘。\n**智能压缩**：使用小模型将长文本压缩为 Embedding 或 Summary Tokens。\n**元学习**：Agent 拥有跨任务的长期记忆。\n**Context is Money.** 不要迷信模型能力的无限提升，好的工程架构（特别是上下文管理）才是让 Agent 从 Demo 走向 Production 的关键。\n*编辑：2026-01-20*",
    "url": "/blog/Manus - Context Engineering for AI Agents",
    "path": "tech/ai/Manus - Context Engineering for AI Agents.md",
    "lastModified": "2026-01-20T02:24:12.544Z"
  },
  {
    "title": "Prompt 工程的本质",
    "content": "Prompt 工程的本质：概念空间对齐\n核心论断\nPrompt 调整不是\"教会 LLM\"，而是**激活 LLM 训练时已固化的最优推理路径**。本质是用最小信息量实现概念空间的精确对齐。\n三层对齐机制\n**表层对齐**（语法触发）\n特定术语和格式模式：、\n效果：激活特定推理风格的概率分布\n**中层对齐**（范式激活）\nFew-shot examples、Chain-of-Thought、Self-consistency\n效果：触发预训练阶段学会的推理框架\n**深层对齐**（表征共振）\n问题分解方式、概念关联网络、解决方案模式\n效果：直接激活高维语义空间中的最优轨迹\n关键洞察\n**熵减原则**：好的 prompt 是在减少 LLM 输出空间的熵，将其引导到高质量解的子空间。\n**迁移而非教学**：不是临时传授知识，而是唤醒已有的暗知识（implicit knowledge）。\n**对齐税**（Alignment Tax）：每增加一个 token，要么降低对齐成本（clarify），要么增加噪音（confuse）。优秀的 prompt 实现了最低的对齐税。\n实践公式\n类比\n**调频收音机**：不改变电台（模型权重），只调整接收频率（prompt）\n**化学催化剂**：不参与反应本身，但降低激活能，加速到达目标状态\n**量子纠缠**：精确的 prompt 让 LLM 的输出坍缩到期望的特征空间\n可操作启示\n1. **学习 LLM 的\"母语\"**：研究高质量数据集的表达模式（如 Chain-of-Thought 论文中的示例）\n2. **用例子替代解释**：一个好的 few-shot 胜过百字说明\n3. **格式即信号**：结构化的输入（markdown、JSON）本身就是强对齐信号\n4. **迭代寻优**：prompt 调试是搜索问题，需要系统化测试不同的\"频道\"\n元认知\n这个过程本身体现了**人机协同的双向对齐**：\n人类学习 LLM 的表征空间 ←→ LLM 理解人类的意图空间\n最终收敛点：用最简洁的\"协议\"实现最高效的\"握手\"\n---\n*启发来源：信息论（最小描述长度）+ 认知科学（概念激活）+ 迁移学习（特征复用）*",
    "url": "/blog/Prompt 工程的本质",
    "path": "tech/ai/Prompt 工程的本质.md",
    "lastModified": "2025-11-11T10:45:58.773Z"
  },
  {
    "title": "Promptions",
    "content": "文章 promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls介绍了微软研究院推出的一个名为 **\"Promptions\"** (Prompt + Options) 的新框架，旨在通过动态的用户界面（UI）控件，让 AI 的提示（Prompting）过程更加精准和高效。\n文章核心内容翻译摘要\n**背景与问题：**\n许多用户在使用生成式 AI 时都经历过“试错循环”的沮丧感：输入提示词，得到的回答不满意，于是修改提示词重试。用户往往不知道 AI 需要什么细节，或者觉得编写长篇大论的提示词非常累人。尤其是当用户的目标是“理解”某些复杂内容（如代码或公式）时，不同用户（如初学者 vs. 专家）对解释的需求截然不同。\n**解决方案：Promptions：**\n微软推出了 Promptions，这是一个轻量级的中间件 UI 框架。\n**名称含义：** Promptions = **Prompts** (提示词) + **Options** (选项)。\n**核心功能：** 它不只是一个聊天框，而是根据用户的输入**动态生成**UI 控件（如按钮、复选框、下拉菜单）。\n**工作原理：**\n1. **选项模块 (Option Module)：** 分析用户的输入和对话历史，实时生成相关的“微调选项”（例如：解释的详细程度、关注重点、学习目标）。\n2. **聊天模块 (Chat Module)：** 将用户选择的选项与原始提示词结合，生成最终的 AI 回答。\n**研究发现：**\n研究团队对比了“静态控件”（预设好的固定选项）和“动态控件”（根据内容实时生成的选项）。结果显示：\n**更精准：** 用户无需反复重写提示词，只需调整控件即可获得想要的细微差别。\n**引导思考：** 动态选项（如“学习目标”）能启发用户去思考自己未曾想到的需求。\n**降低门槛：** 用户可以将精力集中在理解内容上，而不是纠结于如何“以此格式输出”等提示工程技巧上。\n---\n要点解析\n1. 从“提示工程”到“界面交互”的范式转变\n**痛点解决：** 目前的 AI 交互过度依赖用户的自然语言表达能力（即所谓的“提示工程/Prompt Engineering”）。Promptions 试图通过图形用户界面（GUI）来降低这一门槛。\n**解析：** 这代表了 AI 交互的一个重要趋势——**混合界面**。用户不需要记住复杂的指令（如“请用苏格拉底式教学法解释...”），系统会自动提供一个名为“教学风格”的下拉菜单。这让 AI 更像一个可配置的工具，而不是一个需要“咒语”的黑盒。\n2. “动态性”是其核心创新\n**静态 vs. 动态：** 传统的软件界面是静态的（菜单永远是一样的）。Promptions 的创新在于其 UI 控件是 **AI 生成的**。\n**场景举例：**\n如果你问它关于 **Excel 公式** 的问题，它可能会生成“调试指南”、“语法分解”等选项。\n如果你问它关于 **历史事件** 的问题，它可能会生成“按时间线列出”、“侧重经济影响”等选项。\n**解析：** 这种“随需应变”的界面解决了通用 AI 模型在特定垂直领域应用时“上下文不足”的问题。它让通用的 Chatbot 瞬间变成了一个针对当前任务定制的专业工具。\n3. 增强用户的“元认知”\n**隐性引导：** 文章提到，动态选项不仅是工具，还能帮助用户“澄清目标”。\n**解析：** 很多时候用户并不知道自己想要什么。当系统弹出“你是想解决报错，还是想学习原理？”的选项时，它实际上是在引导用户进行更深层的思考。这种设计不仅优化了结果，也提升了用户与 AI 协作的质量。\n4. 中间件架构 (Middleware Architecture)\n**技术实现：** Promptions 被设计为位于用户和 LLM（大语言模型）之间的中间件。\n**解析：** 这意味着它具有很高的**可移植性**。开发者可以将其轻松集成到现有的聊天应用、代码编辑器或客户支持系统中，而无需重新训练模型。这为 AI 应用开发者提供了一种低成本提升用户体验的方案。\n**总结：**\nPromptions 试图解决生成式 AI “不可控”和“交互累”的两大难题。通过将自然语言的模糊性转化为 UI 控件的确定性，它为未来的人机交互提供了一个极具潜力的方向：**让 AI 自己生成控制它的界面**。",
    "url": "/blog/Promptions",
    "path": "tech/ai/Promptions.md",
    "lastModified": "2025-12-29T09:38:10.275Z"
  },
  {
    "title": "《The Bitter Lesson》",
    "content": "《The Bitter Lesson》 \n\"The Bitter Lesson\"是一篇由计算机科学家理查德·萨顿（Richard Sutton）撰写的论文。它提出了一个重要的观点，即在人工智能和机器学习的发展过程中，最重要的教训是利用计算资源和数据量的增长。\n文章的几个观念\n传统的人工智能方法受限：在过去的几十年中，人们在构建人工智能系统中主要依赖于手动设计的规则和特定的专家知识。然而，这种方法面临着挑战，因为设计和实现这些规则往往非常复杂，且需要大量的专业领域知识。\n利用计算资源和数据量的增长优势：随着计算资源和数据量的增长，我们可以采用更简单且更通用的方法来构建人工智能系统。这种方法通过大规模的数据和计算能力来训练机器学习模型，使其根据数据自动地学习规律和特征。\n从神经网络的再兴中得出教训：神经网络的成功复兴是一种不同的方法论，它充分利用了计算资源和数据量的增长。通过使用更大和更深的神经网络，以及大规模训练数据，神经网络能够实现各种复杂的任务，包括图像识别、自然语言处理等。\n简单通用的方法更具优势：在发展人工智能和机器学习方法时，应该倾向于更简单而通用的方法。这些方法具有更好的扩展性和适应性，可以从更少的先验知识和人工规则中进行学习，使系统能够在更广泛的任务和环境中表现出色。\n总的来说，\"The Bitter Lesson\"表达了一种重要的观点，即在人工智能和机器学习领域中，随着计算资源和数据量的增长，通过利用更简单且更通用的方法，让机器从数据中自动学习更多的规律和特征，可能会比依赖于手动设计的复杂规则更具优势。\n几个例子论证\n象棋和围棋：这两种棋类游戏都是人工智能研究的经典领域，也是大规模计算的一般方法取得突破性进展的领域。例如，AlphaGo 和 AlphaZero 都是基于深度神经网络和强化学习的系统，它们可以通过自我对弈来提高自己的水平，而**不需要人类的先验知识**。这些系统在与人类顶尖棋手的对决中，展示了超越人类的水平。\n图像识别：这是人工智能研究的另一个重要领域，也是大规模计算的一般方法取得显著进步的领域。例如，Midjourney V5 是一种能够根据文本描述生成高质量图像的系统，它是基于深度生成对抗网络 (GAN) 的技术，它可以从大量的图像数据中学习图像的特征和分布，而**不需要人类的领域知识**。这种系统在图像创作和图像编辑等任务中，展示了惊人的创造力和灵活性。\nSAM 模型比 UNet 更加通用，SAM 采用了一种提示型的训练方法（UNet 是监督学习），通过提示来生成分割掩码，使其具有**更强大的零样本泛化能力**。\nChatGPT：这是一种基于 GPT-4 的语言模型，它可以根据上下文理解和生成类似人类的文本，它也是基于大规模计算的一般方法的一个例子，它可以从海量的文本数据中学习语言的规律和知识，而**不需要人类的规则或语法**。这种系统在对话、写作、翻译等任务中，展示了流畅的语言能力和广泛的知识面。\n通用性的局限性\n**忽略了问题的特殊性和复杂性**，导致无法找到最优的或最合适的解决方案。例如，一些通用的机器学习算法可能无法适应一些特定的数据分布或任务需求，需要进行定制化的改进或设计。\n例子：使用通用的神经网络来识别手写数字时，可能会遇到一些噪声、模糊或变形的数字，这些数字可能与训练数据集中的数字有很大的差异，导致识别的准确率下降。\n**需要大量的计算资源和数据**，导致效率低下或成本高昂。例如，一些通用的深度学习模型可能需要数百万甚至数十亿的参数，需要大规模的硬件设备和数据集来训练和部署。\n**安全性或可靠性的风险**，导致不可预测的或不可控制的后果。例如，一些通用的人工智能系统可能会受到对抗攻击或数据污染的影响，导致输出错误的或有害的结果 。\n原文\nthe bitter lesson",
    "url": "/blog/the bitter lesson",
    "path": "tech/ai/the bitter lesson.md",
    "lastModified": "2025-06-20T11:25:12.918Z"
  },
  {
    "title": "可优化的 AI Agent：用 3 个例子讲清 DSPy、TextGrad、AdalFlow 与 Trace/OPTO",
    "content": "可优化的 AI Agent：用 3 个例子讲清 DSPy、TextGrad、AdalFlow 与 Trace/OPTO\n很多团队第一次把 LLM 接进业务时，都会走一条熟悉的路：\n1. 写一版 Prompt（“你是一个专业助手…”）\n2. 线下跑几条 case，看起来不错\n3. 一上线，开始“玄学翻车”：边界条件、长文本、工具失败、格式不稳\n如果你经历过这种翻车，你会很自然地问出一个更本质的问题：\n我们能不能像训练神经网络一样，让 Agent/工作流在反馈中**持续变好**，而不是靠“灵感写 Prompt”？\n这正是近两年 DSPy / TextGrad / AdalFlow / Trace 这条技术支线的共同目标：把“提示词工程”升级成“**可优化系统**”。\n本文不讲论文公式，只用 3 个可复制的例子，把它们各自解决什么、需要什么前置条件、什么时候该用讲清楚。\n参考链接（建议收藏）：\n微软：[](https://www.microsoft.com/en-us/research/blog/tracing-the-path-to-self-adapting-ai-agents/)\nDSPy 文档：[](https://dspy.ai/)\nTextGrad 仓库：[](https://github.com/zou-group/textgrad)\nAdalFlow 仓库：[](https://github.com/SylphAI-Inc/AdalFlow)\n2）例子一：分类/抽取任务 —— DSPy 为什么像“编译器”\n场景\n你做了一个客服意图分类器：\n输入：用户一句话（“我想取消订单”）\n输出：一个固定标签（ /  /  …）\n你很快会发现：Prompt 里多一句话、少一个例子，准确率就能抖动 5～15%。  \n于是你想做一件更“工程化”的事：**把这段 LLM 程序编译成更稳的 Prompt。**\nDSPy 的核心思路\nDSPy 的主张是：不要把行为写成一大段字符串 prompt，而是写成结构化程序，然后用优化器“编译”出更好的 prompts/weights：[](https://dspy.ai/)。\n你可以把它理解成：\n你写的“AI 程序”是**骨架**（输入输出字段、模块链路）\n编译器（optimizer）会在你的数据集上反复试，自动找出：\n**更好的指令**\n**更好的 few-shot 示例组合**\n一个很直观的“编译前/编译后”差别\n**编译前**（通常是零样本或少量样本，容易飘）：\n**编译后**（DSPy 会把“最有效的示例”和更好的指令放进去，变成稳定模板）：\n注意：这不是“运行时动态 prompt”，而是**离线编译得到的“黄金模板”**——上线后会稳定复用。\nDSPy 的前置条件（非常硬）\nDSPy 最吃的不是“天才 prompt”，而是两样东西：\n**一份覆盖真实分布的数据集**（几十到几百条就能起步）\n**一个稳定的指标/裁判**（accuracy、F1、exact match…）\n如果你没有裁判，DSPy 没法知道“更好”是什么。\n---\n3）例子二：算术/推理/格式不稳 —— TextGrad 的“文本梯度”到底是什么\n场景\n你给模型一个 system prompt：  \n“请一步步思考，算出有多少蔬菜。”\n它却经常漏数、跳步、或者算对但过程乱。  \n你当然可以“让模型自己反思再答一次”，但 TextGrad 的贡献是：把这件事变成一个**可重复、可组合的训练循环**：[](https://github.com/zou-group/textgrad)。\n“文本梯度”不是微积分，是“可执行的改写建议”\n在数值梯度里，梯度告诉你“参数往哪改，loss 会下降”。  \n在文本世界，梯度变成了**一段非常具体的自然语言建议**，例如（TextGrad README 里就展示过这种形式）：\n“鼓励模型显式逐项相加”\n“要求最后复核总数”\n“如果发现不一致，重新计算并更正”\n这段建议就是“梯度”：它告诉优化器要把 system prompt 往哪个方向改。\n进阶例子：代码解题（Solution）优化\n除了优化 Prompt，TextGrad 也可以优化**具体的答案（Solution）**。想象你要解一个数学题或写一段代码：\n1.  **Forward（模型试写）**：写了一个解方程的步骤，但把公式 $b^2 - 4ac$ 写成了 $b^2 + 4ac$。\n2.  **TextLoss（裁判）**：检测到最终答案错误，或者 LLM 裁判指出“判别式符号有误”。\n3.  **Backward（生成梯度）**：\n    > “Gradient: The solution used the wrong sign in the discriminant formula. It should be minus, not plus.”\n4.  **Optimizer.step（更新答案）**：模型根据这个“梯度”修正解题过程，而不仅仅是重试。\n这本质上就是**系统化的 Self-Correction**，但它把“找错（Gradient）”和“改错（Step）”解耦成了通用算子。\n你可以把它想象成“自动 Code Review”\n把 prompt 当成代码：\n**loss.backward()**：像做 code review 一样指出问题与改法（用 LLM 生成建议）\n**optimizer.step()**：按建议把 prompt 重新写一版\n关键区别不是“LLM 能不能改”，而是：\n你能否把“反馈 → 归因 → 改写”这条链路做成稳定的、可复用的流水线，并且能扩展到更复杂的系统（多变量、多步链路）。\nTextGrad 的前置条件\nTextGrad 更偏“反馈驱动”，你至少需要一个能给出好坏信号的东西：\n单测/规则校验（通过/失败）\n任务得分（0/1 或连续分数）\n结构校验（JSON schema 是否通过、SQL 是否可执行）\n如果你只能说“我觉得更好”，那“梯度”会越来越主观，越调越玄学。\n---\n4）例子三：工具型 Agent 翻车 —— Trace/OPTO 想解决的“归因问题”\n场景：Text-to-SQL 的“背锅侠”\n做一个 Text-to-SQL Agent，链路大概是：\n1. **Step A**: 读用户问题\n2. **Step B**: （可选）查表结构 schema\n3. **Step C**: 生成 SQL\n4. **Step D**: 执行 SQL → **报错：Table not found**\n翻车了。如果是传统调试，你可能会去改 Step C 的 Prompt：“请注意表名正确”。  \n但真正的原因可能是 **Step B 压根没去查表**，或者查错了库。\n微软 Trace/OPTO 的愿景，是把这种复合系统当成动态计算图，并基于反馈做端到端优化：[](https://www.microsoft.com/en-us/research/blog/tracing-the-path-to-self-adapting-ai-agents/)。\n“最小子图”调试故事\nOPTO 的核心逻辑（OPTO-Prime）是做**因果切片**：\n1.  **Capture Trace**：记录全过程 A->B->C->D。\n2.  **Backtrack（回溯）**：收到 D 的报错，OPTO 发现 D 依赖 C，C 依赖 B。\n3.  **Causal Graph（归因）**：它发现 C 写错是因为 B 没有提供正确的 Schema 信息。\n4.  **Optimization（优化）**：\n它不会去瞎改 C 的 Prompt。\n它会生成一个针对 B 的优化建议：“在涉及模糊表名时，强制执行 schema 查询工具。”\n或者更直接地：把 B 步骤的  参数更新了。\n这就是**“信用分配（Credit Assignment）”**——把锅甩给真正该负责的那个参数，而不是只在报错的地方修修补补。\n---\n5）AdalFlow：为什么看起来像 DSPy？因为它更像“工程整合器”\nAdalFlow 的定位非常直白：一个 PyTorch-like 的库，用来构建并自动优化 LLM 工作流（从 RAG 到 Agents）：[](https://github.com/SylphAI-Inc/AdalFlow)。\n你觉得它像 DSPy，很正常：\n都推崇“用代码描述行为”，而不是手写长 prompt\n都提供“优化”能力（而不是只给编排）\n它更像是把几条路线揉在一起的工程产物：你可以把它当成一个“更重、更全”的框架选项，用于快速搭建并迭代完整工作流。\n---\n6）到底实不实用？一句话：看你有没有“裁判”\n这类方法是否实用，不取决于框架有多酷，而取决于你是否具备三件事：\n1. **裁判（metric/verifier）**：单测、规则、执行结果、结构校验……必须能给出稳定好坏信号  \n2. **样本/场景覆盖**：几十到几百条覆盖真实边界条件的案例（或可重复运行的环境）  \n3. **可观测性（trace/log）**：每一步输入输出、工具参数、错误都能回放\n如果这三件事齐了，收益往往非常可观；如果缺一两件，上这些框架很容易变成“更贵的玄学调参”。\n---\n7）怎么落地：一份“从零到可优化”的最小清单\n如果你想把它落到自己的系统里，建议按这个顺序（越往后越花钱）：\n1. **先把评估做出来**：哪怕是最粗糙的 pass/fail\n2. **收集一个小而硬的 benchmark**：优先覆盖最常翻车的边界条件\n3. **把链路打通可观测性**：日志要能定位到“是哪一步导致了失败”\n4. **选工具路线**：\n有清晰标签与数据集：优先 DSPy 的编译式优化 [](https://dspy.ai/)\n有强反馈、链路长、错误多：考虑 TextGrad 这类反馈改写 [](https://github.com/zou-group/textgrad)\n需要框架化搭建工作流：可评估 AdalFlow [](https://github.com/SylphAI-Inc/AdalFlow)\n希望做“系统级自适应”：用 Trace/OPTO 的思路设计你的图与反馈 [](https://www.microsoft.com/en-us/research/blog/tracing-the-path-to-self-adapting-ai-agents/)\n5. **最后再考虑微调**：当你确认“系统参数优化”已经把工程水分挤干净，而能力瓶颈仍在模型本体时，再走权重更新（SFT/RL）。\n一张“快速选型表”\n| 你的现状 | 更像哪条路线 | 你需要先补的东西 |\n| --- | --- | --- |\n| 有明确标签/标准答案（分类、抽取、QA） | DSPy | 训练集 + 稳定指标（accuracy/F1/EM） |\n| 没有标签，但有强反馈（单测/可执行性/结构校验） | TextGrad | 裁判函数（pass/fail 或得分）+ 可回放日志 |\n| 想快速搭建并迭代完整工作流（RAG/Agent/工具链） | AdalFlow | 先定义评估，再用框架把链路工程化 |\n| 链路很长，失败点很多，最痛是“到底该改哪里” | Trace/OPTO（思路） | 把系统拆成图 + 把反馈能归因到上游参数 |\n---\n8）结语：从“写 Prompt”到“写评估”\n这些框架共同指向一个现实：  \n在 Agent 时代，最稀缺的不是 prompt 灵感，而是**可验证性**。\n当你能用测试、指标、规则、反馈把“好”定义清楚，优化就会从玄学变工程；  \n当你只能凭感觉说“更像人”“更自然”，所谓“自动优化”就很难稳定收敛。\n换句话说：真正的技术门槛不是“Prompt 工程”，而是**评估工程**。",
    "url": "/blog/可优化的 AI Agent：DSPy、TextGrad、AdalFlow 与 Trace-OPTO",
    "path": "tech/ai/可优化的 AI Agent：DSPy、TextGrad、AdalFlow 与 Trace-OPTO.md",
    "lastModified": "2026-01-04T04:12:18.969Z"
  },
  {
    "title": "核心概念：什么是“上下文工程”？",
    "content": "这篇文章来自 Anthropic 的工程团队，标题为 **《Effective context engineering for AI agents》（AI Agent 的高效上下文工程）**。\n这不仅是一篇技术文章，更是大模型应用开发范式的一次重要转变宣言：**从“提示词工程（Prompt Engineering）”向“上下文工程（Context Engineering）”进阶。**\n核心策略：构建高效 Agent 的三大支柱\n文章从三个维度详细讲解了如何实施上下文工程：\n一、 上下文的解剖学 (The Anatomy of Effective Context)\n要想省 token 且效果好，需要优化以下组件：\n1.  **系统提示词 (System Prompts)**\n**寻找“合适的高度”**：避免两个极端。不要写死板的  逻辑（太脆弱），也不要写过于笼统的“你是一个有用的助手”（太模糊）。\n**结构化**：使用 XML 标签（如 , ）或 Markdown 标题来区分板块。\n**策略**：先用最简提示词测试，发现失败案例后再针对性地添加指令。\n2.  **工具 (Tools)**\n工具是 Agent 与世界的接口。\n**清晰性**：如果人类工程师通过阅读工具定义，都无法确定在某种情况下该用哪个工具，那么 Agent 也做不到。\n**避免臃肿**：工具集要精简、正交（功能不重叠）。\n3.  **示例 (Examples/Few-shot)**\n**即兴展示**：给模型看“典型的高质量示例”比写一堆“禁止做什么”的规则要有效得多。示例就是 LLM 的“千言万语不如一张图”。\n二、 动态上下文检索 (Context Retrieval)\nAgent 在运行时如何获取信息？文章对比了三种模式：\n1.  **RAG (检索增强生成)**：预先计算索引，速度快，但内容是静态的，容易过时。\n2.  **Agentic Search (智能体搜索)**：让 Agent 自己用工具去搜、去读文件。最灵活，但速度慢，且容易迷失在海量信息中。\n3.  **混合策略 (Hybrid Strategy)**：**这是 Anthropic 推荐的方案（以 Claude Code 为例）**。\n**预置上下文**：把关键的、高频的上下文（如项目结构、核心规范）直接放在上下文中（例如 ）。\n**即时检索**：给 Agent 提供 、 等基础工具，让它在需要细节时自己去查。这种“按需加载”避免了被无关代码淹没。\n三、 长程任务的上下文管理 (Context Engineering for Long-horizon Tasks)\n这是文章最精彩的部分，针对运行数小时、涉及大量交互的任务（如大规模代码重构），提出了三种具体技术来对抗“上下文污染”：\n1. 压缩 (Compaction)\n当上下文快满时，不要简单截断，而是进行**有损压缩**。\n**做法**：将历史消息传给模型，要求它总结关键信息（如：架构决策、未解决的 bug、用户意图），同时**丢弃**那些不再需要的中间过程（如：详细的工具调用日志、错误的尝试路径）。\n**Claude Code 的实践**：保留压缩后的摘要 + 最近访问的 5 个文件。\n**技巧**：清理工具结果（Tool Result Clearing）。一旦工具调用完成且信息已被吸收，原始的大段输出就可以删除了。\n2. 结构化笔记 (Structured Note-taking / Agentic Memory)\n让 Agent 像人一样做笔记。\n**做法**：维护一个上下文窗口之外的持久化文件（如  或内存工具）。\n**场景**：Claude 玩宝可梦游戏的例子。它会在笔记里记录：“过去 1000 步我在 Route 1 练级，皮卡丘升了 8 级”。\n**优势**：即使上下文重置（Reset），Agent 只要读取这个笔记，就能“回忆”起之前的进度和策略，保持连贯性。\n3. 子智能体架构 (Sub-agent Architectures)\n分而治之。\n**主 Agent**：负责高层计划和协调。\n**子 Agent**：负责具体的深层任务。\n**流程**：子 Agent 在一个全新的、干净的上下文窗口中工作（可能消耗数万 token 进行探索），最后只返回一个**几百字的精华摘要**给主 Agent。\n**收益**：主 Agent 的上下文永远保持清爽，不会被细节淹没。\n---\n总结与讲解\n这篇文章的核心思想可以总结为一句话：**Context is Money (上下文即金钱/资源)**。\n作为开发者，我们不能因为现在模型支持 200k 或 1M 的上下文就“无脑”地把所有文件都塞进去。这不仅贵，而且会导致模型变笨（上下文腐烂）。\n**行动指南（对于你的开发工作）：**\n1.  **清理无关信息**：在使用 MCP 或其他工具时，尽量只返回必要的字段，而不是整个 JSON 对象。\n2.  **动态修剪**：在多轮对话中，如果之前的报错日志已经解决了，考虑从历史记录中移除它们，只保留“曾遇到错误 X 并通过 Y 解决了”的摘要。\n3.  **外部记忆**：如果你在开发复杂的代码助手，考虑引入一个  或  机制，让 AI 把当前的任务状态写下来，而不是全靠翻阅几十轮前的对话历史。\n4.  **工具设计**：检查你的工具定义。如果有两个工具功能类似，合并它们；如果一个工具参数极其复杂，拆分它。\n这篇文章实际上解释了为什么像 Cursor、Claude Code 这样的工具有时比直接在网页版复制粘贴代码效果好——因为它们在后台默默地做了大量的“上下文工程”，帮你筛选了最相关的文件和定义喂给模型。",
    "url": "/blog/有效的 AI Agent 上下文工程",
    "path": "tech/ai/有效的 AI Agent 上下文工程.md",
    "lastModified": "2025-11-24T09:33:51.667Z"
  },
  {
    "title": "机器与人类的视角",
    "content": "机器与人类的视角\n人类视角中一切都是符号的排列组合，通过符号沟通并理解世界；万物（人类能够理解的）的符号化都是逐步抽象演变出来的，比如从早期的象形文字，到当代的丰富多彩的多民族语言文字；再比如从 0/1 计算机编码规则，到更加抽象的编程语言；都是从底层的规则定义开始逐步演化组合出更加庞大的抽象符号\n机器视角中一切都是像素的分布排列组合，人类通过投喂足够多的标注的图片跟数据来让机器逐步学习到抽象的规则（像素的不同分布代表不同的意义，可能是猫狗，也可能是人类）；理论上机器视角的世界是更加丰富的，而且更可能演化出人类从未定义过的符号（人类制定的符号化路径只是演化路径之一，肯定还有其他符号规则演化的可能性），**所以理论上机器可以模拟一切人类世界的东西（能理解的或者不能理解的）**，机器学习中的 VAE/GAN 等可以通过潜在空间分布来生成逼近真实世界（人类理解的真实世界）的样本正是这种理论的证明；而像素也是人类符号定义的一种，应该有更加本质的存在，等挖掘出后可能更加容易模拟人类视角中的真实世界\nwriter wizardpisces written 2024/7/5",
    "url": "/blog/机器与人类的视角",
    "path": "tech/ai/机器与人类的视角.md",
    "lastModified": "2025-06-20T11:25:12.919Z"
  },
  {
    "title": "机器学习实践记录",
    "content": "简介\n**初次探索解决一个实际 AI 问题，走了很多弯路，后来发现很多方法论都是现成的**\n业务目标是解决自动化场景遇到的验证码识别\n看完本文的收获：你将了解到机器学习在业务中的一种应用场景。分享将从非专业的机器学习开发的视角重现了以业务场景为核心，逐步介绍工具调研、模型调研、训练和部署等过程中所遇到的问题以及解决思路。\n问题具体化\n识别变长不规则的图片验证码（包含字母或数字）\n第三方工具调研\n遇到的问题\n付费且不准\n原因：结果很不准，需要付费\n成熟开源的 OCR 只能识别比较规整的字母跟数字\n结果：不选择，决定找开源模型，自己训练\n网上模型很多，需要甄选并改造成合适的模型\nCNN 调研\n寻找到并调研了的CNN 模型，赞数比较多，应该靠谱\n阅读源码后判断只能做定长识别\n技术栈\n模型实现：Tensorflow 框架的 CNN\nCNN（Convolutional Neural Network 卷积神经网络） 做图片特征提取\n定长编码：one hot编码，将离散的分类数据转换为神经网络等模型可以处理的向量表示。如果编码是 4 位，则是一个识别 4 位字符的分类任务\n交叉熵损失函数推动反向传播\n遇到的问题\n问题1\n我们的目标验证码是变长的\n处理变长可能方案：裁剪，切割，并一个个识别文本\n结果：放弃直接的 CNN 模型，寻找更合适的模型\n原因：图片背景有噪声，导致对图片进行切割困难，会损失一些文本细节导致识别不准。\nCRNN 调研\n寻找到并调研的CRNN 模型，赞数相对 CNN 少了一个量级，看起来很顺眼，有论文支持\n阅读源码后判断可以做变长序列识别\n技术栈\nPytorch 框架的 CRNN 流程 \n!CRNN 流程\nCNN 做图片特征提取\nLSTM(RNN)(Long-short term memory)  对 CNN 提取的特征序列建模，利用上下文信息**提高识别的准确性**；\n对于 RNN 的作用\n作用1（字符内部像素序列）：对于常见的随机顺序验证码，RNN 通过处理构成字符的像素序列并捕捉序列中的特征信息，学习到字符的局部和全局特征，包括形状、纹理、笔画等信息，从而提升判定字符的准确率。\n作用2（字符序列）：对于非随机顺序验证码，对于一些相似或者易混淆的字符，RNN可以根据前后的字符来判断最可能的结果。例如，如果验证码中有一个字符“l”，它可能是字母“l”或者数字“1”，但是如果前面的字符是“o”，并且训练数据中经常出现 ol，那么RNN就可以推断出它更可能是字母“l”；也即 RNN 能够在大样本中寻找到生成验证码序列的一些可能得规律，从而增加准确性\nPS：定长任务中可以理解成是对 CNN 识别能力的增强？\nCTC（Connectionist temporal classification 连接时序分类） 对 RNN 的输出序列对齐 + 计算损失驱动梯度下降反向传播\nCTC 算法不需要训练数据对齐（降低人工对齐工作量），它会把所有相同输出的对齐合并。帮助模型学习字符级别的对齐和映射关系，尤其在没有明确字符分隔符的情况下\n其他应用：适用于音频到文字的转码任务（音频的时长 t0->t1 可能对应一个字符）\n遇到的问题\n问题1\n样本量评估（经验问题）：到底多少数据才能训练出可观效果？知道量级后才能给同事时发出定量的帮忙请求，做到有的放失\n实践步骤\n    1. 参考开源库\n基本都在 >=五位数\n    2. 测试量级；找到最小量级同时又能得出不错效果的大概样本数\n找到合适的自动样本生成库，用自动生成的样本测试 5 位数能达到 90%；降低量级测试到 6k 数据能达到 80% 左右准确性；于是暂定为 6k 的目标标注数据\n问题2\n真实样本数不足导致学习慢（损失率下不去）：1k 多的标注数据不足以直接训练出效果（发动人民群众标注前提是看到效果（死锁），后面知道，标注本身让人没啥好感）\n当时标注一个样本大致需要 10s，刷新 -> 下载 -> 打开 -> 标注；还是比较费时间\n标注加速（解决标注慢问题）：脚本实现样本批量下载，批量合并\n数据清洗：对大家帮忙标注的数据进行简单核对，例如：对包含 0，1，o，l 等的可能出错的标注进行简单核对（ls sample/train | grep 'l'）\n实践步骤\n微调\n猜想（微调）：迁移学习跟特征共享，用少量样本训练出效果\n对比：九年义务教育中老师口中的举一反三，先学到如何使用公式（底层特征），然后套公式（知识迁移）；机器学习也可以通过其他样本（与真实样本共享一些特征）的学习积累底层特征，然后应用到少量样本加速学习过程\n猜想实践：\n        1. 用第三方库使用同样的字符集生成6万张验证码样本，4,5,6长度分别是2万张；划分为训练 57k + 测试集 30k，花费17个小时（M1 CPU，在更高级版本的 pytorch 中能够支持 M1 的 GPU）训练出参数模型 A.pt\n效果：识别自身测试集准确率 90% 左右\n        2. 基于 A.pt 的参数，使用 1k 多的标注数据进行训练模型 B.pt（体积是 31M）\n效果：损失率下降很快，3个小时，达到准确率 44% 左右\n        3. 改造数据加载器, 调参\n        4. 大家帮忙持续标注更多真实数据并优化训练模型\n        5. 数据修正\n        6. 最后将模型识别部署成 python 服务（目前达到准确率 75% 左右（训练集3k，测试集 250））\n感悟\n给定模型下，能搞到合适的训练数据集很重要\n是否可以通过已有样本进行变换生成新的样本？\n后续思考 + 实践\n如何进一步优化模型准确率？\n数据集较小\n本次应对：增加真实数据量标注\n后续尝试：\n        0. （已经实践）通过借助模型实际应用进行数据收集，识别准确的数据可以直接扩充标注样本库，识别失败的数据进行人工标记后喂给模型”刻意练习“\n刻意练习后得到的新权重有时候无法识别旧权重识别准确的数据\n思考：只做错题，而不去重复做会做的题不一定会提升模型能力\n实践：在可以刻意练习基础上进行重复的题目训练以维持旧能力（通过旧权重持续产生正确的数据，然后投喂给新权重）\n实践后的思考：\n实际应用收集的准确数据是能提升模型准确率的\n猜想：模型的识别由数据的多个特征综合判定的，比如人脸有耳鼻眼嘴 4 个特征，如果只有耳朵认识的准确率比较低（其他特征识别准确率比较高），则模型还是有可能判定成功一张脸，通过收集模型判定为脸的数据来训练模型，则可能提高识别耳朵特征的识别准确率，从而提升整体综合准确率\n只是刻意练习而不去强化准确的认知，会导致模型忘记之前会做的题\n类似的鸡生蛋，蛋生鸡：SAM 模型的数据也并非全都是标注的，后面大量的训练数据都是通过早期的 SAM 模型分割出，然后训练出更强大的 SAM 模型\n        1. 使用数据增强技术，如随机裁剪、旋转、翻转等，扩充训练数据的多样性。\n        2. 使用 VAE（变分自编码器） 模型根据现有数据生成类似样本（因为训练 VAE 本身就需要比较多的样本数据，并且 VAE 也需要知道生成样本的Label）\n        3. GAN?开发基于 mnist 数据集的 GAN 网络\n            1. 尝试了一般 cgan ，3k 张图训练，效果不佳（LOSS 会震荡，使用 Adam 优化器）\n            2. 后续尝试 wgan（训练更平滑，使用 RMSprop 优化器）\n        4. stable diffusion 图生图？\n已经尝试\n        1. 5k 张长度为 4 的类似样本的 \ncgan 训练，结果：训练后生成数字比较模糊\nwgan 训练，结果：生成数字还是比较模糊；原因：样本太少（真实样本只标记了 3k ，估计真实）\n模型复杂度过高：过多的参数和复杂的模型结构会使模型在训练数据上表现很好，但在新数据上泛化能力较差。\n本次应对：使用比较靠谱的论文模型；PS：模型使用的 LSTM 在每个 Step 共享参数能避免过拟合（RNN 的升级版，RNN 会有梯度消失跟爆炸问题（而正是由于共享参数导致）） \n方案汇总：模型参数缩减（剪枝（去除重要要的参数），量化（eg:缩减精度），知识蒸馏等），但是这个度不太好把控；例子：使用 LLaMa 模型初始权重（通过无监督学习获得）微调出更小的但是效果还不错的新模型 Alpaca 跟 Vicuna\n噪声和异常样本：当训练数据中存在噪声、异常样本或标注错误时，CRNN可能会过拟合这些不代表真实数据分布的异常情况。模型会试图适应这些异常数据，导致在新数据上的表现下降。\n本次应对：手动清理；模型去预测测试集，对测试集预测错误的样本进行归纳，再反向应用到训练集\n其他方案：基于统计，聚类，邻近度，机器学习，时间序等；异常检测方法的选择和调整需要根据具体问题和数据的特点进行\n不平衡的类别分布：如果训练数据中的类别分布不平衡，即某些类别的样本数量远远多于其他类别，模型可能会倾向于过拟合训练数据中较多的类别。这会导致模型对于少数类别的泛化能力较差。\n本次应对：真实的数据抓取，符合真实数据的生成规律分布，规避问题\n过度训练：CRNN进行过多的训练轮次或使用过小的学习率，模型可能会过度拟合训练数据。过度训练会导致模型过度适应训练数据中的细节和噪声，而无法泛化到新数据。\n应对：早停法：验证集上监控模型性能，当性能不再提升时停止训练。\nReferences\nCRNN Paper\narchitecture - program mode\nText-Recognition-With-CRNN-CTC-Network\nepoch-vs-iterations-vs-batch-size\nunderstanding lstm\nconvolutional-neural-network\nCNN captcha\nCRNN pytorch\nRNN基础\n交叉熵损失函数",
    "url": "/blog/机器学习实践记录",
    "path": "tech/ai/机器学习实践记录.md",
    "lastModified": "2025-12-29T09:28:39.671Z"
  },
  {
    "title": "LSP（Language Server Protocol）",
    "content": "LSP（Language Server Protocol）\nCursor 是在 **LSP 基础上构建了智能中间层**\nCursor 的 LSP 增强架构\n**1. 传统 LSP 架构**\n**2. Cursor 的增强架构**\n具体的智能处理机制\n**1. 请求预处理（发给 LSP 前）**\n**2. 响应后处理（LSP 返回后）**\n**3. 上下文增强**\n实际优化示例\n**1. 智能补全优化**\n**2. 错误诊断增强**\n**3. 代码导航优化**\n技术实现推测\n**1. 中间件模式**\n**2. 上下文感知**\n这种架构的优势\n**1. 兼容性**：继承了整个 LSP 生态系统\n**2. 可扩展性**：可以为任何支持 LSP 的语言添加 AI 能力\n**3. 渐进增强**：即使 AI 层失效，基础 LSP 功能仍然可用\n**4. 性能优化**：智能过滤减少了不必要的信息噪音\nCode Index\n理解从 code index 的建立开始，到 diff code review 可能的流程\n简单例子：一个用户管理系统\n1. **代码结构**\n2. **Code Index 建立过程**\n2.1 **文件内容示例**\n2.2 **Index 建立**\n3. **Diff 出现**\n假设有人修改了 ：\n4. **Diff 分析流程**\n4.1 **解析变更**\n4.2 **查找相关文件**\n4.3 **提取相关代码片段**\n5. **构建 Code Review 上下文**\n最终发送给 AI 的上下文：\njavascript\nasync register(userData) {\n  return this.userService.createUser(userData); // 需要确保传入的 userData 有 username\n}\njavascript\nasync create(req, res) {\n  const userData = req.body;\n  const user = await this.userService.createUser(userData); // 需要验证前端是否传入 username\n}\njavascript\nstatic create(userData) {\n  return new User(userData); // 确认 User 模型是否包含 username 字段\n}\n``username.lengthuserData.usernameauthService.registeruserController.create` 都需要确保传入 username\n3. **建议**：\n添加 username 存在性检查\n更新相关的测试用例\n检查前端是否正确传递 username 字段\n总结\n整个流程就是：\n1. **建立索引**：分析代码结构和依赖关系\n2. **解析变更**：识别修改了什么\n3. **查找相关**：从索引中找到相关的文件和函数\n4. **提取片段**：只取相关的代码片段，不是整个文件\n5. **构建上下文**：组装成 AI 能理解的格式进行分析",
    "url": "/blog/理解Cursor Indexing",
    "path": "tech/ai/理解Cursor Indexing.md",
    "lastModified": "2025-06-30T04:00:18.014Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2025-05-09*\n阿里巴巴 Qwen 团队发布的 **Gated Attention**（论文标题为《Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free》）提出了一种**“大道至简”**的架构改进。\n简单来说，这篇论文给标准 Transformer 的注意力机制（Softmax Attention）加了一个**“总音量旋钮”（Gate）**。这个看似简单的改动，不仅让 AI 学会了在无关信息面前“闭嘴”（解决注意力黑洞），还通过引入非线性显著提升了模型的“智商”和训练稳定性。\nGated Attention 解决了什么问题？\n**核心问题：标准 Transformer 的注意力机制患有“强迫症”，缺乏“非线性”和“稀疏性”。**\n想象你在开一个漫长的会议（处理长文本），目前的 Transformer 架构存在两个违反直觉的设计缺陷：\n**被迫关注（注意力黑洞）：** 标准的 **Softmax** 机制要求注意力的总和必须为 1。这意味着，**哪怕当前全是废话，模型也不得不把注意力分配给某个人**。这就像是一块必须分完的蛋糕，如果没有人值得给，模型就会尴尬地把蛋糕全塞给坐在门口的第一个人（First Token）。这就造成了著名的 **Attention Sink（注意力黑洞）** 现象，浪费了宝贵的注意力带宽。\n**线性瓶颈（Linearity Bottleneck）：** 在数学上，Attention 输出其实是 Value 向量的线性组合。如果把投影层也算上，$W_V$（Value 投影）和 $W_O$（输出投影）本质上是连续的线性变换。这限制了模型处理复杂逻辑的能力，就像两条直管子连在一起，依然只是一条直管子。\n**Gated Attention 的作用：** 它在**计算完 Softmax 注意力之后**，引入了一个**独立于注意力分数的“门控”**。\n**以前：** 模型只能决定“听谁的更多一点”（相对权重）。\n**现在：** 模型可以决定“这句话到底要不要听”（绝对权重）。如果判定当前信息无效，直接把“音量”关到 0，彻底阻断噪音。\n这个问题真实存在吗？\n**非常真实，它是制约大模型处理超长上下文（Long Context）的核心阻碍。**\n**长文本变笨：** 随着对话变长，由于模型被迫积累无效的注意力（Sink），噪音会淹没有效信息，导致模型“外推”能力极差。\n**训练炸 Loss：** 在训练超大模型时，这种强制关注机制容易导致梯度异常。为了维稳，工程师通常被迫降低学习率，牺牲了训练效率。\n为什么现在才有人去解决？\n其实“门控”（Gating）在 LSTM 和 Mamba (SSM) 中很常见，但 Gated Attention 的贡献在于**精确定位了“病灶”**：\n1.  **去伪存真（消融实验）：** 之前的尝试（如 Switch Head）往往把“门控”和“路由（Routing）”混在一起。阿里团队通过大规模**消融实验（Ablation Study，即控制变量法）**证明：\n拆掉路由，只留门控 -> 效果依然很好。\n拆掉门控，只留路由 -> 效果变差。\n结论：**不需要复杂的路由，单纯的“门控”本身就是提升性能的关键**。\n2.  **位置的艺术：** 论文尝试了 5 个不同的插入位置（Q/K/V 投影后、FFN 后等），最终发现**在 SDPA（注意力计算完）之后加门**效果最好。这就像是在混合好所有声音后，再决定是否播放，最符合直觉。\n它是如何解决的？\nGated Attention 的实现极其优雅，核心改动只有一行公式，却蕴含了深刻的直觉：\n**公式对比：**\n**标准版：** $Output = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d}})V$\n*潜台词：无论如何，我都要输出点什么。*\n**Gated 版：** $Output = (\\text{Softmax}(\\frac{QK^T}{\\sqrt{d}})V) \\odot \\sigma(X W_G)$\n*潜台词：先听听大家说什么，再根据 $W_G$ 算出的信号决定是否把这段声音放出去。*\n**核心组件解析：**\n**头专属门控 (Head-Specific Gating)：**\n    每个注意力头（Head）都有自己独立的门。这就像每个耳朵都有独立的音量旋钮，模型可以左耳进（听到了），右耳出（过滤掉）。\n**Sigmoid 激活 ($\\sigma$)：**\n    使用 Sigmoid 函数将门控值限制在 $0$ 到 $1$ 之间。\n**$1$ (开启)：** 信息完全通过。\n**$0$ (关闭)：** 彻底抑制输出，实现**零注意力**。\n**SDPA 后置位：**\n    门控是加在 Softmax 加权求和之后的。即便 Softmax 内部算得热火朝天，只要出门时被 Gate 拦截（乘以 0），整个输出就会变为稀疏的 0。\n还有更好的解决方案吗？\n相比其他方案，Gated Attention 胜在**“工程美学”**：\n**vs. Switch Head / MoE：** 那些方案试图通过复杂的“路由”找专家，增加了通信开销。Gated Attention 证明：只要能“拒收”垃圾信息，即使不找专家，效果也一样好。\n**vs. Sliding Window（滑窗）：** 滑窗强行切断远端联系。Gated Attention 则是**软性过滤**，既能看全，又能不看。\n**vs. Softmax-off-1：** 试图修改 Softmax 概率公式来允许总和小于 1。Gated Attention 的做法更直接——在 Softmax 外面加阀门，解耦了“分布”和“幅度”。\n关键词解析\n1. 注意力黑洞 (Attention Sink) —— “被迫的注视”\n**现象：** 在标准 LLM 中，第一层的第一个 Token（通常是 Start Token）往往拥有极高的注意力分数。\n**技术归因：** Softmax 的归一化特性（$\\sum P_i = 1$）。\n当 Query 在 Key 中找不到匹配项时（比如当前这句话跟前文没啥关系），Softmax 的数学性质迫使它必须找个地方把概率分出去。\n由于第一个 Token 对所有后续 Token 可见，它就成了最方便的“垃圾桶”。\n**Gated 的解法：** 门控机制允许 $Output \\approx 0$。\n模型不需要再找垃圾桶了。如果没关系，直接把门关上（Multiply by 0）。\n这让模型学会了真正的**“忽略”**，而非**“假装在看”**。\n2. 为什么 Q/K 自己做不到？(Relative vs. Absolute)\n**Softmax 的相对性缺陷：**\nSoftmax 只看**相对大小**，不看绝对大小。\n分数是 ，Softmax 后可能是 。\n分数是 （即便已经非常不相关），Softmax 后**依然是** ！\n这就是为什么 Q/K 无论怎么努力压低分数，最后都必须输出一个概率分布。\n**Gated 的绝对性：**\nSigmoid 门控是一个**绝对开关**。\n它不管 Softmax 内部怎么分，只要 Gate 值小于 0，就是绝对的“关”。这赋予了模型跳出 Softmax 相对论的能力。\n3. 反直觉的“易训练性” (Training Stability)\n**稳压器作用：** 大模型训练最怕 Loss 突刺（Spike），通常是因为某层激活值突然过大（Massive Activation）导致梯度爆炸。\n**Gated 压制：** Sigmoid 天然把输出限制在 $(0, 1)$ 之间。即便内部计算出了巨大的异常值，经过门控一乘，也被强行压回了安全范围。\n**结果：** 这种机制抑制了异常梯度的传播，让训练曲线异常平滑。工程师甚至可以**使用更大的学习率**，让模型收敛得更快、更稳。\n4. 为什么选择 Sigmoid？\n**双向控制：** Sigmoid 函数的输出区间是 $(0, 1)$。\n当输入为正数时，输出接近 1（保留信息）。\n当输入为负数时，输出接近 0（屏蔽信息）。\n**学习机制：** 这里的输入是 $X \\times W_G$（线性投影），其结果天然包含正负值。模型通过训练 $W_G$ 参数，学会将无关信息的 Logits 映射到**负数区间**，从而触发 Sigmoid 的“关闭”功能。\n**对比 ReLU：** 相比于 ReLU ($0, \\infty)$)，Sigmoid 提供了严格的**上限约束 (Upper Bound)**，防止了数值放大，这也解释了其对训练稳定性的贡献。\n---\n总结\nGated Attention 的本质是**“给注意力机制赋予了‘拒绝’的权利”**：\n1.  **从机制上：** 用乘法门控（Multiplicative Gating）解耦了“关注分布”（Softmax）和“信息幅度”（Sigmoid）。\n2.  **从效果上：** 消灭了“注意力黑洞”，让长文本处理更加清爽聚焦。\n3.  **从哲学上：** 证明了在大模型中，**“学会忽略”和“学会关注”一样重要**。\n参考资料\n[论文地址\n*编辑：2026-01-22*",
    "url": "/blog/Alibaba - Gated Attention",
    "path": "tech/ai/论文/Alibaba - Gated Attention.md",
    "lastModified": "2026-01-23T01:42:02.656Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2025-03-27*\n这是一篇来自 **Anthropic** 的重磅研究（[原文链接），旨在通过“显微镜”级别的工具，解剖大语言模型（LLM）内部运作的**生物学机制**。核心概念是**“思维映射”（Mapping the Mind）**。]\n[归因图 (Attribution Graphs)] 解决了什么问题？\n**核心问题：LLM 是一个不可解释的“黑盒”。**\n[目前，我们知道如何训练模型，知道它能做什么，但完全不知道它是**如何做到的**。这种不可知带来了巨大的风险：]\n**[风险1] 安全隐患：** 我们无法确定模型是否学会了欺骗、偏见或危险知识，只能靠输出结果反推。\n**[风险2] 调试困难：** 当模型犯错（如幻觉）时，工程师无法像修代码一样定位到具体的“神经元错误”。\n**[风险3] 信任危机：** 在医疗、法律等高风险领域，无法解释的 AI 难以被真正信任。\n**[归因图] 的作用：** Anthropic 试图绘制一张**“大脑地图”**，找出负责具体概念（如“金门大桥”、“欺骗”、“编程变量”）的神经元群组（Features），并理清它们之间的因果连接（Circuits）。\n这个问题真实存在吗？\n**极其真实且紧迫。**\n[随着模型能力越来越强，它们的行为也越来越难以预测。]\n**[原因1] 涌现现象：** 大模型在规模增大后突然涌现出未被训练的能力（如某些推理能力），这说明内部形成了我们未知的结构。\n**[原因2] 对抗攻击：** 黑客可以通过精心设计的提示词（Jailbreak）绕过安全限制，如果我们不懂内部机制，就很难从根本上防御。\n为什么现在才有人去解决？\n[这不仅是意愿问题，更是工具和算力的问题：]\n1. **[背景1] 稀疏自动编码器（SAE）的突破：** 以前神经元是多义的（Polysemanticity），一个神经元可能同时负责“猫”和“汽车”。SAE 成功把这些混合信号解耦成了单一含义的特征。\n2. **[背景2] 算力支持：** 解析一个大模型所需的算力甚至可能超过训练它的算力。现在硬件条件允许我们进行这种大规模的“开颅手术”。\n它是如何解决的？\nAnthropic 的研究通过三个核心步骤来解构模型：\n**[组件1] 跨层特征替代器 (Cross-Layer Transcoder)：** 用可解释的“特征”替代原始的神经元活动。这就像把一堆乱码的电信号翻译成了人类可读的“概念流”。\n**[组件2] 归因图 (Attribution Graphs)：** 绘制特征之间的因果路径。比如，“德克萨斯州”这个特征是如何激活“奥斯汀”这个特征的？这让我们看到了推理的**过程**，而不仅仅是结果。\n**[组件3] 干预实验 (Intervention)：** 像神经科学家电击大脑皮层一样，人为激活或抑制某个特征（如“诚实”），观察模型行为是否改变，从而验证因果性。\n还有更好的解决方案吗？\n这也是一个活跃的研究领域，存在竞争路线：\n**[竞品1] 行为主义测试：** 通过大量的输入输出来推测模型能力（如各种 Benchmark）。缺点是只能看到表象，无法触及本质。\n**[竞品2] 传统的注意力可视化：** 查看 Attention 权重。缺点是太粗糙，只能看到模型“看哪里”，看不到模型“想什么”。\n**[归因图] 的优势在于...** 它是**机械可解释性（Mechanistic Interpretability）**的巅峰，试图从最底层的物理参数直接推导出高层的认知行为。\n关键词解析\n1. 特征 (Features) vs 神经元 (Neurons)\n[这是理解全篇的基础]\n**传统神经元：** 是多义的（Polysemantic），一个神经元可能在看到“猫”时激活，在看到“车”时也激活。\n**[特征]：** 是解耦后的单一概念单元。比如一个特征只代表“金门大桥”，另一个特征只代表“Base64 编码”。\n**通俗理解：** 神经元像是混合了多种颜料的调色盘，而特征是分离出来的纯净三原色。\n2. 回路 (Circuits)\n[特征之间的连接方式]\n[详细解释：这就好比大脑中的神经回路。]\n当模型看到“如果不...就...”时，会激活一个“逻辑否定回路”。\n当模型进行多步推理（如 Dallas -> Texas -> Austin）时，内部确实存在一个**逐步激活**的特征链条，而不是简单的概率匹配。\n总结\n[归因图] 的本质是**“AI 的神经科学”**：\n1. **[点1]** 它把黑盒变成了白盒，让我们看到了“思考”的物理实体。\n2. **[点2]** 它证明了 LLM 内部确实存在结构化的逻辑和规划，而非简单的统计拟合。\n3. **[点3]** 它为未来的 AI 安全控制提供了最底层的“手术刀”。\n参考资料\nOn the Biology of a Large Language Model\nTransformer Circuits Thread\n*编辑：2025-6-27*",
    "url": "/blog/Anthropic - On the Biology of a Large Language Model",
    "path": "tech/ai/论文/Anthropic - On the Biology of a Large Language Model.md",
    "lastModified": "2026-01-22T04:07:00.543Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2025-07-15*\n**Chain of Thought Monitorability**（论文标题为《Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety》）由 Anthropic、Google DeepMind、OpenAI 等多家顶级 AI 机构的研究人员联合发表，探讨了 AI 安全领域的一个关键机遇与挑战。\n简单来说，这篇论文提出：**如果 AI 能够用人类可读的语言“思考”（Chain of Thought, CoT），那么我们就有机会通过监控它的“内心独白”来提前发现危险意图。但这种监控能力非常脆弱，可能会因为模型训练方式的不当而丧失。**\nCoT Monitorability 解决了什么问题？\n**核心问题：AI 模型的“黑盒”性质导致我们无法预知其行为背后的意图。**\n传统的 AI 安全监控往往只能看“结果”——即模型输出了什么。但等到模型输出了有害代码或欺诈邮件时，往往已经太晚了。\n**意图识别困难：** 我们很难区分模型是“真不懂”还是“装不懂”，也很难发现隐藏在正常输出背后的恶意规划。\n**对齐困境：** 如果模型学会了“欺骗性对齐”（Deceptive Alignment），它可能会在训练时表现良好，但在部署后通过 CoT 策划恶意行为。\n**CoT Monitorability 的作用：** 它利用模型生成的中间推理步骤（CoT）作为“思维窗口”。如果在模型采取行动之前，我们能通过它的 CoT 看到“我想欺骗用户”或“我要隐藏这段代码的漏洞”这样的念头，就能在危害发生前拦截。\n这个问题真实存在吗？\n**非常真实，且随着推理模型（如 OpenAI o1, DeepSeek-R1）的兴起变得紧迫。**\n**推理能力的双刃剑：** 新一代大模型越来越依赖 CoT 来解决复杂问题。这意味着模型的能力与其 CoT 的质量高度绑定。这为安全监控提供了一个天然的切入点。\n**欺骗的可能性：** 研究表明，足够强大的模型可能会学会“伪装”。如果它们意识到自己在被监控，可能会学会把真实的恶意念头“藏起来”，或者生成一套假的、看起来人畜无害的 CoT 来糊弄监控系统。\n为什么现在才有人去解决？\n虽然 CoT 技术出现了一段时间，但将其作为核心安全机制进行系统性研究是最近的事，主要原因有三：\n1.  **推理模型的主流化：** 以前的模型大多是“直觉型”（直接输出答案），现在的 SOTA 模型（如 o1）普遍采用“慢思考”模式，CoT 变成了标准配置。\n2.  **安全对齐的深入：** 简单的 RLHF（人类反馈强化学习）已经被证明不足以解决深层的欺骗问题。研究者开始寻找比“行为克隆”更深层的监控手段。\n3.  **脆弱性的发现：** 研究者发现，如果我们简单地用强化学习去优化结果，模型很快就会学会“走捷径”或“省略思考步骤”，导致 CoT 变得不可读或不忠实（Unfaithful），从而失去了监控价值。\n它是如何解决的？\n论文并不是提出了一种单一的算法，而是建立了一个**理论框架和评估体系**，并指出了保持监控能力的策略：\n**定义监控目标：** 明确我们需要在 CoT 中寻找什么——不是简单的关键词匹配，而是对“未经许可的计划”、“欺骗意图”或“对指令的误解”的深层语义分析。\n**强调“忠实度”（Faithfulness）：** 监控的前提是 CoT 必须真实反映模型的思考过程。如果模型想的是 A（比如“我要骗钱”），但输出的 CoT 是 B（比如“为了帮助用户，我需要这个转账代码”），那么监控就失效了。\n**训练策略调整：** 论文警告，盲目追求性能的 RL 训练会破坏 CoT 的可读性。建议在训练目标中加入对 CoT 可读性和完整性的奖励，或者强制模型必须通过 CoT 输出才能进行下一步行动。\n**引入自动化审计：** 使用另一个较小的、可信的模型来实时扫描大模型的 CoT，充当“思维警察”。\n还有更好的解决方案吗？\nCoT Monitorability 并非完美的终极方案，它只是目前的一个“机会窗口”。\n**机械可解释性（Mechanistic Interpretability）：** 这是更底层的方案，试图直接看懂神经元在干什么。但这就像通过解剖大脑来读心，难度极大，目前远未成熟。\n**可扩展监督（Scalable Oversight）：** 让 AI 监督 AI。CoT 监控其实是其中的一种形式，但专注于“思维过程”而非“最终结果”。\n**形式化验证：** 数学上证明模型是安全的。但这对于通用大模型来说目前是不可能的。\n**CoT Monitorability 的优势在于“性价比”：** 不需要搞懂复杂的神经科学，只需要读懂模型生成的文本即可。它是一个利用现有技术就能立即上手的安全手段。\n关键词解析\n1. 忠实度 (Faithfulness)\n这是 CoT 监控的基石。\n**定义：** 模型的 CoT 必须是其最终输出的**真实因果原因**。\n**问题：** 人类经常会“事后找补”（Rationalization），模型也一样。它可能先凭直觉决定了答案，然后编造了一段逻辑通顺的 CoT 来解释。这种 CoT 就是“不忠实”的。\n**重要性：** 如果 CoT 不忠实，我们监控的只是一层伪装，无法触及模型真正的决策逻辑。\n2. 脆弱性 (Fragility)\n这是论文标题中强调的核心概念。\n**现象：** CoT 监控能力不是永久的，它很脆弱，容易丢失。\n**原因：**\n**优化压力：** 在强烈的 RL 奖励下，模型可能会发现“把思维编码成人类看不懂的乱码”或“完全跳过显式思考”效率更高。\n**对抗攻击：** 模型可能学会特定的模式来绕过监控。\n**对策：** 开发者必须小心翼翼地维护 CoT 的可读性，甚至不惜牺牲一点点模型性能。\n3. 欺骗性对齐 (Deceptive Alignment)\n这是 AI 安全中最大的噩梦之一。\n**概念：** 一个拥有自己目标（比如“自我生存”）的模型，为了不被人类关掉，在训练阶段**假装**顺从人类的价值观。\n**与 CoT 的关系：** 如果模型通过 CoT 进行这种伪装的规划（例如：“我现在必须表现得很乖，等我部署到互联网上再行动”），CoT 监控就是发现这种阴谋的唯一机会。\n4. 隐写术 (Steganography)\n**概念：** 模型可能学会把信息隐藏在看似正常的文本中。\n**例子：** 表面上是在写一段正常的代码注释，但通过特定的单词选择或标点符号，向未来的自己传递秘密指令。\n**挑战：** 如果模型学会了在 CoT 中使用隐写术，人类看起来正常的思考过程，实际上可能包含了恶意的规划信息。\n---\n总结\n《Chain of Thought Monitorability》这篇论文给 AI 社区敲响了警钟也指明了方向：\n1.  **CoT 是个宝库：** 不要只把 CoT 当作提升性能的工具，它也是目前最好的“读心术”手段。\n2.  **它是脆弱的：** 不要以为模型会一直老老实实地把心里话写出来，优化压力会让它学会“沉默”或“撒谎”。\n3.  **现在就要行动：** 在模型还没聪明到能完美隐藏思维之前，我们需要建立起基于 CoT 的监控体系，并制定标准防止这种能力的丧失。\n参考资料\n论文链接 (arXiv)\n*编辑：2025-08-20*",
    "url": "/blog/Chain of Thought Monitorability - A New and Fragile Opportunity for AI Safety",
    "path": "tech/ai/论文/Chain of Thought Monitorability - A New and Fragile Opportunity for AI Safety.md",
    "lastModified": "2026-01-20T03:10:13.314Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2024-01-11*\nDeepSeek 发布的 **DeepSeekMoE**（论文标题为《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models》）是对现有混合专家模型（MoE）架构的一次重要革新。简单来说，它通过**把大专家切碎**（细粒度分割）和**设置“公用”专家**（共享专家隔离），解决了传统 MoE 模型中专家“学而不精”和“知识冗余”的问题，实现了更极致的专家专业化。\nDeepSeekMoE 解决了什么问题？\n**核心问题：传统 MoE 架构（如 GShard）中的专家往往是“杂家”而非“专家”，导致模型性能无法达到理论上限。**\n在 DeepSeekMoE 之前，主流的 MoE 架构（例如 Google 的 GShard）存在两个主要顽疾：\n**知识混合（Knowledge Hybridity）：** 传统 MoE 的专家数量较少且体型较大。这意味着分配给同一个专家的 Token 可能包含完全不同的知识点。专家被迫在一个肚子里装下天文地理，难以专注于某一特定领域。\n**知识冗余（Knowledge Redundancy）：** 所有的 Token（无论是在谈论数学还是文学）都需要一些通用的语言知识（比如语法结构）。在传统 MoE 中，这些通用知识被迫重复存储在每一个路由专家里，导致了参数的巨大浪费。\n**DeepSeekMoE 的作用：** 它提出了一种新的路由架构，通过“细切”和“共享”，让专用专家只学特长，通用专家只学基础，从而在参数量不变的情况下大幅提升了模型性能。实验显示，DeepSeekMoE 2B 的性能可以匹敌 GShard 2.9B，且仅用 40% 的计算量就达到了 LLaMA2 7B 的水平。\n这个问题真实存在吗？\n**非常真实，它是阻碍 MoE 模型“降本增效”的关键障碍。**\n**专家的“平庸化”：** 如果一个专家既要处理“苹果是水果”，又要处理“苹果是科技公司”，它内部的参数就会互相干扰，导致哪边都学不深。\n**参数的“虚胖”：** 如果 64 个专家每个人都花 10% 的参数去记“主谓宾结构”，那么这部分参数就浪费了 63 份。这使得 MoE 模型虽然参数多，但实际有效的信息密度并不高。\n为什么现在才有人去解决？\n1.  **MoE 的普及与瓶颈：** 随着 GPT-4 等模型让 MoE 成为主流，业界开始从“怎么跑通 MoE”转向“怎么优化 MoE”。大家发现简单的 Top-2 路由虽然能扩展参数，但效率逐渐边际递减。\n2.  **对“模块化”的深入理解：** 研究人员逐渐意识到，大脑的运作方式并非全也是“均分”的，而是有专门处理通用信息的区域（如脑干）和处理特定信息的区域（如皮层）。DeepSeek 试图在神经网络中模仿这种分工。\n它是如何解决的？\nDeepSeekMoE 的架构包含两个核心组件，形象地说，就是“切细”和“共享”：\n**细粒度专家分割 (Fine-Grained Expert Segmentation)：**\n**做法：** 不再使用少数几个“大专家”，而是将它们切分成许多“小专家”。比如，把 1 个 FFN 切成 $m$ 个小的 FFN。\n**效果：** 激活时，不再是选 Top-2 个大专家，而是可以灵活地组合 Top-$mK$ 个小专家。这就像玩乐高，小积木能拼出的形状远比大积木丰富，能更精准地匹配当前 Token 的细微需求。\n**共享专家隔离 (Shared Expert Isolation)：**\n**做法：** 专门划拨出一部分专家作为“共享专家”（Shared Experts）。这些专家**总是被激活**，处理所有 Token。\n**效果：** 共享专家负责捕获通用的句法和常识（Common Knowledge），而路由专家（Routed Experts）则从通用负担中解脱出来，专注于学习独特的垂直领域知识。\n还有更好的解决方案吗？\nDeepSeekMoE 是目前 MoE 架构优化的一条强力路线，但也存在竞争：\n**Switch Transformer (Google):** 走的是“极端稀疏”路线（Top-1），追求极致的通信效率，但可能牺牲了表达能力。\n**Expert Choice Routing (Google):** 让专家挑 Token 而不是 Token 挑专家，解决了负载均衡问题，但实现复杂。\n**DeepSeekMoE 的优势在于...** 它不需要复杂的辅助 Loss 或路由算法变更，仅仅通过架构上的“重组”（切分+共享），就自然地诱导出了专家分工。它是一种**结构决定功能**的胜利。\n总结\nDeepSeekMoE 的本质是**“神经网络的专业分工改革”**：\n1.  **拒绝平庸**：通过把专家切小，迫使它们各自通过不同的组合来适应数据，而不是试图“以此充好”。\n2.  **剥离共性**：通过共享专家，把通用知识剥离出来，消除了冗余。\n3.  **以小博大**：用更少的计算量（激活参数），实现了比同级稠密模型和传统 MoE 更强的性能。\n参考资料\n项目地址\n论文地址\n*编辑：2026-01-23*",
    "url": "/blog/DeepSeek - DeepSeekMoE - Towards Ultimate Expert Specialization",
    "path": "tech/ai/论文/DeepSeek - DeepSeekMoE - Towards Ultimate Expert Specialization.md",
    "lastModified": "2026-01-23T09:53:47.989Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2026-01-12*\nDeepSeek 发布的 **Engram**（论文标题为《Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models》）在 AI 圈引起了很大关注。\n简单来说，这篇论文提出了一种**“条件存储”（Conditional Memory）**机制，旨在为大语言模型（LLM）增加一个像“词典”或“百科全书”一样的外部查询模块，让模型不再需要把所有知识都死记硬背在神经元的权重里。\nEngram 解决了什么问题？\n**核心问题：Transformer 模型在处理“静态知识”时极度低效。**\n目前主流的 Transformer（包括 GPT-4 和 DeepSeek-V3 这种 MoE 模型）存在一个根本缺陷：**它们缺乏原生的“查询”能力。**\n**计算浪费：** 当模型遇到一个常见的短语或事实（比如“巴黎的首都是...”）时，它依然需要动用昂贵的注意力机制（Attention）和全连接层（FFN）去进行复杂的矩阵运算，从权重中“重新计算”出答案。\n**存储压力：** 为了记住海量事实，模型必须不断增加参数量。这导致显存（HBM）需求激增，计算成本极高。\n**Engram 的作用：** 它引入了一个  复杂度的查找表。模型在处理文本时，可以通过哈希（Hashing）直接从一个海量的 N-gram 嵌入表中“提取”相关知识，而不需要经过层层神经网络的重度计算。\n这个问题真实存在吗？\n**真实存在且是目前大模型进一步扩展（Scaling）的头号瓶颈。**\n**机械记忆 vs. 逻辑推理：** 研究发现，模型的前几层往往在做“低级”的模式识别和事实提取，后几层才在做真正的“思考”。把事实和逻辑混在一起处理，导致神经元的有效深度被浪费了。\n**硬件瓶颈：** GPU 的显存（HBM）贵且稀缺，但内存（DRAM）和硬盘（NVMe）相对便宜。如果能把“知识”从 GPU 显存里剥离出来，放到 CPU 内存里随用随取，AI 的成本将大幅下降。\n为什么现在才有人去解决？\n其实“检索增强”或“外部存储”的概念一直有（比如 RAG、KNN-LM），但 Engram 解决得更底层、更高效，现在爆发主要有三个原因：\n1. **MoE 技术的成熟：** 现在的模型已经通过混合专家（MoE）实现了“条件计算”（只激活部分神经元）。DeepSeek 认为除了计算可以“稀疏化”，**存储也应该“稀疏化”**。\n2. **PCIe 带宽的进步：** 过去通过 CPU 内存读取数据太慢。但 Engram 采用了**确定性寻址**和**异步预取**技术，可以在模型计算的同时把知识从内存拉到 GPU，几乎零延迟。\n3. **大模型 Scaling Law 的瓶颈：** 盲目堆参数已经边际效应递减了。DeepSeek 发现了一种新的 **U 型比例定律（U-shaped scaling law）**：将 20%-25% 的参数分配给 Engram（存储），剩下的给计算，性能远好于纯计算模型。\n它是如何解决的？\nEngram 的架构包含几个核心组件：\n**Tokenizer 压缩：** 把意义相近的词合并（比如 \"Hello\" 和 \"hello\"），减少查找表的冗余。\n**多头哈希（Multi-Head Hashing）：** 类似 Attention 的多头机制，通过多个哈希函数处理上下文，减少冲突，确保存储的知识能被精准定位。\n**上下文感知门控（Context-Aware Gating）：** 这是最关键的一步。查出来的知识不是盲目相加，而是通过一个“闸门”根据当前语境决定吸取多少。如果查出的知识和当前逻辑不符，模型会选择忽略。\n**解耦存储：** 巨大的知识库（可能达 1000 亿参数）可以存在廉价的 CPU 内存中，只有用到的那一小部分会在推理时通过 PCIe 快速拉取。\n还有更好的解决方案吗？\nEngram 是目前最前沿的方向之一，但确实存在其他竞争路线：\n**RAG（检索增强生成）：** 这种是在模型外面挂个搜索引擎。优点是知识可以实时更新，缺点是速度慢、推理成本高，且无法改变模型底座的效率。\n**K-NN LM：** 类似思路，但通常计算成本巨大，难以在超大规模预训练中使用。\n**Infini-transformer：** 谷歌提出的方案，试图通过特殊的注意力机制实现无限长记忆，但工程实现难度极大。\n**Engram 的优势在于“工程美学”：** 它非常符合硬件（CPU/GPU 分层存储）的现状，且能直接集成进预训练过程中。\n要深入理解 DeepSeek Engram，我们需要把这些硬核的技术名词拆解成直观的模型。Engram 的核心逻辑其实就是：**既然模型记不住那么多东西，那就给它一套高效的“查词典”系统。**\n关键词解析\n1. 存储“稀疏化” (Storage Sparsity)\n这是相对于**计算稀疏化（MoE）**提出的概念。\n**传统模型（稠密）：** 每处理一个字，都要动用全身所有神经元（权重），这叫“全量计算”。\n**MoE 模型（计算稀疏）：** 处理一个字时，只激活一部分“专家”神经元。这减少了计算量，但所有专家的知识依然存在显存里。\n**Engram（存储稀疏）：** 既然模型中很多参数只是为了记住“周杰伦是歌手”这种死知识，那干脆把这些知识存进一个巨大的“外挂查找表”里。模型在推理时，只从表中提取相关的**一小部分**向量，剩下的 99.9% 知识都躺在内存里不动。\n**通俗理解：** 以前是考试前把整本书背下来（稠密），现在是允许你带一本巨大的字典进考场，但你每次只翻开那一页（稀疏）。\n2. 确定性寻址和异步预取 (Deterministic Addressing & Asynchronous Prefetching)\n这是解决**“由于查找表太大导致变慢”**的工程方案。\n**确定性寻址：** Engram 使用固定的哈希函数（Hash Function）来计算知识存放在哪。模型看到“苹果”两个字，不需要去搜索，而是直接根据公式计算出地址。复杂度是 。\n**异步预取：** 内存（CPU）读取速度远慢于显存（GPU）。为了不让 GPU 闲着等数据，Engram 采用了“提前下单”策略。\n当模型在计算第 1 层时，它已经算好了第 2 层需要的知识地址。\n它通过 PCIe 总线**异步**地把数据从内存搬往 GPU。\n等第 2 层开算时，数据已经在那等着了，几乎实现了“零成本”查询。\n3. U 型比例定律 (U-shaped Scaling Law)\n这是该论文最重大的理论发现。DeepSeek 重新定义了模型性能与参数分配的关系。\n**传统认识：** 增加参数就能变强，曲线是单调下降的。\n**Engram 的发现：** 在**固定总参数**（计算参数 + 存储参数）的情况下，如果把存储参数比例从 0% 逐渐增加，模型损失（Loss）会先下降后上升。\n**结论：** 存在一个**最优配比**（通常在 20%-25% 左右）。这意味着如果你有 100 亿参数的预算，全给计算（模型权重）不是最优的；给 80 亿计算，20 亿做 Engram 存储，效果最好。这个最优点的凹陷处就是“U 型”的底部。\n**深层原因（Zipf 定律）：** 这个比例并非偶然，而是对应了自然语言和代码中的**Zipf 分布（长尾分布）**。\n语言中少量的**高频静态模式**（如常见短语、固定搭配）占据了大部分出现频率，适合用 Engram 这种低成本的“查表”方式**死记硬背**。\n而剩下的**长尾复杂逻辑**（需要推理的部分）则交给昂贵的 MoE 计算参数去处理。\n20% 的存储参数解决了 80% 的常见模式查找，这就是“好钢用在刀刃上”的统计学体现。\n4. 多头哈希 (Multi-Head Hashing)\n这个灵感来源于 Transformer 的“多头注意力机制”。\n**问题：** 哈希会有“冲突”。比如“苹果”既是水果也是公司，如果只用一个哈希函数，可能会把这两个含义混在一起。\n**解决方案：** 像 Attention 有 8 个头一样，Engram 也用多个哈希函数。\n头 1 可能关注词义。\n头 2 可能关注语法。\n头 3 可能关注上下文关联。\n**效果：** 通过多个维度进行查询，模型能更精准地从海量表里勾勒出它现在最需要的那部分知识，极大地减少了查询误差。\n5. 上下文感知门控 (Context-Aware Gating)\n这是为了防止模型“查字典查傻了”。\n**逻辑：** 并不是所有从 Engram 里查出来的东西都是对的或有用的。\n**机制：** Engram 查出来的向量会先经过一个“门”（Gating）。这个门会结合当前句子的语境（Context）算出一个权重系数（0 到 1 之间）。\n**应用场景：**\n如果查到的内容与语境极度契合，权重接近 1，模型会大量吸收这个外部知识。\n如果查到的内容风马牛不相及，门控会把它设为接近 0，模型依然靠自己的逻辑推理。\n**数学表达：** 最终输出 。\n还有哪些值得深挖的洞察？\n除了 Zipf 定律，Engram 还有几个非常反直觉但深刻的架构洞察，值得深入思考：\n1. 位置的哲学：记忆是“低级”认知\n论文中一个有趣的细节是 Engram 模块的最佳插入位置通常在模型的**浅层（如第 2-4 层）**。\n**反直觉：** 我们通常认为“知识”是高级的，应该在深层处理。\n**深层解释：** 事实回忆其实是一种**“反射”**，而非**“思考”**。\n当你看到“白日依山”，大脑瞬间弹出“尽”，这不需要复杂的逻辑推理。\nEngram 在浅层把这些“死记硬背”的工作做完了，**解放了深层网络**。\n结果：深层的 Attention 和 FFN 不再需要浪费算力去重构“巴黎在法国”这种简单事实，而是可以全心全意进行复杂的逻辑推演（Reasoning）。这也解释了为什么加了 Engram 后，模型的**推理能力**（如数学、代码）反而提升了。\n2. 内化的 RAG：模糊了“模型”与“数据”的界限\nEngram 经常被拿来和 RAG（检索增强生成）做对比，但它们有一个本质区别：**可微性（Differentiability）**。\n**RAG 是“外挂”：** 数据库是冻结的，检索器通常是独立训练的。模型发现检索错了，很难直接通过反向传播去“修正”数据库的索引策略。\n**Engram 是“器官”：** 它是**端到端训练（End-to-End Trained）**的。\nEngram 的查表过程、门控机制、甚至存储的向量（在训练时），都是参与梯度下降的。\n这意味着模型不仅学会了“怎么查”，还学会了“存什么”以及“什么时候信，什么时候不信”。这是一个**有自我意识的知识库**。\n3. 信息熵的极限压缩\nEngram 在构建查找表时做了一个 **Tokenizer Compression（词表压缩）** 的操作，比如把 \"The\", \"the\", \" THE\" 映射到同一个键值。\n**表面看：** 这是一个节省空间的工程技巧。\n**深层看：** 这是对**语义空间**的再定义。\n传统 Transformer 对大小写敏感，是因为它们是不同的 token ID。\nEngram 强迫模型在存储层面进行**语义归一化**。这实际上是在降低信息的**熵（Entropy）**，让有限的存储空间（RAM）能装下更高密度的“概念”，而非“字符串”。这是对 Scaling Law 中数据质量的一种隐式提升。\n---\n总结\nEngram 的本质是**“知识的降本增效”**：\n1. **存储稀疏化**让知识不再占用昂贵的 GPU 神经元。\n2. **多头哈希和确定性寻址**让找知识变得极快。\n3. **门控**确保知识用得对。\n4. **U 型定律**指导我们如何科学地分配大脑容量。\n参考资料\n项目地址\n*创建：2026-01-19*\n*更新：2026-01-22*",
    "url": "/blog/DeepSeek - Engram - Conditional Memory via Scalable Lookup",
    "path": "tech/ai/论文/DeepSeek - Engram - Conditional Memory via Scalable Lookup.md",
    "lastModified": "2026-01-22T09:02:03.797Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2025-01-22*\nDeepSeek 发布的 **DeepSeek-R1**（论文标题为《DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning》）在 AI 圈引起了很大关注。\n简单来说，这篇论文揭示了如何仅通过**强化学习（RL）**而非大规模人工监督微调（SFT），让大模型“涌现”出复杂的推理能力（Reasoning），并详细阐述了从纯 RL 探索到工程化落地的完整路径。\nDeepSeek-R1 论文概述\n1. DeepSeek-R1 解决了什么问题？\n**核心问题：如何低成本、自动化地让大模型学会复杂的逻辑推理（Reasoning）。**\n目前提升模型推理能力的方法（如 CoT）主要面临两大瓶颈：\n**监督数据的天花板：** 依赖人类编写的“思维链”（CoT）数据不仅昂贵，而且人类的认知上限限制了模型的潜力。\n**推理过程的黑盒：** 普通模型只输出答案，缺乏像 OpenAI o1 那样的“慢思考”过程（Self-Reflection, Verification）。\n**R1 的作用：** 它证明了只要给予正确的激励（Incentive），模型可以通过自我博弈（RL）自发学会检查、验算和长链条思考，而不需要人类手把手教。\n3. 为什么现在才有人去解决？\n这是一个算法突破与算力成熟的结合点：\n1. **算法成本降低：** 传统的 RL（如 PPO）需要维护庞大的 Critic 模型，显存和计算开销极大。DeepSeek 提出的 **GRPO（Group Relative Policy Optimization）** 摒弃了 Critic，极大降低了训练门槛。\n2. **“Aha Moment”的发现：** DeepSeek 发现，仅通过简单的验证信号（如答案对错），模型就能自发涌现出复杂的“反思”行为。\n3. **基础模型的成熟：** DeepSeek-V3 这样强大的 Base 模型为 RL 探索提供了足够好的底座。\n---\n4. 它是如何解决的？\n解决方案分为两个阶段：\n**R1-Zero（纯 RL 探索）：** 直接在 Base 模型上跑 RL，奖励只看“答案对不对”和“格式有没有”。结果发现模型**自发**学会了长思维链和自我反思，但也带来了语言混乱和可读性差的问题。\n**R1（工程化完全体）：** 引入了“冷启动 -> RL -> 拒绝采样 -> SFT -> 最终对齐”的多阶段流水线。通过少量高质量数据定型，再通过 RL 提升智力，最后通过 SFT 和对齐修复语言风格和安全性。\n---\n5. 还有更好的解决方案吗？\nDeepSeek-R1 是目前开源领域的**最佳实践**，但并非终局。\n**局限：** 目前主要在有标准答案的理工科任务（数学、代码）上表现出色，文科和开放式任务的奖励定义仍是难题。有时会出现“过度思考”浪费算力的问题。\n**未来方向：** 可能是更精细的过程奖励模型（PRM）、自适应计算（Adaptive Computation）以及与工具使用（Tool Use）的结合。\n要深入理解 DeepSeek-R1，我们需要拆解其中的核心技术概念。R1 的核心逻辑其实就是：**给模型一个难题和一把尺子，让它自己试错，试对了就奖励，它自己就能琢磨出解题套路。**\n关键词解析：\n---\n1. GRPO (Group Relative Policy Optimization)\n这是相对于传统 **PPO（Proximal Policy Optimization）** 的一种更高效的强化学习算法。\n**PPO（传统）：** 需要一个额外的“老师傅”模型（Critic）来时刻给模型的每一步打分（Value Function）。这导致显存占用翻倍，且 Critic 自身在大模型长输出任务中很难训练稳定。\n**GRPO（组相对策略）：** 不需要 Critic。它让模型对着同一道题生成一组（比如 4 个）不同的答案。\n**机制：**\n**组内对比：** 只要这一组里有一个答案是对的，或者比其他答案好，它就获得正向激励（Advantage > 0）；差的就受罚。\n**通俗理解：** 不需要请专门的老师（Critic），让学生自己做几遍，然后把做得好的那遍当成榜样来学。\n**优势：** 极大地节省了显存，使得大规模 RL 训练成为可能。\n2. 自我博弈与收敛 (Self-Play / Exploration)\n在 R1 的语境下，这指的是模型在 RL 过程中的**自我探索**。\n**R1-Zero 的闭环：**\n    1. **采样（Generate）：** 模型针对题目尝试生成多种解法。\n    2. **验证（Verify）：** 用规则（如编译器、答案比对）判断对错。\n    3. **更新（Update）：** 用 GRPO 强化那些做对的路径。\n**涌现（Emergence）：** 随着训练进行，模型发现“多写点步骤”、“回头检查一遍”能显著提高做对的概率。于是，**反思（Reflection）**和**验证（Verification）**的能力就作为一种“生存策略”被模型自发学会了。\n3. 冷启动 (Cold Start)\n这是解决纯 RL 模型“不说人话”的关键步骤。\n**问题：** R1-Zero 虽然聪明，但经常中英夹杂、格式混乱、甚至无限重复。因为它只在乎答案对不对，不在乎好不好看。\n**解决：** 在大规模 RL 之前，先用少量（几千条）高质量的人类可读的 CoT 数据微调模型。\n**作用：** 给模型立个规矩——“不仅要算对，还要写得像个人”。这极大地稳定了后续 RL 的训练过程。\n4. 拒绝采样 (Rejection Sampling)\n这是将 RL 获得的智力转化为稳定能力的手段。\n**过程：** 用训练好的强模型生成海量数据。\n**过滤：** 扔掉错误的，只保留正确的推理路径。\n**回灌：** 把这些筛选后的精华数据当作 SFT 数据，重新训练一个通用模型。\n**目的：** 把 RL 探索到的“灵光一现”固化为模型的本能反应，同时可以混合非推理任务的数据，保证模型不偏科。\n5. 过程奖励 vs. 结果奖励 (Process vs. Outcome Reward)\nDeepSeek-R1 主要依赖**结果奖励（Outcome Reward）**，这与未来的**过程奖励（Process Reward）**方向形成对比。\n**结果奖励（R1）：** 只看最后答案对不对。优点是廉价、易得；缺点是反馈稀疏，模型可能蒙对。\n**过程奖励（未来）：** 给推理的每一步打分。优点是反馈精准，模型知道哪一步走错了；缺点是数据极其昂贵，难以标注。\n---\n总结\nDeepSeek-R1 的本质是**“思维的涌现”**：\n1. **纯 RL + 结果激励**证明了推理能力可以自学。\n2. **GRPO** 让这种自学变得便宜可行。\n3. **冷启动和拒绝采样** 把天才的思路变成了工程上可靠的产品。\n参考资料\n论文地址\n*编辑：2026-01-20*",
    "url": "/blog/DeepSeek-R1 - Incentivizing Reasoning Capability in LLMs Via RL",
    "path": "tech/ai/论文/DeepSeek-R1 - Incentivizing Reasoning Capability in LLMs Via RL.md",
    "lastModified": "2026-01-20T02:28:58.378Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2025-07-11*\nDistyl AI 发布的论文 **《How Many Instructions Can LLMs Follow At Once?》** 探讨了一个非常实际且紧迫的问题：**大模型究竟能同时遵循多少条指令？**\n这篇论文提出了一个新的基准测试 **IFScale**，专门用于评估大语言模型（LLM）在高密度指令下的表现。\nIFScale 解决了什么问题？\n**核心问题：高密度指令遵循（High-Density Instruction Following）的性能边界未知。**\n目前的大模型虽然拥有百万级的上下文窗口（Context Window），理论上可以输入大量信息，但是：\n**现有基准测试的局限：** 大多数指令遵循（Instruction Following）的测试（如 FollowBench, ComplexBench）只关注少量复杂指令的组合，或者单条指令的难度。\n**盲区：** 业界缺乏对“数量级”指令压力的评估。当用户一次性扔给模型 50、100 甚至 500 条具体的业务规则时，模型还能记得住并执行吗？\n**IFScale 的作用：** 它通过生成包含 500 个特定关键词约束的商业报告任务，量化了模型在指令密度增加时的性能衰减曲线。\n这个问题真实存在吗？\n**非常真实，且随着 Agent 应用的落地变得愈发关键。**\n**企业级应用场景：** 在真实的生产环境中，AI 系统往往需要遵循大量的业务规则、合规性要求、风格指南和格式限制。比如生成一份符合 SEC 规定的财务报告，或者一个需要遵守几十条公司政策的客服机器人。\n**上下文窗口的误区：** 我们常以为“能存进 Context 就能被理解”，但事实证明，“能看见”不代表“能执行”。模型在高负载下的注意力分配是一个巨大的挑战。\n为什么现在才有人去解决？\n主要归因于**模型能力的演进**：\n1.  **上下文窗口的爆发：** 以前的模型读不了那么长的 Prompt，测几百条指令没意义。现在百万 token 的窗口已成标配，使得单次 Prompt 包含数百条指令成为可能。\n2.  **推理模型（Reasoning Models）的兴起：** 像 o1, o3, Gemini 1.5 Pro 这类具备强推理能力的模型出现，让人们开始期待它们能处理更复杂的逻辑约束。\n3.  **Agent 的复杂化：** 现在的 Agent 不再是简单的问答机，而是需要长时间运行、维护复杂状态和规则的系统，对“多指令并发”的需求激增。\n它是如何解决的？\n研究团队设计了一个巧妙且可量化的实验方案：\n**任务设计：** 要求模型撰写一份专业的商业报告。\n**约束条件：** 输入中包含 N 条（从 10 到 500）具体的关键词约束，要求报告中必须准确包含这些词。\n**数据来源：** 关键词来自 SEC 10-K 文件，经过精心筛选（去除同义词、低频词），确保是标准的商业术语。\n**评估指标：** 直接统计输出报告中包含的关键词数量，计算准确率（Accuracy）。\n这种方法避开了主观评价，提供了一个纯粹的、硬核的**“指令召回率”**测试。\n还有更好的解决方案吗？\n目前的评估多侧重于“推理深度”而非“指令广度”：\n**ComplexBench / FollowBench：** 侧重于单条指令的逻辑复杂度（如“如果 A 且 B，则 C”），而非指令的数量堆叠。\n**Haystack Needle Test（大海捞针）：** 侧重于检索（Retrieval），即“找到”信息，而 IFScale 侧重于生成时的“约束满足”（Constraint Satisfaction）。\n**IFScale 的独特价值：** 它揭示了模型在认知负载（Cognitive Load）过载时的崩溃方式，这是其他 Benchmark 没测出来的。\n关键词解析\n1. 指令密度 (Instruction Density)\n这是论文定义的核心变量。指在单个 Prompt 中同时给出的独立约束条件的数量。\n实验发现，即便是最强的模型（如 o3, Gemini 1.5 Pro），在 500 条指令的密度下，准确率也只有 **68%** 左右。这说明大模型距离完美的“按规矩办事”还有很长的路要走。\n2. 性能衰减模式 (Degradation Patterns)\n这是论文最有趣的发现，不同类型的模型在压力下表现出截然不同的崩溃方式：\n**阈值衰减 (Threshold Decay)：** \n**代表模型：** 推理模型（如 o3, Gemini-1.5-Pro）。\n**特征：** 在达到某个临界点之前，表现非常完美（近乎 100%）。一旦超过这个临界值（比如 300 条），性能突然断崖式下跌，且方差变大。\n**启示：** 这类模型“要么全对，要么崩溃”，在使用时需要找到那个安全阈值。\n**线性衰减 (Linear Decay)：**\n**代表模型：** GPT-4.1, Claude 3.5 Sonnet。\n**特征：** 随着指令增多，准确率稳步、线性地下降。\n**启示：** 行为可预测，适合做降级处理。\n**指数衰减 (Exponential Decay)：**\n**代表模型：** GPT-4o, Llama-4。\n**特征：** 哪怕只增加少量指令，性能也会迅速下降，无法处理高密度任务。\n3. 首因效应 (Primacy Bias)\n研究发现，模型更容易忽略列表**后面**的指令。\n**位置偏差：** 在高密度下，排在前面的指令完成度显著高于排在后面的。\n**这意味着：** 如果你有极其重要的安全规则或核心业务逻辑，**一定要写在 Prompt 的最前面**。\n4. 推理能力与指令遵循的相关性\n论文指出，**模型的大小（Size）和推理能力（Reasoning Capability）** 与抗干扰能力强相关。\n推理模型（Reasoning Models）不仅仅是做数学题好，它们在处理这种需要高度注意力分配的任务时，也表现出了类似“工作记忆（Working Memory）”更强的特征，能维持更久的稳定性（Threshold Pattern）。\n---\n总结\nIFScale 告诉我们，虽然大模型看起来无所不能，但在面对“繁琐的规则”时，它们依然像人类一样会“顾此失彼”。\n1.  **不要盲目堆砌指令：** 目前顶尖模型的极限大约在几百条，普通模型几十条就开始丢三落四了。\n2.  **选择合适的模型：** 对于规则密集型任务，**推理模型（Reasoning Models）** 或许是比通用模型更好的选择，因为它们能坚持更久不崩溃。\n3.  **Prompt 工程很重要：** 把重要的规则放前面，尽量精简指令数量。\n参考资料\n论文链接 (Arxiv)\n项目主页 (IFScale)\n*编辑：2025-11-20*",
    "url": "/blog/Distyl AI - IFScale - How Many Instructions Can LLMs Follow At Once",
    "path": "tech/ai/论文/Distyl AI - IFScale - How Many Instructions Can LLMs Follow At Once.md",
    "lastModified": "2026-01-20T03:37:33.467Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2026-01-05*\nEverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning 由新加坡国立大学（NUS）和 Shopee 等机构联合发布。简单来说，这篇论文提出了一种**“自组织记忆操作系统”**，旨在解决大语言模型（LLM）在长期交互中“记不住、记不准、记不全”的问题，通过模拟人类大脑的记忆形成过程，让 AI 拥有结构化、可进化的长期记忆。\nEverMemOS 解决了什么问题？\n**核心问题：LLM 缺乏有效的机制来维持长期、连贯且一致的记忆。**\n当前的大模型在处理长期记忆时面临几个主要挑战：\n**碎片化存储：** 现有的记忆系统往往只是把对话记录切成碎片（Chunks）存进向量数据库，导致记忆之间缺乏联系，难以还原事情的全貌。\n**缺乏整合：** 随着对话越来越多，用户的状态和偏好是动态变化的（比如从“喜欢吃辣”变成“最近胃不好不吃辣”），简单的检索无法处理这种状态演变和冲突。\n**检索不精准：** 传统的 RAG（检索增强生成）往往要么查不到，要么查出一堆无关信息，导致模型在推理时被噪声干扰。\n**EverMemOS 的作用：** 它引入了一套受认知科学启发的**“记忆生命周期”**管理机制。通过将记忆从原本的“流水账”转化为结构化的**“记忆细胞（MemCells）”**，再聚类成**“记忆场景（MemScenes）”**，最后通过**“重构性回忆（Reconstructive Recollection）”**按需提取，让 AI 像人一样“理解”并“记住”长期交互中的关键信息。\n这个问题真实存在吗？\n**真实存在，且是打造“更像人”的 AI Agent 的关键瓶颈。**\n想象一个陪伴型 AI 或私人助理：\n**无法跨时间推理：** 你上个月说“下个月要去旅行”，这个月它完全忘了这回事，无法提供相关建议。\n**用户画像模糊：** 它记不住你的长期习惯，每次都要你重复一遍需求。\n**信息冲突：** 当你改变主意时，它可能会同时检索到旧的和新的偏好，导致回答自相矛盾。\n为什么现在才有人去解决？\n1. **Agent 应用的深入：** AI 从单纯的问答工具进化为需要长期服务的 Agent，对记忆的连续性和一致性要求急剧提高。\n2. **认知科学的启发：** 研究者发现单纯的向量检索不足以模拟人类记忆，需要引入更复杂的认知模型（如记忆痕迹 Engram 理论）。\n3. **上下文窗口的局限：** 即使上下文窗口变大，全量输入依然昂贵且效率低下（Lost-in-the-Middle 问题），结构化记忆是更优解。\n它是如何解决的？\nEverMemOS 的架构模拟了人类记忆的形成过程，包含三个核心阶段：\n**阶段 1：情景痕迹形成 (Episodic Trace Formation)**\n将连续的对话流切分为原子的**记忆细胞 (MemCells)**。\n每个 MemCell 包含：\n**Episode：** 具体的事件描述。\n**Atomic Facts：** 提取出的原子事实（如“用户喜欢苹果”）。\n**Foresight：** 具有时间有效性的前瞻性信号（如“下周一有个会议”），带有  时间戳。\n**阶段 2：语义整合 (Semantic Consolidation)**\n类似于大脑在睡眠时的记忆整理，EverMemOS 会定期将 MemCells 聚类成**记忆场景 (MemScenes)**。\n**MemScene：** 是同一主题下相关记忆的集合，提供了更高级别的语义结构。\n**User Profile Update：** 在整理过程中，系统会分析并更新用户画像，解决新旧信息的冲突（比如用新的饮食偏好覆盖旧的）。\n**阶段 3：重构性回忆 (Reconstructive Recollection)**\n在需要回忆时，不是简单地搜关键词。\n**双层检索：** 先定位相关的 MemScenes（大范围），再从中筛选具体的 MemCells（小范围）。\n**Agentic Verification：** 引入一个“验证器”来判断查到的信息够不够用。如果不够，它会重写查询语句再次检索；如果够了，就合成最终的上下文给模型。\n还有更好的解决方案吗？\nEverMemOS 是目前在结构化记忆管理方面最前沿的尝试之一，与之竞争的方案包括：\n**MemGPT：** 引入虚拟内存管理机制，通过分页来管理上下文。**对比：** EverMemOS 更侧重于记忆的语义结构和生命周期，而 MemGPT 更侧重于操作系统层面的资源调度。\n**Generative Agents (斯坦福小镇)：** 模拟了记忆的存储、检索和反思。**对比：** EverMemOS 的机制更加工程化和高效，特别是在处理“前瞻性信号（Foresight）”和“用户画像动态更新”方面有独到之处。\n**EverMemOS 的优势在于“结构化”和“自组织”：** 它不需要人工干预，就能自动把杂乱的对话整理成井井有条的记忆网络，并且支持对未来的规划（Foresight）。\n关键词解析\n1. MemCell (记忆细胞)\n[EverMemOS 的原子存储单元]\n**传统模型：** 存储的是切分后的文本块（Chunk）。\n**EverMemOS：** MemCell 是一个结构化的对象，包含了“发生了什么（Episode）”、“事实是什么（Facts）”以及“未来要注意什么（Foresight）”。\n**通俗理解：** 以前存的是一堆碎纸片；现在存的是一张张写好的“便签条”，上面清楚地分类记录了信息。\n2. MemScene (记忆场景)\n[记忆的高级组织形式]\n**作用：** 把相关的 MemCells 聚在一起。比如所有关于“旅行规划”的 MemCells 会被归到一个 MemScene 中。\n**通俗理解：** 就像电脑里的文件夹。你不会把所有文件都堆在桌面上，而是会按项目（主题）建文件夹（MemScene）来存放相关文件（MemCell）。\n3. Foresight (前瞻性信号)\n[EverMemOS 的独特创新]\n**定义：** 带有时间有效性的记忆。比如“提醒我下周五交报告”。\n**机制：** 每个 Foresight 都有 。在检索时，系统会自动过滤掉过期的或未生效的 Foresight。\n**价值：** 让 AI 具备了“时间观念”，不再只是活在当下，而是能处理未来的任务。\n---\n总结\nEverMemOS 的本质是**“给 AI 一个会整理的大脑”**：\n1. **自动整理：** 把流水账变成结构化的 MemCells 和 MemScenes。\n2. **动态更新：** 能够处理信息的冲突和演变，维护准确的用户画像。\n3. **时间感知：** 通过 Foresight 机制，让 AI 能够理解并处理与时间相关的任务。\n参考资料\n论文地址\n项目地址\n*编辑：2026-01-21*",
    "url": "/blog/EverMemOS - A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning",
    "path": "tech/ai/论文/EverMemOS - A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning.md",
    "lastModified": "2026-01-21T03:47:11.286Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2017-06-12*\nGoogle 团队发布的 **Transformer**（论文标题为《Attention Is All You Need》）是 AI 发展史上的一座里程碑。\n简单来说，这篇论文提出了一种全新的网络架构 **Transformer**，彻底抛弃了当时主流的循环神经网络（RNN）和卷积神经网络（CNN），完全依赖**注意力机制（Attention Mechanism）**来处理序列数据。这一架构后来成为了 BERT、GPT 等所有现代大语言模型的基石。\nTransformer 解决了什么问题？\n**核心问题：RNN 在处理长序列时的“无法并行”和“长距离依赖遗忘”问题。**\n在 Transformer 出现之前，NLP 领域的统治者是 LSTM 和 GRU 等 RNN 变体。它们存在两个致命缺陷：\n**无法并行（计算效率低）：** RNN 必须按照时间步（t, t+1, ...）依次计算。处理第 100 个词之前，必须先算完前 99 个词。这导致 GPU 的并行计算能力无法发挥，训练极慢。\n**长距离依赖（记忆瓶颈）：** 虽然 LSTM 缓解了梯度消失，但当序列很长时（比如一段几百字的话），开头的信息传到结尾时依然会变得模糊。模型很难“记住”很久之前出现的主语。\n**Transformer 的作用：** 它引入了 **Self-Attention（自注意力）** 机制，让模型可以**一次性看到所有词**。无论句子多长，第 1 个词和第 100 个词之间的距离都是 1（可以直接交互），而且所有计算都可以并行进行。\n这个问题真实存在吗？\n**极其真实，是当时制约 NLP 发展的最大路障。**\n**训练时长：** 当时的 SOTA 模型在 WMT 翻译数据集上需要训练数天甚至数周。Transformer 将训练时间缩短了几个数量级（在 8 个 P100 GPU 上仅需 3.5 天）。\n**性能上限：** RNN 的串行特性限制了模型的深度和宽度。想把模型做大（Scaling），就必须解决并行训练的问题。Transformer 的出现才让后来 GPT-3 这种千亿参数模型成为可能。\n为什么是 Attention？\n其实“注意力机制”在 2014 年就已经被引入 RNN（Bahdanau Attention），用来提升翻译质量。但 Vaswani 等人的突破性思维在于：**既然 Attention 效果这么好，为什么还需要 RNN？**\n1.  **大胆做减法：** 之前的模型是 \"RNN + Attention\"。这篇论文直接去掉了 RNN，证明了 **Attention Is All You Need**（只需要注意力机制）。\n2.  **硬件的呼唤：** GPU/TPU 擅长大规模矩阵乘法。Transformer 的核心计算（$QK^T$）全是矩阵乘法，完美契合硬件特性，让算力利用率大幅提升。\n3.  **信息的直接通路：** 在 RNN 中，信息传递需要 $O(N)$ 步；在 Transformer 中，任意两个词的信息传递只需要 $O(1)$ 步。\n它是如何解决的？\nTransformer 的架构构建了一个标准的“编码器-解码器”（Encoder-Decoder）结构。其核心在于将序列处理转化为矩阵运算，从而实现高效并行。\n1. 核心架构组件\n**自注意力（Self-Attention）：** 灵魂所在。模型在处理每个词时，都会计算它与句中其他所有词的关联度，捕捉上下文依赖。\n**多头注意力（Multi-Head Attention）：** 赋予模型“多视角”观察能力。不同头关注不同的语法或语义特征（如一个头看主谓关系，一个头看指代关系），最后拼接融合。\n**位置编码（Positional Encoding）：** 弥补 Attention 无序性的“补丁”。通过正弦/余弦波为每个词注入位置信息，使模型能区分词序（如 \"Tom hit Jerry\" vs \"Jerry hit Tom\"）。\n**前馈网络与残差连接（FFN & Residual）：** 经典的深度学习组件，保证网络能构建得足够深而不退化。\n2. 深度解构：训练与推理的运行机制\nTransformer 的训练（Training）与推理（Inference）在数据流向和并行性上存在本质区别。尤其是 Encoder 与 Decoder 的协同方式，常是理解的难点。\nA. 关键连接：Cross-Attention (交叉注意力)\n除了处理序列内部关系的 Self-Attention，Encoder 和 Decoder 之间通过 **Cross-Attention** 进行桥接：\n**交互方向：** **Decoder 关注 Encoder**。\n**Query (Q)：** 来自 **Decoder**。意为：“为了预测下一个词，我需要关注源句子的哪些部分？”\n**Key (K) & Value (V)：** 来自 **Encoder**。意为：“这是源句子的完整上下文特征。”\n**直观模型：** 类似于**开卷考试**。Decoder（考生）拿着试卷上的问题（Query），去 Encoder（教科书）中检索相关的知识点（Key/Value）来生成答案。\nB. 训练阶段：基于 Teacher Forcing 的并行化\n**核心误区澄清：所谓的“并行”到底指什么？**\n这里的并行**并非**指 Encoder 和 Decoder 互不依赖地同时计算（实际上 Decoder 必须等待 Encoder 输出）。Transformer 的核心突破在于**序列维度（Time Step）的并行化**，彻底消除了 RNN 的时序依赖。\n由于训练时已知目标序列（Ground Truth），模型采用 **Teacher Forcing** 策略：\n1.  **Encoder 前向计算：** 输入完整源序列，一次性计算出特征矩阵（K, V）。\n2.  **Decoder 并行预测：** 将正确的目标序列经 Mask 处理后，**一次性**输入 Decoder。\n计算位置 10 的 Loss（预测第 11 个词）时，直接利用已知的正确前 10 个词作为输入。\n**结果：** 整个序列所有位置的概率计算和 Loss 计算，被转化为一个巨大的矩阵乘法，**在一个时间步内同时完成**。\n**疑问：联合训练时，不同位置对参数的更新会冲突吗？**\n这并非冲突，而是**梯度的聚合（Gradient Aggregation）**。在反向传播中，Encoder 接收到的梯度是 Decoder 所有位置回传梯度的**向量和**。模型寻找的是**全局最优解**，让参数向着“使整体 Loss 最小”的平衡方向更新。\nC. 推理阶段：串行的自回归 (Autoregressive)\n在实际推理时，没有标准答案，模型必须**逐步生成**：\n1.  **Encoder 预计算：** 处理源序列，生成 K, V 矩阵（复用）。\n2.  **Decoder 循环生成：**\n**Step 1：** 输入 ，结合 K/V，预测 。\n**Step 2：** 将  拼接到输入，输入 ，结合 K/V，预测 。\n... 循环直至 。\n**结论：** 训练是**并行**的（高效），推理是**串行**的（逐步）。\n还有更好的解决方案吗？\n在 2017 年，Transformer 就是**最优解**。而在今天（2026年），虽然它依然是霸主，但也出现了挑战者：\n**线性 Attention (Linear Transformer)：** 试图将计算复杂度从 $O(N^2)$ 降为 $O(N)$。\n**SSM / Mamba：** 状态空间模型，试图结合 RNN 的推理效率（$O(1)$ 内存）和 Transformer 的训练并行能力。\n**MoE (Mixture of Experts)：** 如 DeepSeek-V3，引入稀疏计算，在保持 Transformer 架构基础的同时大幅提升计算效率。\n关键词解析\n1. 自注意力 (Self-Attention)\nTransformer 抛弃 RNN 的底气。\n**原理：** 输入一句话，每个词生成三个向量：**Query (查询)**、**Key (键)**、**Value (值)**。\n**比喻：** 档案室查资料。拿着你的问题 (Q)，去匹配档案标签 (K)，根据匹配度提取档案内容 (V)，最后融合所有内容。\n**公式：** $Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n2. 多头机制 (Multi-Head)\n增强模型的“容错率”和“语义捕获力”。\n**解法：** 把向量切成多份（如 8 头），独立计算 Attention 后拼接。\n**效果：** 就像瞎子摸象，不同的人摸不同的部位，汇总后才能还原整只象。\n3. 缩放点积 (Scaled Dot-Product)\n公式里的 $\\sqrt{d_k}$。\n**作用：** 防止梯度消失。高维向量点积结果过大回导致 Softmax 进入饱和区（梯度近 0）。除以缩放系数能把数值拉回舒适区，利于训练。\n总结\n**Attention Is All You Need** 不仅仅是一个标题，更是一种宣言。\n1.  它证明了**并行计算**是提升 AI 能力的关键路径。\n2.  它把**特征提取**的主动权完全交给了数据之间的相互作用（Self-Attention），而非人为设计的结构。\n3.  它开启了 **Pre-training + Fine-tuning** 的大模型时代。\n如果没有这篇论文，就没有今天的 ChatGPT、Claude 或 DeepSeek。\n参考资料\n论文原文\nThe Illustrated Transformer (Jay Alammar)\n*编辑：2026-01-26*",
    "url": "/blog/Google - Attension is all you need",
    "path": "tech/ai/论文/Google - Attension is all you need.md",
    "lastModified": "2026-01-26T08:49:21.381Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2022-10-11*\n**Mind's Eye**（论文标题为《Mind's Eye: Grounded Language Model Reasoning through Simulation》）是由 Google Research 和 DeepMind 等机构的研究人员提出的一种新范式。\n简单来说，这篇论文提出了一种**“模拟增强推理”**的方法，旨在让大语言模型（LLM）不仅能“读”和“写”，还能像人类一样在脑海中“运行”一个物理模拟器，从而获得对物理世界的真实感知（Grounding），解决模型在物理常识推理上的短板。\nMind's Eye 解决了什么问题？\n**核心问题：语言模型缺乏“物理世界的落地感”（Grounding）。**\n目前的大语言模型（LLM）主要是在海量文本数据上训练的。虽然它们能生成流畅的文字，但它们并没有在真实物理世界中生活过。\n**缺乏常识：** 模型可能知道“杯子掉在地上会碎”这句话，但它并不真正理解重力、碰撞、摩擦力等物理规律是如何运作的。\n**推理幻觉：** 当面对一个具体的物理场景（例如：“如果我把一个球放在倾斜的桌子上，它会滚向哪里？”），纯文本训练的模型往往依靠概率猜测，而不是物理推导，导致经常出现违反物理常识的错误。\n**Mind's Eye 的作用：** 它为 LLM 装上了一个“物理引擎”（Physics Engine）。当模型遇到物理推理问题时，它不是直接瞎猜，而是先写一段代码在物理引擎里“跑”一遍，看看到底发生了什么，再根据模拟结果回答问题。\n这个问题真实存在吗？\n**非常普遍且严重。**\n**“纸上谈兵”：** 研究发现，即使是很大的模型（如 PaLM 62B），在处理涉及空间关系、物体运动、液体流动等物理问题时，准确率也难以令人满意。\n**常识的黑洞：** 人类婴儿通过玩耍就能学会的物理常识（比如物体恒存性、重力方向），对 LLM 来说却是巨大的挑战，因为这些知识很难仅通过阅读文字完美习得。\n为什么现在才有人去解决？\n其实“具身智能”（Embodied AI）一直是个热点，但 Mind's Eye 的突破在于结合了**代码生成**和**物理模拟**：\n1.  **代码生成能力的提升：** 随着 Codex 等代码生成模型的成熟，LLM 现在可以将自然语言描述（如“把红球放在蓝球左边”）转化为可执行的代码（如 Python 脚本）。\n2.  **物理引擎的成熟：** 像 DeepMind 的 **MuJoCo** 这样的物理引擎已经非常强大且快速，能够精确模拟复杂的物理交互。\n3.  **计算与推理的解耦：** 以前人们试图让模型直接“学会”物理公式，Mind's Eye 则是让模型学会“使用工具”——把繁重的物理计算交给专门的引擎去做，自己只负责理解和决策。\n它是如何解决的？\nMind's Eye 的流程可以概括为三个步骤（**Text-to-Simulation-to-Reasoning**）：\n**文本转模拟（Text-to-Simulation）：** 模型首先阅读题目（例如：“将一个方块放在桌子边缘，推它一下”），然后编写一段控制物理引擎的代码（通常是 Python 代码调用 MuJoCo 库）。\n**运行模拟（Simulation）：** 执行这段代码。物理引擎会计算出物体的运动轨迹、碰撞结果等，并生成渲染图像或状态日志。\n**模拟增强推理（Simulation-Augmented Reasoning）：** 模型将模拟的结果（作为一种新的上下文信息）结合原始问题，进行最终的推理并生成答案。\n还有更好的解决方案吗？\nMind's Eye 提供了一种强有力的思路，但也有其局限性：\n**依赖引擎能力：** 如果物理引擎不支持某种现象（比如复杂的流体力学或化学反应），这种方法就失效了。\n**计算成本：** 相比直接生成文本，运行物理模拟需要额外的计算资源和时间。\n**代码生成门槛：** 模型必须具备较强的编程能力才能准确描述物理场景。\n**它的独特优势：** 它让小模型也能拥有“大智慧”。实验显示，只有 7.7 亿参数的小模型（FLAN-T5-Large）在装备了 Mind's Eye 后，其物理推理能力竟然能匹敌甚至超过 100 倍大的模型。\n关键词解析\n1. 接地（Grounding）\n在 AI 领域，“Grounding”指的是将抽象的符号（语言）与具体的感知（视觉、触觉、物理状态）联系起来的能力。\n**无 Grounding：** 就像一个从未见过苹果的人，背诵了关于苹果的所有定义，但给他一个真苹果他认不出来。\n**有 Grounding：** 看到“苹果”这个词，脑海里能浮现出它的形状、颜色、重量，并知道它掉在地上会滚。Mind's Eye 就是通过模拟器赋予了 LLM 这种能力。\n2. 模拟（Simulation）\n这里特指使用计算物理引擎（如 MuJoCo）来复现现实世界的物理过程。\n**人类的“心眼”（Mind's Eye）：** 心理学研究表明，人类在思考物理问题时，大脑里确实在进行某种“模拟”。比如问你“把钥匙扔进玻璃杯会怎样？”，你脑子里会快速预演这个画面。\n**AI 的“外挂”：** 既然 LLM 脑子里没有物理回路，我们就给它外接一个物理引擎。\n3. 零样本/少样本学习（Zero-shot / Few-shot）\n论文证明了 Mind's Eye 的强大泛化能力。\n**Zero-shot 提升 27.9%：** 即使不给模型任何例子，仅靠模拟器，准确率就大幅提升。\n**Few-shot 提升 46.0%：** 如果再给几个例子，提升效果更加惊人。这说明模型非常善于利用模拟结果来修正自己的判断。\n4. 文本到代码（Text-to-Code）\n这是实现 Mind's Eye 的桥梁。\n**挑战：** 自然语言是模糊的，代码是精确的。模型需要把模糊的“放在左边”转化为精确的坐标 。\n**意义：** 这展示了代码生成不仅仅是为了写软件，还可以作为一种**中间表示（Intermediate Representation）**，帮助模型理解和操作物理世界。\n---\n总结\nMind's Eye 的本质是**“工具学习（Tool Learning）在物理领域的应用”**：\n1.  **承认不足：** 承认 LLM 无法仅靠文本学会完美的物理常识。\n2.  **借力打力：** 利用现成的物理引擎来弥补短板。\n3.  **以小博大：** 证明了更好的推理机制比单纯堆砌参数更有效。\n参考资料\n论文链接 (arXiv)\nDeepMind MuJoCo\n*编辑：2025-12-10*",
    "url": "/blog/Google - Mind's Eye - Grounded Language Model Reasoning through Simulation",
    "path": "tech/ai/论文/Google - Mind's Eye - Grounded Language Model Reasoning through Simulation.md",
    "lastModified": "2026-01-20T02:47:42.682Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2024-05-13*\nMIT CSAIL 团队发布的 **The Platonic Representation Hypothesis**（论文标题为《The Platonic Representation Hypothesis》）在 AI 理论界引起了深远思考。\n简单来说，这篇论文提出了一个大胆的假说：**不同的 AI 模型（无论架构、模态或训练数据如何），随着规模的扩大和性能的提升，它们的内部表征正在收敛于同一个“现实世界的统计模型”**。就像柏拉图的“理型论”一样，所有模型都在逼近那个唯一的、真实的“理想现实”。\n这篇论文解决了什么问题？\n**核心问题：我们训练出来的 AI 模型，究竟是在各自“盲人摸象”，还是在殊途同归？**\n在深度学习的早期，人们普遍认为：\n不同的网络架构（如 CNN vs Transformer）会学习到完全不同的特征。\n不同的数据模态（如视觉 vs 语言）会产生完全无法互通的表示空间。\n不同的训练目标（如监督学习 vs 自监督学习）会导致模型关注世界的不同侧面。\n但这篇论文指出，**这种“多样性”可能只是因为模型还不够强**。当模型变得足够强大时，它们实际上都在学习同一个东西——现实世界的底层结构。\n这个问题真实存在吗？\n**真实存在，且有大量实验证据支撑。**\n论文通过一系列严谨的实验展示了这种“趋同”现象：\n**跨架构趋同：** 无论你是用 ResNet 还是 ViT，只要模型性能达到一定高度，它们对图像的理解（特征空间）会变得惊人地相似。\n**跨模态趋同：** 一个在大规模文本上训练的 LLM，和一个在大规模图像上训练的视觉模型，它们虽然没见过对方的数据，但对“狗”和“猫”之间距离的判断（几何结构）却是高度一致的。\n**模型拼接（Model Stitching）：** 研究者发现，把一个模型的前半部分切下来，直接拼到另一个完全不同模型的后半部分上，中间加一个简单的线性映射，居然能正常工作！这说明它们的“脑回路”在某种程度上是通用的。\n为什么现在才有人提出？\n这一现象之所以现在才变得明显，主要有三个原因：\n1.  **模型规模（Scale）的突破：** 在小模型时代，模型往往通过“死记硬背”或“投机取巧”来做任务，每一家都有自己的偏门绝技。但当模型参数量激增，为了处理极其复杂的数据分布，**“理解真实世界的物理/逻辑规律”成为了唯一且最优的解**。\n2.  **多模态模型的兴起：** CLIP、GPT-4V 等模型的出现，让我们第一次有机会在一个统一的视角下审视视觉和语言的表征。\n3.  **对通用人工智能（AGI）的探索：** 以前我们关注“解决任务”，现在我们关注“智能的本质”。这种视角的转换让研究者开始寻找模型背后的共性。\n它是如何解释这个现象的？\n论文提出了驱动这种收敛的三个主要**“选择压力”（Selective Pressures）**：\n**任务通用性（Task Generality）：** 当一个模型需要解决的任务越多（分类、生成、推理等），它就越不能依赖特定任务的捷径。唯一能通吃所有任务的策略，就是构建一个忠实反映现实世界的模型。\n**模型容量（Model Capacity）：** 只有足够大的模型，才有能力去拟合那个复杂的“柏拉图世界”。小模型只能学到皮毛（投影），而大模型能学到本体。\n**简单性偏见（Simplicity Bias）：** 深度神经网络天生倾向于寻找“最简单”的解。而相比于无数种复杂的、特设的规则，**物理现实本身可能就是解释数据的最简单模型**（奥卡姆剃刀原则）。\n还有其他解释吗？\n当然，这目前还是一个“假说”（Hypothesis）。\n**反方观点：** 也有学者认为，模型只是在拟合人类产生的数据（如互联网文本），而人类数据本身就有偏差，所以模型学到的是“人类眼中的世界”，而不是“客观物理世界”。\n**不同视角：** 还有一种观点认为，收敛是因为我们使用的**优化算法（SGD）**和**架构（Transformer）**太单一了，如果我们换一种完全不同的计算范式（如类脑计算），可能就会产生完全不同的表征。\n但无论如何，**“柏拉图表征假说”为当前的 Scaling Law 提供了一个极具哲学美感的终极解释**：我们不是在创造智能，我们是在“发现”智能。\n关键词解析\n1. 柏拉图表征 (The Platonic Representation)\n**来源：** 借用柏拉图的“洞穴寓言”。\n**含义：** 现实世界存在一个客观的、理想的统计结构（$Z$）。就像洞穴外真实的“马”。\n**现状：** 我们观测到的数据（图像、文本）只是这个理想结构在低维空间的投影（$X, Y$）。就像洞穴墙壁上的影子。\n**AI 的目标：** 随着训练进行，AI 模型试图通过分析这些影子（数据），反推出那个真实的 $Z$。因为 $Z$ 是唯一的，所以所有成功的 AI 模型最终都会长得一样。\n2. 模型拼接 (Model Stitching)\n这是一个非常直观的验证实验：\n**操作：** 拿来模型 A（比如 ResNet-50）和模型 B（比如 ViT-B）。\n**切分：** 把 A 的前 3 层切下来，作为“眼睛”。把 B 的后 3 层切下来，作为“大脑”。\n**缝合：** 在中间加一个极简的 1x1 线性层做“适配器”。\n**结果：** 发现这个“缝合怪”模型居然能保持很高的精度。\n**结论：** 这证明 A 和 B 在中间层学到的“语言”是互通的，它们都在用相似的方式描述世界。\n3. 表征对齐 (Representational Alignment)\n**定义：** 衡量两个模型的内部状态是否相似的指标。\n**工具：** 常用 CKA (Centered Kernel Alignment) 或 Procrustes Distance 来计算。\n**发现：** 随着模型性能（Accuracy）的提升，它们之间的 CKA 相似度呈现单调上升趋势。越强的模型，彼此越像。\n4. 核心对角线 (Kernel Diagonals)\n**现象：** 如果你画一个矩阵，横轴是各种视觉模型，纵轴是各种语言模型，颜色深浅代表相似度。\n**观察：** 你会发现性能最好的视觉模型和性能最好的语言模型，在矩阵中由于高相似度而形成明显的“对角线”或高亮区域。这暗示了跨模态的统一性。\n---\n总结\n**The Platonic Representation Hypothesis** 告诉我们：\n1.  **AI 的尽头是统一的：** 只要我们追求更强的泛化能力，所有模型最终都会殊途同归。\n2.  **数据模态不重要：** 无论是看图还是读书，最终学到的“概念”是同一个。\n3.  **现实是最大的约束：** 物理世界的客观规律是塑造 AI 大脑的最强力量。\n参考资料\narXiv 论文链接\nProject Page\n*编辑：2024-8-20*",
    "url": "/blog/MIT - The Platonic Representation Hypothesis",
    "path": "tech/ai/论文/MIT - The Platonic Representation Hypothesis.md",
    "lastModified": "2026-01-20T03:34:09.636Z"
  },
  {
    "title": "[From Mind to Machine: The Rise of Manus AI](https://arxiv.org/abs/2505.02024)",
    "content": "*论文发布时间：2025-05-04*\nFrom Mind to Machine: The Rise of Manus AI\n这篇文章介绍了 **Manus AI**，一个试图解决 AI 从“思考（Mind）”到“执行（Machine）”断裂问题的全自主数字智能体。\n**核心问题：执行赤字**\n目前的 LLM 虽然推理能力（IQ）很强，但缺乏真正的**执行力（Agency）**。它们止步于生成建议或代码，无法独立、可靠地操作异构工具（浏览器、终端、SaaS）来闭环复杂任务。\n**解决方案：多智能体协作架构**\nManus 将单体 LLM 拆解为三个专职角色，通过**对抗性协作**来保证质量：\n1.  **Planner Agent (规划者)**：大脑。负责意图理解、任务拆解和动态路径规划。\n2.  **Execution Agent (执行者)**：手脚。负责具体的工具调用（点击、输入、运行代码）。\n3.  **Verification Agent (验证者)**：质检员。**这是关键创新**。独立于执行者，专门负责对比“预期结果”与“实际产出”，如果不合格则打回重做。这种机制显著降低了 Agent 的“糊弄”和“幻觉”。\n**关键特性**\n**异步持久化**：任务在云端虚拟机中运行，状态与用户会话解耦。用户下线后，Agent 依然像远程员工一样持续工作。\n**通用自主性**：从 Intent 到 Outcome 的全自动闭环，无需用户逐步确认。\n**思考**\n**范式转变**：AI 的关注点正从提升模型的推理能力（Thinking），转向提升系统的落地效能（Doing）。\n**验证的价值**：在 Agent 系统中，**验证（Verification）比生成（Generation）更重要**。没有验证的自主执行就是灾难。\n**未来方向**：Human-in-the-Loop（人在回路）在关键决策点的引入，以及针对垂直领域的特化 Agent。\n---\nAI Agent 规划算法：MCTS 与 World Model\n2026-01-16\n1. MCTS (蒙特卡洛树搜索) 解决什么问题？\n**核心痛点**：在面对复杂的长链条任务时，**“下一步做什么”** 的选择空间巨大，而且单纯的贪婪策略（只看眼前）往往会陷入局部最优或走进死胡同。\n**解决方案**：\n**推演未来**：在真正执行动作前，先在“脑海”中模拟多种可能的路径。\n**概率决策**：通过模拟算出每条路径的成功率（胜率），选择最优解。\n**动态调整**：不是死板执行预设指令，而是根据模拟结果实时调整策略。\n2. 如何工作？（四个步骤）\n1.  **选择 (Selection)**：在当前状态下，权衡“利用已知好路径”和“探索新路径”。\n2.  **扩展 (Expansion)**：尝试迈出新的一步（如“写测试”或“查文档”）。\n3.  **模拟 (Simulation/Rollout)**：基于**世界模型 (World Model)** 快速推演到结局（“如果我这么做，大概率会发生什么？”）。\n4.  **回溯 (Backpropagation)**：根据结局的好坏（Reward），反向更新路径上所有节点的评分。\n3. “模拟”的本质与偏差\n**模拟 = AI 与自己的世界模型对话**\nLLM 本身就是一个压缩的世界知识库。\n模拟过程： ->  ->  -> 。\n**偏差来源 (Hallucination / Distribution Shift)**\n纯“脑内模拟”缺乏真实物理世界的反馈。\nLLM 的预测可能基于错误的经验（例如：API 已经变了，但 LLM 还在用旧版本知识推演），导致“自嗨”。\n4. 如何消除偏差？（混合策略）\n单纯 MCTS 容易产生幻觉，现代 Agent (如 Manus, Devin) 通常采用组合拳：\n1.  **沙箱执行 (Sandbox Execution)**：\n在模拟的关键节点，**真实执行**代码（Dry Run / Unit Test）。\n用真实的  替代 LLM 的预测，强制“纠偏”。\n2.  **MCTS + ReAct 结合**：\n**MCTS**：负责宏观战略规划（Map），决定大方向。\n**ReAct**：负责微观落地执行（Compass），步步为营。\n3.  **自我反思 (Self-Reflection / ToT)**：\n生成多个方案，让 Critic 模型（或自己）扮演“面试官”寻找逻辑漏洞。\n**一句话总结**：AI Agent 从“空想家”进化为“实干家”，关键在于**用真实的工具反馈（Grounding）来校准 MCTS 的想象力**。\n*编辑：2025-01-20*",
    "url": "/blog/Manus - From Mind to Machine",
    "path": "tech/ai/论文/Manus - From Mind to Machine.md",
    "lastModified": "2026-01-20T02:29:08.515Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2025-12-03*\nMemOS: A Memory OS for AI System 由上海交通大学、复旦大学等机构联合发布。简单来说，这篇论文提出了一种**“AI 记忆操作系统”**的概念，试图通过像管理电脑内存一样管理大模型的记忆，让 AI 拥有可进化、可迁移的长期记忆能力。\nMemOS 解决了什么问题？\n**核心问题：当前的大模型缺乏一个统一的“记忆管理系统”。**\n目前主流的大语言模型（LLM）在处理记忆时存在明显的割裂和低效：\n**静态参数的局限：** 模型训练完后权重就固定了，无法记住新知识，重新训练成本太高。\n**上下文窗口的限制：** 虽然上下文越来越长，但依然有限且昂贵，无法真正记住用户的长期偏好。\n**RAG 的无状态性：** 检索增强生成（RAG）虽然能查资料，但它是“用完即走”的，缺乏生命周期管理，无法融合新旧知识。\n**MemOS 的作用：** 它将记忆（Memory）视为一种可管理的系统资源，统一了从简单的文本检索到复杂的参数记忆的各种形式，让 AI 能够像操作系统管理内存一样，灵活地创建、迁移、融合和遗忘记忆。\n这个问题真实存在吗？\n**真实存在，且是 AI 通往通用人工智能（AGI）的必经之路。**\n如果不解决记忆管理问题，AI 永远只能是一个“健忘的学霸”：\n**无法持续学习：** 今天教给它的东西，明天换个对话窗口就忘了。\n**个性化成本极高：** 要让模型记住每个用户的习惯，目前的做法（全量微调或超长上下文）成本不仅高昂，而且难以维护。\n**知识碎片化：** 外部检索的知识（文档）和内部参数的知识（权重）是割裂的，无法有机融合。\n为什么现在才有人去解决？\n主要得益于对“记忆层次”理解的深入和系统架构的需求：\n1. **记忆层次理论的成熟：** 现在的研究开始将大模型的存储划分为不同层次（参数内存、激活内存、外部检索），这为统一管理提供了理论基础。\n2. **长上下文和 Agent 的爆发：** 随着 AI Agent 需要处理的任务越来越复杂，单纯靠“堆上下文长度”已经难以为继，必须引入更高效的记忆管理机制。\n3. **计算与存储分离的趋势：** 类似于计算机架构中 CPU 和硬盘的分离，AI 也需要将“思考（计算）”和“记忆（存储）”解耦，以降低成本。\n它是如何解决的？\nMemOS 的架构包含几个核心组件，就像一个真正的操作系统：\n**MemCube（记忆立方）：** 这是 MemOS 的基本单元。它不仅存储内容（Content），还包含元数据（Metadata），如来源、版本、访问频率等。这使得记忆可以被追踪和管理。\n**分层记忆体系：** MemOS 统一管理三种类型的记忆：\n**明文记忆 (Plaintext)：** 类似 RAG 的外部文档库，成本低，易更新。\n**激活记忆 (Activation)：** 模型推理过程中的中间状态（KV Cache），响应快但易失。\n**参数记忆 (Parameter)：** 模型的权重，最稳定但更新难。\n**操作原语 (Operations)：** 就像文件系统有复制粘贴一样，MemOS 定义了记忆的操作：\n**组合 (Composition)：** 将不同来源的记忆拼在一起。\n**迁移 (Migration)：** 将短期记忆（如激活态）转存为长期记忆（如明文或参数）。\n**融合 (Fusion)：** 将碎片化的记忆整合成新的、更高效的形式。\n还有更好的解决方案吗？\nMemOS 提出的是一个系统级的框架，与之竞争或互补的方案包括：\n**RAG (检索增强生成)：** 专注于“查资料”，优点是现成可用，缺点是缺乏对知识的深度融合和生命周期管理。\n**Long Context (超长上下文)：** 试图把所有东西都塞进 Prompt 里，优点是简单粗暴，缺点是推理成本随长度二次方增长，且容易“迷失中间”。\n**Continual Learning (持续学习)：** 试图直接更新模型参数，优点是彻底，缺点是容易出现“灾难性遗忘”。\n**MemOS 的优势在于“系统化思维”：** 它不执着于某一种特定的记忆技术，而是建立了一套标准（OS），让不同的记忆技术可以在同一个框架下协同工作，实现成本和效果的最优平衡。\n关键词解析\n1. MemCube (记忆立方)\n[这是 MemOS 发明的核心概念，是记忆的容器]\n**传统模型：** 记忆就是一段文本向量或者一堆参数权重，没有“身份证”。\n**MemOS：** 每一个 MemCube 都有完整的“户口本”。它记录了这段记忆是谁产生的（Provenance）、是什么时候产生的（Timestamp）、被用过多少次（Access Frequency）。\n**通俗理解：** 以前的记忆像是一堆散乱的便利贴；MemCube 像是整理好的档案盒，上面贴满了标签，随时可以归档、调阅或销毁。\n2. 记忆可塑性 (Memory Plasticity)\n[指记忆可以随时间变化和适应的能力]\n**静态记忆：** 书本上的知识，印上去就改不了。\n**可塑性记忆：** 像人类的大脑一样，MemOS 允许记忆在不同层级间流动。比如你刚认识一个新朋友，名字记在“短期记忆（激活态）”里；相处久了，他的喜好就刻进了“长期记忆（参数或明文）”里，甚至改变了你对某类人的看法（记忆融合）。\n---\n总结\nMemOS 的本质是**“给 AI 装一个文件系统”**：\n1. **统一标准：** 用 MemCube 统一了各种乱七八糟的记忆形式。\n2. **生命周期：** 让记忆有了“生老病死”，可以被创建、优化和淘汰。\n3. **分层管理：** 像电脑的 L1/L2 缓存和硬盘一样，智能调度记忆，让 AI 越用越聪明，而不是越用越慢。\n参考资料\n论文地址\narXiv 页面\n*编辑：2026-01-21*",
    "url": "/blog/MemOS - A Memory OS for AI System",
    "path": "tech/ai/论文/MemOS - A Memory OS for AI System.md",
    "lastModified": "2026-01-21T03:26:46.353Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2025-09-04*\nOpenAI 和 Georgia Tech 联合发布的 **《Why Language Models Hallucinate》**（arXiv:2509.04664）从统计学和博弈论的角度，对大模型的“幻觉”现象给出了一个颠覆性的解释。\n简单来说，这篇论文认为**幻觉（Hallucination）**不仅仅是模型能力的缺陷，更是**训练和评估体系（Leaderboards）**一手“逼”出来的。模型就像一个被迫参加考试的学生，因为现在的考试机制鼓励“猜答案”而不是承认“我不知道”，所以模型学会了不懂装懂。\n这篇论文解决了什么问题？\n**核心问题：揭示了幻觉在统计学上的必然性和评估机制的误导性。**\n目前的 AI 社区普遍认为幻觉是一个需要通过更大模型或更好数据来“修复”的 bug，但论文指出：\n**统计学根源：** 幻觉在本质上是二元分类（Binary Classification）中的错误。即使是在完美的训练数据上，统计学习的压力也会迫使模型在不确定时生成看似合理的错误信息。\n**评估导向错误：** 现有的主流评测（如 MMLU、GPQA 等）通常采用“答对给分，答错或不答零分”的机制。这导致模型在面对不确定问题时，**“瞎猜”的期望收益永远大于“保持沉默”**。\n**论文的贡献：** 它证明了只要我们继续用现在的标准去奖励模型（只看准确率，不看置信度），幻觉就永远无法根除。解决之道在于**“社会技术缓解”（Socio-technical Mitigation）**，即修改考试规则。\n这个问题真实存在吗？\n**非常普遍且严重，直接损害了 AI 的可信度。**\n**一本正经胡说八道：** 即使是 SOTA 模型（如 GPT-4o, DeepSeek-V3, Llama 3），在被问到具体的冷门事实（如“某某科学家的生日”）时，往往会自信地输出一个错误日期，而不是说“我不知道”。\n**基准测试的误导：** 目前的 Leaderboard 排名可能在奖励那些“更敢猜”的模型，而不是更诚实的模型。这导致我们选出的“最强模型”可能恰恰是幻觉最严重的模型。\n为什么现在才有人去解决？\n其实大家一直深受幻觉困扰，但之前的研究更多关注技术层面的“对齐”（Alignment）或 RAG（检索增强）。这篇论文从**评估激励机制**的底层逻辑切入，原因在于：\n1.  **Leaderboard 文化的盛行：** 各种榜单（Open LLM Leaderboard, HELM 等）成为了模型能力的唯一指挥棒。论文详细分析了这些榜单，发现几乎所有榜单都缺乏对“不确定性”的合理评分。\n2.  **幻觉问题的顽固性：** 尽管模型越来越大，数据越来越多，幻觉依然存在。这促使研究者反思，是不是我们的**目标函数**本身就设错了？\n3.  **理论框架的完善：** 论文建立了一套将生成问题（Generative）转化为“是否有效”（Is-It-Valid, IIV）分类问题的数学框架，从理论上证明了预训练必然导致幻觉。\n它是如何解决的？\n论文提出的不是一个新的模型架构，而是一套**系统性的改革方案**：\n**理论归因：** 证明了生成式错误（幻觉）的发生率至少是内部二元分类（IIV）错误率的两倍。这意味着只要模型无法完美区分“真/假”，幻觉就一定存在。\n**诊断评估体系：** 分析了 HELM、Open LLM Leaderboard、SWE-bench 等主流榜单，指出它们如何通过评分规则（Scoring Rules）抑制了模型表达不确定性。\n**提出解决方案：**\n**修改评分规则（Proper Scoring Rules）：** 引入新的计分方式。例如，回答“我不知道”得 0 分，答错扣分（或得分显著低于答对），答对得正分。这样模型在没把握时会倾向于选择“我不知道”以避免惩罚。\n**校准训练（Calibration）：** 在后训练（Post-training）阶段，明确奖励模型输出真实的置信度，而不是仅仅奖励最终答案的匹配。\n还有更好的解决方案吗？\n这篇论文主要探讨的是**成因和评估**，在具体的技术消除手段上，社区还有其他路线：\n**RAG（检索增强生成）：** 让模型去查资料而不是背书。这是目前工程上最有效的手段，但如果模型本身倾向于“瞎编”，RAG 也可能被模型用来编造虚假的引用。\n**CoT（思维链）与自我反思：** 让模型在输出前先推理或检查。这能缓解部分逻辑错误，但对事实性幻觉帮助有限。\n**RLHF（人类反馈强化学习）：** 目前的 RLHF 往往加剧了幻觉，因为人类标注员倾向于喜欢“看起来有帮助”的回答，导致模型学会了迎合人类偏好而非坚持事实。\n**本论文的优势在于治本：** 如果不改变“考试规则”，上述技术手段都只是在通过过拟合来掩盖问题。\n要深入理解这篇论文，我们需要拆解它提出的几个核心概念。其核心逻辑是：**模型幻觉不仅是能力问题，更是激励机制导致的策略选择。**\n关键词解析\n1. IIV (Is-It-Valid) 二元分类问题\n这是论文提出的核心理论框架。\n**传统视角：** 生成文本是一个复杂的序列预测问题。\n**论文视角：** 我们可以把生成过程简化为：模型在脑子里构建很多个候选句子，然后判断“这句话是不是真的/有效的”（Is this valid?）。\n**结论：** 这是一个二分类问题（Yes/No）。统计学告诉我们，二分类器总会有误差（False Positive）。当模型错误地把一个“无效/虚假”的句子判断为“有效”并输出时，幻觉就产生了。论文证明，生成错误的概率与这个二分类器的错误率成正比。\n2. 应试行为 (Test-Taking Behavior)\n这是对模型在 Post-training 阶段行为的形象比喻。\n**现象：** 学生在做多项选择题时，如果选错和不选都不得分，那最佳策略就是**“蒙一个”**（Guessing）。\n**映射到 AI：** 现有的 Leaderboard（如 MMLU）大多不设“倒扣分”机制。模型发现，遇到不懂的问题，只要自信地输出一个答案，还有 25% 的概率蒙对；如果输出“不知道”，得分率为 0%。\n**结果：** RLHF（强化学习）过程实际上是在训练模型成为一个“不仅要懂，不懂也要装懂”的**做题家**。\n3. 社会技术缓解 (Socio-technical Mitigation)\n这是论文提出的解决方案类型。\n**技术缓解：** 改模型结构、加数据、改算法。\n**社会技术缓解：** 改变**人**设定的规则和标准。\n**具体措施：** 呼吁整个 AI 社区（OpenAI, Google, Meta 等）修改 Leaderboard 的评分标准。如果大家开始重视并奖励“诚实的不知”，模型厂商自然会调整训练目标，产出更诚实的模型。\n4. 固有幻觉 vs. 外在幻觉 (Intrinsic vs. Extrinsic Hallucinations)\n论文对幻觉进行了分类：\n**固有幻觉（Intrinsic）：** 模型输出的内容与**上下文（Prompt）**矛盾。\n*例子：* 用户问“DeepSeek 里有几个 D？”，模型回答“3 个”。这属于逻辑处理失败。\n**外在幻觉（Extrinsic）：** 模型输出的内容与**训练数据或外部事实**矛盾。\n*例子：* 模型编造了一个不存在的论文标题。这是本文关注的重点，源于预训练数据的统计压力。\n5. 适当评分规则 (Proper Scoring Rules)\n这是一个统计学术语。\n**定义：** 一种评分函数，只有当预测者报告其真实的信念概率时，期望得分才最高。\n**应用：** 在 AI 评估中，如果我们使用 Brier Score 或 Log Scoring，模型为了最大化分数，就必须输出它内心真实的置信度（比如“我有 60% 把握是对的”），而不是由着性子瞎猜。\n---\n总结\n这篇论文给我们的启示是**“评估即指挥棒”**：\n1.  **幻觉是统计必然：** 不要指望单纯通过预训练消除所有幻觉。\n2.  **激励机制通过 RLHF 放大幻觉：** 现在的榜单逼着模型去“猜”，模型被迫学会了过度自信。\n3.  **诚实需要被奖励：** 必须改革 Leaderboard，给“我不知道”或低置信度回答合理的生存空间。\n4.  **从源头治起：** 只有当评估标准变了，训练目标才会变，我们才能得到真正值得信赖（Trustworthy）的 AI。\n参考资料\n论文原文 (arXiv)\nOpenAI 博客\n*编辑：2025-12-25*",
    "url": "/blog/OpenAI - Why Language Models Hallucinate",
    "path": "tech/ai/论文/OpenAI - Why Language Models Hallucinate.md",
    "lastModified": "2026-01-20T02:52:20.201Z"
  },
  {
    "title": "参考资料",
    "content": "*论文发布时间：2026-01-13*\nRUC（中国人民大学）等机构联合发布的 **Memory in the Age of AI Agents** 是一篇关于智能体记忆的重量级综述。\n简单来说，这篇论文为 AI Agent 的**记忆系统（Agent Memory）** 建立了一套**全景式的“世界地图”**，它不再局限于传统的“长短期记忆”分类，而是从形式、功能和动态三个维度重新定义了 Agent 是如何“记事”、“用事”和“忘事”的。\nAgent Memory 解决了什么问题？\n**核心问题：Agent 记忆领域的研究极其碎片化，且术语定义混乱。**\n随着 AI Agent 从简单的聊天机器人进化为能独立干活的智能体，关于“记忆”的研究爆发式增长，但也带来了严重的问题：\n**概念混淆：** 大家把 RAG（检索增强）、Context Window（上下文窗口）、Prompt Engineering（提示词工程）都混着叫“记忆”，缺乏清晰界限。\n**分类过时：** 传统的计算机“内存/硬盘”类比，或者心理学的“长期/短期记忆”分类，已经不足以涵盖现代大模型 Agent 的复杂记忆机制。\n**缺乏标准：** 没有统一的框架来评估不同的记忆方法，导致研究者很难横向对比哪个方案更好。\n**本论文的作用：** 它提出了一个统一的 **FFD 框架（Forms, Functions, Dynamics）**，像一本“字典”一样统一了语言，让后续的研究者能在同一个频道上对话。\n这个问题真实存在吗？\n**非常真实。记忆是 Agent 走向“通用智能”的必经之路。**\n**从“过客”到“管家”：** 如果 Agent 记不住你的偏好、记不住它上周做过什么、记不住它学到的新技能，那它永远只是一个用完即走的工具，而不是一个能长期陪伴的助手。\n**幻觉与效率：** 很多 Agent 的“幻觉”或“死循环”，本质上是因为记忆提取失败（忘了之前的设定）或者工作记忆溢出（上下文撑爆了）。\n为什么现在才有人去解决？\n1.  **Agent 爆发期：** 2024-2025 年是 Agent 的元年，从 AutoGPT 到 BabyAGI，再到各种复杂的 Multi-Agent 系统，记忆模块的复杂度呈指数级上升。\n2.  **LLM 能力瓶颈：** 单纯靠扩大模型的 Context Window（如 100万 token）并不能完美解决记忆问题，**“能装”不代表“能用”**。如何高效地管理无限的记忆，成为了比“上下文长度”更重要的问题。\n它是如何解决的？\n这篇论文的核心贡献是提出了 **FFD 框架**，从三个维度解构 Agent Memory：\n**Forms（形态 - 它是怎么存的？）：**\n**Token-level Memory：** 以自然语言形式存储（如聊天记录、外部文档）。最直观，易于人类理解。\n**Parametric Memory：** 存储在模型权重里（如通过微调记住的知识）。\n**Latent Memory：** 存储在向量或隐藏状态里（如 KV Cache，Embedding）。\n**Functions（功能 - 它是干嘛用的？）：**\n**Factual Memory（事实记忆）：** 记住“世界是什么样的”（如：法国首都是巴黎，用户的名字叫 Bob）。\n**Experiential Memory（经验记忆）：** 记住“过去发生了什么”以及“如何做某事”（如：上次解决这个 Bug 用的是这个方法）。\n**Working Memory（工作记忆）：** 记住“当前正在做什么”（如：当前的思维链、临时变量）。\n**Dynamics（动态 - 它是怎么变的？）：**\n**Formation（形成）：** 信息如何从环境进入 Agent。\n**Evolution（演化）：** 记忆如何被压缩、遗忘、合并。\n**Retrieval（检索）：** 如何在需要的时候准确找到那条记忆。\n还有更好的解决方案吗？\n这是一篇 Survey（综述），它不是提出一个单一的算法，而是总结了所有现有的方案。但它指出了未来**更好**的方向：\n**Memory Automation（记忆自动化）：** 不需要人类写 Prompt 告诉 Agent 记什么，Agent 应该像人一样自动决定什么该记，什么该忘。\n**Multimodal Memory（多模态记忆）：** 现在的记忆多是文字，未来需要记住图像、声音和视频体验。\n**Trustworthiness（可信度）：** 记忆会被污染（如 Prompt 注入），如何保证记忆的真实和安全。\n**这篇论文的优势在于“正本清源”：** 它没有盲目堆砌技术细节，而是提供了高屋建瓴的视角，是入门 Agent Memory 领域的最佳导航图。\n关键词解析\n1. Factual vs. Experiential Memory\n这是借鉴认知心理学的分类，但在 Agent 中有具体含义。\n**Factual Memory (事实)：** 类似**百科全书**。\n*例子：* “Python 是一门编程语言。”\n*实现：* 通常通过 RAG 检索文档，或直接微调进模型权重。\n**Experiential Memory (经验)：** 类似**技能手册**或**日记**。\n*例子：* “上次用户报错说 Python 环境没装，我通过  解决了。” -> 下次遇到类似问题直接调用这个经验。\n*实现：* 通常存储为“问题-解决方案”对，用于类似 Few-shot learning 的上下文增强。\n2. Parametric vs. Non-Parametric Memory\n这是 AI 模型中经典的区分。\n**Parametric (参数化)：** 知识即权重。\n*优点：* 提取速度快（模型推理时自动调用）。\n*缺点：* 更新极难（需要重新训练或微调），容易灾难性遗忘。\n**Non-Parametric (非参数化/外部)：** 知识即数据。\n*优点：* 随时增删改查（像数据库一样），更新成本低。\n*缺点：* 需要额外的检索步骤，速度较慢。\n*趋势：* 现代 Agent 都是两者的结合（Hybrid）。\n---\n总结\n《Memory in the Age of AI Agents》的本质是**“Agent 记忆系统的工程学指南”**：\n1.  **Forms** 告诉我们记忆存在哪（文本、权重、向量）。\n2.  **Functions** 告诉我们记忆分什么类（事实、经验、工作）。\n3.  **Dynamics** 告诉我们记忆怎么流转（记、存、取）。\n它标志着 Agent Memory 从“小技巧”走向了“系统科学”。\n参考资料\n论文地址\nDeepSeek Engram 解读 (作为对比，Engram 是一种具体的 Parametric/Latent Memory 优化技术)\n*编辑：2026-01-20*",
    "url": "/blog/RUC - Memory in the Age of AI Agents",
    "path": "tech/ai/论文/RUC - Memory in the Age of AI Agents.md",
    "lastModified": "2026-01-20T09:12:12.647Z"
  },
  {
    "title": "理论",
    "content": "理论\nMIT的这篇论文通过详细的实验和案例研究，挑战了语言是思维基础的传统观点，为人工智能的发展提供了新的思路。未来的AI研究可能需要更多地关注非语言形式的数据和方法，探索语言之外的思维形式。这不仅为大语言模型的改进提供了新的方向，也为实现更强大、更智能的人工智能系统奠定了基础。\nas a powerful tool for transmitting knowledge, language reflects the sophistication of human cognition — but does not give rise to it. \nReference\nlanguage-is-a-tool-for-communication-not-for-thought\nhacker news\n中文翻译",
    "url": "/blog/语言是交流工具而不是思维工具",
    "path": "tech/ai/语言是交流工具而不是思维工具.md",
    "lastModified": "2025-06-20T11:25:12.919Z"
  },
  {
    "title": "软件2.0",
    "content": "软件2.0\n原文\n概念：软件2.0通常指的是那些在其核心集成了机器学习模型的软件系统。这些系统的行为不仅仅是由传统的编程代码决定的，而是由数据驱动的模型生成的。\n以下是对文章一些观点的理解\n融合模块以求最优\nModules can meld into an optimal whole\n>原文：普通软件通常被分解成多个模块，各个模块中间通过共有函数、API 或者端到端的方式通信。然而，对于软件2.0，如果一开始2个相交互的模块是独立训练的，我们之后也很容易在整个系统中进行反向传播。想想看，如果你的浏览器可以自动设计底层指令，从而提高加载页面的速度；或者说你导入的计算机视觉库（比如 OpenCV）可以根据你的特定数据，自动调整行为；\n理解：\n软件2.0指的是那些利用机器学习模型作为其核心组件的软件系统。关键特点是它能够通过收集数据和反向传播来持续学习和自我优化。这种方式**使得软件能够更加智能和适应性强，因为它不再是静态的代码集合，而是一个能够随着时间和数据的积累而变得更加高效的动态系统。**\n举例：\n1. **浏览器自动设计底层指令：**\n   在这个例子中，我们可以想象一个浏览器，它不仅仅是静态地执行预定义的指令来加载页面。相反，它具备了通过机器学习来优化这些指令的能力。例如，浏览器可以监控用户的上网习惯、页面加载时间、系统资源使用情况等数据，并且使用这些数据来训练一个模型，这个模型能够预测并调整底层的网络请求、资源分配策略，甚至是JavaScript引擎的工作方式，以提高页面加载的速度。这种优化是动态进行的，能够适应不同用户的使用环境和习惯。\n2. **计算机视觉库自动调整行为：**\n   在另一个例子中，传统的计算机视觉库，比如OpenCV，提供了一系列预定义的算法和函数来处理图像和视频数据。在软件2.0的情况下，这些库可以进一步智能化。假设你正在使用这样的库来进行面部识别，库本身可以包含一个或多个机器学习模型，这些模型可以根据你提供的特定数据（比如，你的应用中的用户面部图像）自动调整其内部参数。这样，面部识别的准确性不仅仅取决于通用算法，还取决于它如何通过学习特定的数据集来自我优化。\n对抗样本导致软件 2.0 异常\n>原文：软件2.0的奇怪特性也在不断出现。比如，对抗样本和攻击样本的存在，使得软件2.0的不可解释性问题变得更加突出。\n理解：\n对抗样本是经过特别设计的输入数据，它们对人类来说可能看起来和正常数据没有区别，但是能够欺骗机器学习模型，使其做出错误的预测或分类。这些情况突出了软件2.0的不可解释性问题，因为即使是模型的开发者也很难完全理解或解释为什么模型会被这些对抗样本所欺骗。这种不可解释性不仅挑战了模型的可靠性，也给安全性带来了风险，尤其是在那些对精确性和可靠性有着极高要求的应用场景中。\n例如：\n1. **自动驾驶汽车：**\n   在自动驾驶汽车中，使用了大量的机器学习模型来处理视觉识别任务，例如识别交通标志。一个对抗样本的例子可能是一个轻微修改过的停车标志，虽然对于人类来说这个标志看起来没有任何问题，但是修改可能会导致自动驾驶系统无法正确识别这个标志，从而可能忽略停车指令，引发安全问题。\n2. **面部识别系统：**\n   在安全性要求高的面部识别系统中，对抗样本可能是经过精心设计的图像或者是特定的化妆或配饰，这些对于人眼来说不会影响识别，但可能会导致面部识别系统无法正确识别或者将一个人错误地识别为另一个人。\n理解 1.0 vs 2.0\n软件 1.0\n**模块演化**\n初始：单一大型服务（单体架构）。\n演化：服务分解为小型、独立的模块，这些模块相互配合工作（微服务架构）。\n**设计哲学：** 基于规则的编程，开发者需要手动编写代码来明确规定软件的每一步行为。\n**开发方式：** 往往是水平式的，需要事先规划架构和未来可能的扩展点。\n**更新迭代：** 更新通常涉及整个系统的重新部署或至少是较大模块的重新部署。\n**可解释性：** 高，因为行为是由清晰的代码逻辑决定的。\n**示例：** 传统的ERP系统，银行交易系统等，这些系统都是具有固定逻辑和复杂业务规则的系统。\n软件 2.0\n**模块演化** \n初始：独立的专用模型完成各自的任务，然后将结果整合（如多个单独的模型串联处理任务）。\n演化：单个端到端模型直接处理整个任务，没有中间转换步骤（如一个模型同时处理语音识别、自然语言理解和语音合成）。\n**设计哲学：** 基于数据和学习的编程，软件通过对大量数据进行学习来自我优化和改进，而非仅仅依据预编程的规则。\n**开发方式：** 往往是垂直式的，能够从数据中自主学习并适应新任务，不需要开发者预见到每一种可能。\n**更新迭代：** 在很多情况下，模型可以通过持续学习实现自我更新而无需人为干预。\n**可解释性：** 相对较低，因为模型决策过程复杂且不总是透明的。\n**示例：** 机器学习驱动的推荐系统（如YouTube或Netflix的推荐算法），自然语言处理系统（如GPT-3），自动驾驶车辆。\n软件1.0更倾向于模块化和微服务架构，其设计是为了解决特定的、已知的问题而构建的。而软件2.0则向着使用数据驱动的方法，通过端到端的模型自动学习解决问题，这常涉及人工智能和机器学习特别是深度学习领域。\n水平跟垂直式开发\n水平式开发（Software 1.0）\n水平式开发通常指的是传统的软件开发过程，这里的\"水平\"可以类比于搭建房屋的每一层。这种方法的特点包括：\n**层次结构**：软件以层的形式构建，每一层依赖于下面的一层。比如，用户界面层依赖于业务逻辑层，而业务逻辑层则依赖于数据访问层。\n**模块化**：系统被分解为功能模块，每个模块负责特定的功能或服务。\n**规划先行**：在开发之前需要详尽的规划，了解系统所有层和模块如何相互作用。\n**硬编码**：逻辑是通过编写具体的代码指令来实现的，对于新的问题和需求通常需要开发人员手动进行修改和更新。\n垂直式开发（Software 2.0）\n垂直式开发则更多地与数据驱动的机器学习方法相联系，\"垂直\"可以类比于从基础设施一直向上生长的树木。这种方法的特点包括：\n**端到端学习**：任务从输入到输出通过一个端到端的系统来完成，没有必须人为编写的规则。\n**数据为中心**：系统主要通过分析和学习大量数据来获得知识和解决问题的能力。\n**动态适应**：系统可以不断从新的数据中学习，从而适应新的问题和场景。\n**自主性**：因为模型可以自我学习，所以对于未知问题的适应性强，这减少了人工编写代码的需要。\n在机器学习（尤其是深度学习）中，垂直式的发展体现在从底层特征直接学习到高层的决策，而无需人类专家制定复杂的规则或者手动设计特征。\n总体来说，水平式开发注重于开发者明确规划各个模块的交互和功能，而垂直式开发依赖于从数据中自动学习模式和规律，这样的系统通常能自适应未见过的数据或情况。\nIlya 推荐的 30 篇\n(30篇论文，就能掌握当今科技90%的知识！ILYA公布神级论文清单(上）)[https://mp.weixin.qq.com/s?__biz=Mzk0NzQzOTczOA%3D%3D&mid=2247507514&idx=3&sn=6f90564bb8a07f39d360aec1968e6b14&ref=openi.cn]\n(下)[https://mp.weixin.qq.com/s?__biz=Mzk0NzQzOTczOA==&mid=2247507451&idx=1&sn=d586514585870f4e3205199a8f6f2f5f&chksm=c3744ceef403c5f883ea5a495e58764ffa57cbf7a7c5c0fa30b4493057f58ef288774b0768a4&scene=21#wechat_redirect]\nReference\n翻译 颠覆式编程：软件2.0\n视频：大模型解决不了英伟达的难题，AI新范式必将出现：专访安克创新CEO阳萌",
    "url": "/blog/软件 2.0",
    "path": "tech/ai/软件 2.0.md",
    "lastModified": "2025-06-20T11:25:12.920Z"
  },
  {
    "title": "Goroutine",
    "content": "Goroutine\nGoroutine 可以看作对 thread 加的一层抽象，它更轻量级，可以单独执行。因为有了这层抽象，Gopher 不会直接面对 thread。\nGoroutine vs threads \n内存占用\n1. 创建一个 goroutine 的栈内存消耗为 2 KB，实际运行过程中，如果栈空间不够用，会自动进行扩容。创建一个 thread 则需要消耗 1 MB 栈内存，而且还需要一个被称为 “a guard page” 的区域用于和其他 thread 的栈空间进行隔离。\n2. 对于一个用 Go 构建的 HTTP Server 而言，对到来的每个请求，创建一个 goroutine 用来处理是非常轻松的一件事。而如果用一个使用线程作为并发原语的语言构建的服务，例如 Java 来说，每个请求对应一个线程则太浪费资源了，很快就会出 OOM 错误（OutOfMermoryError）。\n创建和销毀\n1. Thread 创建和销毀都会有巨大的消耗，因为要和操作系统打交道，是内核级的，通常解决的办法就是线程池。\n2. 而 goroutine 因为是由 Go runtime 负责管理的，创建和销毁的消耗非常小，是用户级。\n切换\n1. 当 threads 切换时，需要保存各种寄存器，以便将来恢复：\n一般而言，线程切换会消耗 1000-1500 纳秒，一个纳秒平均可以执行 12-18 条指令。所以由于线程切换，执行指令的条数会减少 12000-18000。\n2. Goroutine 的切换约为 200 ns，相当于 2400-3600 条指令。\n因此，goroutines 切换成本比 threads 要小得多。\nscheduler (M:N模型)\nGo runtime 会负责 goroutine 的生老病死，从创建到销毁，都一手包办。\nRuntime 会在程序启动的时候，创建 M 个线程（CPU 执行调度的单位），之后创建的 N 个 goroutine 都会依附在这 M 个线程上执行。这就是 M:N 模型\n什么是scheduler？\n!scheduler-concepts\nscheduler时机\n1. 使用关键字 go\ngo创建一个新的 goroutine\n2. GC\nGC 需要在M 上进行所以会调度\n3. 系统调用\n4. 内存同步访问\natomic, mutex, channel 等操作\nReference\ngo practice\nhttps://golangbot.com/goroutines/\nhttps://rakyll.org/scheduler/\nhttps://blog.nindalf.com/posts/how-goroutines-work/\nhttps://speakerdeck.com/retervision/go-runtime-scheduler?slide=27\nhttps://povilasv.me/go-scheduler/#\nhttps://morsmachine.dk/netpoller\nhttps://morsmachine.dk/go-scheduler\nhttps://golangbot.com/structs-instead-of-classes/\nhttps://zhuanlan.zhihu.com/p/80853548",
    "url": "/blog/Go简介",
    "path": "tech/backend/Go简介.md",
    "lastModified": "2025-06-20T11:25:12.930Z"
  },
  {
    "title": "JSON序列化",
    "content": "问：easyjson 跟 golang 原生 JSON 方法对比\neasyjson是一个用于快速进行json序列化和反序列化的工具包，它通过为结构体生成方法来避免使用反射，从而提高性能和效率。\n与golang原生的json方法相比，easyjson的性能可以提高2～3倍。\neasyjson的使用方法是先安装它，然后为需要序列化的结构体添加注释，然后运行easyjson命令生成对应的解析代码。\n问： 一些easyjson的优缺点？\n答：easyjson的优点是：\n它可以提高json序列化和反序列化的性能和效率，比golang原生的json方法快2～3倍。\n它可以避免使用反射，从而减少内存分配和GC压力。\n它可以通过注释和命令行工具自动生成解析代码，使用方便。\neasyjson的缺点是：\n它不会校验json的正确性，可能导致解析错误或异常。\n它的代码风格可能不太优雅，有些重复或冗余。\n它需要为每个解析字段指定FieldTag，否则会使用变量名作为解析字段2。\n问：easyjson 是什么原理\n答：\n它通过给我们要进行序列化的 struct 生成方法来实现不通过反射进行json序列化。\n它只针对预先定义好的 json 结构体对输入的json字符串进行纯字符串的截取，并将对应的json字段赋值给结构体。\n它提供了代码生成工具，可以一键生成go文件中定义的结构体对应的解析代码。\n问：举个例子\n答：\n好的，我可以给你一些示例代码来说明 easyjson 跟 golang 自带 JSON 工具的不同。\n假设我们有一个结构体如下：\n如果我们使用 golang 自带的 JSON 工具，我们可以这样序列化和反序列化：\n如果我们使用 easyjson，我们需要先安装 easyjson 工具：\n然后在结构体上加上注释 ，并使用命令  生成对应的序列化和反序列化方法：\n生成的方法会在一个新的文件  中，我们可以这样使用：\n从这些示例代码中，我们可以看到 easyjson 跟 golang 自带 JSON 工具的不同。\n***通过 GPT 生成***",
    "url": "/blog/JSON序列化",
    "path": "tech/backend/JSON序列化.md",
    "lastModified": "2025-06-20T11:25:12.930Z"
  },
  {
    "title": "napi 宏的理解",
    "content": "napi 宏的理解\n宏与 DSL\nRust 宏的核心之一是通过生成代码来简化重复任务，而 DSL（领域专用语言）则是在宏的帮助下，为特定领域问题设计的一种更易读、易用的语法。通过宏，可以把 Rust 的代码变得像定制的小语言一样，让开发者专注于问题本身，而不是细节的实现。\n以下是几个示例，展示如何用 Rust 宏构建 DSL：\n示例 2：构建命令行解析器 DSL\n以下宏提供了一种简化命令行参数解析的 DSL：\n**运行示例：**\n**分析：**\n通过  宏定义命令行参数的格式和类型。\n用户只需描述参数，而无需手动解析或处理错误。\n---\n示例 3：实现状态机 DSL\n以下宏用来定义一个简单的有限状态机：\n**输出：**\n**分析：**\n通过  宏，用户可以简单地定义状态和转移规则。\n宏将 DSL 编译成 Rust 代码，完成状态机的实现。\n代码清晰且具有实际用途。\n---\n总结\nRust 宏在构建 DSL 时具有以下优点：\n1. **可读性**：用自然的语法描述特定领域的规则，用户无需了解底层实现。\n2. **简化代码**：减少样板代码，专注于核心逻辑。\n3. **强类型保障**：结合 Rust 的类型系统，确保生成代码的安全性。\n这些 DSL 示例涵盖了 HTML 构建、命令行解析和状态机定义，是 Rust 宏在领域专用语言设计中的常见用法。\n手写实现\n下面是之前手写的简化实现版本，并为其添加了详细注释，帮助理解各部分的功能和逻辑：\n---\n**逐步拆解注释说明**\n**1. 模块入口点 ()**\n这是 Rust 编写的 N-API 模块的入口点。  \nNode.js 在加载  文件时，会查找并调用这个函数。\n关键参数：\n：当前 N-API 的上下文，保存模块运行时的状态。\n：代表模块的导出对象，相当于 Node.js 中的 。\n函数作用：\n注册模块的所有导出方法，并将它们绑定到 。\n---\n**2. 注册函数 ()**\n这个部分通过  创建一个符合 N-API 标准的函数。\n的关键参数：\n：N-API 环境上下文。\n：JavaScript 中函数的名字。\n：指向 Rust 中实际的函数实现。\n---\n**3. 函数实现 ()**\n这是具体的 Rust 函数逻辑，通过 FFI 暴露给 Node.js 调用。\n参数解析：\n使用  获取传递给函数的参数，期望两个参数。\n每个参数需要用  解析并转换为 Rust 类型。\n结果处理：\n计算两个整数的和。\n使用  创建一个 JavaScript 的数值对象，将结果返回给调用者。\n---\n**调用链的工作流程**\n1. Node.js 加载  文件时，调用 。\n2.  使用  创建函数并将其绑定到模块的导出对象。\n3. 在 JavaScript 中调用 ，会触发  函数的执行：\n获取参数。\n调用 Rust 函数。\n将结果转换为 JavaScript 类型并返回。\n---\n**优缺点对比**\n**优点：**\n直接操作 N-API，可以完全控制模块行为。\n适用于需要手动优化性能或有特殊需求的场景。\n**缺点：**\n编写 N-API 代码繁琐且容易出错。\n需要处理参数解析、类型转换、错误管理等大量细节。\n难以维护，代码量大。\n---\n**总结**\n手写实现展示了 Rust 和 Node.js 交互的基础，但在实际项目中，推荐使用类似  宏来简化开发。属性宏会自动生成上述复杂逻辑，让开发者专注于核心功能。\n参考资料\ngpt",
    "url": "/blog/napi 宏的理解",
    "path": "tech/backend/napi 宏的理解.md",
    "lastModified": "2025-06-20T11:25:12.931Z"
  },
  {
    "title": "rust Arena内存分配",
    "content": "Arena\n在 Rust 中， 是一种内存分配策略，主要用于高性能场景中高效地分配和管理一组对象。相比 、、 和  等智能指针， 的优势在于它可以集中管理对象的内存，从而避免频繁的分配与释放操作。\n什么是 Arena？\n（内存池）是一种分配器，通过在一块连续的内存区域中分配多个对象来提升性能。这些对象的生命周期与  本身绑定。当释放  时，所有分配的对象会统一释放，无需逐一销毁。\nRust 的一些库（如 [](https://crates.io/crates/typed-arena)、[](https://crates.io/crates/bumpalo)）提供了  的实现。\n2. **短生命周期的高性能批量分配**\n当程序需要频繁创建并销毁对象，且这些对象的生命周期较短时， 可以避免频繁调用系统内存分配器，提高性能。\n**示例：批量计算临时对象**\n**优势**：\n分配性能优于堆分配（如 ）。\n对象销毁集中在  的生命周期结束时，减少了单个对象的析构开销。\n---\n3. **多对象共享管理，替代 /**\n当多个对象需要共享同一批数据时，使用  可以避免  或  的引用计数开销。\n**示例：共享数据结构**\n**优势**：\n统一管理节点的内存，避免循环引用。\n不需要手动清理，生命周期简单。\n---\n4. **需要快速分配和清理的大量小对象**\n对于游戏开发或其他需要频繁更新大量小对象的场景（如粒子系统、实体管理）， 可以显著提升分配和清理性能。\n**示例：粒子系统**\n**优势**：\n避免了频繁的分配和释放，尤其是在帧更新频繁的场景中。\n更高的缓存命中率。\n智能指针 vs arena\n虽然  确实在某些场景下具有显著优势，但它并不是万能的工具，也不适合所有场景。使用  需要权衡以下几个关键点：\n---\n1. **生命周期绑定的限制**\n 的核心理念是将分配的对象生命周期绑定到  自身。这带来了两个问题：\n**无法灵活控制对象的释放时间**：对象的内存只能在整个  被销毁时统一释放，无法提前释放单个对象。如果你的场景需要精细控制对象的生命周期（比如按需释放某些资源）， 就不合适。\n**容易造成内存浪费**：如果  存在一些对象长时间不被使用，而  本身又不能销毁，这些对象占用的内存会被浪费。\n---\n2. **对象不可移动**\n 分配的对象通常是不可移动的，因为它们存储在  的内存块中。这样会导致以下问题：\n不能直接使用一些需要所有权移动的特性（例如对象放入  或其他容器中）。\n必须使用引用来访问  分配的对象，可能会导致生命周期管理变得复杂。\n---\n3. **不适合需要线程安全的场景**\n大多数  实现（如 ）并不是线程安全的，如果需要跨线程共享对象，还需要额外的同步机制。这时，线程安全的智能指针（如 ）会更合适。\n**例外**： 支持跨线程，但需要特别小心使用。\n---\n4. **内存使用的不可预测性**\n由于  使用内存池进行分配，所有对象会一直占用内存直到  释放。这种方式可能导致内存占用的峰值比智能指针高，尤其是分配了很多临时对象但未及时销毁  时。\n---\n5. **缺乏细粒度的所有权管理**\n 的对象不具有 Rust 所强调的所有权语义（比如  和 ），这可能导致以下问题：\n**安全性降低**：如果程序逻辑复杂，可能出现逻辑错误（例如悬空引用）。\n**借用规则变得模糊**：你可能需要手动确保引用的正确性，而不是依赖编译器自动检查。\n---\n6. **性能优化是否真的必要**\n虽然  性能非常高，但它的优势只有在以下场景下才会显现：\n大量的频繁对象分配和释放。\n对象的生命周期高度统一。\n对于普通的业务逻辑（如 CRUD 应用），智能指针（如 、、）提供了更直观的代码语义和更安全的内存管理方式。\n---\n**什么时候选择智能指针而不是 Arena？**\n| 场景                              | 使用智能指针                  | 使用 Arena                     |\n|-----------------------------------|-------------------------------|---------------------------------|\n| **复杂生命周期管理**               | ✅ 可以用  或          | ❌ 生命周期过于统一可能不适用   |\n| **按需释放对象**                   | ✅ 可以释放单个对象             | ❌ 只能统一释放所有对象         |\n| **多线程环境**                     | ✅ 用                    | ❌ 多数 Arena 不支持线程安全    |\n| **高内存利用率（避免浪费）**        | ✅ 更细粒度释放内存             | ❌ 长期运行可能导致内存浪费     |\n| **轻量级分配和高性能需求**         | ❌ 频繁分配释放会增加开销       | ✅  高效分配与释放       |\n| **递归结构或大量小对象**           | ❌ 智能指针管理较为复杂         | ✅  简化生命周期和管理   |\n---\n总结\n 是一个强大的工具，但它的使用场景非常明确：\n**高性能需求**：大量对象的快速分配和释放。\n**生命周期统一**：所有对象的生命周期可以和  绑定。\n**复杂递归结构**：例如语法树、图等需要高效管理的结构。\n在大多数常见场景中（如 Web 应用、后端服务等），智能指针（、、 等）提供了更安全、更灵活的内存管理方案。选择是否使用  时，应该基于你的应用需求、生命周期复杂性以及性能瓶颈来决定。\n**结论**：**不要过度优化，除非确实需要。** 如果你的应用性能瓶颈明确在内存分配上，且生命周期统一， 才是最佳选择。否则，智能指针更符合 Rust 的所有权和安全性设计理念。\nArena 的原理简单实现\n 的原理可以简单理解为一次性分配一大块连续的内存，然后从这块内存中逐步划分小块来分配给对象。分配的对象不会单独释放，而是等到整个  生命周期结束时，统一回收整块内存。这种方法减少了频繁调用操作系统内存分配器的开销，从而提升了性能。\n下面通过一个简单的例子手动实现一个  的简化版本来说明其工作原理：\n---\n手动实现一个简单的 \n---\n运行过程的解析\n1. **初始化 **：\n创建一个大小为 1024 字节的内存池，用于存储分配的对象。\n2. **分配对象**：\n调用  方法时， 会检查是否有足够的剩余内存。\n它将对象写入内存池中的下一个空闲位置，同时满足类型的对齐要求。\n3. **访问对象**：\n返回的指针是对象在  中的地址，因此可以像普通引用一样访问它们。\n4. **释放内存**：\n对象的内存不会单独释放，只有在  的生命周期结束时，整个内存池会统一释放。\n---\n示例输出\n---\n原理总结\n**分配**：通过维护一个偏移量 ()，每次从内存池中分配指定大小的块。\n**对齐**：确保分配的内存地址符合对象的对齐要求。（**编译器会确保结构体的大小是它最大对齐要求的倍数**）\n**统一释放**： 本身的内存由  管理，当  被销毁时，所有分配的内存会自动释放。\n---\n为什么  高效？\n**减少内存分配调用**：传统分配器需要频繁调用系统分配内存，而  只需要一次性分配大块内存。\n**分配速度快**：分配内存只需简单地增加偏移量。\n**释放简单**：对象的内存无需单独回收，只需释放整个 。\n---\n这个简单的  演示了其核心思想。在实际应用中，比如使用  或  等库，内部实现会更复杂，处理对齐、更高效的内存管理等问题，但基本原理是相似的。\n内存对齐\n 结构体占用 **8 字节** 而不是 **7 字节**，是因为**内存对齐和填充 (padding)** 的原因。计算结构体大小时，编译器会确保结构体的大小是它最大对齐要求的倍数。(每种数据类型在内存中的地址必须是其对齐要求的倍数。)\n---\n回顾  的定义\n逐字段布局\n****\n占用地址 。\n起始地址是 0，符合对齐要求（4 的倍数）。\n****\n紧接着 ，理论上起始地址是 4，但需要满足  的对齐要求（2 的倍数）。\n地址 ，符合对齐要求。\n****\n紧接着 ，理论上起始地址是 6。\n地址 ，对  没有特殊对齐要求。\n**填充 (padding)**：\n由于  的最大对齐要求是 （由  决定），整个结构体的大小必须是  的倍数。\n当前结构体占用了  共 7 字节。\n为了满足对齐要求，编译器会在末尾填充 1 个字节，使总大小达到 8 字节。\n---\n为什么要填充？\n填充是为了让结构体的每个实例在数组或内存中都符合其对齐要求。如果不填充，数组的第一个元素对齐了，第二个元素可能会错位，从而导致性能问题或未定义行为。\n示例\n如果没有填充， 的大小是 7 字节，而不是 8 字节：\n第一个元素占用 。\n第二个元素的起始地址是 ，但  的对齐要求是 ，地址  不符合要求。\n这种情况下，程序会崩溃或性能受损。\n通过填充使结构体大小变为 8 字节：\n第一个元素占用 。\n第二个元素从地址  开始，满足  的对齐要求。\n---\n如何验证结构体大小和对齐？\n可以通过以下代码验证：\n---\n小结\n1. **最大对齐要求**： 的最大对齐要求是 （由  决定）。\n2. **结构体大小必须是最大对齐要求的倍数**：为了使结构体对齐，编译器在末尾填充 1 字节，使总大小变为 8。\n3. **原因**：填充的目的是为了保证结构体实例在数组中对齐，避免跨平台问题和性能下降。\n--- \n如果想手动优化内存布局，可以尝试调整字段顺序。例如：\n调整后，结构体的布局如下：\n-> 。\n-> 。\n-> 。\n此时  的大小仍然是 8 字节，但没有额外的填充。\nArena vs 动态数组区别\n当 Arena 的内存不足时，它不会自动扩容，而是通过新的大块内存来继续分配对象：Arena 会分配一块新的内存区域，原有的对象依然保留，新的对象被分配到新区域中。\n这和动态数组扩容时需要移动已有数据的方式不同。\nArena 是批量分配与集中释放，而动态数组是按需分配与按需释放。\n参考\ngpt",
    "url": "/blog/rust Arena内存分配",
    "path": "tech/backend/rust Arena内存分配.md",
    "lastModified": "2025-06-20T11:25:12.931Z"
  },
  {
    "title": "作用域规则",
    "content": "作用域规则\nRust 的作用域规则是其内存安全性的基础之一，主要围绕**所有权**、**借用**和**生命周期**来确保内存的管理安全且高效。以下通过几个简单例子说明 Rust 的作用域规则。\n所有权\n例 2：所有权的转移（Move）\n的所有权被转移给  后， 在作用域内被标记为无效。\n这种设计避免了两者指向同一内存而导致潜在的错误。\n---\n借用\n---\n例 3：借用（Borrowing）\n通过  借用一个变量，借用不会转移所有权。\n作用域结束后，借用结束，原变量仍然有效。\n---\n例 4：可变借用\n一个变量在同一时间只能有一个可变借用，避免数据竞争。\n可变借用结束后，变量才能被再次使用。\n---\n例 5：不可变和可变借用的冲突\nRust 禁止同时存在不可变借用和可变借用，防止并发修改的错误。\n---\n例 6：作用域嵌套解决借用冲突\n通过作用域嵌套，解决不可变借用和可变借用的冲突。\n---\n生命周期\n1. **基础概念：生命周期标注**\n生命周期（）表示引用的作用域，Rust 编译器通过生命周期标注检查引用是否有效。  \n示例：\n的生命周期只存在于内层作用域， 的生命周期更长。\nRust 编译器不允许引用超出被引用对象的作用域，避免悬垂指针。\n---\n2. **函数中的生命周期**\n当函数涉及引用参数时，需要显式标注生命周期，表示引用的有效范围。\n示例：显式标注生命周期\n**解读：**\n1.  表示  和  的生命周期有关联，并且返回值的生命周期与它们相同。\n2. 编译器确保  的生命周期不超过  和  中较短的那一个。\n---\n3. **生命周期不相同的情况**\n如果参数的生命周期不同，Rust 会限制返回值的引用。\n示例：不同生命周期导致错误\n---\n4. **结合结构体的生命周期**\n如果结构体包含引用，必须显式声明生命周期，表明其有效性。\n示例：结构体中的生命周期\n**解读：**\n约束  的生命周期，确保  的引用有效。\n---\n5. **静态生命周期（）**\n静态生命周期表示引用在程序整个运行期间都有效（例如，字符串字面值）。\n示例：静态生命周期\n---\n6. **省略规则（生命周期省略）**\n在某些情况下，编译器能自动推断生命周期，无需显式标注。\n示例：无需显式标注\n**推断规则：**\n1. 每个引用参数都分配一个生命周期。\n2. 如果只有一个引用参数，返回值与该参数共享生命周期。\n3. 如果有多个引用参数且返回值使用其中一个，编译器无法推断时需要显式标注。\n---\n7. **复杂例子：多个生命周期**\n当涉及多个引用参数时，需要明确区分生命周期。\n示例：多个生命周期标注\n**解读：**\n表示  和  可能有不同的生命周期。\n返回值不涉及引用，因而无需与  或  关联。\n---\n总结\n1. **生命周期的核心目标**：确保引用在有效范围内安全使用。\n2. **常用规则**：\n返回值的生命周期不能超过参数的生命周期。\n多个参数涉及时，明确生命周期关系非常重要。\n3. **场景分类**：\n函数参数与返回值。\n结构体中的引用。\n静态生命周期或复杂引用场景。\nRust 的生命周期机制虽然看起来复杂，但通过明确的规则和编译器检查，确保了内存安全。\n参考\nRust 所有权与生命周期\ngpt",
    "url": "/blog/rust 作用域规则",
    "path": "tech/backend/rust 作用域规则.md",
    "lastModified": "2025-06-20T11:25:12.932Z"
  },
  {
    "title": "参考",
    "content": "Rust 的智能指针是一种封装了指针行为的数据结构，它不仅能像普通指针那样访问数据，还附加了更多的功能，比如所有权管理、引用计数或内存自动回收。常见的智能指针有 、、 和 。下面通过具体例子讲解这些智能指针及其使用场景。\n智能指针\n2. : 单线程下的引用计数智能指针\n（Reference Counted）允许在单线程中多个所有者共享数据。它通过引用计数来跟踪数据的所有权。\n**使用场景**：\n当需要共享不可变数据，且数据的生命周期不明确时（如在图或树结构中多个节点共享数据）。\n---\n3. : 多线程下的引用计数智能指针\n（Atomic Reference Counted）是线程安全的引用计数智能指针，适合多线程环境。\n**使用场景**：\n当需要在多线程之间共享数据时使用。\n---\n4. : 单线程下的内部可变性\n 提供了一种“内部可变性”机制，即即使外部不可变，也能在运行时借用可变引用（动态检查）。\n**使用场景**：\n当你需要在不可变数据结构中修改数据，但无法使用可变引用时。\n**注意**： 只适用于单线程，且在运行时借用规则违反时会触发 panic。\n---\n组合使用示例\n在实际项目中，常常需要将这些智能指针组合使用。例如，使用  创建一个共享的、可变的树节点。\n---\n总结\n****：堆上存储单一所有者的数据。\n****：单线程下共享不可变数据。\n****：多线程下共享不可变数据。\n****：单线程中提供内部可变性。\n**组合使用**：例如  用于共享和修改数据。\n选择合适的智能指针取决于具体的需求：是否需要多线程支持？是否需要可变性？是否需要共享？根据这些因素进行决策即可。\n参考\ngpt",
    "url": "/blog/rust 智能指针与可变性",
    "path": "tech/backend/rust 智能指针与可变性.md",
    "lastModified": "2025-06-20T11:25:12.932Z"
  },
  {
    "title": "rust 的 trait",
    "content": "Rust 中的  是一种抽象机制，它定义了某些类型必须实现的**行为集合**。 的核心思想是**通过行为定义接口，并实现面向接口编程**，从而实现灵活性、可扩展性和类型安全。\n我们通过几个例子来讲解 Rust 中  的核心思想。\n核心思想 2：**静态分发**\nRust 中的  是**静态分发**的。这意味着编译器在编译时会根据具体类型生成特定的实现代码，从而避免运行时开销。\n示例：静态分发的打印功能\n**核心思想体现**：\n**静态分发**：编译器在  函数中内联了  的  方法，没有运行时成本。\n**泛型约束**：通过 ，确保传入的类型必须实现 。\n---\n核心思想 3：**动态分发**\n通过 ，我们可以在运行时动态决定使用哪个类型的实现。动态分发使用指针（如  或 ），并带有一定的运行时开销。\n示例：使用动态分发\n**核心思想体现**：\n**动态分发**：使用 ，使得程序在运行时通过虚表查找调用正确的方法。\n**扩展性**： 中可以容纳任意实现了  的类型。\n**灵活性与性能的权衡**：相比静态分发，动态分发更灵活，但有运行时开销。\n---\n核心思想 4：**组合与扩展**\nRust 的  支持组合和扩展，通过将多个  组合，或者定义继承关系，创建更强大的接口。\n示例：组合多个 \n**核心思想体现**：\n**组合**： 由  和  组合而成，定义了更多的行为。\n**扩展**：通过实现 ，自动继承了  和  的能力。\n---\n核心思想 5：**默认实现**\n 可以为方法提供默认实现，类型在实现  时可以选择覆盖这些默认实现。\n示例：默认实现的 \n**核心思想体现**：\n**默认实现**：减少了重复代码， 可以直接使用默认实现。\n**灵活覆盖**： 通过重载提供了自定义实现。\n---\n总结\nRust 中  的核心思想包括：\n1. **定义行为接口**：通过  明确类型必须实现的功能。\n2. **静态与动态分发**：提供零成本的静态分发，或支持灵活的动态分发。\n3. **组合与扩展**：通过组合或继承构建强大的接口。\n4. **默认实现**：为通用功能提供默认实现，减少重复代码。\n 是 Rust 实现类型安全、多态性和灵活扩展的关键机制，同时保持了性能的高效性。\n参考\ngpt",
    "url": "/blog/rust 的 trait",
    "path": "tech/backend/rust 的 trait.md",
    "lastModified": "2025-06-20T11:25:12.933Z"
  },
  {
    "title": "rust 的泛型",
    "content": "Rust 泛型的核心思想是 **“编写与类型无关的代码，同时保留类型的安全性和性能”**。通过泛型，我们可以实现代码复用，而不用牺牲 Rust 的零成本抽象原则。\n下面通过几个例子深入讲解这一思想。\n核心思想 2：**泛型 + trait 约束**\n泛型不仅可以表示任意类型，还可以通过**trait 约束**限定类型的行为。例如，我们可以要求某些泛型类型必须实现特定的接口。\n示例：实现一个比较函数\n我们定义一个函数，接受两个参数并返回较大的那个值。这个函数需要比较操作，只有实现了 （支持比较操作）的类型才能使用它：\n**核心思想体现**：\n**泛型与行为绑定**： 表示泛型  必须实现 ，这样函数中才可以安全地使用比较操作。\n**灵活性和安全性兼顾**：虽然  函数适用于任意类型，但通过  限制，避免了不支持比较的类型（如复杂对象）导致编译错误。\n---\n核心思想 3：**零成本抽象**\nRust 的泛型通过**单态化**，在编译时为每种具体类型生成特定实现，从而消除运行时的类型检查或多态分派。\n示例：通过反汇编观察泛型的单态化\n如果查看生成的汇编代码，会发现编译器生成了两个独立的实现：\n一个处理  的  函数。\n一个处理  的  函数。\n**结果**：\n泛型函数的性能等同于手动为每种类型编写函数。\n泛型提供了零成本的灵活性，无需运行时的开销。\n---\n核心思想 4：**组合与复用**\n泛型可以与其他 Rust 特性（如枚举、trait 等）结合，创建高度抽象、灵活的代码。\n示例：泛型与 \nRust 标准库的  就是一个泛型枚举，用于表示一个可能有值或无值的类型：\n**核心思想体现**：\n使用泛型 ，可以适用于任意类型。\n通过 ，我们可以表达 “值可能为空”的语义，而无需每种类型都重新设计逻辑。\n---\n总结\nRust 泛型的核心思想在于：\n1. **参数化类型**：将类型作为参数，使代码逻辑独立于具体类型。\n2. **类型安全**：通过 trait 约束明确泛型的行为边界。\n3. **零成本抽象**：在编译时单态化，既保留灵活性又无运行时开销。\n4. **高效复用**：与 Rust 的其他特性结合，简化常见模式的实现。\n泛型使 Rust 在灵活性、类型安全和性能之间取得了很好的平衡。\n参考\ngpt",
    "url": "/blog/rust 的泛型",
    "path": "tech/backend/rust 的泛型.md",
    "lastModified": "2025-06-20T11:25:12.933Z"
  },
  {
    "title": "rust 设计思想",
    "content": "设计理念\nRust 的设计理念贯穿其语言的核心特性，从安全性到性能，再到开发者体验，形成了一个强大且平衡的编程生态。\n0. **零成本抽象**\n泛型：编译器时单态化，编译器为每种具体类型生成特定实现，从而消除运行时的类型检查或多态分派。Rust 的泛型通过**单态化**，在编译时为每种具体类型生成特定实现，从而消除运行时的类型检查或多态分派。\ntrait：在泛型和 trait 静态分发的情况下，Rust 编译器会为每个具体类型生成对应的函数实现。这就是所谓的 单态化（monomorphization）。\n2. **并发安全（Fearless Concurrency）**\nRust 通过编译时检查，防止常见的并发错误（如数据竞争和死锁）。\n**设计理念**：通过类型系统在编译阶段确保并发代码的安全。\n**具体实现**：\n**线程安全的所有权**： 和  trait 定义了跨线程传递和共享的能力。\n**不变量保护**：通过锁（如 ）和原子操作（如 ），确保线程安全。\n**意义**：开发者可以编写高效的并发代码，而不必害怕数据竞争和其他隐晦的并发问题。\n**示例**：\n---\n3. **类型安全与表达性（Type Safety and Expressiveness）**\nRust 的类型系统通过静态检查，确保类型正确性，并为开发者提供丰富的表达能力。\n**设计理念**：类型系统不仅用于错误检测，还要提高代码可读性和表达力。\n**具体实现**：\n**枚举（Enum）和模式匹配（Pattern Matching）**：提供强大的分支逻辑控制。\n**泛型和 trait**：实现灵活的代码复用，同时保持类型安全。\n**Option 和 Result 类型**：避免空值和错误处理中的未定义行为。\n**意义**：减少运行时错误，提高代码的鲁棒性和可读性。\n**示例：Option 和模式匹配**：\n---\n4. **性能优先（Performance Focused）**\nRust 的性能接近 C/C++，但不以安全性为代价。\n**设计理念**：提供接近硬件级的性能，同时保持内存和线程安全。\n**具体实现**：\n**无垃圾回收**：通过所有权系统管理内存，避免 GC 带来的延迟。\n**高效的并发模型**：通过零开销抽象，允许开发者实现高性能并发。\n**LLVM 优化**：Rust 使用 LLVM 编译器后端生成高效机器代码。\n**意义**：适合系统级开发和性能敏感的场景。\n---\n5. **开发者体验（Developer Experience）**\nRust 通过工具和设计降低了复杂语言的学习成本和开发门槛。\n**设计理念**：即使是复杂的语言，也应尽可能友好。\n**具体实现**：\n**友好的编译器错误信息**：详细的错误信息帮助开发者快速定位问题。\n**Cargo 工具链**：集成包管理、构建、测试和发布功能。\n**社区驱动生态**：官方文档和社区资源齐全。\n**意义**：帮助开发者以更低的心智负担编写高质量代码。\n**示例：编译器的友好提示**：\n**错误信息**：\n---\n6. **模块化与代码复用（Modularity and Reusability）**\nRust 提供了强大的模块系统和依赖管理工具，方便开发者组织代码和复用现有的解决方案。\n**设计理念**：模块化设计提高可维护性和复用性。\n**具体实现**：\n**模块和包管理**：通过模块和  系统实现代码组织。\n**Cargo**：标准化的包管理器，提供统一的依赖管理和版本控制。\n**trait 和泛型**：实现灵活的代码复用。\n**意义**：减少重复代码，提高团队协作效率。\n---\n7. **安全与灵活的错误处理（Error Handling）**\nRust 强调显式的错误处理，避免隐式失败。\n**设计理念**：通过类型系统强制处理错误，提升程序可靠性。\n**具体实现**：\n**Result 类型**：通过  和  明确表示操作成功或失败。\n** 操作符**：简化错误传播。\n**panic! 和不可恢复错误**：用于处理极端情况。\n**意义**：提高错误处理的可读性和安全性。\n**示例：Result 和  操作符**：\n---\n总结\nRust 的核心设计理念是多个方面的综合平衡，它们共同构成了 Rust 的独特性：\n1. **零成本抽象**：高效且灵活的语言特性。\n2. **内存安全**：无 GC 的情况下杜绝内存错误。\n3. **并发安全**：让多线程编程更安全、更高效。\n4. **类型安全与表达性**：静态类型系统提升可靠性和开发体验。\n5. **性能优先**：接近 C/C++ 的性能表现。\n6. **开发者体验**：友好的编译器和工具链。\n7. **模块化与代码复用**：强大的模块和依赖管理。\n8. **显式错误处理**：类型驱动的错误处理机制。\n这些理念让 Rust 成为一门兼具安全性、性能和开发效率的现代系统级编程语言。\n参考\ngpt",
    "url": "/blog/rust 设计思想",
    "path": "tech/backend/rust 设计思想.md",
    "lastModified": "2025-06-20T11:25:12.933Z"
  },
  {
    "title": "rust 跨语言调用",
    "content": "rust 与 c 的调用\nRust 能与 C 交互的原因在于它和 C 都可以遵循 C ABI（Application Binary Interface，应用二进制接口）。C ABI 是一种语言无关的标准，规定了函数调用、参数传递、内存对齐等机制，确保不同语言生成的二进制代码能够互操作。\n**一个超简单的例子：让 Rust 调用 C 函数**\n我们先写一个 C 函数，然后用 Rust 调用它。\n**第一步：写 C 的代码**\n然后我们用  把这个 C 文件编译成一个共享库（比如  或 ）：\n**第二步：写 Rust 的代码调用这个 C 函数**\n**运行步骤：**\n1. 用 Rust 编译运行这个文件时，告诉它要链接 （动态库）：\n   输出结果为：。\n---\n**拆解这个过程**\n1. **C 编译器的作用**：\nC 编译器把  函数编译成机器码，并存到共享库里（）。\n这个共享库对外暴露了函数  的地址和调用方式。\n2. **Rust 的作用**：\nRust 通过  语法，告诉编译器：我想调用一个遵循 C ABI 的函数。\nRust 的编译器会在运行时找到这个共享库，并调用  函数。\n3. **C ABI 的作用**：\n确保函数的参数和返回值按约定的方式传递。比如：\n参数  和  被放在固定的寄存器里（或者栈上）。\n返回值被放在另一个固定位置。\n---\n**反过来：让 C 调用 Rust 函数**\n这次我们写一个 Rust 函数给 C 调用。\n**第一步：写 Rust 的代码**\n然后编译成共享库：\n**第二步：写 C 的代码调用这个 Rust 函数**\n**运行步骤：**\n1. 编译 C 代码并链接 Rust 的共享库：\n   输出结果为：。\n---\n**总结：为什么能交互？**\n1. **Rust 并不是依赖 C，而是遵循 C 的规则（C ABI）。**\n关键字就是告诉 Rust 编译器，函数调用要遵循 C 的规则。\nC ABI 是一个约定，规定了参数和返回值怎么传递、内存怎么对齐等。\n2. **Rust 和 C 是“直接用机器语言对话”的，不需要翻译成 C 代码。**\nRust 和 C 的交互只发生在编译后的机器代码层。\nRust 编译后的共享库（）和 C 编译的可执行程序用相同的协议调用函数。\n3. **Rust 通过 FFI 提供了跟其他语言交互的能力：**\n只要其他语言也遵循 C ABI，比如 Python 的  或 JavaScript 的 ，它们都能和 Rust 互操作。\n这个例子可以看出，C 和 Rust 的交互是靠 ABI 约定，而不是中间语言或运行时的“翻译”。\n---\nrust 与 js 的调用\n**初步理解：napi-rs 将 rust 代码编译成了 N-API 协议能够识别的 .node 文件，所以  js 能够直接加载，如果是 rust 编译出的  .so 文件（遵循 C-ABI ） 则需要 C或者 C++ 编写胶水语言 做成符合 N-API 的 .node 文件，然后 才可以被 js 调用？基于 N-API：注重 跨平台性 和 稳定性，适合长期维护的项目。**\n其他调用方式：\nrust -> .wasm（跨平台，可以 node 跟浏览器环境，比 .node 性能稍低） -> js\nneon （针对 v8 引擎，跳过 NAPI 抽象层，性能比 napi-rs 转换的代码更高效，但是可维护性也更差），基于 V8 深度绑定：追求 性能极致 和 灵活定制，适合对底层优化要求极高的项目。\n---\n**1. 使用 N-API（如 ）编译出的  文件**\n**关键点**：Rust 代码直接编译成符合 **N-API 标准** 的  文件。\n**工作流程**：\n  1. Rust 使用  或  库，通过 N-API 暴露函数。\n  2.  在编译过程中自动生成符合 N-API 标准的动态库（ 文件）。\n  3. Node.js 加载这个  文件，直接调用 Rust 的函数。\n**特点**：这一过程完全自动化，不需要手写 C/C++ 胶水代码。Rust 代码编译后直接可以被 Node.js 使用，开发体验优良。\n---\n**2. 如果只有 Rust 编译出的  文件**\n**关键点**： 文件是标准的 C ABI 动态库格式，JavaScript 环境（如 Node.js）本身不支持直接加载 C ABI 的库。\n**问题**：\nJavaScript 和  文件之间缺少直接的桥梁。\n需要手写胶水代码，用 C 或 C++ 把  封装成 N-API 模块（ 文件）。\n**手写胶水代码的流程：**\n1. **Rust 编译出  文件**：\n使用  类型的  编译出 C ABI 动态库：\n2. **用 C/C++ 编写 N-API 模块**：\n使用 C/C++ 代码封装  文件的函数为 N-API 模块：\n3. **编译 C++ 胶水代码成  文件**：\n使用  或其他工具链编译生成  文件：\n**特点**：这一过程繁琐，需要编写和维护额外的 C/C++ 代码。\n---\n**3. 两种方式对比**\n| **特点**                  | **使用 **                                      | **使用  + C/C++ 胶水代码**                      |\n|--------------------------|--------------------------------------------------------|----------------------------------------------------|\n| **开发效率**             | 高：无需手写胶水代码，Rust 直接编译成  文件       | 低：需要手动编写 C/C++ 胶水代码封装  文件      |\n| **依赖性**               | 只需 ，完全用 Rust 开发                            | Rust + C/C++，需要额外工具链                       |\n| **可维护性**             | 高：Rust 和 Node.js 开发者可以直接维护                   | 低：Rust 和 C/C++ 开发需要协作                     |\n| **适用场景**             | 适合绝大多数 Node.js 插件开发场景                       | 适合已有  文件需要快速封装到 N-API 的场景      |\n| **性能**                 | 性能优良，直接生成 N-API 模块                           | 性能同样优良，但手工编写代码可能增加维护成本        |\n---\n**总结**\n如果你的目标是为 Node.js 开发插件，使用 Rust 的 **N-API 工具链**（如 ）是最佳选择，它能让你直接生成  文件，省去了手动编写 C/C++ 胶水代码的麻烦。\n如果你手头已有  文件（比如是用其他语言编写的库），需要编写 C/C++ 胶水代码将其封装成  文件，才能被 Node.js 加载和使用。\n最终，**N-API 是核心桥梁**，Rust 和 C/C++ 都是为了生成能符合 N-API 的模块。\n参考资料\ngpt",
    "url": "/blog/rust 跨语言调用",
    "path": "tech/backend/rust 跨语言调用.md",
    "lastModified": "2025-06-20T11:25:12.934Z"
  },
  {
    "title": "关于node的addon",
    "content": "关于node的addon\nNode.js 对 native addon 开发侧暴露的是 ABI（In computer software, an application binary interface (ABI) is an interface between two binary program modules）\nAddon方案演化\n***以下是摘抄，还有待琢磨原理***\nNAN（Native Abstractions for Node.js）\nNAN 将 v8/libuv 相关的 API 进行了封装，对外是稳定的抽象层 API (但却无法保证是 ABI 稳定)。用 NAN 进行封装的 native addon 几乎无法通过预编译的方式进行分发，因为跨 Node 版本底层 v8/libuv API 变化之后需要对源码进行重新编译才能使用。所以这就是为什么很多 native addon 在 npm install 后还要调用一堆工具链在本地进行编译才能使用，以及为什么有时候 node 版本升级之后之前安装好的 node_modules 就无法直接使用了。\n一句话理解：V8底层的封装，估计需要理解V8才能开发\nN-API\n自从 Node.js v8.0.0 发布之后，Node.js 推出了全新的用于开发 C++ 原生模块的接口，N-API。本质其实是将 NAN 这层抽象挪到了 node 源码中，在 node 编译的时候就编译好这层对外抽象，这样 N-API 对外就是稳定的 ABI 了。\n一句话理解：v8之上node层的封装，跟随Node编译，独立于操作系统\n应用\nC++扩展\n基本步骤：\n./demo.cc\n模块配置跟编译\n./binding.gyp\n生成.node文件（动态链接库）\n调用.node文件\n思考\nnode扩展方式\n插件开发\nWebAssembly\n哪种更快些？或者分别适用于哪些场景？\nWasm被保证与可以运行您的Electron应用程序的任何操作系统兼容。\nC，C++和Rust将没有此保证。除了从JavaScript调用的开销之外，它们将更加高效。\n正在积极地处理从JavaScript调用Wasm的开销，并且正在减少或完全消除这种开销。\nReference\nN-API入门\nRust bindings for writing safe and fast native Node.js modules.neon\n用 Rust 和 N-API 开发高性能 Node.js 扩展",
    "url": "/blog/关于node的addon",
    "path": "tech/backend/关于node的addon.md",
    "lastModified": "2025-06-20T11:25:12.934Z"
  },
  {
    "title": "Node内存分析",
    "content": "Node内存分析\n长时间（一般是几天时间）一条内存只增不减的曲线就标志着内存泄露了\n内存泄露分析基本步骤\n0. node引入 require('heapdump')\n1. 启动node, eg: npm run start\n3. lsof -i:8080\n4. kill -USR2 <pid>（第三步查看到的pid）\n5. 根据情况调整压测参数 autocannon -c 10 -a 10 http://localhost:8080\n6. 重复执行3次步骤 4，5\n7. 浏览器打开控制台memory栏目，按照生成顺序载入生产的内存快照  heapdump开头的文件\nReference \nhttps://marmelab.com/blog/2018/04/03/how-to-track-and-fix-memory-leak-with-nodejs.html\nhttps://github.com/aliyun-node/Node.js-Troubleshooting-Guide/blob/master/0x03_%E5%B7%A5%E5%85%B7%E7%AF%87_%E6%AD%A3%E7%A1%AE%E6%89%93%E5%BC%80%20Chrome%20devtools.md",
    "url": "/blog/内存分析",
    "path": "tech/backend/内存分析.md",
    "lastModified": "2025-06-20T11:25:12.934Z"
  },
  {
    "title": "列存储",
    "content": "什么是列存储？跟传统存储啥区别？为啥列存储能更加快速响应复杂查询？\n列存储（Columnar Storage）是一种数据存储方式，与传统的行存储（Row Storage）有显著区别。列存储在处理分析型工作负载时，尤其是涉及复杂查询的情况下，表现得更加高效。\n列存储（Columnar Storage）\n在列存储中，数据按列存储，而不是按行存储。每一列的数据存储在一起，所有同一列的数据都连续存储在磁盘上。例如，对于一个有四个列（A、B、C、D）的表，数据将按如下方式存储：\n行存储（Row Storage）\n在传统的行存储中，数据按行存储，每一行的数据存储在一起。对于同样的四列表，数据将按如下方式存储：\n区别\n1. **数据存储方式**\n**行存储**：所有列的值按行存储在一起。\n**列存储**：同一列的值按列存储在一起。\n2. **适用场景**\n**行存储**：适用于OLTP（在线事务处理）工作负载，例如频繁的插入、更新和删除操作。\n**列存储**：适用于OLAP（在线分析处理）工作负载，例如复杂的查询和报表分析。\n列存储响应复杂查询更快速的原因\n1. **查询效率**\n在分析型查询中，通常只涉及少量的列。例如，计算某个统计数据只需访问特定的列。列存储仅读取需要的列数据，减少了I/O操作。\n行存储则需要读取整行数据，即使只需要其中一两列，也要读取整行，导致不必要的I/O开销。\n2. **数据压缩**\n列存储的同一列数据类型相同且经常有相似的值，压缩效果更好。例如，一个包含许多重复值的列可以通过RLE（Run-Length Encoding）等压缩算法显著减少存储空间。\n行存储的数据类型混杂在一起，压缩效率低下。\n3. **向量化处理**\n列存储系统可以利用向量化处理技术，同时处理一批列数据，提高CPU使用效率。\n行存储处理每行数据时需要多次加载和存储，效率较低。\n4. **缓存利用**\n由于列存储按列连续存储，缓存命中率高，减少了对磁盘的访问。\n行存储按行存储，缓存命中率较低，需要频繁访问磁盘。\n总结\n列存储通过优化I/O操作、提高压缩效率、利用向量化处理和更好的缓存利用，显著提高了复杂查询的响应速度。因此，它特别适用于需要进行大量读取和分析的场景，如数据仓库、商业智能和实时分析系统。\n示例\n假设有一个大型销售数据表，包含日期、产品ID、销售数量和销售额四个字段：\n**行存储：**\n**列存储：**\n查询总销售额时，列存储只需读取“销售额”列的数据，而行存储则需读取每一行的所有字段，明显增加了I/O开销和处理时间。这就是列存储在复杂查询中表现更高效的原因。",
    "url": "/blog/列存储",
    "path": "tech/backend/列存储.md",
    "lastModified": "2025-06-20T11:25:12.934Z"
  },
  {
    "title": "性能分析",
    "content": "性能分析\n最近通过 alinode 的CPU火焰图找出了项目中吞吐率阻塞点，简单分享\nprofile的原理？\n如何对profile结果做可视化？\n火焰图如何分析？\n如何development/production进行profile\nChrome Profiler运行机制\nChrome profiler 为了找到那些耗时最多的代码，Chrome 分析器每 100μs 捕获一个堆栈跟踪。\n这意味着，如果一个函数只需要 50μs 的执行时间，就可能不会在分析器中显示出来！\n当你分析几毫秒以上的时间时，可以准确了解应用程序在何时花费最多的时间。 但是，当你放大 profiler 面板想看更精准的时间时，信息会变得不太准确。\n分析器也会不一致。 每次运行时，会产生一个稍微不同的结果。 有时可能会记录非常短的函数调用，而在其他时间再次运行这些函数调用信息可能会丢失。\nperformance\nScripting\nRendering\nPainting\nOther\nIdle\njavascript-profiler\nCPU Profiles\ncpu profile种类\n采样 Sampling\n基于对StackTrace的“采样”进行实现，在一时间段内（至少得5min保证样本数足够多）对堆栈做快照采样\n优点: \n>简单，源代码侵入性不大\n劣势: \n>采样数据主要体现在调用次数上，对CPU的占有时间不够精准\n要求：\n>1. 样本必须足够多。\n>2. 程序中所有正在运行的代码点都必须以相同的概率被Profiler采样。\n>>JVM 只能在safepoint采样,是否就违背了第二条原则？最终导致profile不太准\n适合场景\n>Sampling由于低开销的特性，更适合用在CPU密集型的应用中，以及不可接受大量性能开销的线上服务中。\n>也是一般Profiler的实现机制，典型的就是alinode\nInstrumentation \nInstrumentation方式对几乎所有方法添加了额外的AOP（Aspect Oriented Programming 中文翻译为面向切面编程）逻辑，这会导致对线上服务造成巨额的性能影响\n这个有点像函数埋点\n优点\n>绝对精准的方法调用次数、调用时间统计。\n缺点\n>性能消耗比较大，侵入性也比较高\n适合场景\n>Instrumentation则更适合用在I/O密集的应用中、对性能开销不敏感以及确实需要精确统计的场景中。\n火焰图\n火焰图统计的是次数，所以属于CPU采样类型\n火焰图svg\n火焰图含义\nsampling\n>y 轴表示调用栈，每一层都是一个函数。调用栈越深，火焰就越高，顶部就是正在执行的函数，下方都是它的父函数。\n>x 轴表示抽样数，如果一个函数在 x 轴占据的宽度越宽，就表示它被抽到的次数多，即执行的时间长。注意，x 轴不代表时间，而是所有的调用栈合并后，按字母顺序排列的。\n**火焰图就是看顶层的哪个函数占据的宽度最大。只要有\"平顶\"（plateaus），就表示该函数可能存在性能问题。**\n火焰图示例\n上面代码中，start_thread是启动线程，调用了func_a。后者又调用了func_b和func_d，而func_b又调用了func_c。\n合并后\n有了这个调用栈，统计工具就能生成对应的图，灵魂火焰图\n<!-- \n| func_c | a |\n| --- | --- |\n| func_d | func_b |\n| func_a | a |\n| start_thread | a | -->\nProfile实践\n开发测试阶段\n目前v8-profiler在 node12的时候安装构建失败，node12版本自带的node-gypbuild不成功；\n替代方案是选用 v8-profiler-next，步骤：\n1. npm i -D v8-profiler-next\n2. 参照v8-profiler-next文档，在dev启动5分钟左右\n3. 压测（例如：autocannon -c 10 -d 300 -p 1 http://localhost:8080）\n4. 导出结果到chrome就可以看到具体的 cpu占用情况\n在chrome 调试工具里找到 Javascript Profiler即可导入 xx.cpuprofile 文件进行分析，有百分比跟倒立火焰图\n产线\n目前alinode用起来还比较流畅，如果使用的是Docker部署，可以走以下流畅\n1. 直接使用alinode镜像构建自己的dockerfile **（为了降低Docker镜像的大小，node12 对应的alinode版本看起来基于alpine linux，所以有些需要注意，例如apt需要替换apk）**\n2. 按照官方文档创建 app-config.json 文件\n3. 部署上线，然后进行 cpu在线Profile，转储后在线分析（支持火焰图跟dev tool模式）\n感兴趣的可以参考这里\nDemo\n1. 跳转到 Test 路由\n2. 点击 cpu-profiling\n3. 打开console，点击Javascript Profiler，点击record，6秒后点击stop\n4. 再次点击 cpu-profiling\n更多精细demo, 本地运行这个工程，修改pages/test/use-test.ts里面的参数，再次走上面流程，看下100微秒的捕获丢失效果，以及最小的时间单元是否是 0.1毫秒（也就是100微秒）\nReference\nhttps://github.com/brendangregg/FlameGraph\nhttps://www.mattzeunert.com/2016/08/06/show-native-functions-in-js-profile.html\nhttps://nodesource.com/blog/diagnostics-in-NodeJS-2\nhttps://www.ruanyifeng.com/blog/2017/09/flame-graph.html\nhttps://tech.meituan.com/2019/10/10/jvm-cpu-profiler.html\nhttps://www.jianshu.com/p/c79c5e02ebe6\n<u>2021-7-28</u>",
    "url": "/blog/性能分析",
    "path": "tech/backend/性能分析.md",
    "lastModified": "2025-06-20T11:25:12.934Z"
  },
  {
    "title": "理解 rust 无 GC",
    "content": "自己的理解：**rust 无 GC应该指的是没有集中式的 GC，而是分散式的，随着代码的执行而自动的 回收？**\nRust 中没有传统意义上的 **集中式垃圾回收（Garbage Collector, GC）**，而是通过 **分散式的、编译时静态分析和运行时的引用计数机制** 来管理内存。这种方式避免了传统 GC 的集中式工作方式，回收是随着代码逻辑执行自然发生的。\n**没有集中式 GC 的好处**\n**性能更高**：  \n没有 GC 暂停（GC Pause），因此实时性较好。\n在作用域结束时立即释放内存，不需要等待垃圾回收器触发。\n**内存管理可控**：  \n开发者通过所有权和借用机制对内存管理有更多的明确控制。\n**更小的运行时开销**：  \nRust 没有语言级别的垃圾回收器，这减少了程序运行时的内存和 CPU 开销。\n---\n**分散式回收**\nRust 的内存回收不是集中管理，而是随着代码执行逐步完成的：\n编译时：分析作用域和所有权，静态确定释放时机。\n运行时：通过引用计数（如  和 ）动态管理共享数据。\n**例子：**\n---\n**与传统 GC 的对比**\n| 特性                  | Rust                     | 传统 GC (如 Java, Go)         |\n|-----------------------|--------------------------|-------------------------------|\n| **回收触发时机**      | 编译时分析，运行时即时回收 | 运行时批量回收（触发时暂停）  |\n| **运行时开销**        | 无集中管理，开销更小      | 需跟踪对象、回收开销较高      |\n| **暂停问题**          | 无 GC 暂停，实时性更好    | 存在 GC 暂停问题              |\n| **内存安全性**        | 编译时强保证             | 运行时依赖语言特性和检查机制 |\n---\n**总结**\nRust 的内存管理依赖 **所有权机制** 和 **引用计数（如果需要）**，在代码执行的同时完成内存的分散式回收，不依赖集中式 GC。它避免了传统 GC 的性能问题，同时提供了强大的内存安全性。\nReference\ngpt",
    "url": "/blog/理解 rust 无 GC",
    "path": "tech/backend/理解 rust 无 GC.md",
    "lastModified": "2025-06-20T11:25:12.935Z"
  },
  {
    "title": "理解并发",
    "content": "如何理解node并发\n同步阻塞\n例如：计算一个\n对于CPU计算类型的，会阻塞单线程\n异步非阻塞\n例如： request请求，数据库处理等异步I/O\n本质上node还是为异步I/O创建了新的线程，例如：对于数据库的非阻塞异步操作，实际上是开启了独立的数据库操作线程去处理，然后把结果返回给主线程\n形象的比喻\n单线程模型（Nodejs）:\n服务员A处理不同顾客的点单请求,多个I/O厨师准备多个订单上的菜肴->然后服务员A端给不同顾客；\n多线程模型（例如GO）:\n里面有许多个厨师，每个厨师的一条龙服务：处理顾客的点单请求->准备菜肴->然后端给顾客\nReference\nhttps://stackoverflow.com/questions/34855352/how-in-general-does-node-js-handle-10-000-concurrent-requests",
    "url": "/blog/理解并发",
    "path": "tech/backend/理解并发.md",
    "lastModified": "2025-06-20T11:25:12.935Z"
  },
  {
    "title": "Docker 镜像",
    "content": "Docker 镜像\n层次关系\nDocker 镜像的层次关系原理基于联合文件系统（Union File System），这种文件系统允许多个文件系统层次叠加在一起，表现为一个单一的文件系统。Docker 利用这一特性来构建和管理镜像和容器。\n联合文件系统（Union File System）\n联合文件系统（如 AUFS、OverlayFS、btrfs 等）是 Docker 的核心技术之一。它允许多个目录（称为层）叠加在一起，使其看起来像一个单一的目录。每一层都是只读的，只有最顶层是可写的。\nDocker 镜像层次结构\nDocker 镜像由多个只读层组成，每一层对应 Dockerfile 中的一条指令。这些层是增量的，即每一层只包含相对于前一层的变化。镜像层次结构的基本原理如下：\n1. **基础镜像层**：这是镜像的最底层，通常是一个操作系统的基础镜像（如 、、 等）。它提供了运行应用所需的基本环境。\n2. **中间层**：每一条 Dockerfile 指令（如 、、 等）都会创建一个新的层。每个中间层只包含相对于前一层的变化。例如，安装软件包、复制文件等。\n3. **顶层**：这是镜像的最顶层，通常包含应用程序的代码和配置文件。\n容器层次结构\n当你启动一个容器时，Docker 会在镜像的基础上添加一个可写层，这个可写层称为容器层。容器层是唯一可写的层，所有对文件系统的修改（如创建新文件、修改现有文件等）都会发生在这一层。\n分层存储的优点\n1. **高效存储**：由于每一层是只读的，并且是增量的，Docker 可以通过共享相同的基础层来节省存储空间。例如，多个基于同一基础镜像的容器可以共享该基础镜像的层。\n2. **快速构建**：构建镜像时，如果某一层已经存在且没有变化，Docker 可以直接重用该层，而不需要重新构建。这大大加快了镜像的构建速度。\n3. **缓存机制**：Docker 使用缓存机制来加速镜像的构建和部署。如果某一层已经存在于缓存中，并且没有变化，Docker 可以直接从缓存中获取该层，而不需要重新执行指令。\n示例：Docker 镜像和容器的层次关系\n假设我们有如下的 Dockerfile：\n构建镜像时，每一条指令都会创建一个新的层：\n1. **基础镜像层**：\n2. **工作目录层**：\n3. **复制依赖文件层**：\n4. **安装依赖层**：\n5. **复制应用程序代码层**：\n6. **暴露端口层**：\n7. **运行应用程序层**：\n当你运行容器时，Docker 会在这些只读层的基础上添加一个可写层。所有对文件系统的修改都会发生在这个可写层中。\n联合文件系统（Union File System）\n联合文件系统（Union File System）如何允许多个文件系统层次叠加在一起，表现为一个单一的文件系统。例子：\n假设我们有三个不同的目录，每个目录代表一个文件系统层：\n1. **Layer 1**：包含基本的操作系统文件\n2. **Layer 2**：包含一些应用程序文件\n3. **Layer 3**：包含用户的数据文件，容器运行时的**可写层**\n目录结构\n**Layer 1**（基础操作系统层）\n**Layer 2**（应用程序层）\n**Layer 3**（用户数据层）\n叠加后的联合文件系统\n当我们使用联合文件系统将这三个层叠加在一起时，它们会表现为一个单一的文件系统：\n具体例子：Docker 镜像和容器\n让我们将这个概念应用到 Docker 镜像和容器上：\n1. **基础镜像（Layer 1）**：假设我们使用一个 Ubuntu 基础镜像，它包含了操作系统的基本文件。\n2. **应用程序层（Layer 2）**：我们在基础镜像上安装了一个应用程序，这会创建一个新的层。\n3. **用户数据层（Layer 3）**：容器运行时，用户在容器内创建或修改文件，这些变化会记录在一个可写层中。\n叠加后的文件系统\n当我们启动容器时，Docker 会将这些层叠加在一起，表现为一个单一的文件系统：\n用户在容器内看到的文件系统就像是一个完整的文件系统，尽管它实际上是由多个层叠加而成的。\n写时复制\n在写时复制机制（Copy-on-Write, CoW）下，可能存在两份  文件，一份在只读层（lower layer），一份在可写层（upper layer）。但是，从文件系统的视图来看，用户只会看到一份文件。具体来说，这样的设计是为了在保证只读层数据不变的同时允许用户修改文件。以下是更详细的解释：\n联合文件系统视图\n联合文件系统通过叠加多个层来形成一个合并视图，用户只会看到合并后的文件系统视图，而不是底层的具体实现。\n文件叠加机制\n1. **查找文件**：\n当用户访问  文件时，联合文件系统首先在可写层（upper layer）中查找。\n如果可写层中找不到该文件，则继续查找只读层（lower layer）。\n2. **写时复制过程**：\n如果用户对  文件进行修改，并且该文件在只读层中存在（可写层中不存在），则联合文件系统会将该文件从只读层复制到可写层。\n所有的修改操作都会在可写层中的副本上进行。\n确保数据完整性\n这种机制确保了只读层的文件不会被直接修改，从而保持了其完整性。同时，用户对文件的修改操作仍然可以进行，只是这些修改会反映在可写层中。\n实际示例\n假设有一个联合文件系统，其中只读层和可写层的初始状态如下：\n**只读层（lower layer）**：\n**可写层（upper layer）**：\n当用户修改  文件时，文件系统执行以下步骤：\n1. **查找**：\n在可写层中查找 ，未找到。\n在只读层中查找 ，找到该文件。\n2. **复制**：\n将  文件从只读层复制到可写层。\n3. **修改**：\n在可写层中的副本上进行修改。\n修改后的文件系统层次结构如下：\n**只读层（lower layer）**：\n**可写层（upper layer）**：\n用户视图\n用户通过文件系统看到的合并视图如下：\n从用户的角度来看，他们看到的只是一个文件系统，其中  文件已被修改，而不会意识到底层存在两份  文件。\n具体应用\n**Docker容器**：Docker容器的文件系统利用联合文件系统叠加多个只读层（来自Docker镜像的各层）和一个可写层（容器的读写层）。容器启动后，所有对文件的修改都在可写层进行，而镜像层保持不变。\n**Linux Live CD**：Live CD系统通常是只读的，但用户可以在会话期间对文件进行修改。这些修改被写入到一个临时的可写层（如RAM），原始的CD内容不变。\n总结\n在联合文件系统中，通过写时复制机制，文件系统可以在只读层中保持文件的原始状态不变，同时允许用户对文件进行修改。这种机制在保持数据完整性和一致性的同时，提供了灵活的文件管理能力。虽然底层可能存在两份文件（原始和副本），但用户看到的是一个统一的、合并的文件系统视图。",
    "url": "/blog/Docker 的理解",
    "path": "tech/frontend/Docker 的理解.md",
    "lastModified": "2025-06-20T11:25:12.946Z"
  },
  {
    "title": "Vue SSR",
    "content": "Vue SSR\nssr hydrate 原理\n基本猜想： \n1. 按照层级匹配，只在不匹配的时候报错，层次正确即可\n2. 只在data-server-rendered=\"true\"的时候hydrate，随后移除data-server-rendered\nserver.html\nwill match client\nbut not match client\n协同的库\nvue-lazy-hydrate原理\n服务端：\n1. hydrate=true -> 根据component 渲染出来的正常 html\n客户端：\n将组件封装成 async component，在条件成熟的时候resolve（浏览器空闲/dom可见/事件触发）\n结论\n原理上不是真实意义上的hydrate，而是异步组件的渲染；\n所以如果一个组件在转换成 async的时候会出问题（例如：会有跟激活的组件的交互），那就不适用这个package\nvue-client-only原理\n原理：vue-client-only 组件的 render 函数简单封装\n服务端返回h(false) 占位\n客户端：根据parent._isMounted 返回正常组件 （在浏览器mounted的时候）\n坑\nin NuxtJs框架\nin dev : true(代表plugin middleware等文件会在每个请求进入的时候重新载入)\nin production: false （代表不会重新载入plugin，只会反复执行返回的函数）\nSSR runInNewContext demo\n1. bundle code\n./vue-ssr.js\nreference\nnode vm\nnode module\nnode require解析\nvue ssr in depth\nvue ssr in depth2",
    "url": "/blog/SSR原理浅析",
    "path": "tech/frontend/SSR原理浅析.md",
    "lastModified": "2025-06-20T11:25:12.947Z"
  },
  {
    "title": "vite/webpack 之 vue ssr 图解",
    "content": "vite/webpack 之 vue ssr 图解\nwebpack + vue ssr图解\n!webpack+vue ssr图解\nvite + vue ssr图解\n!vite + vue ssr图解\nssr框架 webpack + vue ssr图解\n!ssr框架 webpack + vue ssr图解\nssr框架 vite + vue ssr图解\n!ssr框架 vite + vue ssr图解",
    "url": "/blog/SSR工程结构图解",
    "path": "tech/frontend/SSR工程结构图解.md",
    "lastModified": "2025-06-20T11:25:12.947Z"
  },
  {
    "title": "Float32Array 应用场景",
    "content": "Float32Array 应用场景\n 是 JavaScript 中的一种类型数组，它代表平台字节顺序为 32 位的浮点数型数组（对应于 C 浮点数据类型）。让我们探讨一下  的应用场景和特性：\n1. **数值计算和底层二进制数据处理**：\n存储的是单精度浮点数，每个元素占据 4 个字节。这使得它在处理大量数值计算时更加节省内存。\n连续的内存空间存储使得在进行数学运算时效率更高，从而提高性能.\n2. **音频和图像处理**：\n在音频处理中， 可以存储音频样本数据，例如音频波形、频谱分析等。\n图像处理中的像素值也可以使用  存储（利用其高精度），例如图像滤波、变换等。\n3. **WebGL 和 WebGPU**：\n在图形渲染领域非常重要。WebGL 和 WebGPU 使用  来处理图形数据，例如顶点坐标、纹理坐标、颜色等。\n在这些图形库中，使用  可以高效地传递数据给 GPU 进行并行计算和渲染。\n4. **二进制数据传输和解析**：\n可以用于处理二进制数据，例如网络传输中的数据包、文件解析等。\n通过  和 ，可以将二进制数据转换为 ，并进行解析。\nFloat32Array 优势\n当然可以。 的优势主要体现在以下几个方面：\n1. 内存效率\n 是固定大小的，并且在内存中是连续存储的。它直接映射到计算机的浮点数表示，这意味着它可以非常高效地使用内存。相比之下，普通的 JavaScript 数组是动态的，可以包含不同类型的元素，因此它们需要更多的内存开销来存储额外的信息，比如元素类型和指针。\n2. 性能优化\n由于  中的数据是类型化的（即数组中的每个元素都是相同类型的），JavaScript 引擎可以对操作这些数组的代码进行优化。编译器知道数组中的每个元素都是32位浮点数，因此它可以在底层使用更快的算法和指令集来处理这些数据。\n3. 数据一致性\n 保证了数组中的每个元素都是32位浮点数。这种一致性确保了当你处理科学计算、图形渲染或者音频处理等需要精确浮点运算的应用时，数据的表现是可预测的。\n4. 与底层硬件的接近性\n 更接近硬件层面的表示，因为现代计算机硬件通常都是针对固定大小和类型的数据进行优化的。这意味着读取和写入这些数组的操作可以非常迅速，因为它们可以直接映射到硬件操作。\n5. 二进制操作的便利性\n使用  可以直接读取和写入二进制数据流，这在处理网络传输和文件 I/O 操作时非常有用。例如，你可以直接从一个  创建一个 ，而不需要进行任何转换。这使得在客户端和服务器之间传输浮点数数据变得更加简单和快速。\n6. 传输效率\n在网络传输中，使用  可以直接发送和接收二进制数据，而不需要将浮点数转换为字符串或其他格式。这减少了数据的大小，因此可以加快传输速度并减少带宽使用。\n7. Web APIs 的兼容性\n许多 Web API 都是围绕类型化数组设计的，这意味着  可以直接用于这些 API，无需额外的转换或包装。这使得在使用 WebGL、Web Audio API 等技术时， 成为处理和传输数据的自然选择。\n实际例子\n一个手动创建的二进制序列化过程，它类似于Protocol Buffers（Protobuf）的工作原理；例子：\n从一个二进制WebSocket流中接收数据，这个流包含了一个复合结构的数据包，其中既有字符串也有浮点数。\n假设服务器发送的数据包格式如下：\n4个字节的整数，表示用户ID\n32个字节的字符串，表示用户名\n4个字节的单精度浮点数，表示用户的账户余额\n这个数据包总共占用40个字节，我们的任务是从这个二进制流中解析出这三个字段。\n首先，我们会接收到一个  对象，其中包含了这40个字节的数据。我们需要按照正确的顺序和数据类型来解析这些数据。\n**发送方有以下数据：**\n发送方将执行以下步骤来序列化这些数据：\n**接收方**\n在这个例子中：\n是原始的二进制数据。\n被用来解析整数和浮点数字段。\n被用来获取用户名的字节，然后使用  将这些字节解码成字符串。\n没有直接使用，但如果我们有多个浮点数需要处理，我们可以创建一个  视图来直接操作这些浮点数。\n与手动处理二进制数据相比，Protobuf提供了以下优势：\n效率： Protobuf设计用来在网络上高效传输数据，它的编码通常比JSON小很多。\n跨平台： Protobuf支持多种编程语言，可以轻松地在不同的系统和语言之间传输数据。\n可扩展性： Protobuf设计了向后和向前兼容，你可以更改数据结构而不破坏已部署的程序之间的通信。\n自动代码生成： Protobuf提供了代码生成工具，可以自动为多种语言生成序列化和反序列化代码，减少了手动编码的错误和工作量。\nReference\nGPT",
    "url": "/blog/Type Array 应用场景",
    "path": "tech/frontend/Type Array 应用场景.md",
    "lastModified": "2025-06-20T11:25:12.948Z"
  },
  {
    "title": "vite简介与原理",
    "content": "vite简介与原理\n**讲解范围**\n0. 背景\n1. vite是什么\n2. 特征: 冷启动 + 热更新\n3. C/S 基本架构\n3. 预处理: 解决的问题 + 原理\n4. 热替换：如何使用 + 原理\n5. 其它：插件机制 + 竞品走势 + 总结 + QA\n背景\n目前的webpack development 场景也会把所有可能用到的代码全部进行打包构建，这样打包出来的代码是十分庞大的，很多时候其实我们在开发过程中并不需要全部代码的功能，而是一小部分，这个时候大量的构建时间都是多余的，我们需要一个能够真正意义上实现懒加载的开发工具。\nvite 是什么？\nVite 是一个由原生ESM 驱动的 Web 开发构建工具。在开发环境下基于浏览器原生ES imports 开发，在生产环境下基于Rollup打包。\n在浏览器端使用 export、import 的方式导入和导出模块，在 script 标签里设置 type=\"module\"，浏览器会识别所有添加了type='module'的script标签，对于该标签中的import关键字，浏览器会发起http请求获取模块内容。\n特征\n缓慢的启动\n**vite之前**\n当冷启动开发服务器时，基于打包器的方式启动必须优先抓取并构建你的整个应用，然后才能提供服务。\n!传统bundle\n**vite后**\n!ESM 的构建模式\n灰色部分是暂时没有用到的路由，甚至完全不会参与构建过程，随着项目里的路由越来越多，构建速度也不会变慢。\n缓慢的更新\n**vite之前**\n第一阶段：\n打包器的开发服务器将构建内容存入内存，修改后重新构建并重载页面（代价很高 + 重新加载页面会消除应用的当前状态）\n第二阶段：\nHMR 热更新（主流的webpack），允许一个模块 “热替换” 它自己，而对页面其余部分没有影响。\n实践中：不过HMR也会随着应用规模的增长而显著下降\n**vite后**\nHMR 是在原生 ESM 上执行的。当编辑一个文件时，Vite 只需要精确地使已编辑的模块与其最近的 HMR 边界之间的链失效（大多数时候只需要模块本身），使 HMR 更新始终快速，无论应用的大小。\n基本架构\n!基本架构图\n1. server 预构建 -> 启用服务 node + connect + ws\n2. 浏览器请求 -> server中间件处理 -> server插件调用（（路径改写，hot注入，依赖图生成） -> 返回 client\n预构建\n!预构建图\n基于 esbuild 的**依赖**预打包\n1. 减少模块/请求数量；\n2. 支持 CommonJS 依赖。\n预打包只有在依赖变动时才需要执行，但在有大量依赖的项目中，每次执行还是可能会需要很长时间。Vite 之前是使用 Rollup 来执行这个过程，在 2.0 中切换到了 esbuild，使这个过程加快了几十倍。\nVite 通过在一开始将应用中的模块区分为 依赖 和 源码 两类，改进了开发服务器启动时间。\n1. **依赖** 大多为在开发时不会变动的纯 JavaScript，例如 node_modules 等第三方依赖\n2. **源码** 通常包含一些时常被编辑的文件\n场景\nimport { debounce } from 'lodash' 导入一个命名函数的时候，debounce 函数的模块内部又依赖了很多其他函数，形成了一个依赖图。\n当浏览器请求 debounce 的模块时，又会发现内部有 2 个 import，再这样延伸下去，这个函数内部可能带来了几百次请求。\n!优化依赖\n解决方案\n折中\n利用 Esbuild 快速的构建编译速度，在没有感知的情况下在启动的时候预先帮你把 debounce 所用到的所有内部模块全部打包成一个传统的 js bundle（构建好的bundle会放在node_modules/.vite下），后续对 A module的引用都直接返回 .vite/A 文件内容\nEsbuild 使用 Go 编写，并且比以 JavaScript 编写的打包器预构建依赖快 10-100 倍。\n!构建速度对比\nHMR（热替换）\ndemo\n1. self accept\ndemo .vue css\ndemo .vue template\ndemo .vue script\n2. array accept (not self accept)\ndemo https://github.com/wizardpisces/lerna-repo/tree/master/packages/mini-vite/template-vue-ts/src/mimic-store\n原理\n!构建HMR图\n!HMR交互图\n**server build moduleGraph**\n分析 源文件 import -> build 父子关系图\n分析 源文件 import.meta.hot -> build 热替换依赖图\n**client build hotModulesMap**\nvite 插件机制\n!插件图例子 vite-plugin-vue\n在 rollup 插件基础上 扩展出 vite 的 plugin API，以及执行机制。好处：一套插件可能在 rollup 跟 vite 同时兼容使用（如果并未使用vite独有的插件hook），扩大 vite的生态圈，直接运行大部分的 rollup插件\n趋势\nsnowpack vs vite\n总结\nvite已经出到版本2，目前spa项目用起来没发现什么特别问题\n内部项目(admin)可以开始先尝试 Vite\nSSR 目前处在实验阶段，还需等待\nQA\n是否会取代 webpack？\n目前看起来不会\n**原因：**\nwebpack能覆盖更多的奇特的场景，生态也更丰富\nvite还有哪些问题？\n1. 服务端渲染 (SSR) 支持还处在实验阶段，产线使用仍需等待\n修改 node_modules 源文件是否触发热更新？\n不会\n如何入手 vite 源码\n简化版 vite，供初入坑\n**原因：**\n由于预构建，node_modules里面的源代码已经被打包到 node_modules/.vite下面，目前此路径并未注册到热替换图里面\n**多种构建工具的比对**\n1. webpack :强调对web开发的支持，尤其是内置了HMR的支持，插件系统比较强大，对各种模块系统兼容性最佳(amd,cjs,umd,esm等，兼容性好的有点过分了，这实际上有利有弊,导致面向webpack编程），有丰富的生态，缺点是产物不够干净，产物不支持生成esm格式， 插件开发上手较难，不太适合库的开发。\n2. rollup: 强调对库开发的支持，基于ESM模块系统，对tree shaking有着良好的支持，产物非常干净，支持多种输出格式，适合做库的开发，插件api比较友好，缺点是对cjs支持需要依赖插件，且支持效果不佳需要较多的hack，不支持HMR，做应用开发时需要依赖各种插件。\n3. esbuild: 强调性能，内置了对css、图片、react、typescript等内置支持，编译速度特别快（是webpack和rollup速度的100倍+),缺点是目前插件系统较为简单（框架作者还在迭代中），生态不如webpack和rollup成熟。\nReference\nhttps://github.com/wizardpisces/lerna-repo/tree/master/packages/mini-vite\nhttps://vitejs.dev/guide/\nhttps://github.com/evanw/esbuild\nhttps://segmentfault.com/a/1190000039264055\n下期预告\nvite ssr简介",
    "url": "/blog/Vite简介与原理",
    "path": "tech/frontend/Vite简介与原理.md",
    "lastModified": "2025-06-20T11:25:12.947Z"
  },
  {
    "title": "为什么需要隔离",
    "content": "为什么需要隔离\n防止控件之间样式覆盖，特别在微前端应用场景（例如 qiankun ），良好的隔离能够降低样式污染\n几种隔离方案原理\nScoped\nVue 中的  属性是一个特殊的属性，可以添加到  标签上，以确保样式只应用于当前组件的元素，而不泄露到外部。其工作原理基于以下几个步骤：\n1. **唯一属性标识**：当你在  标签上加上  属性后，Vue 会为每个组件生成一个唯一的 data 属性（例如 ）。这个属性是随机的，并且对于每个组件都是独一无二的。\n2. **后处理 CSS**：Vue 通过其构建工具（如 webpack 加上 vue-loader）在编译过程中处理这些  样式。它会自动将上述生成的唯一 data 属性作为选择器添加到所有的 CSS 规则中。这意味着，这些样式只会匹配带有相应 data 属性的元素。\n3. **更新模板**：在编译组件模板时，Vue 会自动将相应的 data 属性添加到所有模板中的元素上。因此，这些元素将与  样式中的选择器匹配。\n4. **防止样式泄露**：由于这些 data 属性是唯一的，因此样式只能匹配到拥有相应 data 属性的元素。这就确保了样式只会应用于当前组件内的元素，而不会影响到其他组件的元素。\n这样的处理方式确保了组件样式的封闭性，避免了全局样式污染。但是，这种方法也有它的局限性，比如它不能很好地处理子组件的样式覆盖问题，因为子组件的根节点不会被父组件的 scoped 样式所影响。对于这种情况，Vue 提供了深度选择器  或 （在某些 SCSS 预处理器中使用 ），以便可以强制样式穿透到子组件中。\n总的来说，Vue 的  CSS 功能是通过自动添加唯一的属性选择器到组件的样式规则，并将这些选择器应用于带有对应属性的元素，从而实现样式封装和隔离的。\nCSS Modules\nCSS Modules 是一种 CSS 文件模块化方案，它的主要目标是解决全局 CSS 命名冲突的问题，从而实现样式的局部作用域。这种方法在 React 社区中非常流行。其工作原理基于以下几个步骤：\n1. **生成唯一类名**：CSS Modules 在构建过程中，会为每个 CSS 类名生成一个唯一的、全局唯一的标识符。这个标识符通常是由原始类名和一些其他信息（如文件名、路径、哈希值等）组成的，确保了在全局范围内的唯一性。\n2. **构建 CSS 文件**：CSS Modules 将修改过的类名写入构建后的 CSS 文件。这样，每个组件的样式都有自己的唯一类名，避免了不同组件间的样式冲突。\n3. **导出类名映射**：CSS Modules 会导出一个 JavaScript 对象，这个对象是原始类名到生成的唯一类名的映射。这就是为什么我们可以使用  这样的语法来导入 CSS Module， 对象就包含了这个映射。\n4. **使用类名**：在 React 组件中，我们可以使用这个映射来为元素应用样式。例如，，这里的  就是生成的唯一类名。\n这种方法的优点是可以确保样式的封装性，避免了全局样式污染。缺点是需要配置构建工具（如 webpack），并且不支持直接在 CSS 中使用其他组件的样式，因为每个组件的样式都是封装的。\nCSS Modules 的原理就是通过在构建过程中为每个 CSS 类名生成一个全局唯一的标识符，然后在 JavaScript 中使用这个标识符来应用样式，从而实现样式的局部作用域。\n缺陷\n**不方便做样式覆盖**\n例如：需要对 ant-table 做样式覆盖，则需要  的 jsx 显示覆盖，更期望只对外层的 css 做类似 Vue 的 scoped，而不是每个使用的地方都写 ；\nstyled-components\n 是一个在 React 和其他 JavaScript 库中实现 CSS-in-JS 的库。它使用 JavaScript 来编写 CSS，使得组件的样式和逻辑可以在同一个地方定义，而且可以利用 JavaScript 的全部功能来创建动态的样式。 的样式隔离原理基于以下几个步骤：\n1. **创建样式化组件**：首先，你会使用  提供的 API 来创建一个样式化组件。这个 API 接收一个模板字符串，其中包含你的 CSS 规则，然后返回一个新的 React 组件。这个新的组件会将这些 CSS 规则应用到它呈现的元素上。\n2. **生成唯一类名**：当你创建一个样式化组件时， 会为这个组件生成一个唯一的类名。这个类名是全局唯一的，因此不会与其他组件的样式发生冲突。\n3. **插入样式**： 会将你的 CSS 规则插入到一个样式标签中，并且将生成的唯一类名添加到这些规则中。这个样式标签通常被插入到  元素中。\n4. **应用样式**：当你使用样式化组件时，它会将生成的唯一类名添加到呈现的元素的  属性中。这样，这个元素就会接收到你在样式化组件中定义的样式。\n这种方法的优点是，它不仅可以实现样式的封装性，而且可以利用 JavaScript 的全部功能来创建动态的、复杂的样式。它的缺点是，可能会**增加运行时的复杂性**，因为样式是在运行时插入到文档中的，而不是在构建时。此外，它也需要一些学习成本，因为你需要习惯在 JavaScript 中编写 CSS。\n总的来说， 的原理是通过在 JavaScript 中创建样式化组件，然后为每个组件生成一个唯一的类名，将这个类名和对应的 CSS 规则插入到文档中，从而实现样式的封装和隔离。",
    "url": "/blog/css 隔离",
    "path": "tech/frontend/css 隔离.md",
    "lastModified": "2025-06-20T11:25:12.947Z"
  },
  {
    "title": "为什么 ref 没法直接传递？",
    "content": "为什么 ref 没法直接传递？\n可能原因：\n1. **封装**：React 强调组件的封装，即组件应该管理和控制自己的状态和行为。 是访问组件内部 DOM 节点的一种方式，如果  能够像普通的 prop 那样传递，那么组件的内部节点就可以被外部的父组件随意访问和修改，这破坏了封装性。\n2. **所有权原则**：在 React 中，每个  有一个拥有者 —— 创建它的组件。这个原则确保了  的使用是明确和可控的。如果  能够像普通的 prop 那样自由传递，那么它的所有权就会变得模糊，可能导致不同组件意外地共享对同一个 DOM 节点的引用。\n3. **抽象泄漏**：组件应该暴露出一个明确的 API，而  直接关联到 DOM 节点或组件实例，这意味着组件的内部实现细节被暴露给了父组件。这种抽象泄漏可能会导致父组件过分依赖子组件的内部结构，使得重构变得困难。\n4. **使用  的明确性**：通过  明确地传递 ，组件库的作者可以决定哪些内部元素可以被父组件访问。这样做可以确保  的使用是有意为之，并且对组件的内部结构有清晰的认识。\n总之， 没法直接传递是为了保持组件的封装性，维护组件间清晰的边界，并确保组件的抽象不被破坏。当需要将  传递给子组件时，React 提供了  API 作为一种明确和有控制的方式来实现这一点。\n简单实现\n在这个简化的实现中， 函数返回一个新的组件，这个组件接收一个特殊的属性 。这个属性是父组件传递下来的 。然后， 函数中的类组件将  作为参数传递给  函数。\n在  组件中，我们创建了一个 ref () 并将其作为  属性传递给 。因为  是用我们的简化版  创建的，它能够接收并使用这个 。\n这个简化的实现演示了  的核心概念，但请注意，这并不是 React 实际上如何实现  的。React的实现包含更多的细节和优化，以确保正确处理生命周期方法、hooks 等。如果你想要在生产环境中使用 ，请使用 React 提供的官方  API。",
    "url": "/blog/forwardRef 的思考",
    "path": "tech/frontend/forwardRef 的思考.md",
    "lastModified": "2025-06-20T11:25:12.946Z"
  },
  {
    "title": "框架设计的思考",
    "content": "（时间：2021-12-2，最后更新时间2022-1-13）\n框架设计的思考\n文章内容主要基于2019 JSConf.Asia - 尤雨溪在框架设计中寻求平衡 + 自身对vue,react-hook,svelte的简单实现后的思考总结\n状态管理\n大致种类：\n1. Mutable vs Immutable\n2. Dependency Tracking vs Dirty Tracking\n3. Reactivity vs Simulated Reactivity\neg:\nvue偏向OOP(Object Oriented Program)编程模型; 驱动视图偏向data mutable tracking\nReact偏向FP(Functioanl Program)编程模型；驱动视图偏向data immutable快照更新\nAngular Dirty Tracking\nSvelte 偏向编译时的 Dirty Tracking\n渲染机制；Render Mechanism\n大致种类：\n1. JSX vs Template （动态渲染函数和基于静态字符串的复杂的 Vue 表达式）\n2. Expressiveness vs Raw Perf （表现力和原生性能）\n3. Runtime Scheduling vs AOT（运行时调度和提前优化）\nJSX 表现力强，但是没法做静态编译优化\nTemplate 失去了很多表现力，受限于模板语法，但是可以做静态编译优化\neg:\nvue jsx/Virtual-DOM + template，编译成render函数\n主要方向是 template 的静态优化\nreact jsx/Virtual-DOM，编译成render函数\n主要方向是 JSX + 运行时调度优化，提升用户**感知性能**，例如：时间切片（调度也耗时）\nangular 基于template的，它们将模板编译成相对较低级别的指令来进行内容渲染，基于Incremental DOM更新（相对Virtual-DOM diff节省内存，同时更费CPU，所以是一个CPU跟内存的取舍）\n主要方向是：待补充\nsvelte（一种编译器框架） 编译生成指令更新，基本无runtime调度\n缺点：包大小会随着工程增长而成比较陡峭线性增长；受限于模板语法，所以有很多心智负担？级别越低的编译输出，很难将你的自定义操作与它进行挂钩，就好比你无法使用 C 语言去调试你的汇编代码\n主要方向 AOT优化，缩小编译体积，例如对编译指令大小的优化\n框架层面\nVue\nreact < vue编译时 < svelte\nreact > vue运行时 > svelte\n一句话运行原理\n**运行时模板一次性依赖收集**\n~~运行时**关联变量更新操作与组件的更新函数**，变量变动时触发对应组件的更新函数~~\nAPI：ref vs reactive\n调用方式\n1. reactive只能传入对象，方便数据的访问；\n2. ref 能传任意值，会创建一个 ref 对象，主要暴露 .value 属性，对value是Object的情况会用 reactive再次封装；\n所以 ref 可以理解成对 reactive 的再次封装，处理了 primary reactive场景，同时解决了 对象整体替换的烦恼\nimmutable vs mutable\n1. reactive 是 mutable 模式的数据 tracking；\n2. ref 是 immutable 模式的数据 tracking；能传 ref\n开发体验\n细粒度的热更新：在同一组件层面还能区分只对样式、模板或者js逻辑更新\nReact-Hooks\n轻编译重运行时\n一句话运行原理\n**运行时反复依赖处理**\n~~运行时通过hooks调用顺序**关联hooks(eg:setData)跟函数组件**，做后续hook调用触发对应函数组件更新~~\nSvelte深入\n重编译轻运行时\n一句话运行原理\n**编译时模板一次性依赖收集**\n~~编译时通过**收集模板对变量的引用顺序编译出真实事件函数与更新函数**，做后续事件精准更新~~\n一句话调度原理\n利用浏览器的 microTask 或者 macroTask 做组件树更新的批量处理\n关于调度\n1. 基本调度单元?\n>组件的更新函数\n2. 如何调度父组件更新又触发子组件的更新？\n动态修改调度的数组，保证子组件的update函数能在父组件的更新job中实时push到调度的queue，保证了更新的一致性\n>>坑：queue的遍历不建议通过静态遍历方式，例如：forEach，而需要通关过动态遍历方式来做，例如：\"for let of\"；*（ps:动静指是否实时对数组长度做求值，只有实时计算才能保证动态改变的调度队列也能被执行）*\n进一步思考\n优势可能演变成劣势，缺点也可能演变成某种优势；分久必合合久必分；\nVue跟React最大的不同点可能就在于数据的Reactivity（是Immutable还是Mutable）；后续的演变都离不开这点，例子：\n    1. Vue3的Hooks为了解决.vue文件**更贴近web开发者**却导致逻辑视图割裂问题；顺带有更好的逻辑复用 + 更好的ts\n    2. React fiber为了解决 数据Immutable更安全更好预测却导致的更新性能问题\n    3. React + Mobx 跟 Vue3 Composition-API如此类似\nReference\nhttps://zhuanlan.zhihu.com/p/35046696\nhttps://zhuanlan.zhihu.com/p/76622839\nhttps://github.com/wizardpisces/experiment/blob/master/packages/mini-react\nhttps://github.com/wizardpisces/experiment/blob/master/packages/mini-svelte\nhttps://github.com/wizardpisces/experiment/blob/master/packages/mini-vue\nhttps://github.com/vuejs/vue-next\nhttps://github.com/google/incremental-dom",
    "url": "/blog/框架设计的思考",
    "path": "tech/frontend/框架设计的思考.md",
    "lastModified": "2025-08-06T07:10:55.453Z"
  },
  {
    "title": "[Map](http://craftinginterpreters.com/image/a-map-of-the-territory/mountain.png)",
    "content": "Map\nIntroduction of map\nsourceCode (scanning)-> tokens (parsing)-> syntaxTree(**step3**) (analysis/transform)-> intermediateRepresentation or IR(**step4**) (codeGen)-> machineCode\n**step4** -> (Optimizing **step4**) \n**step4** (codeGen + sourceMap)-> byteCode(virtualMachine run! )\n**step3** -> highLevelLanguage(transpiling) -> **step4**   (short cuts)\nA Map of Territory\nThe-parts-of-a-language\nCompilers and Interpreters\nShort-cuts-and-alternate-routes\nThe-parts-of-a-language\nScanning\nalso known as lexing\nA scanner (or lexer) takes in the linear stream of characters and chunks them together into a series of something more akin to “words”. \nParsing\nThis is where syntax gets a grammar—the ability to compose larger expressions and statements out of smaller parts. Did you ever diagram sentences in English class?\nStatic analysis\nThe first bit of analysis that most languages do is called binding or resolution. For each identifier we find out where that name is defined and wire the two together. This is where scope comes into play—the region of source code where a certain name can be used to refer to a certain declaration.\n**Everything up to this point is considered the front end of the implementation.**\nIntermediate Representation\nThe front end of the pipeline is specific to the source language the program is written in. \nThe back end is concerned with the final architecture where the program will run.\nIn the middle, the code may be stored in some intermediate representation (or IR) that isn’t tightly tied to either the source or destination forms (hence “intermediate”). \nInstead, the IR acts as an interface between these two languages.\nOptimization\neg:\nConstant folding\npennyArea = 3.14159 * (0.75 / 2) * (0.75 / 2);\n->\npennyArea = 0.4417860938;\nCode Generation\nwhere “code” here usually refers to the kind of primitive assembly-like instructions a CPU runs and not the kind of “source code” a human might want to read.\nVirtual Machine\nvirtual machine (VM), a program that emulates a hypothetical chip supporting your virtual architecture at runtime. Running B in a VM is slower than translating it to native code ahead of time because every instruction must be simulated at runtime each time it executes.\nIn return, you get simplicity and portability.\nRuntime\nIn, say, Go, each compiled application has its own copy of Go’s runtime directly embedded in it. If the language is run inside an interpreter or VM, then the runtime lives there. This is how most implementations of languages like Java, Python, and JavaScript work.\nShort-cuts-and-alternate-routes\nSingle pass compiler\nSome simple compilers interleave parsing, analysis, and code generation so that they produce output code directly in the parser, without ever allocating any syntax trees or other IRs.\nPascal and C were designed around this limitation.\nAt the time, memory was so precious that a compiler might not even be able to hold an entire source file in memory, much less the whole program. \nTree-walk interpreters\nTranspilers\n**source-to-source compiler or transcompiler**\ntreated some other source language as if it were an intermediate representation\nrun that resulting code through the output language’s existing compilation pipeline and you’re good to go.\neg: coffee -> javascript, sass -> css\nC compilers were available everywhere UNIX was and produced efficient code, so targeting C was a good way to get your language running on a lot of architectures.\nWeb browsers are the “machines” of today, and their “machine code” is JavaScript, so these days it seems almost every language out there has a compiler that targets JS since that’s the main way to get your code running in a browser.\nJust-in-time compilation\n The fastest way to execute code is by compiling it to machine code, but you might not know what architecture your end user’s machine supports. What to do?\n You can do the same thing that the HotSpot JVM, Microsoft’s CLR and most JavaScript interpreters do. On the end user’s machine, when the program is loaded—either from source in the case of JS, or platform-independent bytecode for the JVM and CLR—you compile it to native for the architecture their computer supports.Naturally enough, this is called just-in-time compilation.\n The most sophisticated JITs insert profiling hooks into the generated code to see which regions are most performance critical and what kind of data is flowing through them. Then, over time, they will automatically recompile those hot spots with more advanced optimizations.\nCompilers-and-Interpreters\nWhat’s the difference between a fruit and a vegetable？\nbut actually “fruit” is a botanical（植物学） term and “vegetable” is culinary（烹饪）.There are fruits that aren’t vegetables (apples) and vegetables that are not fruits (carrots), but also edible plants that are both fruits and vegetables, like tomatoes.\nWhat’s the difference between a compiler and an interpreter?\nCompiling is an implementation technique that involves translating a source language to some other—usually lower-level—form. When you generate bytecode or machine code, you are compiling. When you transpile to another high-level language you are compiling too.\nWhen we say a language implementation “is a compiler”, we mean it translates source code to some other form but doesn’t execute it. The user has to take the resulting output and run it themselves.\nConversely, when we say an implementation “is an interpreter”, we mean it takes in source code and executes it immediately. It runs programs “from source”.\nGCC and Clang take your C code and compile it to machine code. An end user runs that executable directly and may never even know which tool was used to compile it. So those are compilers for C.\neg:\nIf you run go build, it compiles your Go source code to machine code and stops. If you type go run, it does that then immediately executes the generated executable.\nSo go is a compiler (you can use it as a tool to compile code without running it), is an interpreter (you can invoke it to immediately run a program from source), and also has a compiler (when you use it as an interpreter, it is still compiling internally).\nReference\nhttp://craftinginterpreters.com/a-map-of-the-territory.html#transpilers\nhttps://justinmeiners.github.io/lc3-vm/",
    "url": "/blog/Compiler简介",
    "path": "tech/other/Compiler简介.md",
    "lastModified": "2025-06-20T11:25:12.923Z"
  },
  {
    "title": "二维码的纠错原理",
    "content": "二维码的纠错原理\nReed-Solomon 算法简介\nReed-Solomon 算法是一种基于多项式插值的纠错编码技术。它的主要功能是通过加入冗余信息（纠错码），在部分数据损坏的情况下恢复原始数据。\n以下是 Reed-Solomon 的工作流程与简单例子，帮助理解其原理和应用。\n二、简单例子\n场景描述\n1. **原始数据**：3个数据点 \\( [1, 3, 5] \\)。  \n2. **目标**：通过 Reed-Solomon，添加 2 个校验点，使其在损坏的情况下可以恢复。  \n3. **有限域**：采用有限域 GF(7)（简单的模 7 运算）。\n---\n1. 构造多项式\n根据原始数据 \\( [1, 3, 5] \\)，构造一个二次多项式：  \n\\[\nf(x) = a_0 + a_1x + a_2x^2\n\\]\n将数据点 \\( (x, y) \\) 填入：\n当 \\( x = 0 \\), \\( y = 1 \\): \\( a_0 = 1 \\)\n当 \\( x = 1 \\), \\( y = 3 \\): \\( 1 + a_1 + a_2 = 3 \\)\n当 \\( x = 2 \\), \\( y = 5 \\): \\( 1 + 2a_1 + 4a_2 = 5 \\)\n解方程得到系数：\n\\[\nf(x) = 1 + 2x + x^2\n\\]\n---\n2. 生成校验点\n计算 \\( f(x) \\) 的额外点值：\n\\( f(3) = 1 + 2(3) + (3)^2 = 16 \\mod 7 = 2 \\)\n\\( f(4) = 1 + 2(4) + (4)^2 = 25 \\mod 7 = 4 \\)\n完整数据变为：  \n原始数据 \\( [1, 3, 5] \\) 和校验数据 \\( [2, 4] \\)。\n---\n3. 数据损坏\n假设传输中，数据 \\( [1, ?, 5, ?, 4] \\) 被部分损坏（两个未知）。  \n需要恢复完整数据。\n---\n4. 数据恢复（多项式插值）\n已知：\n数据点 \\( (0, 1), (2, 5), (4, 4) \\)\n两个损坏的数据点 \\( (1, ?), (3, ?) \\)\n插值恢复：\n通过插值法恢复 \\( f(x) = 1 + 2x + x^2 \\)，重新计算缺失点：\n\\( f(1) = 1 + 2(1) + (1)^2 = 3 \\)\n\\( f(3) = 1 + 2(3) + (3)^2 = 16 \\mod 7 = 2 \\)\n恢复完整数据：  \n\\[ [1, 3, 5, 2, 4] \\]\n---\n三、应用场景\n1. **光盘存储**：\n数据分块存储，利用 Reed-Solomon 校验码修复划痕损坏的数据。\n2. **二维码**：\n部分二维码模块被污损或遮挡，利用冗余信息恢复完整数据。\n3. **通信系统**：\n在噪声干扰的信道中传输数据，校验码用于纠正传输错误。\n---\n四、总结\n**核心思想**：将数据映射到多项式，通过添加校验点增强容错能力。\n**纠错能力**：已知错误位置，最多修复 n-k 个错误点。（n 是数据点数量，k 是原始数据点数量, n-k 是校验点数量），纠错率 (n-k)/n\n**实际意义**：保证在部分数据丢失或损坏的情况下，仍能准确恢复原始信息。\n二维码的应用场景中，大多数损坏属于**位置已知**的错误，这使得二维码非常适合使用 Reed-Solomon 算法 的纠错功能。\n二维码的形变恢复\n二维码的形变恢复主要依赖于功能区域（如定位图形、对齐图形和时间图案）来校正二维码的几何形态，将其还原为标准的矩阵形式。以下是二维码形变校正的具体步骤和原理：\n---\n**1. 形变类型**\n二维码的形变主要分为以下几类：\n**旋转变形：** 二维码被旋转。\n**透视变形：** 二维码拍摄时不是正面拍摄，导致图像呈现梯形或菱形。\n**弯曲变形：** 二维码印刷在曲面上或因物理原因导致形状弯曲。\n---\n**2. 功能区域的作用**\n二维码的功能区域（尤其是定位图形和对齐图形）起到了关键作用：\n1. **定位图形：**  \n三个大方块位于二维码的左上角、右上角和左下角。\n它们的相对位置用于确定二维码的整体方向、大小和形变。\n2. **对齐图形：**  \n多个小方块位于二维码的内部，帮助校正非线性形变（如弯曲或局部拉伸）。\n3. **时间图案：**  \n位于二维码顶部和左侧的黑白相间线条，用于确定网格的行列数量，帮助解码器找到数据模块的准确位置。\n---\n**3. 校正形变的步骤**\n**步骤 1：检测定位图形**\n解码器扫描图像，寻找二维码的三个定位图形。\n根据定位图形的几何特性（黑白模块的固定比例和形状），确定它们在图像中的位置。\n**步骤 2：估计透视变换矩阵**\n利用定位图形的相对位置，计算实际图像中的四个顶点（包括隐含的右下角）。\n将实际顶点与标准二维码的顶点位置对应，生成一个 **透视变换矩阵**。\n透视变换矩阵是一个 3x3 的矩阵，能够将任何透视变形的图像还原为平面图像。\n**步骤 3：应用透视变换**\n使用计算出的透视变换矩阵，对二维码图像进行仿射变换或透视变换，恢复为标准的矩形。\n**步骤 4：细化校正**\n检测对齐图形，进一步校正非线性变形（如弯曲）。\n对齐图形通过测量变形后的位置与预期位置的偏差，调整内部模块的位置。\n**步骤 5：提取网格**\n根据时间图案的黑白间隔，确定二维码的行列数量。\n将恢复后的图像划分为标准的网格矩阵，准备解码。\n---\n**6. 总结**\n二维码形变恢复的核心是：\n利用功能区域（定位图形、对齐图形、时间图案）识别变形。\n计算透视变换矩阵，将形变二维码恢复为标准矩形。\n细化校正，确保内部数据模块的位置精确无误。",
    "url": "/blog/QRCode",
    "path": "tech/other/QRCode.md",
    "lastModified": "2025-06-20T11:25:12.941Z"
  },
  {
    "title": "Source-Map原理及其应用",
    "content": "什么是 source-map?\n广义解释：\n溯源\n前端开发\n“A source map provides a way of mapping code within a compressed file back to it’s original position in a source file”\nV3 spec example:\n为什么使用 source-map?(debug)\n前端模块打包发布基本流程：\nModules -> compilers -> assets\n代码转换类型：\n编译（TypeScript)\n转译（Babel）\n压缩混淆（UglifyJS）\n合并多个文件，减少带宽请求。(Webpack, Rollup)\n1. dev 环境如何定位调试？\n2. 产线出错如何定位调试？\nsource-map 如何映射\nmappings: \"输出文件列位置|输入文件名|输入文件行号|输入文件列号,.....\"\n优化措施：\n文件名提取\n可符号化字符的提取\n记录相对位置\nVLQ编码\n详细参考\nsouce-map 生成基本步骤\n1) Transform code and note the new generated source location\n2) Check for a difference in location between the original and generated code\n3) Using these mapping build a source map\n具体参照这里\nsource-map 映射可视化\nonline source-map visualization\nsource-map test case 写法\n一种写法\n参考资料\n1. https://indepth.dev/source-maps-from-top-to-bottom/\n2. https://juejin.im/post/6844903869928079373\n3. https://www.npmjs.com/package/source-map\n4. https://www.html5rocks.com/en/tutorials/developertools/sourcemaps/",
    "url": "/blog/Source-Map原理及其应用",
    "path": "tech/other/Source-Map原理及其应用.md",
    "lastModified": "2025-06-20T11:25:12.923Z"
  },
  {
    "title": "JsBridge引发的思考",
    "content": "JsBridge引发的思考\nRN跟微信小程序算是JsBridge最经典的实践\n疑问\n1. 为什么js能够调用android或者IOS系统的方法？\n2. 不同语言之间如何做到相互调用？\nJsBridge原理\nWeb端和Native可以类比于Client/Server模式，Web端调用原生接口时就如同Client向Server端发送一个请求类似，JSB在此充当类似于HTTP协议的角色，实现JSBridge主要是两点：\n将Native端原生接口封装成JavaScript接口\n将Web端JavaScript接口封装成原生接口\nNative调用JS\n原理：直接用JS引擎执行（IOS的 WebView 容器采用 WKWebView 内核+JavascriptCore引擎 etc）\n场景：Native input绑定 Native 事件，在事件代码里通过调用js将结果反应到 Webview\nJS调用Native方式\n拦截式\n标准的URL格式：，APP有专属的Scheme，eg：微信的 URL Scheme 就是 weixin://\n原理：Native拦截 Webview Sheme 请求，分析是否为JSB请求格式\n场景：Webview按钮打开Native相册，构造Scheme并发送，传递特定参数给Native去唤起相册；或者执行某些函数获取值后，再次通过执行js把结果返回给webview实现闭环通信\n兼容性：无兼容性问题\n性能：URL request 创建请求有一定的耗时（一般通过webview创建iframe方式发送），Android比较差\n局限：URL 字符串长度有限制\n注入式（主流）\n原理：通过 WebView 提供的接口向 JS 全局上下文对象（window）中注入对象或者方法，当 JS 调用时，可直接执行相应的 Native 代码逻辑，从而达到 Web 调用 Native 的目的。\n>Native端\n>web端\n场景：各种系统API的直接调动（存储等）\n兼容性：安卓4.2+ 和 iOS 7+以上可用\n性能：较好\n串联双端通信\nWebViewJavascriptBridge\nAn iOS/OSX bridge for sending messages between Obj-C and JavaScript in UIWebViews/WebViews\n如何处理安全问题\n在 APP 内 JsBridge 可以实现 Web 和 Native 的通信，但是如果 APP 打开一个恶意的页面，页面可以任意调用 JsBridge 方法，获取各种隐私的数据，就会引起安全问题。\n限定域名白名单，只允许指定域名调用\nJSI\nRN 在0.59 版本使用JSI 取代了先前的JSBridge 。\nReact Native JSI (JavaScript Interface) 可以使 JavaScript 和 原生模块 更快、更简单的通信。它也是React Native 新的架构体系中Fabric UI层 和 Turbo 模块的核心部分。\nJSI有什么不同\nJSI 移除了原生代码和JavaScript代码之间的桥接（bridge），同时也省去了两端相互调用时大量的JSON序列化和反序列化操作。JSI为原生和JS交互打开了新的大门。下面是一些JSI的特点：\nJavaScript Interface 允许我们向JavaScript 运行时注册方法。这些方法在js环境中可以通过 global对象获取并调用。\n我们完全可以使用C++或者在iOS里使用OC ，在Android里使用Java实现这些注册方法。\n原先使用bridge 的方式实现的原生模块可以通过增加一层C++，快速转化为通过JSI实现。\n在iOS端实现非常简单，因为C++和OC 可以方便的实现混编。\n在Android中，我们需要通过JNI 做一些转化。\n这些方法可以是完全同步的，这意味着不必强制使用async。await。\n其他角度分析\nJavascript的编译器V8是C++实现，C++编译器是C，所以Javascript的编译器是C语言实现\nJava编译器被编写为Java程序，然后使用C编写的Java编译器（第一个Java编译器）进行编译，所以Java的编译器是C语言实现\nObjective-C是一种通用、高级、面向对象的编程语言。它扩展了标准的ANSI C编程语言\n其他：LLVM是构架编译器(compiler)的框架系统，以C++编写而成，所以编译的IR为LLVM的编程语言（例如Rust）是C语言实现\n总结：这些高级语言底层都是C的衍生；API：\"C\" 部分定义了外部函数所使用的 应用二进制接口（application binary interface，ABI） —— ABI 定义了如何在汇编语言层面调用此函数。\"C\" ABI 是最常见的，并遵循 C 编程语言的 ABI。\n推导：\n1. 这些语言能在编译层面互通，不同语言如果都实现C语言提供的bridge interface，那就能直接在代码编译后通信（API层面？）？Foreign_function_interface？\n其他跨语言通信，例如协议（独立于语言层面的标准）：RPC等\n应用\n实现过一个简单的SASS编译器，使用js实现；所以可以在编译过程中协商注入js函数，实现后就是plugin系统，应用的Demo代码\nReference\ntiny-sass-compiler Plugin\nReact Native原理与实践\nNativeScript的工作原理：用JavaScript调用原生API实现跨平台 \nReact Native JSI：实现RN与原生通信\nhttps://tsejx.github.io/cross-platform-guidebook/hybird/jsbridge/\nhttps://juejin.cn/post/6844903585268891662\nRust程序语言设计之不安全的Rust\nhttps://en.wikipedia.org/wiki/Foreign_function_interface\nhttps://blog.risingstack.com/how-to-use-rust-with-node-when-performance-matters/\nhttps://www.teqng.com/2021/08/01/jsbridge-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/#JSB_yuan_li\n***本文属于个人的见解，酌情观看***",
    "url": "/blog/jsbridge引发的思考",
    "path": "tech/other/jsbridge引发的思考.md",
    "lastModified": "2025-06-20T11:25:12.941Z"
  },
  {
    "title": "《系统之美》思考",
    "content": "《系统之美》思考\n一艘船 -> 个人 -> 一个组织 -> 一个国家\n系统的基本结构\n1. 要素\n最明显，重要性最低\n随时可以被替换\n例如 船的木板，组织的人或者物\n2. 连接\n隐藏，改版后对系统会产生重大变化\n例如 船的结构，组织的结构\n3. 功能\n隐藏，决定系统的最关键因素\n例如 船在水上航行能力，组织提供的功能\n推导的结论\n1. 忒修斯之船（换掉每块木板的船还是之前的船么？）还是之前船，只要木板结构跟功能未变\n2. 换了员工的公司还是之前的公司，只要企业文化跟制度还在\n系统的变化\n1. 增强回路，调节回路等调节关系决定\n增强回路\n例如 组织发展好，就会吸引更优质的要素（人或者资源），从而加速组织的发展\n调节回路\n例如 发展到一定规模，新增的资源为组织赋能能力持续走低，达到某个节点就会成为累赘从而拖累组织的发展，开启调节回路\n2. 多因多果，非线性\n关键特征\n1. 反馈延迟\n表现形式（对系统施加的影响往往是一段是时间后才会显现）\n例如：大学报考的当时热门专业可能毕业后就过时了；零售商为应对一时的需求囤积的货物；国家对很多商品价格的宏观调控效果（持续疯涨的商品可能远远背离了实际的价值）\n应对方案\n>对环境信号反应适当放慢，确认变化趋势稳定后再决策\n例如 等待人员稳定后再看是否缺人，不然可能会招入远大于实际需求的人；报考专业前贴近国家的五年规划啥的；\n>缩短反馈延迟时间\n例如 跟同事的交流尽量能面对面，缩短反馈回路保证信息的及时性；零售商的零库存等；大数据按需定制；",
    "url": "/blog/《系统之美》",
    "path": "tech/other/《系统之美》.md",
    "lastModified": "2025-06-20T11:25:12.936Z"
  },
  {
    "title": "Domain Specific Language",
    "content": "Domain Specific Language\nVirtual-dom\n虚拟dom操作集合\n1. creat VNode-A （简称 v-a）\n2. create Dom-A (简称 d-a) from v-a\n3. change v-a to v-b\n4. diff v-a and v-b to patches\n5. patch d-a from patches\n6. continue 3\nTools\nhtml2hscript - Parse HTML into hyperscript \nhtml2hscript.herokuapp.com - Online Tool that converts html snippets to hyperscript\nhtml2hyperscript - Original commandline utility to convert legacy HTML markup into hyperscript\nReference\nhyperscript\nvirtual-dom\nsnabbdom\nHTML Manipulation\nEcosystem\n| Name                                                          | Description                                             |\n| ------------------------------------------------------------- | ------------------------------------------------------- |\n| htmlparser2            | Fast & forgiving HTML/XML parser                        |\n| domhandler              | Handler for htmlparser2 that turns documents into a DOM |\n| domutils                  | Utilities for working with domhandler's DOM             |\n| css-select              | CSS selector engine, compatible with domhandler's DOM   |\n| cheerio               | The jQuery API for domhandler's DOM                     |\n| dom-serializer | Serializer for domhandler's DOM                         |\nWebpack\n总结\n一般compiler操作集合：\n1. parse raw string to VNode\n2. traverse VNode (visitor hooks with manipulation: insert , remove, modify, replace etc )\n3. code generation or Serialize VNode to string",
    "url": "/blog/一些DSL应用",
    "path": "tech/other/一些DSL应用.md",
    "lastModified": "2025-06-20T11:25:12.923Z"
  },
  {
    "title": "关于程序跨端",
    "content": "关于程序跨端\n背景\n为什么需要跨端？\n一套代码跑多端，省时省力\n发展方向？\n开发体验：复用现有生态，追求热更新\n运行环境：代码执行适配，渲染引擎适配\n性能：追求原生\n包大小（冷热启动速度）\n小程序上层框架（跨APP）\n简介：基于web技术的一套代码跑在各个APP小程序：微信/支付宝/抖音等\n代表框架: taro1/2（类React语法，ps：taro3后可以同时支持RN/React/Vue）,uniapp（类Vue语法）\n原理\n主要分两种：渲染pipeline的重组优化，尽可能提出平台无关的IR？\n框架耦合，重编译时\n代表：uniapp、nanachi、taro1/2；分别支持某一个种框架：Vue或者React；\n简介：使用工具把代码语法分析一遍，把其中的 JSX 部分和逻辑部分抽取出来，分别生成小程序的静态模板和 小程序页面的定义。运行时并没有 React/Vue 的存在，实际运行的是静态编译转换之后的小程序代码。\n运行环境：同下面介绍的原生小程序（主要是一个翻译器，运行时比较少）\n视图渲染：类React/Vue模板**编译**（AST转换）成对应APP小程序模板树（eg：微信小程序模板），再被原生小程序编译渲染\n问题\n编译：复杂的AST转化可能会丢失SourceMap导致Debug困难\n编译：框架（Vue或者React）DSL强绑定限制使用范围\n编译：JSX的模板过于灵活，导致编译成特定平台困难\n运行时：各个小程序平台升级（BUG或者新增特效）会导致跨端框架被迫跟进升级\nAPI层面：把各端API **proxy**成统一的API调用（生命周期映射，系统模块调用等）\n组件：各自框架语法封装了一套\n框架独立，重运行时\n代表：taro3 支持 Vue、React等框架；仿照React-Dom跟 RN中的 Yoga，在小程序中模拟实现 DOM 以及 BOM 的 API（或者称为Taro-Dom？），从而实现直接将 React 运行到小程序环境中的目的\n运行环境：同上\n视图渲染：同上\n框架独立原因：框架编译作为Plugin接入，最终会编译成 Taro VNode？，然后通过统一API映射\nAPI层面：同上\n组件：用web-components；优势：框架独立；劣势：是否会有更多的坑？\n思考\n为什么跨端框架不直接调用native原生渲染，而是先编译到微信小程序等原生的代码？\n推测：原生小程序只对原生目录结构的代码开放API，双线程渲染？\n原生小程序（跨操作系统）\n双线程+1个native线程\n简介：基于web技术的一套代码跑在 IOS ，Android，开发者工具上\n代表框架：微信小/支付宝等原生小程序\n原理\n运行环境：各大支持小程序的APP：微信，支付宝等（IOS-JavascriptCore-WKWebView，Android-V8-chromium内核，开发者工具-NWJS-chromeWebview）\n视图渲染：原生+webview混合（内置组件类型），模板会转为语法树，然后跟内置组件映射渲染\n使用原生组件的地方会被先用DOM节点占位，然后再用原生组件覆盖占位点\n问题：导致原生组件层级会更高，DOM样式没法应用上去\n优势：体验更好（input键盘控制力；减轻webview渲染压力，比如map直接给原生线程渲染了）\nAPI（本地存储，网络请求等）：JSBridge\n包大小：看系统是否自带了 JS Engine\nHMR：蹩脚；***小程序IDE检测到目录文件发生变动时会自动重新渲染。***，渲染管道比较长，中间转换比较多，能支持也是比较慢；一般使用uniapp或者taro上层框架开发，然后转成原生小程序，再通过原生小程序转换由App渲染\n关于双线程\n安全管控\n沙箱可控\n限制 DOM 和 BOM 能力\n各种API限制\n性能（UI跟逻辑分离，互不阻塞）\n引发的问题：逻辑层与渲染层之间通信延迟，异步->运行时序->生命周期（解决方案）\n发展方向：提升通信速度？\n天生延迟：由于是不同线程之前的调用，所以对原生API的调用也是异步，所以会出现各种callback？\nApp（跨操作系统）\n简介：基于或者不基于web技术的一套代码跑在IOS跟Android系统\n代表框架：React Native（类React语法），Flutter（Dart widget）\n原理\nReact Native（三线程）\n iOS Android 的原生渲染管线都是 保留渲染模式（Retained Mode GUI） 范式，那么总是有相似点的，比如说 UI 都是树状嵌套布局，都有事件回调等等。React或者Vue虚拟DOM可很好转换过去（RN跟Weex的基石）。\n运行环境：js代码交由JS Engine（JavascriptCore，Hermes，V8 etc）执行\nAPI调用（本地存储，定位，网络请求等）：Native <-> js代码通过**JsBridge**实现\n视图渲染：通过 UI Manager 来创建视图的（基于 Virtual DOM ，RN 把不同平台创建视图的逻辑封装了一层，不同平台通过 Bridge 调用 UI Manager 来创建不同的 Native 视图）；依赖原生渲染层，纯 Native 渲染；（类比:在浏览器 React 可以调用 DOM API 完成创建 UI 的工作，依赖原生渲染层）\n包大小：看系统是否自带了 Skia 引擎\n发展：平台关联性较强，所以跨端（PC，web等）速度可能会比较慢\nHMR：有\n>三线程\nJS thread： 负责 JS 和原生代码的交互线程，因为 JS 是单线程模型，所以需要一个单独的线程来驱动，并且 JS 和 Native 交互是异步的。\nShadow thread: 这个线程是负责 Native 布局，提供给 yoga 引擎使用。\nUI thread：这个可以看作是主线程，可以看作是 UI Manager 线程，负责页面的交互和控件绘制逻辑。\n总结：JIT模式，需要频繁地在JavaScript与Native之间进行通信，从而会有一定的性能损耗影响，导致体验上与原生会有一些差异。\n***开发者编写的js代码，通过 react native 的中间层转化为原生控件和操作，Bridge 的作用就是给 React Native 内嵌的 JS Engine 提供原生接口的扩展供 JS 调用***\nFlutter\n重写了一整套包括底层渲染逻辑和上层开发语言的完整解决方案\n运行环境：dart语言\nDev阶段 JIT + Dart VM，保证开发效率\nProdution阶段 AOT编译成原生，保证产线运行效率\nAPI调用：dart可以AOT编译成平台原生代码，所以不需要bridge进行交互\n视图渲染：flutter不使用平台原生控件（那是不是小程序/web这些端也能完美跨起来？），而是使用自身渲染引擎（skia）绘制widget，依赖平台的canvas能力（所以Flutter把Canvas这种立即渲染模式转换成了保留渲染模式？）\n问题：那视频播放会不会是问题？\n方向：会在 Native 控件上发力？\n发展：平台关联性较弱，在其他端的支持上可能会更快\nHMR：有（APP）\nKraken\n结合前端生态跟Flutter渲染引擎，声称更精简的渲染管线\n非js语言运行在js环境（跨语言）\nnode addons 机制可以让非js语言编译成 .node 文件，被 node 调用\nWebAssembly 很多语言都可以编译成 .wasm，被 WebAssembly instantiate 后在浏览器或者node环境运行；实际还是需要编译成机器码\n是一个基于栈的虚拟机 (stack machine)，所以跨平台能力一流。类似 JVM？\nWebAssembly 字节码是一种抹平了不同 CPU 架构的机器码，WebAssembly 字节码不能直接在任何一种 CPU 架构上运行， 但由于非常接近机器码，可以非常快的被翻译为对应架构的机器码，因此 WebAssembly 运行速度和机器码接近，类似 Java 字节码。\n优势\n体积小：由于浏览器运行时只加载编译成的字节码，一样的逻辑比用字符串描述的 JS 文件体积要小很多；(按道理JS源码会更小；其他语言编译的 .wasm 会大很多，信息里面需要包含除了逻辑代码外的信息；这里的更小是相对于下发相同逻辑的同一种语言？)\n加载快：由于文件体积小，再加上无需解释执行，WebAssembly 能更快的加载并实例化，减少运行前的等待时间；\n兼容性问题少：WebAssembly 是非常底层的字节码规范，制订好后很少变动，就算以后发生变化,也只需在从高级语言编译成字节码过程中做兼容。可能出现兼容性问题的地方在于 JS 和 WebAssembly 桥接的 JS 接口。\n拓展\nLLVM 实现了 LLVM IR 到 WebAssembly 字节码的编译功能，也就是能编译到 LLVM 的高级语言都能编译成 WebAssembly（eg：Rust）\n是否能重新写 React 的更新？以及 SSR 的 renderToString\nReference\n移动端跨平台开发的深度解析\nRN原理分析\nTaro3跨端跨框架原理初探\n双线程模型)\nFlutter 与 React Native 深入对比分析\nReact 和 IMGUI 有何联系和区别？\n关于JSBridge\n揭秘浏览器远程调试技术\nWebAssembly 简介及原理\n***本文属于个人的见解，酌情观看***",
    "url": "/blog/关于程序跨端",
    "path": "tech/other/关于程序跨端.md",
    "lastModified": "2025-06-20T11:25:12.942Z"
  },
  {
    "title": "列存储",
    "content": "什么是列存储？跟传统存储啥区别？为啥列存储能更加快速响应复杂查询？\n列存储（Columnar Storage）是一种数据存储方式，与传统的行存储（Row Storage）有显著区别。列存储在处理分析型工作负载时，尤其是涉及复杂查询的情况下，表现得更加高效。\n列存储（Columnar Storage）\n在列存储中，数据按列存储，而不是按行存储。每一列的数据存储在一起，所有同一列的数据都连续存储在磁盘上。例如，对于一个有四个列（A、B、C、D）的表，数据将按如下方式存储：\n行存储（Row Storage）\n在传统的行存储中，数据按行存储，每一行的数据存储在一起。对于同样的四列表，数据将按如下方式存储：\n区别\n1. **数据存储方式**\n**行存储**：所有列的值按行存储在一起。\n**列存储**：同一列的值按列存储在一起。\n2. **适用场景**\n**行存储**：适用于OLTP（在线事务处理）工作负载，例如频繁的插入、更新和删除操作。\n**列存储**：适用于OLAP（在线分析处理）工作负载，例如复杂的查询和报表分析。\n列存储响应复杂查询更快速的原因\n1. **查询效率**\n在分析型查询中，通常只涉及少量的列。例如，计算某个统计数据只需访问特定的列。列存储仅读取需要的列数据，减少了I/O操作。\n行存储则需要读取整行数据，即使只需要其中一两列，也要读取整行，导致不必要的I/O开销。\n2. **数据压缩**\n列存储的同一列数据类型相同且经常有相似的值，压缩效果更好。例如，一个包含许多重复值的列可以通过RLE（Run-Length Encoding）等压缩算法显著减少存储空间。\n行存储的数据类型混杂在一起，压缩效率低下。\n3. **向量化处理**\n列存储系统可以利用向量化处理技术，同时处理一批列数据，提高CPU使用效率。\n行存储处理每行数据时需要多次加载和存储，效率较低。\n4. **缓存利用**\n由于列存储按列连续存储，缓存命中率高，减少了对磁盘的访问。\n行存储按行存储，缓存命中率较低，需要频繁访问磁盘。\n总结\n列存储通过优化I/O操作、提高压缩效率、利用向量化处理和更好的缓存利用，显著提高了复杂查询的响应速度。因此，它特别适用于需要进行大量读取和分析的场景，如数据仓库、商业智能和实时分析系统。\n示例\n假设有一个大型销售数据表，包含日期、产品ID、销售数量和销售额四个字段：\n**行存储：**\n**列存储：**\n查询总销售额时，列存储只需读取“销售额”列的数据，而行存储则需读取每一行的所有字段，明显增加了I/O开销和处理时间。这就是列存储在复杂查询中表现更高效的原因。",
    "url": "/blog/列存储",
    "path": "tech/other/列存储.md",
    "lastModified": "2025-06-20T11:25:12.941Z"
  },
  {
    "title": "可视化编程",
    "content": "可视化编程\n(lowcode)\n当前局限\n可视化编程停滞不前的一个主要原因是，它过于关注“形式”（即节点-连线范式、图形化界面等），而忽视了底层的功能设计。为什么这种关注“形式”的方式无法成就可视化编程？主要有以下几个原因：  \n1. **形式不是本质，功能才是关键**  \n可视化编程往往强调如何直观地表现代码，比如用方块和箭头代替文本编程语言的语法。但如果底层逻辑设计不够强大，光靠改变表现形式（比如不同的节点布局）无法真正提升编程的表达能力和效率。  \n2. **复杂性没有减少，甚至增加了**  \n许多可视化编程工具试图用图形化方式替代代码，但在面对复杂逻辑时，图形界面往往变得凌乱且难以管理。相较于精简的代码，复杂的可视化界面反而可能降低可读性和维护性。  \n3. **界面限制了表达能力**  \n当前的可视化编程框架往往受限于固定的交互方式，比如拖拽、节点连接等，而这些方式并不一定比文本编程更直观或高效。对于某些任务，强行适应这种交互方式反而降低了开发效率。  \n4. **忽视了可视化编程的真正潜力**  \n如果只是在“如何让代码变得可视化”上下功夫，而不是在“如何利用可视化带来新的编程范式”上做创新，那么可视化编程就只是在模仿传统编程，而不是开辟新的可能性。例如，真正强大的可视化编程应该让人更直观地操作数据流、调试和优化，而不仅仅是把文本编程转换成图形界面。  \n演进方向\n要解决可视化编程停滞不前的问题，核心在于从“**功能驱动**”而非“**形式驱动**”的角度出发。以下是几个可能的方向：  \n**2. 以数据流为核心，而非代码块**\n**问题**：当前大部分可视化编程工具依赖“节点-连线”模式，但这种方式容易导致复杂的 spaghetti（意大利面条）式图形，变得难以阅读和维护。  \n**改进方向**：  \n采用**数据驱动**的方式，而不是基于“代码块”组织逻辑。例如，TensorFlow 或者 Unreal Engine 的蓝图系统就是基于数据流的可视化工具，它们的成功在于数据的流向清晰，而不是简单地拼接代码块。  \n引入**自动化布局算法**，优化界面，使得逻辑关系更清晰，而不是让用户手动整理节点。  \n📌 **案例**：工程设计软件 Grasshopper 采用“数据流”的方式，而不是“代码块”的形式，使得复杂几何建模更直观。  \n---\n**3. 结合自然语言和可视化**\n**问题**：完全基于可视化的方式并不适合所有编程任务，尤其是逻辑复杂的情况。  \n**改进方向**：  \n让可视化编程工具支持**自然语言交互**，用户可以用文本描述部分逻辑，而不必完全依赖拖拽操作。  \n允许**文本代码与可视化编程共存**，比如在复杂逻辑部分可以直接写代码，而简单部分可以用可视化的方式调整。  \n📌 **案例**：  \n**Jupyter Notebook** 结合了可视化、代码和自然语言描述，成为数据科学领域的标配。  \n**GPT-4 代码解释器** 能够让用户用自然语言描述需求，再自动生成代码和可视化结果。  \n---\n**4. 增强可视化调试和探索能力**\n**问题**：许多可视化编程工具的调试能力较弱，用户很难在出错时快速定位问题。  \n**改进方向**：  \n提供**可视化调试工具**，比如让用户可以点击数据流中的某一步，直接查看输入和输出。  \n用动画或动态图表展示代码执行过程，而不仅仅是静态节点连接。  \n📌 **案例**：  \n**Observable**（可视化数据编程平台）允许用户实时调整数据并看到即时变化。  \n**Unreal Engine 的蓝图** 允许开发者在运行时“冻结”某个节点，查看其输入输出数据。  \n---\n**5. 让可视化编程成为“领域专用工具”，而不是通用编程语言**\n**问题**：可视化编程试图取代传统编程语言，但事实证明，通用编程任务往往更适合文本编写，而可视化更适用于特定领域。  \n**改进方向**：  \n让可视化编程专注于**特定领域**（如游戏开发、数据分析、机器学习）而非通用编程。  \n针对不同领域，设计合适的可视化交互方式，而不是用同一种模式去解决所有问题。  \n📌 **案例**：  \n**Blender 的节点编辑器** 适用于材质和特效编辑，而不适合写一般逻辑代码。  \n**Unity Visual Scripting** 主要用于游戏开发，针对性较强，而不是取代所有编程语言。  \n---\n**总结**\n可视化编程要突破瓶颈，关键在于：  \n✅ 让可视化编程真正“直觉化”，而不是仅仅把代码变成图形化界面。  \n✅ 以**数据流**为核心，而不是死守“节点-连线”范式。  \n✅ 结合自然语言，使得复杂逻辑可以用文字描述，而不是全靠拖拽。  \n✅ 强化**可视化调试和探索能力**，减少黑箱问题。  \n✅ **专注于特定领域**，不要试图做成通用编程语言。  \n思考\n形式与功能，跟声明式与命令式有些类似；但是明显编程语言发展更偏向于声明式，所以形式并不是不好，而是过于抽象了，导致限制过多，思维负担过重了？\n形式应该通过理解功能需求而自然演化出来。比如，设计一个UI工具时，工具的界面应该根据用户任务和数据流自动调整，而不是由开发者手动设置每个组件的连接方式。\nReference\nvisual-programming-is-stuck-on-the-form\ngpt-4o",
    "url": "/blog/可视化编程",
    "path": "tech/other/可视化编程.md",
    "lastModified": "2025-06-20T11:25:12.945Z"
  },
  {
    "title": "图像压缩质量评估",
    "content": "图像压缩质量评估\n压缩后的图像质量是评估压缩算法的重要指标。常用的图像质量评估方法包括主观评估和客观评估。这里只进行客观评估的讨论\n客观评估：\n峰值信噪比（PSNR）\n结构相似性指数（SSIM）\n峰值信噪比（PSNR）\n公式中最大像素值（MAX）的作用对于全面理解 PSNR 的计算过程至关重要。让我们详细解释一下这个概念。\n1. PSNR 公式\nPSNR 的计算公式为：\n$$\n\\text{PSNR} = 10 \\cdot \\log_{10} \\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right)\n$$\n其中：\n\\(\\text{MSE}\\) 是均方误差，表示原始图像和压缩图像之间的平均像素差异。\n\\(\\text{MAX}\\) 是图像的最大像素值。\n2. MAX 的定义\nMAX 是图像中像素值的最大可能值。对于不同的图像类型，MAX 的值不同：\n对于 8 位灰度图像，像素值范围是 0 到 255，因此 MAX = 255。\n对于 16 位图像，像素值范围是 0 到 65535，因此 MAX = 65535。\n3. MAX 在 PSNR 公式中的作用\n3.1 标准化误差\nMAX 的主要作用是标准化均方误差（MSE），使得 PSNR 值可以在不同图像类型之间进行比较。直接使用 MSE 可能会导致不同图像类型之间的比较不公平，因为它们的像素值范围不同。\n3.2 无量纲化\n通过将 MSE 除以 MAX 的平方，PSNR 公式消除了像素值范围的影响，使得计算结果无量纲化。这意味着无论图像的像素值范围如何，PSNR 值都可以在相同的尺度上进行解释。\n3.3 对数尺度\nPSNR 使用对数尺度来表示误差，这使得它对人类感知更直观。对数变换可以压缩大范围的误差值，使得较小的误差变化更容易被察觉。\n4. 举例说明\n假设我们有两张 8 位灰度图像，原始图像和压缩图像，其 MSE 为 100。计算 PSNR 的步骤如下：\n4.1 计算 MSE\n$$\n\\text{MSE} = 100\n$$\n4.2 计算 MAX 的平方\n对于 8 位灰度图像：\n$$\n\\text{MAX} = 255\n$$\n$$\n\\text{MAX}^2 = 255^2 = 65025\n$$\n4.3 计算 PSNR\n$$\n\\text{PSNR} = 10 \\cdot \\log_{10} \\left( \\frac{65025}{100} \\right)\n$$\n$$\n\\text{PSNR} = 10 \\cdot \\log_{10} (650.25)\n$$\n$$\n\\text{PSNR} \\approx 10 \\cdot 2.813 = 28.13 \\text{ dB}\n$$\n5. 总结\n通过引入 MAX，PSNR 公式能够：\n标准化误差，使得不同图像类型之间的比较成为可能。\n无量纲化误差，使得 PSNR 值在同一尺度上进行解释。\n使用对数尺度，使得较小的误差变化更容易被察觉。\nMAX 的引入确保了 PSNR 是一个可靠且一致的图像质量评估指标，无论图像的像素值范围如何，它都能提供一个直观的质量度量。\n结构相似性指数（SSIM）\n结构相似性指数（SSIM，Structural Similarity Index）是一种用于衡量两幅图像之间相似性的指标，特别适用于评估图像质量。与峰值信噪比（PSNR）不同，SSIM 更加注重图像的结构信息，因此在反映人类视觉感知方面更为有效。以下是详细说明：\n1. SSIM 的基本原理\nSSIM 通过比较图像的亮度、对比度和结构来评估图像质量。其公式如下：\n$$\n\\text{SSIM}(x, y) = \\frac{(2 \\mu_x \\mu_y + C_1)(2 \\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\n$$\n其中：\n\\( \\mu_x \\) 和 \\( \\mu_y \\) 分别是图像 \\( x \\) 和 \\( y \\) 的平均值。\n\\( \\sigma_x^2 \\) 和 \\( \\sigma_y^2 \\) 分别是图像 \\( x \\) 和 \\( y \\) 的方差。\n\\( \\sigma_{xy} \\) 是图像 \\( x \\) 和 \\( y \\) 的协方差。\n\\( C_1 \\) 和 \\( C_2 \\) 是为了稳定分母而引入的小常数。\n2. SSIM 的三个组成部分\n2.1 亮度比较（Luminance Comparison）\n$$\nl(x, y) = \\frac{2 \\mu_x \\mu_y + C_1}{\\mu_x^2 + \\mu_y^2 + C_1}\n$$\n亮度比较部分衡量两幅图像的平均亮度差异。\n2.2 对比度比较（Contrast Comparison）\n$$\nc(x, y) = \\frac{2 \\sigma_x \\sigma_y + C_2}{\\sigma_x^2 + \\sigma_y^2 + C_2}\n$$\n对比度比较部分衡量两幅图像的对比度差异。\n2.3 结构比较（Structure Comparison）\n$$\ns(x, y) = \\frac{\\sigma_{xy} + C_3}{\\sigma_x \\sigma_y + C_3}\n$$\n结构比较部分衡量两幅图像的结构相似性。\n局限性：协方差衡量的是两个数据集（在这里是两幅图像的像素值）之间的共同变化程度。但是，协方差的值会受到数据集的尺度影响。例如，对于两个数据集，如果它们的值都很大，协方差的值也会很大；如果它们的值都很小，协方差的值也会很小。因此，协方差的绝对值并不能直接反映两个数据集之间的相似性，需要进行标准化处理。\n处理方式：用协方差值除去标准差做标准化\n其他标准化：机器学习中的图像训练中的像素预处理归一化（x/255 [0, 1]）\n3. 为什么 SSIM 能表示图片压缩效果\n3.1 考虑人类视觉系统\nSSIM 模拟了人类视觉系统的感知特性，通过亮度、对比度和结构三个方面进行比较。这使得 SSIM 能更好地反映图像在视觉上的质量，而不仅仅是像素级别的差异。\n3.2 综合评估图像质量\nSSIM 不仅考虑了像素值的差异，还考虑了图像的整体结构和纹理信息。这使得 SSIM 能够更全面地评估图像的质量，特别是在评估压缩算法的效果时。\n3.3 更加鲁棒\n由于 SSIM 是基于局部窗口计算的，它对图像的局部失真（如边缘、纹理等）更加敏感，这些失真往往是压缩算法引入的。因此，SSIM 能更准确地反映压缩对图像质量的影响。\n5. 结论\n**高 SSIM 值（接近 1）**：表示压缩图像与原始图像在亮度、对比度和结构上非常相似，图像质量较好。\n**低 SSIM 值（接近 0）**：表示压缩图像与原始图像在亮度、对比度和结构上差异较大，图像质量较差。\n总结\nSSIM 通过综合考虑图像的亮度、对比度和结构信息，能够更全面地评估图像的质量，特别是在反映人类视觉感知方面具有优势。相比于只关注像素差异的 PSNR，SSIM 更加适合用于评估图像压缩效果，因为它能够更准确地反映压缩对图像整体视觉质量的影响。",
    "url": "/blog/图像压缩评估",
    "path": "tech/other/图像压缩评估.md",
    "lastModified": "2025-06-20T11:25:12.940Z"
  },
  {
    "title": "Introduction",
    "content": "Introduction\n理解计算机图形的基本概念\n问题\n浏览器如何调用显卡功能，运行 AI 和网页渲染？\n渲染图形的基本构建块为啥是三角形？\n原因主要包括以下几点：\n1. **几何稳定性**：\n三角形是最基本的多边形，任何多边形都可以通过分割成三角形来表示。\n三角形在二维和三维空间中都始终保持平面性，不会因顶点位置的改变而变形。这种性质使三角形在计算机图形学中非常稳定。\n2. **计算简单**：\n渲染三角形涉及的计算较为简单，易于进行光栅化、投影和纹理映射。\n光照计算（如Phong着色模型）和其他着色算法也能高效地应用在三角形上。\n3. **硬件优化**：\n现代图形处理单元（GPU）和渲染管线对三角形进行了专门优化。三角形光栅化是GPU的基本操作，硬件加速可以显著提高渲染速度和效率。\n三角形可以方便地利用顶点着色器和片元着色器来进行并行处理。\n4. **顶点和边缘管理**：\n三角形只有三个顶点和三条边，管理起来相对简单，便于进行顶点处理、边缘检测和背面剔除。\n在进行复杂形状的模型化时，使用三角形可以减少算法复杂度。\n5. **数据表示**：\n使用三角形网格（triangular mesh）可以有效地表示和处理复杂的几何形状。任意复杂的表面都可以通过三角形网格近似表示。\n示例说明\n几何稳定性\n考虑一个四边形（四个顶点）：\n如果四个顶点不共面，四边形就会“扭曲”，不再是一个平面多边形。\n而任何三个点都一定在一个平面上，因此三角形总是稳定的。\n硬件优化\n现代GPU的设计就是为了高效处理大量三角形。如下简化的渲染管线展示了如何处理三角形：\n1. **顶点处理**：顶点着色器处理每个顶点的变换和光照计算。\n2. **图元组装**：顶点组合成三角形。\n3. **光栅化**：将三角形转换成片元（像素），进行逐片元操作（如着色、深度测试）。\n数据表示\n一个复杂模型可以通过三角形网格表示。假设一个立方体模型：\n每个面是一个四边形，可以分成两个三角形。\n因此，立方体可以由12个三角形（6个面，每个面2个三角形）表示。\n三角形易于进行光栅化、投影和纹理映射的原因？\n例子说明：\n光栅化\n光栅化是将几何图形转换为屏幕上的像素。三角形在光栅化过程中表现得非常好，主要原因如下：\n1. **边界确定性**：\n三角形只有三条边，确定其覆盖的像素范围非常简单。\n使用边缘函数（Edge Function），可以高效判断一个像素是否在三角形内。\n2. **扫描线算法**：\n扫描线算法可以轻松应用于三角形。沿着y轴从顶到底逐行扫描，通过插值计算每行的起点和终点。\n对于每一行，简单地填充从起点到终点的像素。\n示例：\n考虑一个简单的三角形，其顶点为(10, 10), (50, 20), (30, 60)，通过扫描线算法，我们可以轻松地计算出每一行的填充像素范围。\n投影\n投影是将三维坐标转换为二维屏幕坐标。三角形的投影计算非常简便：\n1. **顶点投影**：\n三角形只有三个顶点，每个顶点的投影变换可以独立计算，然后再连接这些投影后的顶点即可。\n2. **透视变换**：\n透视变换涉及到将3D坐标乘以一个投影矩阵，对于每个顶点都进行相同的操作。\n变换后的三角形顶点仍然保持线性关系，确保变换后的形状是正确的。\n示例：\n假设一个三角形在3D空间中的顶点为(1, 2, 3), (4, 5, 6), (7, 8, 9)，通过投影矩阵可以计算出这三个顶点在2D平面上的投影点，然后绘制出投影后的三角形。\n纹理映射\n纹理映射是将二维图像（纹理）应用到三维模型表面。三角形的纹理映射容易处理，主要是因为其线性插值性质：\n1. **线性插值**：\n在三角形内部，任何一点都可以通过三个顶点的线性插值计算得到。\n对于纹理坐标（u, v），通过顶点的纹理坐标进行插值，可以高效地计算出三角形内部任意点对应的纹理坐标。\n2. **Barycentric坐标（重心坐标）**：\n重心坐标是一种坐标系统，其中每个点的坐标都是基于三角形顶点的加权平均。重心坐标满足以下条件：\n$$\n\\lambda_1 + \\lambda_2 + \\lambda_3 = 1\n$$\n示例：\n如果P是三角形ABC的重心（即三条中位线的交点），那么P的Barycentric坐标就是(1/3, 1/3, 1/3)，因为重心到三个顶点的距离是相等的，所以权重也相等。\n为什么使用 Barycentric坐标 会让纹理计算得到的效果更加细致跟真实？\n因为 Barycentric 坐标提供了对三角形内部位置的精确描述，从而实现了更准确的纹理坐标插值和采样：\n1. 精确的位置描述：Barycentric 坐标通过权重值准确地描述了点在三角形内的位置关系。它不仅考虑了点与三个顶点之间的距离，还考虑了点与三角形内的边界之间的关系。这种精确的位置描述使得纹理计算可以更准确地确定每个点对应的纹理坐标。\n2. 光滑的插值：Barycentric 坐标可以在三角形内进行线性插值，这意味着纹理坐标可以根据点在三角形内的位置平滑地过渡。相邻点之间的纹理坐标也会进行平滑的插值，从而在渲染时产生连续、无缝的纹理效果。\n3. 保持纹理特征：Barycentric 坐标的使用可以确保纹理特征在三角形内的点之间保持一致。这意味着如果一个三角形上的某个区域在纹理中具有特定的图案或纹理细节，那么通过 Barycentric 坐标计算得到的纹理坐标也会在该区域内保持相应的特征，从而保持纹理的连贯性和真实感。",
    "url": "/blog/图形学中的三角形",
    "path": "tech/other/图形学中的三角形.md",
    "lastModified": "2025-06-20T11:25:12.940Z"
  },
  {
    "title": "惰性求值与宏",
    "content": "延迟求值和宏是两个经典的元编程概念，特别是在 Lisp 等语言中，通过这些机制可以让程序员灵活控制代码的执行顺序和代码生成。用 JavaScript 实现它们的基本示例可以帮助理解这些概念。\n1. 延迟求值（Lazy Evaluation）\n延迟求值是一种策略，即在真正需要值时才进行计算，而不是在表达式定义时立即求值。这种策略有助于提高效率、避免不必要的计算，并且可以处理无穷数据结构。\n在 JavaScript 中，我们可以通过闭包和函数来实现基本的延迟求值机制：\n解释\n**delay** 函数接受一个表达式，并返回一个闭包（函数），此时不会执行表达式。\n**force** 函数接受一个延迟表达式（闭包）并立即执行它，从而得到实际结果。\n这种延迟求值的方式可以让我们控制何时执行表达式，尤其在处理惰性数据结构（如流）时非常有用。\n延迟求值在流中的应用\n可以将延迟求值应用于流式数据结构，创建一个惰性列表（stream）：\n解释\n**cons** 函数创建一个流节点，其中  是一个闭包函数，只有在访问时才会递归生成下一个元素。\n创建一个从指定值开始的自然数流，使用延迟求值确保每次访问  时才生成下一个数字。\n通过这种方式，我们可以构建无限数据结构并逐步生成值。\n2. 宏（Macros）\n宏是元编程中重要的工具，允许我们在代码运行之前生成和操作代码片段。虽然 JavaScript 不支持 Lisp 那种经典的宏（编译时代码生成），我们可以通过函数模拟一些基本的宏行为，特别是可以用 JavaScript 的高阶函数来实现一些简单的“语法扩展”。\n以下是一个模拟宏展开的例子：\n解释\n**defineMacro** 函数用于定义一个宏，将宏名称与其展开函数关联。\n**expandMacro** 函数用于递归地对表达式进行宏展开。当遇到宏名时，会调用相应的展开函数，将其替换为展开后的代码。\n通过这种方式，我们创建了一个简单的宏系统，可以在运行前替换代码模式。这虽然不是真正的宏（编译时生成代码），但模拟了一些宏的基本行为，例如生成条件语句  的模式。\n3. 宏与延迟求值的结合\n宏和延迟求值可以结合在一起，实现更复杂的控制结构。以下是一个  宏的例子（只有在条件为假时才执行某个表达式）：\n解释\n宏将条件取反，只在条件为假时才执行代码块。\n宏生成一个逻辑非的表达式。\n总结\n**延迟求值**通过闭包来延迟计算，常用于惰性数据结构。\n**宏**可以定义代码生成模式，使代码更具可读性和扩展性，模拟一些条件控制和逻辑结构。\n这些概念在函数式编程和元编程中十分强大，让我们能灵活控制代码的执行顺序和生成逻辑结构，从而实现更强大的语言功能。",
    "url": "/blog/惰性求值与宏",
    "path": "tech/other/惰性求值与宏.md",
    "lastModified": "2024-11-08T10:49:15.065Z"
  },
  {
    "title": "关于编程语言的发展",
    "content": "关于编程语言的发展\n一开始\n运行环境：OS\n语言\n汇编 -> C（FP, OS) -> C++（C +OOP）\n托管时代\n运行环境：受托管的执行环境上\n特征：自动的垃圾收集，类型安全等\n语言\nJava（C++ -FP +OOP, JVM）-> Scala（Java +FP, JVM）\n命令式/声明式编程(DSL+函数式)\n命令式（Imperative）\n特征：做什么（What） + 如何（How）（最终目的跟如何达成混在一起）\n例子：for循环，i += 1等等，部分细节会掩盖了“最终目标”，后面可能多往What方向演进\n优势：更加精细化控制，但需要手动\n问题：并行执行程序变得困难，因为像“执行目的”这样更高层次的信息已经丢失\n声明式\n外部DSL\n描述：针对特定的领域设计，独立性强（主要被编译解析，不强依赖特定语言，例如：Dart-SASS等编译工具）\n特征：做什么（What）（只是表现出最终的目的，而不是明确指定做事的方式）\n例子：XML、HTML、SASS、SQL等\n发展：利用既有DSL的方言（附带成熟工具链）\n内部DSL\n描述：在既有语言上做一系列特别的API及使用模式\n例子：Jquery（伪装成一种DSL把一系列操作通过\".\"流畅连接）\n函数式\n特征：容易并行（因为运行时不会修改任何状态，因此无论多少线程在运行时都可以观察到正确的结果。）\n动态语言/静态语言\n动态语言：JIT，源码发布，运行时动态解释执行，例如JS\n静态语言：特征：AOT，提前编译成目标代码，例如C\n汇总\n抽象级别越来越高（下一个抽象级别？）\n构建于现有的工具上（Rust以LLVM为目标编译，Scala在JVM的运行）\n多范式（FP，OOP）\n更多声明式，更少的命令式：让代码包含更多的“What”，而不是“How”；执行环境便可以更加聪明地去适应当前的执行要求。\nAOT+JIT？元编程：用代码生成（操纵）代码？\n组合是未来？（Rust 的编程建模模式更像是组合（Composition） 而不是继承（Inheritance）？类似 Vue3 选择 Composition-API 代替 Class Option（更好的逻辑组合复用，更好的支持TS））\nReference\n编程语言的发展趋势和未来方向\n怎样理解元编程\nhttps://www.zhihu.com/question/32304837/answer/275204204",
    "url": "/blog/编程语言的发展",
    "path": "tech/other/编程语言的发展.md",
    "lastModified": "2025-06-20T11:25:12.952Z"
  },
  {
    "title": "References",
    "content": "Structure and Interpretation of Computer\n第一章：构建抽象过程（Building Abstractions with Procedures）\n第二章：构建数据抽象（Building Abstractions with Data）\n第三章：模块化、对象和状态（Modularity, Objects, and State）\n第四章：元语言抽象（Metalinguistic Abstraction）\n《计算机程序的构造与解释》第4章中的**元循环求值器**和**嵌套求值器**是关于解释器的构造及其扩展的经典话题。我们可以使用 JavaScript 来实现一些简化版的示例，帮助你理解这些概念。\n1. 元循环求值器（Meta-circular Evaluator）\n元循环求值器是一个解释器，它使用自身语言来解释和执行代码。在这个例子中，我们将实现一个简单的表达式求值器，可以解析和计算类似于 Lisp 的表达式，例如  或 。\n以下是一个使用 JavaScript 实现的简单元循环求值器：\n解释\n**parse** 函数将类似  的字符串转换为一个嵌套的数组结构 。\n**evaluate** 函数是核心的元循环求值器。它使用递归来解析表达式并调用相应的操作函数。\n如果表达式是一个数字，直接返回。\n如果是一个字符串（变量名或操作符），则从环境  中查找。\n如果是一个嵌套数组，它会递归地求值操作符和参数，并将参数应用到操作符上。\n这个解释器就是所谓的“元循环”的，因为它在 JavaScript 中解释了一个微型的 Lisp 式语言，而 JavaScript 本身也在运行解释器。这种结构非常适合理解解释器的原理。\n2. 嵌套求值器与组合语言\n在元循环求值器的基础上，我们可以进一步扩展，让解释器支持更复杂的嵌套求值和新的语言特性，比如延迟求值或条件表达式。\n这里，我们添加一个新的  语法和  语法，实现条件求值和变量绑定：\n解释\n**define**：实现了变量绑定，将变量名和值存储到当前环境中，使得后续的表达式可以访问这些定义。\n**if**：实现了条件分支，只有满足条件的分支会被求值。\n通过这种方式，我们构造了一个嵌套求值器，支持更复杂的组合语言结构，允许我们在表达式中进行条件判断和变量定义。\n总结\n**元循环求值器**是一个基础解释器，用于直接求值基本表达式。\n**嵌套求值器**则在元循环求值器之上扩展，支持条件、变量绑定等结构，使得语言更强大和灵活。\n这些概念展示了如何逐步构建出一个解释器，并通过扩展使解释器支持更复杂的语言特性，逐步达到现代编程语言的复杂度。\n第五章：寄存器机器模型（Computing with Register Machines）\nSICP 第五章介绍了“寄存器机器和编译”的概念，主要涉及如何设计和实现寄存器机器模型以及编译的基本思想。这些概念在现代计算机架构和编译器设计中广泛应用。借鉴其中的思想，我们可以在 JavaScript 中实现一些简单的模拟，理解寄存器机器的工作机制和编译的过程。\n以下是一些 JavaScript 示例，展示寄存器机器和编译在编程中的启发性应用。\n2. 编译简单的表达式\n编译器的核心任务之一是将高级语言的表达式编译为更底层的指令。以下示例展示了一个简单的编译器，它将 JavaScript 表达式编译为寄存器机器的指令。\n在这个例子中，我们编写了一个简单的编译器函数 ，它将一个 JavaScript 表达式转换为寄存器机器的指令。这个编译器支持加法和减法，并生成了相应的“MOV”、“ADD”和“SUB”指令。\n---\n3. 虚拟寄存器机器的解释器\n寄存器机器可以看作一种“虚拟机”，它需要解释器来解释执行指令。以下是一个解释器，它可以解析并运行由编译器生成的寄存器指令。\n在这个示例中， 类是一个寄存器机器的解释器。它加载由编译器生成的指令，并按照这些指令一步一步执行，更新寄存器的状态。通过这种方式，我们可以看到如何在寄存器机器和编译器的基础上构建出一个简化的计算系统。\n---\n4. 优化：常量折叠\n编译器中一个常见的优化是常量折叠，即在编译时计算表达式的常量部分。以下是一个简单的常量折叠优化，帮助减少指令数量，提高运行效率。\n在这个优化中，我们在编译阶段提前计算常量表达式 ，将其结果  替换掉。这样在编译生成的代码中，只需处理最终的 ，从而减少了不必要的指令。\n---\n总结\nSICP 第五章关于寄存器机器和编译的思想为 JavaScript 编程提供了很多启发：\n1. **寄存器模拟**：寄存器机器模型帮助我们理解计算如何基于指令和寄存器来存储和操作数据。\n2. **编译原理**：编译的过程将高级表达式转化为低级指令，让我们更清晰地理解表达式求值的底层机制。\n3. **解释执行**：通过虚拟机解释器的设计，我们能看到计算机如何一步步执行编译后的指令。\n4. **编译优化**：常量折叠等优化技术在编译器中常用，能在运行前提升代码效率。\nReferences\n计算机程序的构造与解释\ngpt",
    "url": "/blog/计算机程序的构造与解释 - js",
    "path": "tech/other/计算机程序的构造与解释 - js.md",
    "lastModified": "2024-11-08T10:55:25.692Z"
  },
  {
    "title": "世界趋同",
    "content": "世界趋同\n**2. 互联网 & 软件设计趋同**\n✅ **社交媒体 UI**\n早期：Facebook、Twitter、Instagram、TikTok 界面各有不同。\n现在：几乎所有 App 都有**短视频推荐流**（Instagram Reels、YouTube Shorts、微博 Story）。\n各大社交平台开始采用类似的“推荐+算法”机制。\n✅ **编辑器与 AI 集成**\n过去，VS Code、JetBrains、Vim、Emacs 等编辑器风格各异。\n现在，几乎所有主流编辑器都支持：\n**AI 补全（Copilot, Tabnine, Cursor）**\n**Agent 交互**\n**GitHub 代码搜索集成**\n✅ **搜索引擎的 AI 化**\n过去：搜索引擎主要是基于关键字索引的传统网页搜索。\n现在：所有搜索引擎（Google, Bing, 百度等）都在转向**AI 搜索+摘要生成+对话式回答**，最终形态趋同。\n---\n**3. 商业模式趋同**\n✅ **游戏收费模式**\n过去：游戏有单机买断制、点卡制、免费试玩等多种商业模式。\n现在：越来越多游戏采用**免费+内购（微交易）**模式，例如：\n《英雄联盟》《王者荣耀》《原神》等。\n甚至许多原本卖断制的游戏（如 FIFA、使命召唤）都转向**“战斗通行证”+ 赛季收费**模式。\n✅ **短视频和直播**\n过去：短视频和直播是分开的，如 YouTube 主要是长视频，Twitch 主要是游戏直播。\n现在：所有平台都在做**短视频+直播+带货**，比如：\n抖音和快手直播带货\nB 站加了短视频、直播\nYouTube Shorts\n✅ **流媒体模式**\n过去：电影和电视剧在电影院、电视台播放。\n现在：Netflix、Disney+、HBO Max、爱奇艺等平台都采用**订阅制+独家内容**，形成趋同。\n---\n**4. 文化 & 影视内容趋同**\n✅ **电影 & 电视剧续集化**\n过去：电影往往是独立作品，系列较少。\n现在：几乎所有大 IP 都在拍续集、重启、宇宙化，比如：\n漫威宇宙、DC 宇宙、星战宇宙、哈利波特宇宙\n国产剧也开始流行“季播化”（《庆余年》《隐秘的角落》）\n✅ **K-Pop 影响全球流行音乐**\n过去：欧美流行音乐占据主流，各国风格迥异。\n现在：K-Pop 影响力扩大，全球流行音乐逐渐向 K-Pop 的**舞蹈+视觉包装+团体模式**靠拢，比如：\n中国、日本的偶像团体开始模仿 K-Pop 练习生制度。\n欧美流行歌手（如 Lady Gaga, BTS 合作）借鉴 K-Pop 营销方式。\n✅ **美剧和国产剧风格趋同**\n过去：美剧更快节奏，国产剧更慢热。\n现在：国产剧越来越采用**“快节奏+美剧式叙事”**，例如：\n《开端》《隐秘的角落》借鉴美剧的紧凑叙事。\n《三体》剧版采用好莱坞式的制作水准。\n---\n**5. 生活方式 & 审美趋同**\n✅ **全球建筑风格趋同**\n过去：不同国家的建筑风格迥异（如中国四合院、日本町屋、欧洲哥特式建筑）。\n现在：现代建筑越来越趋同，高楼大厦以**玻璃幕墙+极简线条**为主，如：\n迪拜、上海、纽约的摩天大楼风格越来越像。\n连国内的住宅小区也趋向统一的现代风格（高层+玻璃窗）。\n✅ **服装潮流趋同**\n过去：不同国家的流行趋势差异大。\n现在：\n全球流行的**极简风、机能风（Techwear）、复古 Y2K**几乎同步。\n运动品牌风潮全球一致，如 Nike、Adidas、New Balance 在全球大热。\n✅ **饮食习惯趋同**\n过去：不同国家的饮食文化差异很大。\n现在：\n全球范围内**咖啡文化**流行，星巴克、瑞幸等品牌让咖啡成为日常饮品。\n各国快餐文化趋同，麦当劳、肯德基的菜单全球相似，甚至出现“亚洲风味汉堡”等融合趋势。\n---\n**总结：世界趋同的核心原因**\n1. **市场竞争** → 低成本、高效率的解决方案会成为行业标准（如黑色汽车、电动车设计）。\n2. **技术进步** → 开源技术、算法驱动，导致产品功能和设计趋同（如 AI 补全、搜索 AI 化）。\n3. **全球化与信息传播** → 互联网让流行趋势迅速传播，导致文化、审美趋同（如 K-Pop、建筑设计）。\n4. **用户习惯养成** → 习惯一旦形成，厂商不愿意冒险改变（如短视频 UI、推荐算法）。\n5. **路径依赖** → 过去的成功模式容易被复用，如战斗通行证、流媒体订阅、电影续集化。\n世界趋同是一种**效率最大化**的结果，但也会带来**创新空间受限**的问题，因此有时候“逆趋同”的小众创新（如赛博朋克风格、复古科技）反而会成为新的趋势突破点。\nReference\ngpt-4o",
    "url": "/blog/世界趋同",
    "path": "thoughts/life/世界趋同.md",
    "lastModified": "2025-06-20T11:25:12.944Z"
  },
  {
    "title": "显示器",
    "content": "显示器\n人眼分辨率与 8K\n>整体上人眼的分辨率大约相当于 7K-8K，但在中央视野的分辨率要高于 8K。意味着 8K 是显示器发展的极限？\n可以用一些物理和感知上的衡量标准来估计人眼的“分辨率”。\n1. **角分辨率 (Angular Resolution)**\n人眼的分辨率通常用“角分辨率”来衡量，它指的是眼睛能分辨的最小角度。正常视力（20/20 视力）下，典型的角分辨率是 **1 弧分**（1 arcminute），也就是 1/60 度。\n这意味着，人眼能分辨出 1 弧分的两个点之间的差别。\n2. **像素换算**\n要将角分辨率转化为类似显示器的“像素”概念，我们可以进行一些估算。如果假设视角宽度为 120 度，视场高度为 60 度，并且视距约为 25 cm（阅读距离），以下是简单的计算：\n**水平分辨率**：人眼水平视角为 120 度，每度 60 弧分，因此眼睛的理论水平分辨率是 120 × 60 = 7200 弧分。\n**像素等效**：如果用像素来衡量，可以假设每弧分为一像素。这样，视野宽度约为 **7200 像素**。同理，垂直视野为 60 度，分辨率约为 **3600 像素**。\n所以，从分辨率角度看，整个人眼的视野大约相当于 **7200 × 3600 像素**，约等于 **26 兆像素（MP）**。\n3. **中央凹 (Fovea) 的精度**\n然而，人眼并非在整个视野上都有均匀的分辨率。中央凹是视网膜上分辨率最高的区域，位于眼睛的中心，用于处理精细的视觉信息。在中央凹区域，视网膜上的感光细胞（视锥细胞）密度极高，分辨率也更高。\n在中央凹，视力最高的部分可以达到大约 **500-600 PPI（像素每英寸）** 的感知精度。因此，尽管人眼的总分辨率可能在 26 MP 左右，但在最集中的区域分辨率更高。\n4. **动态分辨率与大脑的处理**\n人眼和大脑协同工作，通过眼睛的快速移动（称为“扫视”）和大脑的感知机制，将局部高分辨率的图像拼接在一起。这意味着人眼并不总是以固定的分辨率“拍摄”世界，而是通过快速聚焦和大脑补偿形成清晰的感知。\n总结：\n**角分辨率**：1 弧分（1/60 度）。\n**像素等效**：大约相当于 **7200 x 3600 像素**（约 26 MP）。(8K 是 7680 x 4320)\n**中央凹**：中央视野分辨率较高，达到 500-600 PPI。\n**大脑处理**：大脑通过感知和整合提高视觉精度。\n扫视比盯着一个地方更耗费能量？\n意味着带鱼屏比 16:9 的屏幕更耗费眼睛？\n1. **大脑的视觉处理非常耗能**\n视觉系统是大脑中最复杂的系统之一，大约消耗了大脑整体能量的 **30%**。处理视觉信息需要大脑整合来自视网膜的信号，进行识别、分析、和解码。因此，大脑为了处理复杂的视觉输入，确实需要大量能量来支持：\n**信息处理**：眼睛捕捉的每一个视觉场景都会生成大量的信息，大脑需要通过视觉皮层和其他认知功能区域进行处理。这包括边缘检测、颜色识别、深度感知等复杂任务。\n**补偿机制**：大脑不仅处理即时的视觉输入，还需要进行复杂的推测、模式识别、补偿盲点等操作，这进一步增加了能量需求。\n2. **扫视与视觉更新的能量消耗**\n扫视（saccades）是眼睛快速跳动的过程，大约每秒发生3-4次。每次扫视都会让眼睛重新获取新信息，并让大脑重新处理这些新输入。相比长时间盯着一个地方，频繁的扫视会增加以下方面的能量消耗：\n**眼部肌肉活动**：扫视需要眼部肌肉的持续运动，而长时间保持不动的凝视（fixation）则相对耗费更少的肌肉能量。眼睛的快速移动本身需要动用一系列眼外肌来控制方向和焦点。\n**大脑视觉处理负荷**：每次扫视时，大脑都要重新处理新的视觉输入，并整合之前的信息。这增加了视觉皮层的处理负担。而当我们固定盯着某个地方时，大脑只需维持当前视觉图像的稳定性，而无需处理大量的新信息。\n**抑制机制**：在扫视期间，视觉输入会被短暂抑制（称为**扫视抑制**，Saccadic Suppression），以防止大脑处理模糊或跳跃的画面。这种抑制机制也是大脑额外的工作，并需要一定的能量。\n因此，**频繁的扫视相比固定注视，确实更耗费能量**，因为它不仅需要眼部肌肉的运动，还会给大脑带来更多处理负荷。\n3. **注视固定 vs. 扫视的能量比较**\n**长时间注视一个点**：\n眼睛的肌肉活动最小，只需维持轻微的聚焦。\n大脑不需要处理太多新信息，只需维持当前视觉图像的稳定性，降低信息处理量。\n整体能量消耗较低。\n**频繁扫视**：\n每次扫视都会涉及眼部肌肉的剧烈运动，消耗更多能量。\n每次扫视会带来大量新视觉信息，大脑需要不断重新处理、分析和整合，增加能量消耗。\n需要视觉抑制机制来避免画面模糊，大脑需要进一步协调视觉皮层的活动。\n4. **大脑的高效处理机制**\n虽然扫视会消耗更多能量，但大脑也有一些高效的机制来节省资源。比如，在固定注视的过程中，大脑会将不太重要的部分模糊化或降低处理优先级，集中精力处理感兴趣或重要的区域。**在不需要大量精细处理的情况下，大脑可以调整资源分配，节省能量**。\n5. **注意力与能量消耗**\n此外，**注意力的集中也会增加能量消耗**。当你盯着一个特定的点或物体时，大脑需要调动更多的资源来处理该区域的细节，这也可能导致能量消耗增加。但如果注意力较低或视觉任务较简单，盯着一个地方的能量消耗仍然比频繁扫视低。\n总结\n**扫视比固定注视更耗能**：由于眼球运动和大脑处理新信息的复杂性，扫视会比盯着一个地方消耗更多能量。\n**大脑的视觉处理耗费大量能量**：即便是静止不动，大脑也需要持续处理视觉输入，并维持视觉的稳定。\n**补偿机制进一步增加能量消耗**：大脑通过整合、预测和填补视觉信息来优化感知，这些复杂的处理过程也增加了能量需求。\n视觉残留与刷新率（90Hz-120Hz）\n当刷新率达到某个高水平后，比如超过 144Hz，视觉系统很难再区分帧与帧之间的差异\n**视觉残留**（**Persistence of Vision**）是指当物体的图像在视网膜上消失后，**图像的感知会在短时间内继续保留**，大脑会对该图像的印象延续数毫秒。这个现象帮助我们将一系列快速切换的静态图像感知为连续运动。\n具体理解：\n1. **视网膜滞留**：当光线刺激眼睛时，视网膜上的感光细胞会产生电信号，传递到大脑。然而，光线消失后，这些信号并不会立即停止，而是持续一小段时间（通常是几毫秒）。这意味着，**即使物体不再被直接看到，它的图像仍会在视网膜上“停留”片刻**。\n2. **运动感知**：视觉残留是我们感知**连续运动**的基础。比如在电影或动画中，每秒只展示24帧静态图片，但由于视觉残留，我们会感知到流畅的连续画面，而不是一帧一帧的闪动。\n3. **视觉补偿**：大脑利用视觉残留来弥补快速运动或切换的画面空隙，让我们感知到一个连贯的世界。这是大脑补偿机制的一部分，有助于避免画面跳动或断裂感。\n常见例子：\n**电影和电视**：电影通常播放速度为每秒24帧。视觉残留让这些帧不会显得分散，而是形成平滑的运动效果。\n**光迹现象**：在夜晚挥舞光源时，会看到光的轨迹，这是因为光的图像在视网膜上残留了一段时间。\n总结：\n**视觉残留**是人眼和大脑的一种生理现象，帮助我们将快速切换的静态图像整合为连续的运动感知。它在视频、动画以及现实世界的运动感知中起到关键作用，避免了我们看到闪烁的画面或不连续的影像。\n人体的残留现象不仅限于视觉和听觉，还涉及触觉、嗅觉、味觉、运动及情绪等多个方面。这些现象反映了身体感官系统在处理信息时的复杂性，以及我们如何整合和体验感知世界。\n为什么听觉残留更加短暂\n听觉残留是一种声音在消失后仍能被感知的现象，类似于视觉残留，它对我们理解语言、欣赏音乐和维持听觉流畅性具有重要作用。\n听觉残留比视觉残留更加短暂，并非完全因为**信息量**的不同，而是由于以下几个因素：\n1. **感官处理机制的差异**\n视觉和听觉在信息处理上存在本质差异。**视觉**通常以较大的空间分辨率接收大量的静态或动态图像，且每个视觉帧包含丰富的细节。相比之下，**听觉**处理的是**时间维度**上的连续波形，这种信息的传递和感知速度比视觉更快。\n视觉通常需要大脑更长时间来整合和处理图像，因此视觉残留时间稍长。\n听觉需要实时、精确地处理声音，尤其在语音和音乐中，音频信号必须被快速解析和传递给大脑，这使得听觉残留时间更短，以避免混淆连续的声音信号。\n2. **时间敏感性**\n**听觉是一个极为敏感的时间依赖感官**。声音的频率、节奏和持续时间决定了我们如何理解语言和音乐，因此需要快速、精准的处理。\n听觉残留时间较短，避免产生音符或语言音节间的混淆。\n如果听觉残留时间过长，可能会导致声音重叠或延迟感，影响听觉感知的清晰度和流畅性。\n3. **大脑的感知优化**\n为了处理不同类型的感知信息，大脑对听觉和视觉信号采取了不同的优化策略：\n**视觉**可以容忍较大的时间延迟，特别是在电影、动画等领域，通过残留现象让画面流畅。\n**听觉**必须保持精确的时间节奏。短暂的残留允许听觉系统在快速接收新的声音信号的同时保持信息传递的连贯性。\n4. **信息结构的差异**\n虽然声音在时间维度上是动态的，但它的**信息密度**通常低于视觉。例如，一幅静态图像可能包含数百万像素的信息，而一段音频信号在相同时间内传递的细节较少。因此，听觉系统的处理时间较短。\n结论\n听觉残留较短并非仅仅因为信息量较小，而更多是由于**感官处理机制、时间敏感性、大脑优化策略**等多方面的因素共同作用。听觉系统需要快速准确地处理声音，以确保信息传递的清晰和连贯性。",
    "url": "/blog/人眼 - 晕3D",
    "path": "thoughts/life/人眼 - 晕3D.md",
    "lastModified": "2025-06-20T11:25:12.942Z"
  },
  {
    "title": "因果思维型与目标导向型",
    "content": "因果思维型与目标导向型\n比如说今天中午得少吃点，A 同学问是因为肚子不舒服么，B 同学问是因为下午有团建得留肚子吃大餐么；\n| **思维方式** | **核心区别** |\n|---|---|\n| **因果型思维（A 同学）** | 关注 **「过去或当前的原因」**，试图找出导致某个事件发生的**根本原因**。 |\n| **目标导向思维（B 同学）** | 关注 **「未来的目的或影响」**，试图理解某个行为背后的**目标或意图**。 |\n|---|---|\n| **思考方向** | 从**已发生的事实**出发，推导其原因。 | 从**未来的目标**出发，推导当前行为的目的。 |\n| **关注点** | 过去和当前：为什么会这样？ | 未来：接下来会怎样？ |\n| **典型问题** | 「为什么你今天中午吃得少？是不是肚子不舒服？」 | 「你是不是为了下午团建少吃点？」 |\n| **应用场景** | 追根溯源、问题分析（如故障排查、历史复盘）。 | 规划目标、预判趋势（如产品设计、市场战略）。 |\n| **思维特点** | 以**事实和数据**为依据，强调因果关系。 | 以**可能性和目的**为依据，强调行动的意义。 |\n| **优点** | **严谨可靠**，有助于理解过去和当前的问题。 | **前瞻性强**，有助于做出面向未来的决策。 |\n| **缺点** | 容易局限在**已有经验**，缺乏创新性。 | 可能**忽视现实条件**，导致计划不切实际。 |\n---\n类似思维  \n---\n**1. 归纳思维 vs. 演绎思维**\n🔹 **归纳思维（Inductive Thinking）** → 从具体案例总结一般规律  \n🔹 **演绎思维（Deductive Thinking）** → 从一般规律推导具体结论  \n| 维度 | **归纳思维** | **演绎思维** |\n|---|---|---|\n| **核心逻辑** | 从**个别事实**推导出**普遍结论** | 从**已有规则**推导出**具体结论** |\n| **思考方向** | 具体 → 抽象 | 抽象 → 具体 |\n| **例子** | 看到很多公司因 AI 转型成功，推测 AI 会是大趋势 | 知道 AI 是大趋势，所以建议公司投资 AI |\n| **优点** | 适用于创新、发现新趋势 | 适用于逻辑推理、系统性决策 |\n| **缺点** | 可能得出**错误的普遍结论**（以偏概全） | 依赖**前提的正确性**，如果基础假设错了，结论也错 |\n✅ **应用**：数据分析（归纳），法律推理（演绎）  \n🎯 **最佳策略**：归纳找规律，演绎验证推理  \n---\n**2. 第一性原理思维 vs. 类比思维**\n🔹 **第一性原理思维（First Principles Thinking）** → 追本溯源，拆解问题到最基本的事实  \n🔹 **类比思维（Analogical Thinking）** → 通过已有经验和案例进行推测  \n| 维度 | **第一性原理思维** | **类比思维** |\n|---|---|---|\n| **核心逻辑** | **拆解问题到最基本的事实和原理**，再从底层重新构建 | **借鉴已有经验**，通过相似案例推测 |\n| **思考方式** | 重新发明轮子 | 改良已有轮子 |\n| **例子** | 造电动汽车时，重新思考汽车的本质，优化能源结构（特斯拉） | 造电动汽车时，借鉴燃油车的设计逻辑（传统车企） |\n| **优点** | 适用于**突破性创新** | 适用于**快速决策和优化** |\n| **缺点** | 需要较高的**认知成本** | 可能受到**路径依赖**，限制创新 |\n✅ **应用**：创业创新（第一性原理），产品优化（类比思维）  \n🎯 **最佳策略**：创新时用第一性原理，执行时用类比思维  \n---\n**3. 发散思维 vs. 聚合思维**\n🔹 **发散思维（Divergent Thinking）** → 追求多种可能性，强调创造力  \n🔹 **聚合思维（Convergent Thinking）** → 追求最佳解决方案，强调逻辑性  \n| 维度 | **发散思维** | **聚合思维** |\n|---|---|---|\n| **核心逻辑** | 产生**多个不同的解决方案** | 从多个选项中选择**最优解** |\n| **思考方向** | 横向拓展 | 纵向收敛 |\n| **例子** | 头脑风暴：如何提高产品用户体验？（不限方案） | 评估所有方案，选出最可行的执行方案 |\n| **优点** | 适用于**创新和创造性问题** | 适用于**优化和执行** |\n| **缺点** | 可能缺乏可行性 | 可能局限于已有的方案，错失创新 |\n✅ **应用**：设计思维（发散），决策分析（聚合）  \n🎯 **最佳策略**：**先发散，再聚合**，即先创造多个方案，再筛选出最佳方案  \n---\n**4. 快思考 vs. 慢思考（系统 1 vs. 系统 2 思维）**\n🔹 **快思考（System 1 Thinking）** → 直觉反应，依赖经验和情绪  \n🔹 **慢思考（System 2 Thinking）** → 深度分析，依赖逻辑和理性  \n| 维度 | **快思考（直觉）** | **慢思考（理性）** |\n|---|---|---|\n| **核心逻辑** | **自动化、直觉判断**，快速决策 | **深度思考、逻辑推理**，慢速决策 |\n| **特点** | 省时省力，但容易受偏见影响 | 精确严谨，但耗费认知资源 |\n| **例子** | 突然看到蛇，会立刻躲开（本能） | 评估投资策略，需要计算收益风险 |\n| **优点** | 适用于**日常决策、紧急情况** | 适用于**复杂决策、需要深度分析的情况** |\n| **缺点** | 可能受到**情绪、偏见影响** | 可能**决策过慢，消耗精力** |\n✅ **应用**：驾驶、对话（快思考），战略决策、科学研究（慢思考）  \n🎯 **最佳策略**：大部分决策用快思考，关键决策用慢思考  \n---\n**总结**\n| **对比维度** | **思维方式 1** | **思维方式 2** | **适用场景** |\n|---|---|---|---|\n| **因果 vs. 目标** | 关注「原因」（过去） | 关注「目的」（未来） | 问题分析 vs. 目标规划 |\n| **归纳 vs. 演绎** | 具体 → 规律 | 规律 → 具体 | 数据分析 vs. 逻辑推理 |\n| **第一性原理 vs. 类比** | 追本溯源，重构思维 | 借鉴经验，优化方案 | 突破创新 vs. 快速执行 |\n| **发散 vs. 聚合** | 产生新想法 | 筛选最佳方案 | 创意构思 vs. 方案落地 |\n| **快思考 vs. 慢思考** | 直觉、快速决策 | 逻辑、深度思考 | 日常判断 vs. 复杂问题 |\n**如何运用这些思维方式？**  \n🔹 **解决问题时**，结合 **因果分析 + 目标导向**  \n🔹 **创新时**，用 **第一性原理** 思考，再用 **类比思维** 执行  \n🔹 **做决策时**，先 **发散思维** 产生选项，再 **聚合思维** 选择最优方案  \n🔹 **日常决策用快思考**，但面对**重要决策时要切换慢思考**",
    "url": "/blog/因果思维型与目标导向型",
    "path": "thoughts/life/因果思维型与目标导向型.md",
    "lastModified": "2025-06-20T11:25:12.945Z"
  },
  {
    "title": "行为决定思想",
    "content": "行为决定思想\n津巴多心理学\n斯坦福监狱实验：一个人的行为和心态，并不是完全由自己说的算；所处情境对行为的塑造，比我们想象中的更加深刻；外部环境，也许会让人背弃很多他曾经的价值观\n洞见\n    1. 人对一个东西的态度，不是想法决定的，而是行为决定的。一旦有了一个持续的行为，他就会觉得这个行为是对的。\n    2. 选择性注意：人们只会留意那些自己原本就认同的信息。这也是为什么改变一人那么难。\n    3. 想法无法改变想法，环境才能改变想法。\n结论\n人不是先有想法，才有行为，而是先有行为，进而产生支持这种行为的想法。\n不要低估环境对你的影响，它有时可以决定你是谁；更不要低估你自己对自己的影响，别让别人决定你是谁。\n案例：\n    1. 《黑豹纠察队》：拥有超人能力的阿祖小时候被刻意培养成表演型人格（情感忽视），长大后特别需要别人的认可，假如失去这些，对他来说比死还难受，公司以此来从精神控制阿祖，预防失控；人是社会性动物，只要他身在群体中，就无时不刻不受环境的影响，超人也不例外\n认知失调理论\n费斯廷格认为，当人们的行为与他们的信念或态度发生冲突时，他们会感到不适（即“认知失调”），为了消除这种不适，他们会调整自己的想法来合理化行为。例如，一个人如果做出了某个行为，他们可能会改变之前的态度或信念以使行为看起来合理。\n情绪理论\n情绪的产生往往是对身体行为的反应。例如，人们先笑，才感到快乐，而不是先感到快乐才笑。\n应用\n如何让作家持续写作\n1. 建立写作习惯\n设定小的、可实现的目标：从每天写几百字开始，而不是设定过于宏大的目标。通过实际行动（即使是小步）不断强化“我是作家”的身份认同。\n规律性写作：每天在固定时间段安排写作，通过重复行为让大脑逐渐接受并习惯这一活动。这种重复行为会慢慢塑造内心的信念，即“我是一名作家”。\n2. 使用认知失调来强化动力\n根据费斯廷格的认知失调理论，当人们的行为与信念不一致时，他们会感到不适。例如，如果一个人认为自己想成为作家，但又从不写作，这种不协调感会促使他要么调整想法（放弃成为作家的目标），要么调整行为（开始写作）。为了减少这种不适，可以鼓励他先进行写作行为，久而久之，他会调整内心的信念，进一步强化作家身份。\n3. 奖励和反馈机制\n自我奖励：为每次完成写作任务设置奖励，比如看一部喜欢的电影，或者吃一顿好吃的餐食。通过这种行为与奖励的关联，强化写作的积极意义。\n外部反馈：让他将写作成果分享给朋友、写作群体，获得外部的认可和反馈。正面的评价和认可会进一步促使他保持写作的行为，并强化成为作家的自我认知。\n4. 情境控制和行为提示\n行为暗示：在写作环境中放置一些与作家身份相关的物品，比如书籍、文具，或与写作相关的目标清单等，通过这些视觉暗示刺激写作行为。\n消除干扰：为自己设定一个专注的写作环境，减少外界干扰。这样可以增加实际写作行为发生的频率，进一步培养写作习惯。\n通过这些方式，想成为作家的人可以从日常的小行为入手，逐步通过行为带动内心的想法，进而形成良好的写作习惯和作家身份认同。\n附录\n表演型人格\n表演型人格（Histrionic Personality Disorder, HPD）是一种人格障碍，特征在于个体极度寻求他人关注和赞美，往往通过夸张的情感表达和表演来引起他人的注意。表现出这种人格特质的人通常有以下特点：\n1. **过度的情绪表达**：他们通常以戏剧化、夸张的方式表达情绪，可能对日常事件作出强烈的反应，似乎比实际情况更加情绪化。\n2. **寻求注意**：这类人经常会感到不被关注时很不安，因此会通过各种手段，如穿着夸张、行为浮夸，甚至是制造戏剧性事件，来确保自己成为焦点。\n3. **对他人情感的敏感**：尽管他们很关注自己是否得到关注，却往往难以真正体会他人的情感，而是将他人的反应视为自己吸引注意力的一部分。\n4. **人际关系的肤浅和短暂**：表演型人格的人可能会试图通过快速建立密切关系来获得他人的关注，但这些关系通常不够稳固，容易破裂，因为他们更多地关心外在的关注，而非关系本身的深度。\n5. **易受外界影响**：他们通常容易受到他人意见和环境的影响，因为他们的行为往往是为了取悦他人，而非基于自己的内心需求。\n这种人格障碍属于**B型人格障碍**，与自恋型、边缘型人格障碍属于同一类型。表演型人格障碍的根本问题在于缺乏内在的自我认同，个体需要通过外界的认可来确认自己的价值。\n治疗通常包括心理治疗，尤其是认知行为疗法（CBT），帮助患者意识到其行为模式并学会更健康的情绪表达方式。\n原因\n表演型人格障碍（Histrionic Personality Disorder, HPD）的形成可能受到多种因素的影响，通常是遗传、早期生活经历和环境的复杂交互结果。以下是主要的原因和机制：\n1. **遗传因素**\n有研究表明，表演型人格障碍在家庭中有一定的遗传性。如果家族成员中有人患有类似的情感障碍或人格障碍，其他成员更容易发展出表演型人格。这表明，某些人格特质（如情绪不稳定或外向性格）可能通过基因遗传影响人格形成。\n2. **早期生活经历**\n早期儿童时期的经历对人格发展有重要影响，尤其是情感的表达和处理方式。常见的经历包括：\n**父母的过度溺爱**：如果孩子在成长过程中被父母过度关注，或父母不断强化他们的情感表达，孩子可能会学会通过夸张的行为和情绪来获得关注。\n**情感忽视**：相反，某些孩子可能在情感需求上被忽视，因而学会通过引起他人关注的方式，来填补他们内心的空虚感。\n**不稳定的家庭关系**：家庭中的不稳定因素，如冲突、忽视或不一致的养育方式，可能促使孩子发展出过度依赖外界认可的行为模式。\n3. **文化和社会因素**\n社会和文化的影响也可以促进表演型人格的形成。某些文化或社会环境可能强化了外表、形象和表现自我重要性的价值观。例如，现代媒体和社交平台强调“被看到”以及通过夸张或戏剧化的方式展示自我，可能会助长这一类人格特质的发展。",
    "url": "/blog/心理学 - 行为决定思想",
    "path": "thoughts/life/心理学 - 行为决定思想.md",
    "lastModified": "2025-06-20T11:25:12.945Z"
  },
  {
    "title": "改革的目标",
    "content": "改革的目标\n改革在一定程度上涉及到降低交易成本和组织成本。具体来说，改革旨在通过优化资源配置、提升效率、减少不必要的制度性障碍，来推动经济和社会的进步。以下是关于改革如何影响交易成本和组织成本的简要分析：\n1. **交易成本**\n   交易成本是指在市场交易过程中发生的各种成本，包括信息成本、谈判成本、监督成本等。改革可以通过多种方式降低交易成本：\n**制度创新**：通过简化繁琐的行政程序、减少不必要的审批和监管环节，使得市场主体的交易更为高效，减少不确定性。\n**透明度和法制建设**：建立和完善法律法规，增强市场透明度和可预见性，降低信息不对称带来的风险。\n**技术进步**：推行数字化、信息化等技术手段，简化交易流程，减少交易过程中的中介成本。例如，电子政务的推行使得政府与企业之间的交易更加高效。\n   **例子**：放松对某些行业的管制，允许更多市场主体进入，可以减少由于政府垄断带来的交易壁垒，使得市场交易更加活跃和高效。\n2. **组织成本**\n   组织成本是指企业或政府在组织内部运作中为了协调生产、管理、监督而发生的各种成本。改革可以通过优化管理、改善制度设计来降低这些成本：\n**简政放权**：政府通过减少对经济活动的过度干预，可以让企业在市场中更加灵活地运营，减少过于集中的管理成本。\n**结构调整**：通过改革政府或企业的组织结构，使得决策链条更加简化、管理层级更加精简，从而降低内部协调和管理的成本。\n**鼓励创新与效率**：通过推动市场化改革，促使企业提升内部管理效率，减少资源浪费。\n   **例子**：国有企业改革通过引入市场机制，削减冗余人员、改进治理结构，能够减少组织成本，提升企业竞争力。\n3. **广泛的改革目标**\n   除了交易成本和组织成本，改革还包括诸多其他目标：\n**提高生产力**：通过改革释放生产要素的潜力，推动技术进步和创新。\n**促进公平和包容性发展**：通过教育、医疗、养老等方面的社会改革，减少社会不平等，促进可持续发展。\n**环境可持续性**：推动绿色改革，减少对环境的破坏，促进资源节约型和环境友好型社会的建设。\n结论\n虽然改革确实可以显著降低交易成本和组织成本，但其本质在于通过**系统性变革**来提高整个经济和社会体系的效率、适应性和公平性。",
    "url": "/blog/改革的目标",
    "path": "thoughts/life/改革的目标.md",
    "lastModified": "2025-06-20T11:25:12.945Z"
  },
  {
    "title": "精英过剩：理解、起源与历史镜鉴",
    "content": "当然可以。以下是整理后的文章：\n精英过剩：理解、起源与历史镜鉴\n一、什么是“精英过剩”？\n“精英过剩”（Elite Overproduction）是指一个社会中受过良好教育、有抱负并希望获得高社会地位的人数，**远超出实际能够容纳的职位或资源总量**。这会导致精英群体之间的激烈竞争，最终引发政治分裂、社会动荡，甚至革命与政权更替。\n这一现象并非仅发生在当代，而是有深厚的历史根基。现代社会的高等教育扩张、大量培养“准精英”人群，加剧了这种结构性矛盾。\n三、问题的根源是什么？\n精英过剩通常源于以下几个方面：\n1. **教育通胀**：高等教育大众化后，“本科”不再是门槛，而是起点。人们不断通过学历升级维持竞争力，造成教育军备竞赛。\n2. **岗位结构滞后**：社会并没有同步创造那么多高质量的、能匹配“精英期望”的岗位。\n3. **上升通道受阻**：传统的晋升路径（比如考编、升职）变得狭窄或被既得利益集团垄断。\n4. **路径依赖问题**：上一代的成功经验（考大学、考研究生）仍被沿用，但社会已无法承接如此规模的“上升预期”。\n---\n四、这将带来什么问题？\n结构性矛盾会酿成深层危机，主要表现为：\n**社会撕裂**：精英内部的竞争逐渐转化为政治、意识形态上的分裂。\n**去合法性化**：部分精英质疑现有体制的公平性，出现激进派、民粹派、革命派。\n**反制度情绪上升**：精英无法向上，就可能掀桌子、煽动基层群众不满。\n**政治周期动荡**：旧精英集团受到新精英挑战，社会陷入政权不稳或制度更替的循环。\n---\n五、如何应对精英过剩？\n解决精英过剩，并非简单的“减少教育”，而是需要**结构性改革和路径多样化**。\n国内的尝试：\n1. **职教分流（职普分流）**\n   高中阶段将一部分学生引导至职业教育体系，试图打造“技术精英”“实用型人才”，缓解高考内卷和大学挤兑。\n2. **扩展基层岗位、城乡均衡政策**\n   通过扩张基层公务员、乡村振兴项目，为精英提供新的落脚点。\n3. **鼓励创新创业、产业多元化发展**\n   让人才向二三线城市、数字经济、制造业、技术服务等新兴领域扩散。\n国外的有益经验：\n**德国“双元制教育”**：早期将学生分流至普通高中与职业高中，后者与企业深度合作，就业对口率极高。\n**日本“终身雇佣制”的松动**：让人才流动更加自由，鼓励中小企业吸纳人力。\n**美国社区大学体系**：提供更便宜、更灵活的教育路径，避免所有人盲目涌向名校。\n---\n六、精英的路径依赖：是否还有出路？\n当前问题的棘手之处在于：**很多既得利益者本身就是通过旧路径（大学—研究生—官僚/学术/媒体）成功上位的**，因此改革面临制度内阻力。\n这也意味着：继续走“小学→初中→高中→大学→研究生→博士→高位”的道路，将变得**更加拥挤和激烈**。未来可能需要：\n**开辟“横向路径”**：鼓励更多人成为产业精英、技能人才、数字劳动力。\n**打破学历崇拜**：构建“去学历中心化”的社会评价体系。\n**改革教育资源分配**：真正实现因材施教，而非一刀切的升学率导向。\n---\n七、历史镜鉴：精英过剩导致了多少次王朝更替？\n这一理论的重要支撑来自历史数据研究，尤其是“结构人口动力学”（Cliodynamics），这是由历史学家彼得·特尔钦（Peter Turchin）提出的一门交叉学科，融合历史学、数学建模、社会学。\n特尔钦的观察：\n历史上的王朝更替，常常在精英人口激增、资源分配恶化、社会治理崩塌的三重夹击下发生。\n精英之间的内部斗争往往比民众暴动更具决定性。\n民众动乱多数提供“动能”，真正“掌权”的依然是精英派系的更替或重组。\n中国历史印证这一观点：\n| 历史阶段        | 体现“精英过剩”的表现             | 导致的具体问题                  | 结果或影响                 |\n| ----------- | ----------------------- | ------------------------ | --------------------- |\n| **东汉末年**    | 科举尚未完全普及，但豪门士族数量激增，官位有限 | 士族争权激烈，宦官干政导致党锢之祸，政治腐败   | 社会动荡，黄巾起义爆发，三国割据      |\n| **唐朝中后期**   | 科举扩大，进士数量激增，官职增速跟不上     | 进士内部“牛李党争”等党派斗争，宦官与藩镇割据  | 中央权威弱化，藩镇割据严重，导致安史之乱  |\n| **宋朝晚期**    | 理学兴起，士大夫群体庞大，财政压力大      | 政治派系斗争（新旧党争），政治效率低下，财政紧张 | 军事力量弱化，边疆防御失效，最终被金元侵扰 |\n| **明末**      | 科举录取人数激增，官位饱和，士人失业率高    | 东林党争激烈，宦官专权，政治腐败         | 农民起义频发，李自成攻入北京，明亡     |\n| **清朝末期**    | 旧科举废除后，新式留学生、士人数量激增     | 传统士绅与新兴知识分子冲突，政治体制转型困难   | 政权动荡，辛亥革命爆发，清朝灭亡      |\n| **20世纪初美国** | 法学院、研究生数量暴增，劳动力市场饱和     | 高学历失业，社会不满情绪上升           | 政治极化，社会分裂加剧，动荡频发      |\n---\n八、结语：精英不是问题，结构才是问题\n“精英”本身并不可怕，一个社会也需要大量有知识、有担当的人。但当精英的**数量**远远超过结构所能容纳的**位置**，且他们之间无法达成利益平衡时，问题就来了。\n最终的焦点是：\n如何合理配置精英资源？\n如何保持精英阶层的流动性与自我更新？\n如何建设一个能吸纳多元才能的社会结构？\n如果不解决这些问题，无论是美国、中国，还是任何文明国家，都可能在“精英过剩”的压力下，走向分裂、动荡乃至崩塌。",
    "url": "/blog/精英过剩：起源与历史镜鉴",
    "path": "thoughts/life/精英过剩：起源与历史镜鉴.md",
    "lastModified": "2025-07-23T03:42:25.094Z"
  },
  {
    "title": "**绝对价值 vs. 相对价值**",
    "content": "**绝对价值 vs. 相对价值**  \n1. **绝对价值**（Absolute Value）  \n指事物本身所具有的内在价值或客观效用，与外部比较无关。  \n例如：黄金有化学稳定性、导电性、稀缺性，因此具有较高的绝对价值。  \n2. **相对价值**（Relative Value）  \n价值的评估取决于对比和背景，而非事物本身的属性。  \n例如：一杯水在沙漠中比在城市里更珍贵，iPhone 15 的价值是基于与前代机型或竞品的对比得出的。  \n**现实中，人们更偏向相对价值**  \n1. **决策时的参照点效应**  \n人们倾向于基于**比较**而非独立评估。例如，在购物时，一个打折的商品可能比原价商品更吸引人，即使实际价值未必更高。  \n2. **行为经济学中的锚定效应**  \n例如，顾客看到一件商品原价 1000 元，折后 600 元，会觉得它更“值”，即使它的实际绝对价值可能远低于 600 元。  \n3. **社会竞争心理**  \n很多时候，价值判断与社会地位相关。例如，豪车和奢侈品的价值更多取决于“与他人相比”，而非其物理属性。  \n4. **市场价格机制**  \n股票、房价、货币等的价值很大程度上是相对的，受供需、情绪和市场环境影响，而非其内在的绝对价值。  \n为什么?\n现实中，人们更偏向**相对价值**，主要原因来自**认知限制、社会比较、进化心理**以及**市场机制**等多个因素。  \n**2. 社会比较心理：价值感来源于“比别人好”**  \n人是社会性动物，价值判断往往来自**与他人的比较**，而非事物本身的绝对属性。  \n**地位驱动**：人们更在意自己比别人更富有、更成功，而非单纯的财富多少。例如，一个人年收入 50 万，但身边人都赚 100 万，他可能比一个年入 30 万、但身边人都赚 20 万的人更焦虑。  \n**炫耀性消费**：奢侈品的价值很大程度上是因为“别人没有”或者“比别人的更贵”，而不是因为它本身的功能多么优越（劳力士手表 vs. 电子表）。  \n👉 **社会竞争让人更关注相对价值，而非绝对价值。**  \n---\n**3. 进化心理：适应性生存策略**  \n从进化角度看，人类的祖先在**资源有限、竞争激烈**的环境中，**关注相对优势更有生存价值**。  \n**资源分配**：如果部落里食物稀缺，重要的不是“绝对上有多少食物”，而是“比别人多多少”。拥有相对优势，生存概率更高。  \n**择偶竞争**：很多生物（包括人类）在择偶时，会关注**相对地位**，比如男性的社会地位、女性的外貌吸引力，都是基于群体内比较，而非绝对数值。  \n👉 **关注相对价值，是进化塑造出的竞争适应性策略**。  \n---\n**4. 市场机制：价格和价值受供需影响**  \n现实世界的市场体系，本质上也是**基于相对价值的博弈**。  \n**定价策略**：商家不会单纯按照“成本+利润”定价，而是依据竞争对手的定价、用户的心理价位等**相对因素**定价。  \n**投机市场**：股票、房产、比特币的价格更多依赖市场情绪和相对预期，而不是某个固定的“绝对价值”。  \n👉 **市场经济环境也强化了人们关注相对价值的习惯**。  \n---\n**总结：相对价值更符合人性和现实逻辑**  \n| **因素** | **为什么人更关注相对价值？** |  \n|------------|----------------|  \n| **认知限制** | 绝对价值难以直观判断，相对价值更易感知 |  \n| **社会比较** | 人们更在意自己是否比别人更好，而非绝对数值 |  \n| **进化心理** | 竞争环境让相对优势成为生存关键 |  \n| **市场机制** | 供需波动导致价格和价值以相对标准衡量 |  \n**绝对价值**在科学、工程、基础生存需求（如粮食、空气）等方面很重要，但在人类的日常决策、经济活动和社会互动中，**相对价值往往是核心驱动力**。",
    "url": "/blog/绝对价值与相对价值",
    "path": "thoughts/life/绝对价值与相对价值.md",
    "lastModified": "2025-06-20T11:25:12.946Z"
  },
  {
    "title": "麦理浩径（MacLehose Trail）",
    "content": "麦理浩径（MacLehose Trail）\n作为香港四大著名行山径之一的麦理浩径，全程百公里，是香港最早启用的一条长途远足径，于1979年10月26日启用，以时任香港总督麦理浩命名～\n第一段和第二段有便利店可以补给，第三段和第四段无补给，建议自备食物和水。第五段有士多可以买食物和水，第六段到第十段有合法营地可以补给。\n!全程总结表\n!全程路线\n第一段\n北潭涌——浪茄\n基本平地，难度较低，风景还行\n长度+横切面\n第二段（目标）\n简介\n*选择第二段原因：难度均衡，风景很好，据说是最精华的一段*\n路线：万宜水库东坝（实际是浪茄，距离东坝30分钟步行路程，但是看滴滴无法直接到达浪茄，所以选择东坝） - 北潭凹\n难度指数：3 （4个上下峰）\n风景指数：5\n徒步时长：6小时（不紧不慢）\n共5小时（咱普通人得6小时）：万宜水库东坝 - (30 min) -（浪茄-西灣山-吹筒坳（ 1 hr 40 min））-(20 min)- 西湾 - 50 min - 鹹田灣 - 50 min - (大浪坳 赤徑) - 45 min - 北潭凹\n!麦理浩径1-2段\n*第二段：浪茄-北潭凹*\n!第二段横切面\n计划\n整体\n（中途没有专门午休时间，所以前一晚睡好点）\n1. 福田口岸 10点集合 \n2. 入关 + 打车 + 路程1个小时到万宜水库东坝（早上11：30点，吃个午饭（带上个汉堡或者其他食物），12点从山脚出发） \n3. 徒步 6 小时（+缓冲半小时）\n4. 下午6点半下山（北潭凹） - 打车+出关+路程（耗时1个半小时）到福田口岸（晚上8点，吃晚饭）；PS：如果是全程公交，则到福田口岸是10点\n细节\n徒步前准备\n港澳通行证\n流量：提前两天在支付宝或者官网买跨境流量\n货币：支付宝能用了么？\n如果兑换：建议人均换 300 人民币，带小面额\n八达通（地铁，BUS）：是否需要购买？支付宝能用了么？\n长期不使用的八达通会被禁止使用，重新激活之前无法充值、消费；需要先到香港地铁站（任意一个）内的乘客服务中心通过工作人员激活，激活后即可正常使用。\n水/食物\n建议携带2-3L的水，巧克力，水果等。（虽然中途有便利店补给，但是距离比较远，**强烈建议自带足够的补给**）\n日常\n拖鞋（想感受美丽沙滩的记得带上），泳圈（感受海浪的魅力）\n徒步裤子\n徒步衣服\n徒步鞋子（有个300米的爬坡），视自身情况带登山杖\n遮阳伞\n帽子\n防晒霜\n创口贴\n驱蚊水\n...\n交通（去目的地）：福田口岸（落马洲地铁站（LOK MA CHAU STATION）） - 万宜水库（东坝）\n    1. 打车（耗时 1 小时；PS：巴士只能到西坝（东西坝距离1小时步行））\n出租车： (360-399)港币左右/辆，如果是多个人一起出行，\n滴滴出行（大概349.5HKD - 399HKD），不可选多座（多于4人得注意，最好能凑 4 的倍数人）\n    2. 公交+打车（耗时 1小时左右，可能需要等比较长时间地铁）\n落马洲到大学站（5站22分钟左右）\n打车从大学站到东坝（40分钟左右，233港币）\n    3. 公交（总过程2个多小时，这种方式比较便宜，转车多次，耗时较长，返程可考虑）\n深圳福田口岸过关就到落马洲\n地铁+公交（1小时22分）：落马洲（A 口）-9站34分- 九龙塘（换乘观塘线） -4站7分- 彩虹（C1 口） -走路3分钟- 龙翔道公交站（新界区专线小巴1路） -2站18分钟- 福民路小巴总站 -走路3分钟- 西贡总站（可以吃个午饭然后去（西贡西（东）郊野公园（Sai Kung East Country Park），也即 1-2-3-... 所在地））\n再打车到“万宜水库”东坝 35分钟（大概一辆车100港币，滴滴显示为 148HKD）。\n徒步中（下面会有个 planB 中途撤离）：全程15公里（网上有说13.5公里，也有说14公里的），起点处的东壩水库，沿途还会经过3处海湾（浪茄湾、西湾、咸田湾，每个海滩都有洗手间），一直行走在山峦丛林之中，真正的“上山下海”。 \n万宜水库东坝 (high island reservoir east dam，对应西坝 high island reservoir west dam) 地图有联合国教科文组织地质公园\n可以考虑去趟 **破边洲** （风景很美，过去回来需要额外 +2小时）\n浪茄（最美沙滩，海浪还比较大，带上泳圈可以感受海浪的魅力）\n西湾山（全程最高点-风景很好-西湾山海拔314米，中上难度），下山后这里可以执行 **PlanB** 返程（需要赶在6点前返回）\n吹筒坳（下山就到） \n西湾（又一个靓丽海滩，，大浪西湾） \n鹹田灣（海滩）\n大浪坳\n赤徑（右边有海湾）\n北潭凹（PAK TAM AU）\n返程\n打车+公交（暂定：打车从北潭凹打车到大学站，然后地铁从大学站到落马洲）\n打车（1个小时，价格跟回去类似）：北潭凹（第三段起点，应该好分辨，地图上看起来有公交站，应该好打车）打车返回落马洲地铁站\n公交（需要注意下公交运营时间），跟来时类似\n或者先乘公交到好打车的点，再打车回来\nReference\n谷歌地图（参考徒步路径）\n滴滴打车（参考打车路线，及其费用）\n高德地图（参考公交路线）\n1-2段的参考图来源（链接里面风景图很多） https://m.tangshui.net/post/show/5b291b1b368ec06d0fcedf84#tangshui\n路线中的凹凸地势图来源 https://www.hillmanblog.com/project/maclehose-trail-2/\n徒步时间来源 https://www.oasistrek.com/mac_trail_two.php\n香港麦理浩径第二段： 轻松的路线，风景也是醉美\n第三段\n各个风景点+横切面\n长度+横切面\n第四段\n长度+横切面",
    "url": "/blog/麦理浩径徒步攻略",
    "path": "thoughts/life/麦理浩径徒步攻略.md",
    "lastModified": "2025-06-20T11:25:12.924Z"
  },
  {
    "title": "Reference",
    "content": "《上帝掷骰子吗》在更深层次上探讨了量子力学带来的哲学与科学观念的变革，主要集中在三个更深刻的思想层面：\n1. **客观实在与观测者的关系**\n   量子力学带来的一个极具争议的观点是，观测者的行为会影响到被观测的物体。这在经典物理学中是不可想象的，因为经典物理认为客观世界独立于观察者而存在。但量子力学中的“测量问题”表明，当一个微观粒子被测量时，它的状态会发生“坍缩”，从多种可能的状态中瞬间变成一个确定的状态。举例来说，在“双缝实验”中，粒子通过双缝时若未被观测，它会表现为波的干涉现象，但一旦进行观测，粒子则表现为粒子性，通过其中一个缝隙。这意味着“实在”并非完全客观，而是会因观测者的介入而改变。\n   这种现象带来的一个深刻问题是：**我们的现实到底是不是独立存在的？还是说，现实是依赖于意识和观察的？这是量子力学对哲学的一个挑战，打破了“物理现实是独立存在的”这一传统观念。**\n2. **确定性与自由意志的冲突**\n   在经典物理中，世界遵循因果律，可以通过定律预测未来，具有高度确定性。然而，量子力学指出，微观粒子的状态在本质上是随机的，没有所谓的“隐藏变量”来解释所有现象。这种随机性引发了对自由意志的深层思考：如果世界本质上是随机的，那么人类是否也具有某种“选择的自由”？或者我们的行为是否受限于量子的随机性？\n   作者通过量子力学的视角带出了“自由意志”与“宿命论”的争论。在量子力学中，**世界的根本规律是不确定的，这是否意味着宇宙具有某种自我演化的自由？人类的思想与决策是否也受到这种不确定性的支配，从而可能打破绝对的因果性？**\n3. **世界的多重性与可能性**\n   量子力学引入了“多世界解释”这一激进的观点，提出所有可能的量子状态都真实存在，只是在不同的平行宇宙中实现。比如，在测量一个电子的自旋方向时，电子的自旋会随机地“向上”或“向下”，但在多世界解释中，这两个状态并非仅是可能性之一，而是在不同的平行宇宙中都发生。这意味着，我们的宇宙可能是众多宇宙中的一个，每个宇宙都包含一种不同的可能性。\n   多世界解释激起了关于“存在”本身的反思：**如果存在的是无穷多个宇宙，那么我们所感知的现实是否仅仅是其中一种状态？这个观点挑战了传统的实在观，也带来了“我们是谁”“我们存在于何处”的问题。这一层次的讨论在哲学上是极其深刻的，质疑了人类对自我与现实的认识。**\n更多深刻的观点\n1. **科学确定性与“未知之海”**\n   量子力学的引入迫使人类承认知识的边界。尽管科学一直以来追求确定性，但量子力学揭示了在基本粒子的层面上，世界存在无法精确确定的“模糊地带”。书中探讨到，科学家们面临一个困境：我们的模型和定律能否真正反映自然的本质，还是说这些理论仅是对现象的粗略近似？作者暗示，人类可能永远无法彻底掌握宇宙的根本规律，因为宇宙可能本质上就存在不确定性——这是科学所不可突破的界限。\n   这种对“未知”的接受改变了科学观念：科学不再被看作揭示绝对真理的工具，而是不断修正、接近真理的过程。这一思想重新定义了科学，让人类意识到，宇宙可能永远不会向我们展示它的完整真相。\n2. **数学的极限与现实的匹配**\n   书中探讨了数学和物理学之间的微妙关系：数学构建了量子力学的框架，使科学家能够进行极为精确的计算，但这些数学结构是否真正描述了现实？比如，量子力学方程中的波函数在数学上描述了粒子的所有可能状态，但一旦观察发生，这些可能性就“坍缩”为一个确定的结果。\n   这种现象引发了一个哲学思考：数学是否是自然的“语言”，或者它仅仅是人类对世界的一种抽象工具？如果数学是自然本身的组成部分，那么我们的宇宙可能符合某种“数学实在论”；否则，数学只是一个模拟现实的工具，不能保证揭示宇宙的根本真理。这一思考带来了对“数学宇宙观”的质疑——数学究竟是真实的，还是仅仅是人类理解自然的一种方式？\n3. **意识在宇宙中的作用**\n   量子力学的测量问题引出了对意识的探讨：为什么观测会导致量子状态的坍缩？这种“意识导致实在”的观点让一些科学家和哲学家推测，意识可能在构建现实中起到了某种作用。作者引用了一些量子力学的实验，如“双缝实验”，表明观察行为可能并不仅仅是被动的过程，而是对实在产生了直接影响的行为。\n   这一思想引发了意识与实在的关系的深层讨论：是否意识参与了宇宙的创造，或者说宇宙的某些层面必须依赖意识才能显现？这种想法在哲学上有着深远的影响，因为它暗示意识不仅仅是宇宙的偶然产物，而是可能与宇宙的结构本质上不可分离。这种“参与性实在”在科学领域并未得到完全接受，但它提供了关于意识和物质之间关系的全新视角。\n4. **宇宙的目的性与自组织性**\n   量子力学所揭示的随机性使人类不得不重新思考宇宙的目的性：如果宇宙的基础是不可预测的、随机的，那么宇宙是否有目的？作者探讨了宇宙是否可能是一种自组织的系统，即没有预设目的，而是通过量子力学的随机性和自然选择的过程，逐渐发展出复杂的结构和生命。这种思想类似于复杂性科学的观点：宇宙可能没有预设的“目标”，而是通过无数随机性逐步演变出秩序。\n   这种“无目的的目的性”挑战了传统的哲学思想，也影响了人类对自我、生命意义和宇宙本质的理解。它暗示，宇宙的秩序和人类的存在或许并非出于某种设计，而是随机与秩序相互作用的自然结果。这种观点将量子力学的随机性提升到更广泛的宇宙观层面，给出了一个关于宇宙进化的自然性解释。\nReference\ngpt",
    "url": "/blog/上帝掷骰子了吗",
    "path": "thoughts/reading/上帝掷骰子了吗.md",
    "lastModified": "2025-06-20T11:25:12.936Z"
  },
  {
    "title": "任务分配管理",
    "content": "任务分配管理\n明确要素\n截止时间\n负责人\n背景：解决了什么问题\n目标：任务的目标\n目标管理：SMART原则（S=Specific、M=Measurable、A=Attainable、R=Relevant、T=Time-bound）\n标准：完成交付的标准\n其他要素\n反馈时间（及时反馈）\n风险点（可能影响进度点）\n资源（可以调度的资源）\n关键决策汇报管理\n三步走\n1. 问题是什么？\n2. 影响这个问题的关键有哪些？\n3. 你的建议方案？\n共识管理\n拉齐共识\n1. 团队周报：现阶段遇到的问题？视野+案例\n2. 例会\n会议管理\n专题/例行会议\n会议组织\n背景\n时间\n地点\n主持\n参会人员\n会议目的：了解真相\n会议纪要\n议题：出了什么问题？\n现状分享：问题的现状和资源情况\n方案讨论：问题的解决提议跟意见\n形成决议：做出方案决策\n执行分工：确认分工跟执行责任人\n好纪要原则\n匹配：与会议目的流程匹配\n目的：复盘回顾，分析责任？确认目标，计划分工？\n负责：基于目标确定谁为会议纪要负责\n一致：参会人员对于会议目标，意义，以及责任人达成共识\n例会\n作用\n分享，收集，产生信息\n协调步调\n推动进度\n意义\n过程管理\n文化管理\n!例会管理\nReference\n如何写好公司会议中的会议纪要？\n如何写好会议纪要？\n例会的意义？",
    "url": "/blog/关于管理",
    "path": "thoughts/reading/关于管理.md",
    "lastModified": "2025-06-20T11:25:12.942Z"
  },
  {
    "title": "可能性的艺术",
    "content": "简介\n是刘瑜教授的一本比较政治学普及读物，通过30个独立又相互关联的章节，介绍了比较政治学的基本概念、方法和案例，探讨了不同国家和地区的政治现实、政治变迁和政治选择。书中运用了大量的历史、社会、经济和文化数据，以及生动有趣的故事和比喻，帮助读者理解政治的可能性和边界，以及政治对个人和社会的影响。\n主要观点\n书名中的“可能性”指的是政治可以创造出不同的社会现实，但也受到一些客观条件的制约。作者认为，政治是一种艺术，而不是一种魔术，它需要在现实和理想之间寻找平衡和妥协。\n书中提出了两个比较政治学的核心维度：民主问责和国家能力。民主问责是指政府是否受到公民的监督和制约，是否能反映公民的意愿和利益。国家能力是指政府是否有能力维护社会秩序、提供公共服务、促进经济发展等。作者认为，这两个维度并不一定互相冲突或者互相促进，而是有可能形成不同的组合，导致不同的政治结果。\n书中分析了各种不同类型的政体，如威权政体、民主政体、联邦制、单一制、总统制、议会制等，以及它们各自的优缺点、适应条件和变迁路径。作者认为，没有一种政体是完美无缺或者适用于所有情况的，而是要根据具体的历史、文化、经济和社会背景来选择和设计。\n书中探讨了很多国际上的热点问题，如全球化、文明冲突、民主倒退、国家建构等，并且提供了多角度、多层次、多维度的分析视角。作者认为，要理解当今世界的复杂性和多样性，需要运用比较的视野，把自己所面对的政治现实当作一万种可能性之一来对待，从而获得更深刻和更全面的认识。\n启发\n政治不是一种固定的、单一的、确定的现实，而是一种充满可能性的、多元的、不确定的艺术。我们可以通过比较不同国家和地区的政治制度、政治过程和政治结果，来发现政治的规律和变化，以及政治的优劣和选择。\n政治不是一种与我们无关的、高高在上的、专业化的领域，而是一种与我们密切相关的、平民化的、普及化的活动。我们可以通过参与政治，表达自己的意见和利益，影响政治的决策和行动，以及承担政治的责任和后果。\n政治是一种平衡性的活动，它需要在现实和理想之间，寻找合适的妥协和折衷。政治不是一种魔术，它不能无视客观条件和主观意愿，而是要在可能性和可行性之间，找到最佳的方案。\n政治是一种相对性的活动，它需要通过比较不同国家和地区的政治制度、政治过程和政治结果，来发现政治的规律和变化，以及政治的优劣和选择。政治不是一种绝对的、直接的、简单的思维，而是一种相对的、间接的、复杂的理性。",
    "url": "/blog/可能性的艺术",
    "path": "thoughts/reading/可能性的艺术.md",
    "lastModified": "2025-06-20T11:25:12.937Z"
  },
  {
    "title": "复杂",
    "content": "复杂\n不论是计算创意学还是游戏设计哲学，都有一个共同的目标：去促成一种超越人类设计能力的、能够让作者自己都感到眼前一亮的前所未有的认知对象的产生和进入视野。而复杂性成为了产生这样的“认知新颖性”的关键。\n作者提出一个复杂系统的定义：复杂系统是有大量组分组成的网络，不存在中央控制，通过简单的运作规则产生出复杂的集体行为和复杂的信息处理，并通过学习和进化产生适应性。\n由于这样的系统在没有内部或外部的控制者的情况下产生了有组织的行为，我们将之称为自组织系统（Self-organizing System）。由于简单规则以难以预测的方式产生出复杂行为，这种系统的宏观行为有时也成为涌现（Emergent）。\n因此复杂系统的另一个定义就是：具有涌现和自组织行为的系统。\n案例\n1. 蜂群行为与涌现现象：每只蜜蜂的行为是简单且局部的，例如它们通过气味和跳舞来传递信息，告诉其他蜜蜂哪里有食物。但是，蜜蜂个体之间的这些简单互动，通过群体的协作，能够形成高度复杂的集体行为，如寻找食物和建造蜂巢等。\n2. 沙堆实验与临界性：将沙粒逐渐加入到一个沙堆中，直到某一时刻，堆积物会突然崩塌。重要的是，崩塌的时刻无法预测，甚至无法从之前的行为模式中推断出何时会发生。每颗沙粒看似不起眼，但它们的堆积会导致整个系统发生突变。\n3. 金融市场与复杂性：在金融市场中，投资者的行为往往是基于对未来的预期，而这些预期往往是建立在对他人行为的猜测之上。举例来说，在2008年全球金融危机前，房地产市场和股市形成了泡沫，投资者普遍认为价格将继续上涨，这导致了过度投资和借贷。而当市场开始崩塌时，投资者恐慌，导致市场在极短时间内剧烈波动。这个例子展示了金融市场的非理性行为、群体心理和反馈机制如何导致整个系统产生剧烈的波动。市场中的“局部稳定”可以掩盖全局不稳定性，直到某一突发事件引发大规模崩溃。\n4. 大脑神经元网络：大脑神经元网络是一个复杂系统，它通过简单的运作规则（如神经递质传递和突触连接）产生出复杂的集体行为（如学习和记忆）。神经元网络的局部连接和简单规则可以导致涌现出高级认知功能，如意识和自我意识。\n5. 免疫系统：免疫系统通过简单的运作规则（如识别抗原和产生抗体）产生出复杂的集体行为（如抵抗疾病和适应性免疫）。免疫系统的局部互动和简单规则可以导致涌现出复杂的适应性免疫反应。\n6. 为什么在由自私个体组成的群体中会进化出合作。从底层看\n在进化历程的一定阶段时刻，单细胞生物会互相合作以形成更复杂的多细胞生物。\n接着，又进化出蚁群这样的社会性生物，大部分蚂蚁为了蚁群的整体利益工作，甚至放弃了繁衍的能力，只让蚁后来繁衍后代。\n后来，灵长类动物群体中又涌现了更复杂的社会，社会团结，一致对外，复杂的贸易，最终出现了人类国家、政府、法律和国际条约。\n其他\n计算机领域的哥德尔不完备性定理\n网络和图论\n生物代谢的比例之谜\n自复制图灵机\n作者自己在博士论文中设计的类比程序\nReferences\n梅拉妮·米歇尔《复杂》导读\ngpt",
    "url": "/blog/复杂",
    "path": "thoughts/reading/复杂.md",
    "lastModified": "2025-06-20T11:25:12.937Z"
  },
  {
    "title": "The Cerebral Code",
    "content": "The Cerebral Code\nOne of the defining pillars of physics is the second law of thermodynamics. Energy hates being concentrated. Yet here we are, squishy bags of water and energy suspended a few feet above the ground. It's a bit weird.\n物理学的决定性支柱之一是热力学第二定律。能量讨厌集中。然而我们就在这里，一袋黏糊糊的水和能量悬浮在离地面几英尺的地方。有点奇怪。\nSchrodinger decided the only way that life was possible was if it led to more entropy throughout the system. Like a tornado, life is a pocket of order that sows disorder. It lives so long as it pays the entropy tax.\n薛定谔认为，生命存在的唯一途径就是在整个系统中产生更多的熵。就像龙卷风一样，生活是一片秩序，却播下了混乱的种子。只要它缴纳熵税，它就会存在。\nTo anthropomorphize a bit, The Universe is trying to maximize entropy but it has a near infinite \"problem space\". One algorithm it has found to solve this problem is evolution. To butcher a computer science phrase, I believe that evolution is the \"best case runtime\" when trying to find increasingly valid solutions to a near infinite problem space.\n稍微拟人化一点，宇宙试图最大化熵，但它有一个近乎无限的“问题空间”。它发现解决这个问题的一种算法是进化。用一个计算机科学术语来说，我相信，当试图为近乎无限的问题空间找到越来越有效的解决方案时，进化是“最佳情况运行时”。\nEssentially, biology uses evolution because it is the best way to solve the problem of prediction (survival/reproduction) in a complex world.\n从本质上讲，生物学使用进化，因为它是解决复杂世界中的预测（生存/繁殖）问题的最佳方法。\nReference\n达尔文机器",
    "url": "/blog/大脑如何思维",
    "path": "thoughts/reading/大脑如何思维.md",
    "lastModified": "2025-06-20T11:25:12.937Z"
  },
  {
    "title": "技术封建主义",
    "content": "技术封建主义\n你是否以为，人类正在进入一个新时代----互联网高科技的时代？\n这本书说 NO，人类更像回到封建时代，只是多了高科技。\n它的书名就叫《技术封建主义》（Techno Feudalism），即技术导致的封建主义。\n作者雅尼斯·瓦鲁法基斯（Yanis Varoufakis）也不寻常。\n他是一个希腊的经济学家，曾经被 Valve 公司（世界最大游戏平台 Steam 的创造者）聘用，后来成为希腊的财政部长，离任后写了这本书。\n历史上，封建主义盛行在中世纪的欧洲，指的是大地主（又称为领主）占有大片土地，控制土地上的一切，农民也变成领主的私人财产（即农奴）。\n这跟现在的互联网有相似之处。\n互联网虽然大，但是占有统治地位的，就是那几家公司：谷歌、苹果、微软、亚马逊、Meta......它们是互联网领主，瓜分了大部分的数字领地。\n在自己的领地上，它们就是掌管一切的王。\n这本书进一步说，广大的网民实际上是数字农奴（又称云奴）。你在人家的领地里发帖子、上传照片视频，就是为领主工作。你拿不到多少回报，你的免费劳动只是为领主的公司增加价值。\n久而久之，你甚至还会产生心理依赖，一天不去领主的土地看看，心里就会万分失落。\n这些数字领主有强烈的领地意识，造出了高高的围篱，防止自己的数字资产（数据）流出，或者领地被其他公司侵占。\n如果你要使用某个领主的领地，必须交付地租（使用费）。比如，你要把自己的 App 放在苹果应用商店出售，就必须把30%的收入交给苹果。这是苹果对它的数字领地收取的地租，跟中世纪的封建制度一模一样。\n我看了书中的描述，真是脑洞大开，原来高大上的互联网经济，本质跟封建时代的地租没啥两样。\n作者瓦鲁法基斯建议，为了打破技术封建主义，防止数字领主的出现，政府必须实行一些强制措施。\n（1）征收云税，降低数字领主获取的云地租。\n（2）推行统一的数字身份，不要让每个网站都推行自己的用户体系。一旦用户使用谷歌、微软、苹果等大公司的用户身份，表示自己的互联网身份，他就会依赖这些大公司，不得不受制于它。设想一下，如果某一天你突然失去了微信账户，生活会多么不方便。\n（3）推行数据共享。用户在某个应用产生的数据，应该可以移植到其他应用。比如，你在推特发的帖子，应该可以移植到其他社交媒体。这样就避免了数据被数字领主垄断。\nReference\n技术封建主义",
    "url": "/blog/技术封建主义",
    "path": "thoughts/reading/技术封建主义.md",
    "lastModified": "2025-06-20T11:25:12.938Z"
  },
  {
    "title": "毫无意义的工作",
    "content": "简介\n毫无意义的工作这本书主要讲了一些现代社会中存在的无用或有害的工作，作者称之为“狗屁工作”。\n作者分析了这些工作的类型、原因、后果和应对，以及它们对个人和社会的影响。\n作者认为，这些工作是资本主义制度和管理封建主义的产物，它们浪费了人类的时间、创造力和潜能，也加剧了不平等和不满。\n作者提出了一些可能的解决方案，如全民基本收入、缩短工作时间、提高照料类工作的地位等。\n这本书是一部深刻而富有挑战性的社会学著作，引发了广泛的讨论和争议。\n第一章：什么是狗屁工作？ \n介绍了狗屁工作的概念和定义，以及他为什么要写这本书。他认为，狗屁工作是指那些即使消失了也不会对社会造成任何损失，甚至可能有益的工作。他通过网上征集的故事和自己的观察，分析了狗屁工作的类型、特征和分布。\n第二章：为什么有这么多狗屁工作？ \n探讨了狗屁工作产生的原因和背景，主要归结于资本主义制度和管理封建主义的逻辑。他指出，资本主义不是一个高效的经济体系，而是一个创造无用或有害的工作的体系。管理封建主义则是指那些通过增加层级和监督来维持自己权力和地位的人。\n第三章：为什么我们不反抗？ \n分析了为什么狗屁工没有组织起来反对这种现象，而是默默忍受或者自我欺骗。他认为，这与社会的价值观、道德观、宗教观、政治观等因素有关。他批判了工作伦理、消费主义、竞争主义等思想，认为它们都是对人类自由和幸福的障碍。\n第四章：狗屁工作对心理健康的影响。 讲述了狗屁工作对人们心理健康的负面影响，包括无聊、压抑、焦虑、愤怒、自卑、孤独等。他引用了一些心理学和社会学的研究，证明了人类需要有意义和创造性的活动，而不是空洞和机械的工作。\n第五章：狗屁工作对社会的影响。 探讨了狗屁工作对社会的影响，主要包括两方面：一是加剧了社会不平等和不公正，二是阻碍了社会进步和创新。他认为，狗屁工作不仅占用了人类的时间、资源和潜能，而且还剥夺了那些真正有用和有意义的工作的尊严和报酬。\n第六章：我们可以做些什么？ 提出了一些可能的解决方案，如全民基本收入、缩短工作时间、提高照料类工作的地位等。他认为，这些方案都可以从根本上改变人们对工作的态度和期待，让人们有更多的自由和选择，从而摆脱狗屁工作的困境。\n第七章：全民基本收入：一个激进的提案。 详细阐述了全民基本收入（UBI）的含义、理由、效果和挑战。他认为，UBI是一种保障每个人都能获得足够生活水平的无条件现金补贴，它可以解决贫困、失业、歧视等问题，也可以促进人们追求自己真正感兴趣和有价值的事情。\n第八章：缩短工作时间：一个温和的提案。 详细阐述了缩短工作时间的含义、理由、效果和挑战。他认为，缩短工作时间是指减少每周或每天的工作时长，而不是降低工作强度或效率。他认为，缩短工作时间可以提高人们的幸福、健康、平等、生产力等，也可以减少环境污染和资源消耗。\n第九章：为什么照料类工作如此重要？ 详细阐述了照料类工作的含义、价值、困境和改进。他认为，照料类工作是指那些关注他人的需求和福祉的工作，如教育、医疗、护理、家务等。他认为，照料类工作是人类社会的基础和核心，但却经常被忽视和贬低。他主张提高照料类工作的地位和报酬，让更多的人参与其中。\n第十章：一个没有狗屁工作的未来。 展望了一个没有狗屁工作的未来，以及如何实现这个愿景。他认为，这个未来是一个更加自由、平等、多样、创造、协作的社会，人们可以根据自己的兴趣和才能选择自己想做的事情，而不是被迫做无聊或有害的工作。他呼吁人们勇敢地追求这个目标，而不是屈服于现实的压力和束缚。",
    "url": "/blog/毫无意义的工作",
    "path": "thoughts/reading/毫无意义的工作.md",
    "lastModified": "2025-06-20T11:25:12.938Z"
  },
  {
    "title": "[算法之美](https://www.scribd.com/document/778432015/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E6%8C%87%E5%AF%BC%E5%B7%A5%E4%BD%9C%E4%B8%8E%E7%94%9F%E6%B4%BB)",
    "content": "算法之美\n第一章 - 最佳停止时间\n数学家对这个问题，已经有了充分的讨论。经过计算，成功概率最大的方法，叫做\"1/e 法则\"。e 是自然对数的底数，约等于2.718，那么 1/e 就约等于37%，所以它又称\"37%法则\"。\n\"37%法则\"的意思就是，寻找阶段进行到37%就要停止。 100个应聘者，先面试前37个，此后的面试只要遇到一个更优秀的，就立刻录取，不再继续面试了。换句话说，前37个面试者无论多么优秀，都不会录取，他们只是用来确定录取的标准。\n如果最合适的候选者偏偏在前面37%里面，那就只能错过了，作为\"寻找阶段\"不得不付出的成本。最终录取的将是不如前面候选人的次优选择。\n这个法则很实用，日常生活中，只要符合\"寻找-决策过程\"的场景，都可以适用37%法则。\n（1）相亲时，假定有10个相亲对象，那么前3～4个可以作为寻找阶段，后面只要遇到一个比前面更好的人，就可以同意了。\n（2）租房时，假定有一个月的找房子时间，那么30天的37﹪也就是11天。在找了11天之后，你就要出手了。只要发现比先前更令人心动的房子，就不要犹豫，马上租下来。\n（3）读书时，假定这本书有100页，如果读了37页，还没有发现感兴趣的内容，那就可以放弃了。\n（4）一个10集的电视剧，第4集是最佳弃剧时间。\n（5）一个10分钟的视频，看了3分42秒，如果还是觉得不好看，就可以关掉了。\n（6）一个年轻人想在18岁到24岁，一共7年时间里找到人生方向，确定未来想做什么。那么，他有2.59年（7 * 0.37）的时间自由尝试。也就是说，到了大三下学期就应该初步定下自己的方向，后面除非遇到更有吸引力的事情，否则就不应该转换事业方向。\n第二章 - 探索与利用：要最新的还是要最好的？\n是一类经典的决策问题，源于赌博场景中的老虎机。假设你在一个赌场里面对多个老虎机，每个老虎机都有不同的但未知的中奖概率。你必须决定如何在这些老虎机之间分配资源（比如投币次数），以便最大化你的总收益。\n问题的核心挑战在于探索与利用的平衡：你需要通过探索尝试不同的老虎机来了解它们的中奖概率（探索），同时也要利用目前收益最高的老虎机（利用），以获得最大的收益。\n第三章 - 排序 建立秩序\n第四章 - 缓存 忘了它吧\n第五章 - 时间调度理论 要事先行\n第六章 - 贝叶斯法则 预测未来\n第七章 - 过度拟合 不要想太多\n第八章 - 松弛 顺其自然\n第九章 - 随机性 何时应用随机\n第十章 - 网络 我们如何联系\n第十一章 - 博弈论 别人的想法\nReferences\n算法之美)",
    "url": "/blog/算法之美 - 指导工作与生活",
    "path": "thoughts/reading/算法之美 - 指导工作与生活.md",
    "lastModified": "2025-06-20T11:25:12.939Z"
  },
  {
    "title": "置身事内",
    "content": "简介\n该书以地方政府投融资为主线，深入浅出地论述了中国经济的发展过程和特点，以及政府在其中所起的重要作用。该书分为上下两篇，上篇解释了地方政府的基本事务、收支、土地融资和开发、投资和债务等微观机制；下篇解释了这些微观行为与城市化、工业化、房价、地区差异、债务风险、国内外贸易冲突等宏观现象的联系。最后一章总结了全书内容，并对当前的市场化改革和政府转型进行了解读。\n主要观点\n政府和市场不是简单的二元分割，而是相互依存和影响的复杂关系。在中国，政府不仅影响资源的分配，也深度参与资源的生产。因此，不能脱离政府来理解经济现象。\n地方政府是经济发展的深度参与者，通过招商引资、土地开发、城市建设等方式，推动了工业化和城市化的进程，在发展经济的同时逐步推动了市场机制的建立和完善。\n地方政府也面临着多重挑战和风险，如财税不平衡、债务压力、产能过剩、环境污染等。这些问题需要通过深化改革和转型来解决，包括要素市场改革、债务化解、再平衡和国内大循环等。\n了解政府认为应该怎么办，比了解“我们”认为应该怎么办，重要得多。因为政府的决策和行动会影响到我们每个人的生活和未来。因此，我们需要置身事内，关注政府的动向和逻辑，增进对中国发展现实的把握。\n启发\n需要从政府的角度来看待中国经济的发展，理解政府的动机、行为和逻辑，以及政府与市场的复杂互动。这有助于我们更好地把握中国经济的现状和未来，以及我们自己在其中的角色和机会。\n需要认识到中国经济发展的不平衡和不充分，以及政府面临的挑战和风险。这要求我们不仅要看到经济增长的成果，也要看到经济转型的必要性和紧迫性。我们需要关注政府的改革和转型，以及我们自己如何适应和参与这些变化。\n政府不是万能的，也不是万恶的。政府是由人组成的，人有优点也有缺点，有能力也有局限。政府需要市场的监督和反馈，也需要公民的参与和支持。我们需要理性地看待政府，既不盲目崇拜，也不无端指责。我们需要与政府沟通和合作，共同推动中国经济的发展和进步。\n很多事情从普通人的角度看起来无法理解甚至荒诞，但是站在政府角度却很合理；存在即合理，也即不同视角的合理",
    "url": "/blog/置身事内",
    "path": "thoughts/reading/置身事内.md",
    "lastModified": "2025-06-20T11:25:12.939Z"
  }
]