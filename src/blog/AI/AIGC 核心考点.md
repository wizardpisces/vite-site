# 特斯拉的 FSD 的技术栈
* RegNet 

# CLIP 模型
CLIP（Contrastive Language-Image Pretraining）是一种基于对比学习的多模态模型，旨在将计算机视觉和自然语言处理领域结合起来。

CLIP成为了计算机视觉和自然语言处理这两大AI方向的“桥梁”，AI领域的多模态应用有了经典的基石模型。

# 模型工厂
gpt 据说是一次训练成功，说明背后的模型工厂流水线很稳定（能够通过改变参数就得出预期的训练结果，对于模型的控制很稳定）
模型流水线

# STNs 神经网络

STN（Spatial Transformer Networks）可以简单理解为通过CNN来自动学习转换矩阵（参考下面**齐次坐标**），使得原图和转换矩阵运算后，能够被掰正。可插入到深度神经网络中的模块，它可以自主学习如何执行空间变换以增强模型的性能。

STN包括三个主要的部分：

* Localisation Network：这是STN的第一部分，负责学习如何执行最优的变换。基本上，它是一个卷积神经网络，可以输入图像并通过自己的层来预测变换的参数。例如，如果我们要执行仿射变换，Localisation Network可能会输出六个参数，定义了需要进行的平移、缩放、旋转和错切 的仿射矩阵。

* Parameterised Grid Generator：这一部分会接收Localisation Network输出的参数，并使用这些参数生成一个对每个像素进行变换的坐标网格。例如，如果输入图像需要旋转，则网格会生成旋转后每个像素的新位置。
  * 例如 
  grid = F.affine_grid(theta, x.size())
  ```
  图片像素：
  [(0,0), (0,1), (0,2),
 (1,0), (1,1), (1,2),
 (2,0), (2,1), (2,2)]

 -> 使用右移一位的仿射变换矩阵生成的仿射变换网格：

 [(0,1), (0,2), (0,3),
 (1,1), (1,2), (1,3),
 (2,1), (2,2), (2,3)]
```

* Sampler：F.grid_sample(x, grid) 部分使用网格生成的新坐标去索引输入图像，生成变换后的输出图像。如果新坐标点不在像素点的精确位置上，则会使用一些插值方法来估计新坐标对应的像素值。


[STNs 网络源码](https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html#spatial-transformer-networks-tutorial)
# 齐次坐标
在齐次坐标系统中，一个 n 维空间的点用 n+1 维的坐标来表示。在笛卡尔坐标系中，我们可以用两个坐标 (x, y) 来表示这个点。但是在齐次坐标系中，我们会用三个坐标 (x, y, z) 来表示这个点。这里的 z 坐标通常被称为“齐次坐标”。
* 属性：齐次坐标的一个关键属性是，如果你将所有坐标都乘以同一个非零的实数，那么这个点的位置并不会改变。例如，在齐次坐标系中，(2, 4, 2) 和 (1, 2, 1) 实际上表示的是同一个点。
* 应用：非常适合用来表示几何变换，如平移、旋转和缩放。
## 举例说明
[常见变换矩阵](https://img-blog.csdnimg.cn/20210715201056712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMjQzNzUw,size_16,color_FFFFFF,t_70)
```
假设我们有一个二维空间中的点 P，其笛卡尔坐标是 (x, y)。我们想将这个点在屏幕上向右移动 5 个单位，并向上移动 3 个单位。在笛卡尔坐标系中，这个操作会是：

P' = (x + 5, y + 3)

但是，如果我们想通过矩阵乘法来应用这个平移，我们就不能直接使用二维坐标，因为矩阵乘法不支持直接添加一个常数（如平移向量）。这就是齐次坐标发挥作用的地方。

我们首先将点 P 转换为齐次坐标，增加一个额外的维度：

P_homogeneous = (x, y, 1)

现在，我们可以使用一个 3x3 的变换矩阵来表示平移：

T = | 1 0 5 |
    | 0 1 3 |
    | 0 0 1 |

将变换矩阵 T 乘以齐次坐标 P_homogeneous，我们得到：

P'_homogeneous = T * P_homogeneous
               = | 1 0 5 |   | x |
                 | 0 1 3 | * | y |
                 | 0 0 1 |   | 1 |

               = | x + 5 |
                 | y + 3 |
                 |   1   |

这个结果的前两个坐标 (x + 5, y + 3) 就是应用平移后的新位置，与我们直接在笛卡尔坐标系中计算的结果相同。但是，使用齐次坐标，我们可以通过矩阵乘法来实现这个变换，这对于计算机图形学的应用非常重要，因为硬件通常针对矩阵运算进行了优化。

最后，我们可以将齐次坐标转换回笛卡尔坐标，只需要简单地忽略最后一个坐标（在这个例子中是 1）即可。所以，点 P 在平移变换后的笛卡尔坐标是 (x + 5, y + 3)。

这个例子展示了齐次坐标如何简化和统一几何变换的表示和计算，特别是在需要连续应用多个变换时。通过使用齐次坐标，我们可以将多个变换合并到一个矩阵中，然后一次性应用到点上，这在三维图形渲染中尤为重要。
## 为什么齐次坐标有这个特性


齐次坐标有这个特性是因为它们引入了一个额外的维度来代表点的“权重”，这允许坐标表示在尺度上不是唯一的。在齐次坐标系统中，一个点的位置由比例而非绝对值确定，这意味着只有坐标的比例（或比例关系）是重要的，而不是它们的实际数值。

这个特性的数学基础是射影几何中的概念，射影几何研究的是对象的几何属性在投影变换下的不变性。在射影空间中，一个点可以通过不同的齐次坐标来表示，只要这些坐标是成比例的。这是因为射影空间中的点是通过从原点出发的射线来定义的，而每个射线与给定的超平面（例如，二维空间中的线或三维空间中的平面）相交于一点。齐次坐标中的额外维度允许我们表示这样的射线，并通过交点来确定原始空间中的点。

在实际应用中，这个特性使得齐次坐标非常适合表示包括无穷远点在内的所有可能位置，这是在笛卡尔坐标系中不可能做到的。例如，在齐次坐标中，点 (x, y, z) 在 z 不为零时可以表示为笛卡尔坐标 (x/z, y/z)，而当 z 为零时，这个点可以被认为是在无穷远处。

在计算机图形学中，齐次坐标的这个特性允许我们使用矩阵乘法来表示和组合各种变换，包括那些在传统笛卡尔坐标系中需要特殊处理的变换，如平移。通过简单地将变换矩阵与齐次坐标向量相乘，我们可以应用复杂的变换序列，而无需为每种变换类型编写单独的代码。这种统一的处理方式简化了算法，并允许硬件和软件优化这些操作。



# SD U-Net的整体架构

# ResNet
* ResNet 是什么网络？对于 ResNet 模型，通常会使用 ImageNet 数据集进行训练，ImageNet 是一个包含超过一千万张图像的大规模图像数据集，涵盖了一千个不同类别的图像。
  * 任务：
    * 基础能力：图片分类（基础能力）
    * 改造后能完成的任务：目标检测（Object Detection）（迁移学习：在网络的顶部添加额外的卷积层和分类器），语义分割（Semantic Segmentation），实例分割（Instance Segmentation），图像风格转换（Image Style Transfer），图像超分辨率（Image Super-Resolution） 等

* 残差的作用？
  >缓解梯度消失问题：由于梯度可以通过短路路径直接传播，它减少了梯度通过多个层时潜在的消失问题。

  >加速训练：网络可以通过学习残差函数更加容易地训练，尤其是在非常深的网络中。

  >提升准确度：有了残差连接，网络可以学习得更深，在许多视觉识别任务中可以提升性能。

* 既然“短路”的部分不是选择性的，而是恒定的，那不就是数据同时通过并训练正常训练路径跟短路路径，这不是会让网络的训练耗时更长么，为什么还说加速了网络训练？

  > 短路连接（或跳跃连接）和主路径都会在每次前向和反向传播过程中同时进行计算。残差网络（ResNets）并不会节省计算时间。相反，由于引入了额外的跳跃连接，他们其实可能会稍微增加整体的计算成本。

  > “加速”在这里指的是残差网络的收敛速度，即网络如何快速降低训练损失并提高验证性能。这个收敛加速是通过处理一些影响深度网络训练的问题实现的。

  >1. **梯度消失和梯度爆炸问题**：深度神经网络往往难以训练，因为梯度，也就是损失函数相对于参数的导数，可能会变得非常小（梯度消失）或非常大（梯度爆炸）。这使得优化算法难以用有效的步骤来更新参数。通过引入短路连接，ResNets 允许梯度在不受阻碍的情况下流经多个层。这更容易保持梯度的大小适中，从而更有效地进行权重更新。

  >2. **恒等映射的保留**：ResNets的一个关键思想是:如果深层网络可以在不损失性能的情况下给出一个恒等映射，那么任何增加深度的尝试都不应减少网络的性能。通过引入短路连接，网络可以学习到的映射更接近于恒等映射，这可以帮助保证深度增加进而性能增加。

  >所以，当我们说ResNets "加速"训练时，是指它们通过解决这些问题增强了收敛性，从而更快地达到良好的训练和验证性能，而不是它们减小了每个迭代的计算成本。

* 网络啥时候走短路，啥时候走正常训练路径呢？
  >残差网络中的每一层都由两个主要部分组成：正常路径（或称为转换路径）和短路或跳过的路径（跳跃连接）。这两条路径在最后合并，通常是通过简单的加法操作进行合并。

  >通过这种架构，网络可以灵活地选择在某个特定层采取什么样的行动。如果有一层的输出与其输入没有太大差异（即："residual"，或残差，接近于零（则说明正常路径的输出趋近0），残差 = 残差块输出（=残差路径（一般是输入）+正常路径输出） - 残差块输入），那么网络可能选择大部分使用跳跃连接的路径，把那个层的输出直接传递到下一个层。这样的结果大致是，这一层并没有做太多的改变，只是曲线救过了梯度消失问题。在某种程度上，你可以把这种行为看作是网络"关闭"了这个层的部分操作。
  
  >另一方面，如果某一层的输入和输出之间有很大的差异（即："residual"，或残差，不接近于零），那么网络可能会更多地使用正常路径，而不依赖跳跃连接。这就意味着网络实际上对数据进行了一些有用的变换。

  >这个决定是通过训练数据自然产生的，不是通过人工采取主动调整的。所有这一切的目标都是为了最小化训练数据和网络预测之间的误差。通过训练自然而然地使得一条路径的影响力增强，另一条路径的影响力减弱。



[ResNet 详细解释](https://zhuanlan.zhihu.com/p/31852747)

# 深度学习成就人物
如果说深度学习三巨头Hinton、LeCun、Bengio是T0级别，那么何恺明（ResNet）毫无疑问T1级别。

* Geoffrey Hinton（杰弗里·辛顿）：他被认为是深度学习领域的先驱之一，提出了反向传播算法，并在神经网络和深度学习领域做出了众多重要贡献。他的工作为现代深度学习的发展奠定了基础。

* Yann LeCun（杨立昆）：他是卷积神经网络（CNN）的先驱之一，提出了卷积神经网络的概念，并在计算机视觉、模式识别和自然语言处理等领域做出了重要贡献。

* Yoshua Bengio（约书亚·本吉奥）：他是深度学习领域的杰出科学家之一，提出了深度置信网络（DBN）和深度生成模型等重要概念，并在深度学习理论和算法方面做出了许多开创性工作。

[何恺明目前的学术成果是否够得上计算机视觉领域历史第一人？](https://www.zhihu.com/question/424149824/answer/2296707462)


# Resource
* [SD 详解](https://zhuanlan.zhihu.com/p/632809634)
* [DeepLearing-Interview-Awesome-2024](https://github.com/315386775/DeepLearing-Interview-Awesome-2024)