
# 正在编辑....

# 几种生成方式
* 已有图片 + 文字描述
* 多图片各自短视频生成合并成长视频
* 已有视频 + 风格替换


# AI 视频生成发展
RNN -> GAN -> 自回归(transformer) -> diffusion -> sora/V-JEPA
* RNN：循环神经网络，是一种能够处理序列数据的神经网络结构，可以捕捉视频帧之间的时间依赖关系，生成连续的视频序列。RNN的代表模型有LSTM、GRU等。
* GAN：生成对抗网络，是一种无监督的生成模型框架，通过让两个神经网络相互博弈来进行机器学习。GAN可以生成视觉逼真度高的视频，但控制难度大、时序建模较弱。GAN的代表模型有DCGAN、WGAN、Pix2Pix、PatchGAN等。
* 自回归（transformer）：自回归模型是一种基于概率的生成模型，可以根据已有的数据预测下一个数据的概率分布。自回归模型可以实现细粒度语义控制，时序建模能力强，但计算量大。自回归模型的代表模型有Transformer、BART、GPT等。
* [diffusion](https://learnopencv.com/denoising-diffusion-probabilistic-models/) ：扩散模型是一种基于能量最小化的生成模型，可以将数据从高维空间扩散到低维空间，再从低维空间还原到高维空间。扩散模型可以生成高质量、高分辨率的视频，但生成速度慢。扩散模型的代表模型有DDPM、DVAE、Score-based等。 
    * dVAE 的基本思想是，首先使用 VAE 将原始数据编码成离散的潜在变量，然后使用扩散模型将潜在变量逐步加入噪声，最后使用逆向过程从噪声中重建潜在变量和原始数据
* sora/V-JEPA：sora是一种基于扩散模型的视频生成方法，可以从文本生成视频，具有高效、稳定、可控的特点。V-JEPA是一种基于自回归模型的视频生成方法，可以从图片生成视频，具有高质量、高分辨率、高连贯性的特点。

V-JEPA 是一种视频自监督学习的方法，它可以通过观看视频来学习视觉表示。它的特征预测方法是，让模型能够预测视频中一个区域（称为目标区域y）的特征表示，这个预测基于另一个区域（称为源区域x）的特征表示。这样，模型可以学习视频中不同时间步骤之间的语义关系，以及视频中的高级概念信息。

# Sora
Sora 架构
* 什么是 Patches？ - ViT
* 为什么可以做到多种分辨率？- 动态分辨率 NaViT 多尺寸分辨率训练及特征融合训练
* 视频数据怎么提取 patches？- 空间 patches - ViViT - 针对视频数据 patches 提取
* 主体架构？ -扩散 transformer - Dit - UNet 采用归纳偏置保持空间局部性和平移等边性，transformer 逐步学习更远像素点
* 提示词是否优化？- 提示词扩写 - GPT4