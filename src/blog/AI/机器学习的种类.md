# 监督学习
在监督学习中，训练数据既包含输入特征也包含输出标签（或称为目标值）。算法通过学习这些数据，建立输入和输出之间的映射关系，以便能够预测新输入数据的输出。监督学习常用于分类和回归问题。常见的监督学习算法包括线性回归、逻辑回归、决策树、支持向量机、朴素贝叶斯、神经网络等。

# 半监督学习
在半监督学习中，训练数据包含少量的带有标签的样本和大量的无标签样本。这种方法结合了监督学习和无监督学习的优势，可以在标签数据有限的情况下提高模型的性能。

# 无监督学习
不依赖于标注过的训练数据来学习数据的特征。算法能够自动地从数据中发现结构和模式，而不需要任何外部的指导或标签。无监督学习特别适合于那些**难以获取标注数据的场景**，或者当我们对数据的结构和关系知之甚少时。

* 聚类分析：通过聚类算法，如K-means或层次聚类，将数据分组，使得同一组内的数据点相似度高，不同组间的数据点相似度低。这有助于识别数据中的自然分布和结构。
* 降维技术：如主成分分析（PCA）、核PCA、t-SNE等，通过降低数据的维度来提取关键特征，同时尽量保留数据的重要信息。
* 自编码器（动态训练参数）：一种神经网络，通过重构输入数据来学习数据的有效表示。自编码器的隐藏层可以捕捉数据的关键特征。
  * 例子：MNIST 数据的压缩与解压中，对于每一张图像，编码器的函数都是试着去学习一种可以将图像的数千个像素点的信息压缩到一个32维的向量中的方法，而解码器则试图从这个32维的表示中重新构造出原始图像。训练过程中要达到的目标就是使得解码后的图像尽可能的接近原始的输入图像。 **输入图像数据本身就是训练目标**
* 对比学习：通过比较数据点之间的相似性和差异性，学习数据的特征表示。提供了更多的灵活性和效率（比如服务于迁移学习），尤其是在处理大规模未标注数据集时
  * 对比学习的目标（**可以被视作迁移学习的准备步骤**，以下给定的 SimpleCNN 例子可以在训练好后在最后一层加上全连接层进行图像分类的迁移学习，具体可以查看迁移学习的示例代码例子）
    * 表征学习： 对比学习主要用于无监督的场景，因此它最主要的目标是学习出能够捕捉数据本质特征的表达（表征），而不是像监督学习那样直接用于分类或回归任务。这些特征可以用于下游任务，例如图像识别、目标检测或自然语言处理任务等。
    * 数据增强的效果理解： 利用数据增强创建的不同视图，模型学习到的特征应该是对这些视图改变具有不变性的。这意味着无论图像如何旋转或变色，模型都可以识别出图像中的同一对象。

**对比学习示例**

PyTorch实现的对比学习的简单代码示例。模型的目标是学习一个特征表示，这个表示能够区分不同的数据样本。具体来说，模型通过最大化正样本（经过数据增强的相似图像）之间的相似度来实现这一点，同时最小化与负样本（不同的图像）之间的相似度。不依赖于外部的标注信息。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import CIFAR10

# 定义一个简单的卷积神经网络
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc = nn.Linear(64 * 8 * 8, 128)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = F.relu(self.fc(x))
        return x

# 数据增强
data_transforms = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
])

# 加载CIFAR-10数据集
train_dataset = CIFAR10(root='./data', train=True, download=True, transform=data_transforms)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# 初始化模型和优化器
model = SimpleCNN()
criterion = nn.CosineEmbeddingLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 训练模型
for epoch in range(5):
    for i, (inputs, _) in enumerate(train_loader):
        # 数据增强生成正样本
        inputs_augmented = data_transforms(inputs)
        # 计算原始输入和增强输入的特征表示
        features_original = model(inputs) # shape = (batch_size, 128)
        features_augmented = model(inputs_augmented) # shape = (batch_size, 128)
        # 设置标签，正样本为1，负样本为-1
        labels = torch.ones(inputs.size(0)).to(inputs.device) # shape = (batch_size)
        # 计算损失并优化
        loss = criterion(features_original, features_augmented, labels) # label 是一个标签值，当 label=1 时，两个样本被视为应该相似；当 label=-1 时，两个样本被视为不相似。
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')
```


# 强化学习
强化学习是一种通过与环境的交互来学习的方法。在这种学习方式中，算法通过尝试不同的行为来探索环境，并根据每次行为的结果（奖励或惩罚）来调整其策略，以最大化长期累积的奖励。强化学习常用于控制问题，如机器人导航、游戏AI等。（Python Gym库提供了一种简单有效的方式来设计和评估强化学习算法，特别适用于那些需要快速了解和应用新技术的场合。）

三种方式：
* 基于价值：例如 Q-learning
* 基于策略
* 价值与策略结合

## 例子

### 基于策略的代码例子

```python
# 只有两个状态（0和1），并且在每个状态下，智能体都可以选择两个动作（A和B）。如果智能体在状态0下选择动作A，它将获得1的奖励并转移到状态1，如果它选择动作B，它将获得0的奖励并保持在状态0。在状态1下，无论智能体选择哪个动作，它都将获得0的奖励并返回状态0。

# 定义状态、动作和奖励
states = [0, 1]
actions = ['A', 'B']
rewards = {(0, 'A'): 1, (0, 'B'): 0, (1, 'A'): 0, (1, 'B'): 0}

# 定义策略
policy = {}

# 对于每个状态，选择能获得最大奖励的动作
for state in states:
    best_action = None
    best_reward = float('-inf')
    for action in actions:
        reward = rewards[(state, action)]
        if reward > best_reward:
            best_action = action
            best_reward = reward
    policy[state] = best_action

# 打印策略
print(policy) # 代码给出的结果是 {0: 'A', 1: 'A'}，意味着不管在哪个起点，下一个 action 是A就是最好的选择？
```
### 基于价值的例子

```python
# Q-learning是一种无模型的强化学习方法，它通过学习一个动作价值函数（Q函数）来估计在给定状态下采取某个动作的期望回报。
import numpy as np

# 定义环境的奖励矩阵R
R = np.array([
    [-1, -1, -1, -1, 0, -1],
    [-1, -1, -1, 0, -1, 100],
    [-1, -1, -1, 0, -1, -1],
    [-1, 0, 0, -1, 0, -1],
    [0, -1, -1, 0, -1, 100],
    [-1, 0, -1, -1, 0, 100]
])

# 初始化Q矩阵，所有值为0
Q = np.zeros(R.shape)

# 学习参数
gamma = 0.8  # 折扣因子，值较高，代表我们更重视长期奖励
alpha = 0.1  # 学习率

# Q-learning算法
for episode in range(10000):
  state = np.random.randint(0, 6)  # 随机选择一个初始状态
  while state != 5:  # 直到达到目标状态5
      # 选择所有可能动作中的一个
      possible_actions = np.where(R[state] >= 0)[0]
      # print(possible_actions, np.where(R[state] >= 0)) #一种可能的输出 [1 2 4] (array([1, 2, 4]),)
      action = np.random.choice(possible_actions)

      # 计算Q值
      next_state = action
      Q[state, action] = R[state, action] + gamma * Q[next_state].max()

      # print(state,action)

      # 更新状态
      state = next_state

# 归一化Q矩阵
Q = Q / Q.max()

print("训练后的Q矩阵:")
print(Q)
```
```
训练后的Q矩阵:
[[0.    0.    0.    0.    0.8   0.   ]
 [0.    0.    0.    0.64  0.    1.   ]
 [0.    0.    0.    0.64  0.    0.   ]
 [0.    0.8   0.512 0.    0.8   0.   ]
 [0.64  0.    0.    0.64  0.    1.   ]
 [0.    0.    0.    0.    0.    0.   ]]
```

# 迁移学习
迁移学习是机器学习中的一种策略，它涉及将在一个任务上已经学习到的知识应用到另一个任务上。换句话说，你拿一个在大量数据上经过预训练的模型，然后将其应用到一个与之相关但数据相对较少的新任务上。这种做法可以加快模型的训练速度，提高模型的性能，尤其是当新任务的数据有限时。

## 迁移学习应用例子
使用Python中的PyTorch库对新的图像分类任务进行微调。这里所使用的预训练模型是ResNet，它经过在ImageNet数据集上的预训练，我们将其应用到一个新的分类任务上（如猫狗分类）。

```python
import torch
from torchvision import models, transforms, datasets
from torch import nn, optim

# 加载预训练的ResNet模型
model = models.resnet18(pretrained=True)

# 冻结模型的所有权重，以防止在训练新任务时被修改
for param in model.parameters():
    param.requires_grad = False

# 然后，我们用新任务的类别数目替换模型的最后一个全连接层（例如，如果新的任务是区分猫和狗，那么输出应为2）：
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 2) # 替换模型的最后一个全连接层，意味着同时丢弃了模型最后一层的权重？

# 之后，我们通过在新数据集上进行训练来微调这个模型。首先，设置一个数据加载器和数据变换：
data_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

train_data = datasets.ImageFolder(root='path_to_training_data', transform=data_transforms)
train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)

# 最后是训练循环。注意我们只优化了最后一层，因为其它层已经被冻结了：


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
```

# Reference
* gpt
* [AI by Doing](https://aibydoing.com/notebooks/chapter11-01-lab-introduction-and-examples-of-reinforcement-learning)