# 卷积与全连接网络

## 为什么有了全连接却又诞生了卷积

* 参数共享：
卷积网络通过卷积操作利用参数共享的机制，这意味着在处理图像时，同一个卷积核（滤波器）会应用在整个图像上。这种方法大大减少了模型的参数数量，使得卷积网络在处理图像等高维数据时更加高效。

* 局部连接：
卷积层的神经元仅与输入数据的一个局部区域相连接，这与全连接层的每个神经元连接到输入数据的所有元素形成鲜明对比。局部连接使得卷积网络能够捕捉到输入数据中的局部特征，这在图像识别等任务中非常有用。

* 空间层次结构：
CNNs 通过多个卷积层和池化层（pooling layers）的堆叠，能够自然地学习输入数据的空间层次结构。在这种架构中，底层卷积层可能专注于学习边缘或纹理等低级特征，而高层卷积层则能够组合这些低级特征来识别更复杂的模式。

* 减少过拟合：
由于卷积网络通常有更少的参数和内置的正则化效果（如参数共享和池化），它们在处理复杂图像任务时比全连接网络更不容易过拟合。

* 适用于图像数据：
图像数据具有很强的空间结构特征，卷积网络能够有效地利用这一点。在图像数据中，相邻的像素通常相关性较高，而远离的像素相关性较低。卷积网络通过卷积层直接对这种空间结构进行建模，而全连接网络则没有这种直接的建模能力。

平移不变性：
* 卷积网络具有一定程度的平移不变性，这意味着即使图像中的对象发生了平移，卷积网络仍然能够识别出来。这是因为同一个卷积核在整个图像上滑动，学习到的特征对于图像中的不同位置是一致的。

总结来说，虽然全连接网络在某些任务中表现良好，特别是输入数据的维度较低或者没有明显的空间结构时，但在处理图像这样的高维空间数据时，卷积网络由于其结构上的优势，通常能够提供更好的性能。

## 卷积比全连接能减少过拟合原因

* 参数越少，越不容易过拟合
  * 卷积的参数共享使得训练模型参数更少
  * 卷积下采样（池化），减少了特征图的尺寸，有助于降低模型复杂度
* 越抽象，越不容易过拟合：卷积空间局部性使得识别训练过程更加抽象

想象一下，你在做一个拼图游戏。全连接网络就像是你没有任何线索，只能靠猜来拼接每一块拼图。而卷积网络则像是给了你一些提示：哪些拼图块可能属于天空、哪些可能是树木等等。

在全连接网络中，每个输入信息点都要与每个输出点相连，这就像是拼图中的每一块都要试着与其他所有块相匹配来找到正确位置，这不仅耗时而且容易出错。如果你只有少量的拼图块（也就是数据），你可能会发现一些看似合适的匹配方式，但实际上它们并不是正确的大图景的一部分。这就是过拟合，即你的网络太过于适应你手头上的这些数据，而无法泛化到新的数据上。

卷积网络通过关注局部特征来减少这种风险。它就像是在告诉你：“不用看所有的拼图块，只看这一小块就好。”比如说，它可能只关注一块小区域内的拼图，这样就减少了错误匹配的机会。由于它只需要学习局部特征而不是整个图像的复杂模式，所以它需要的线索（也就是参数）就少得多。

简而言之，卷积网络通过专注于图片中的小部分（局部特征）并在整个图片中重复使用这些信息，减少了需要学习的内容量，从而减少了过拟合的风险。这就像是利用同样的线索来解决拼图的不同部分，而不是每个部分都重新来过。

## 卷积的参数共享

想象一下你用一个橡皮图章在一张纸上盖印。无论你在纸上的哪个位置盖印，图章的图案都是相同的。这就是参数共享的基本概念。

在卷积神经网络中，一个“橡皮图章”就是一个卷积核（或者叫滤波器）。这个卷积核包含了一些参数，它在输入数据（比如一张图片）上移动，每次移动都会应用相同的参数来检测特定的特征，比如边缘或者角落。这样，不管这些特征出现在图片的哪个位置，卷积核都能用相同的方式来识别它们。

如果不使用参数共享，那么每个位置的特征都需要一个独立的“图章”，这会导致需要很多很多的图章（参数）。但有了参数共享，你只需要一个图章就可以检测整张图片的相同特征，这大大减少了所需的参数数量。

*PS: 卷积的参数量级: C_in * C_out * Kernel_size（卷积核） * Kernel_size + C_out（偏置）*

## 卷积跟全连接网络的结合
* 图像分类：图像分类是指将输入的图像分配到预定义的类别中，如猫、狗、飞机等。图像分类的常用模型是卷积神经网络（CNN），它由多个卷积层、池化层和全连接层组成。卷积层和池化层负责提取图像的局部特征，而全连接层负责将这些特征整合成最终的分类结果


# VIT 是什么
[VIT](https://blog.csdn.net/lsb2002/article/details/135320751) - Google推出了VIT（Vision Transformer）：一个和Bert几乎一致，同时不添加任何卷积结构的图像分类模型。VIT在Transformer上的成功，证明了可以用统一的模型，来处理不同领域（语言/图像/视频）的任务，进而开启了多模态模型研究的新篇章。

## VIT vs CNN

[VIT 彻底赢了 CNN 么](https://www.zhihu.com/question/531529633)：
* transformer全局感受野，在大图片或者说找东西时效果好（类似近视眼，能够感受图像大轮廓）。
  * 注意力机制的缺点：抓重点信息，忽略不重要的信息，数据少的时候，注意力机制效果不如bilstm，bilstm序列短（比如验证码的长度？）的时候效果也比注意力机制好
* cnn局部感受野，对细节处理较好（理解像素级别的问题，例如 医疗影像）。

## 几个疑问
1. [如何理解 self-attention（transformer）更像low-pass filter，而conv（CNN）反而更高频一点？](https://www.zhihu.com/question/531529633)
    * self-attention机制通过计算输入序列中每个元素之间的相似度，来建立全局的依赖关系。这样，self-attention可以有效地捕捉图像或视频中的全局信息，但也会导致特征图变得过于平滑，丢失一些细节信息。因此，self-attention可以看作是一个低通滤波器，它只保留低频信息，而过滤掉高频信息。
    * 卷积操作则是通过在图像或视频上滑动一个小的窗口，来提取局部的特征。这样，卷积可以有效地捕捉图像或视频中的细节、纹理、边缘等高频信息，但也会忽略一些全局的信息
2. [如何从数学上对比两个神经网络的相似度？](https://www.zhihu.com/question/531529633/answer/3291275883)
  * 通过对神经网络进行特征向量编码进行相似度对比？特征向量如何反映神经网络对输入数据的变换和抽象？
    * 通过定义 [CKA (Centered Kernel Alignment)](https://zhuanlan.zhihu.com/p/66315878) 指标比较不同网络结构中的特征相似度：当在不同模型架构的层对之间绘制CKA相似度矩阵时，可以观察到ViT具有相对均匀的层相似度结构。相比之下，ResNet 模型在相似性结构上显示出明显的层级差异，较低层和较高层之间的相似度分数较小。这些结果说明，ViT 在整个模型中具有高度相似的表征，而 ResNet 模型在较低层和较高层之间的表征相似性要低得多。

## 其他概念
* 卷积层与卷积核：一个包含 64 个 3x3 的卷积核的卷积层（卷积核作用于 3 个 in_channels，然后叠加成为 out_channel）：self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)

# Reference

* GPT4
* https://easyai.tech/ai-definition/cnn/
