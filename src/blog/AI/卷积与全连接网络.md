# 卷积与全连接网络

## 为什么有了全连接却又诞生了卷积

* 参数共享：
卷积网络通过卷积操作利用参数共享的机制，这意味着在处理图像时，同一个卷积核（滤波器）会应用在整个图像上。这种方法大大减少了模型的参数数量，使得卷积网络在处理图像等高维数据时更加高效。

* 局部连接：
卷积层的神经元仅与输入数据的一个局部区域相连接，这与全连接层的每个神经元连接到输入数据的所有元素形成鲜明对比。局部连接使得卷积网络能够捕捉到输入数据中的局部特征，这在图像识别等任务中非常有用。

* 空间层次结构：
CNNs 通过多个卷积层和池化层（pooling layers）的堆叠，能够自然地学习输入数据的空间层次结构。在这种架构中，底层卷积层可能专注于学习边缘或纹理等低级特征，而高层卷积层则能够组合这些低级特征来识别更复杂的模式。

* 减少过拟合：
由于卷积网络通常有更少的参数和内置的正则化效果（如参数共享和池化），它们在处理复杂图像任务时比全连接网络更不容易过拟合。

* 适用于图像数据：
图像数据具有很强的空间结构特征，卷积网络能够有效地利用这一点。在图像数据中，相邻的像素通常相关性较高，而远离的像素相关性较低。卷积网络通过卷积层直接对这种空间结构进行建模，而全连接网络则没有这种直接的建模能力。

平移不变性：
* 卷积网络具有一定程度的平移不变性，这意味着即使图像中的对象发生了平移，卷积网络仍然能够识别出来。这是因为同一个卷积核在整个图像上滑动，学习到的特征对于图像中的不同位置是一致的。

总结来说，虽然全连接网络在某些任务中表现良好，特别是输入数据的维度较低或者没有明显的空间结构时，但在处理图像这样的高维空间数据时，卷积网络由于其结构上的优势，通常能够提供更好的性能。

## 卷积比全连接能减少过拟合原因

* 参数越少，越不容易过拟合
  * 卷积的参数共享使得训练模型参数更少
  * 卷积下采样（池化），减少了特征图的尺寸，有助于降低模型复杂度
* 越抽象，越不容易过拟合：卷积空间局部性使得识别训练过程更加抽象

想象一下，你在做一个拼图游戏。全连接网络就像是你没有任何线索，只能靠猜来拼接每一块拼图。而卷积网络则像是给了你一些提示：哪些拼图块可能属于天空、哪些可能是树木等等。

在全连接网络中，每个输入信息点都要与每个输出点相连，这就像是拼图中的每一块都要试着与其他所有块相匹配来找到正确位置，这不仅耗时而且容易出错。如果你只有少量的拼图块（也就是数据），你可能会发现一些看似合适的匹配方式，但实际上它们并不是正确的大图景的一部分。这就是过拟合，即你的网络太过于适应你手头上的这些数据，而无法泛化到新的数据上。

卷积网络通过关注局部特征来减少这种风险。它就像是在告诉你：“不用看所有的拼图块，只看这一小块就好。”比如说，它可能只关注一块小区域内的拼图，这样就减少了错误匹配的机会。由于它只需要学习局部特征而不是整个图像的复杂模式，所以它需要的线索（也就是参数）就少得多。

简而言之，卷积网络通过专注于图片中的小部分（局部特征）并在整个图片中重复使用这些信息，减少了需要学习的内容量，从而减少了过拟合的风险。这就像是利用同样的线索来解决拼图的不同部分，而不是每个部分都重新来过。

## 卷积的参数共享

想象一下你用一个橡皮图章在一张纸上盖印。无论你在纸上的哪个位置盖印，图章的图案都是相同的。这就是参数共享的基本概念。

在卷积神经网络中，一个“橡皮图章”就是一个卷积核（或者叫滤波器）。这个卷积核包含了一些参数，它在输入数据（比如一张图片）上移动，每次移动都会应用相同的参数来检测特定的特征，比如边缘或者角落。这样，不管这些特征出现在图片的哪个位置，卷积核都能用相同的方式来识别它们。

如果不使用参数共享，那么每个位置的特征都需要一个独立的“图章”，这会导致需要很多很多的图章（参数）。但有了参数共享，你只需要一个图章就可以检测整张图片的相同特征，这大大减少了所需的参数数量。

## 卷积跟全连接网络的结合
* 图像分类：图像分类是指将输入的图像分配到预定义的类别中，如猫、狗、飞机等。图像分类的常用模型是卷积神经网络（CNN），它由多个卷积层、池化层和全连接层组成。卷积层和池化层负责提取图像的局部特征，而全连接层负责将这些特征整合成最终的分类结果
# Reference

* GPT4
* https://easyai.tech/ai-definition/cnn/
