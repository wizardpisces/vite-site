# 反向传播
有一个简单的神经元函数 y = w * x，模拟计算梯度和进行反向传播的过程。

一次权重更新过程（给定初始数据输入 x = 1，w = 10，实际输出是 10，期望输出 y1 = 2 则 目标 w 为 2）：
```
输入 x = 1
初始权重 w = 10
计算输出 y = w * x = 10 * 1 = 10
```
计算损失：
```
给定期望输出 y1 = 2
计算损失函数 loss = (y1 - y)^2 = (2 - 10)^2 = 64
```
计算梯度：
```
损失函数对权重 w 的梯度：dL/dw = 2 * (y1 - y) * (-x)
将具体数值代入：

dL/dw = 2 * (2 - 10) * (-1) = 2 * (-8) * (-1) = 16
因此，梯度为 dL/dw = 16。
```
反向传播：
```
使用梯度下降法更新权重 w：
w = w - learning_rate * dL/dw
假设学习率（learning rate）为 0.1，将梯度代入：

w = 10 - 0.1 * 16 = 10 - 1.6 = 8.4
更新后的权重为 w = 8.4。
```

# CGAN MNIST 训练步骤
1. 固定 generator （ real_label = [batch_size, 10] 的对真实 label的 one-hot 编码 ）
    * 用真数据训练 output_label = Discriminator(real_image)，d_real_loss = BCELoss(out_label,real_label)
    * 用虚假数据（噪音 + 真实标签 [batch_size, noise_dim（满足0~1正态分布）] + real_label = [batch_size, noise_dim+10] = z_tensor ）
    * 训练 fake_image = Generator(z_tensor) 得出 fake_image( Tensor[batch_size, 1, 28, 28])
    * 再次 out_label = Discriminator(fake_image) ，d_fake_loss = BCELoss(out_label,fake_label(全0))
    * 计算 D_loss = d_real_loss + d_fake_loss 反向传播，更新 Discriminator
2. 固定 discriminator
    * 由 fake_image = Generator(z_tensor)
    * 由 Discriminator(fake_image) 得出 out_label ，g_loss = BCELoss(out_label,real_label)
    * 计算 G_loss = g_loss 反向传播，更新 Generator

# 预处理

* 归一化：一种常见的图像预处理操作，它用于将图像的像素值归一化为均值为0、标准差为1的分布，或者只将数据收窄到 -1 ~ 1 之间。常用于 CNN 网络数据预处理
    * 加速训练：常用的激活函数如 Sigmoid 和 Tanh 在输入值较大或较小的区域会饱和，导致梯度接近或完全为零，从而使梯度下降变得非常缓慢或停滞。通过将像素值缩放到 -1 到 1 的范围，可以使输入值位于激活函数的线性区域，避免梯度饱和问题，提高网络的训练效果。
    * 模型稳定性：在优化算法中，例如梯度下降法，较大的梯度值可能导致参数更新过大，从而使优化过程不稳定甚至发散。通过将像素值缩放到 -1 到 1 的范围，可以将梯度控制在较小的范围内，提高优化算法的数值稳定性，使模型更容易收敛。
    * 数据分布一致性：将像素值缩放到 -1 到 1 的范围可以使不同图像之间的像素分布更加一致。这样做的目的是确保输入数据的统计特性在整个训练集上是相似的，从而提高模型的泛化能力。
    * 推广：Batch Normalization (BN) 层作用类似，但是应用在**训练阶段**，对每个小批量数据进行标准化

# 损失函数
## 交叉熵

* 熵：阿根廷 1/4概率打进决赛 ，1/2 概率获得冠军，1/8 获得冠军，则有 f(1/8) = f(1/2) + f(1/4)，f(x) := 信息量，推出可能的 f(x) := -log(x) （log 2为底单调上升，加负号才则单调向下）
* 交叉熵：KL 散度是一种用于衡量两个概率分布之间差异的度量，KL(P || Q) = Σ(P(i) * log(P(i) / Q(i)))，固定分布 P 的时候 KL 散度可以化简为交叉熵 KL(P || Q) = Σ(P(i) * log(P(i) / Q(i))) = -Σ(P(i) * log(Q(i))) = -H(P, Q)；可以很好的用于机器学习损失计算

### 问题
* 回归跟分类区别？
    * 分类例子：识别图片是猫还是狗
    * 回归例子：通过特征1-n预测房价
    * 思考：分类跟回归的区别是目标的 离散跟连续 区别？还是说输出的label之间是否有“距离度量”？
* 为什么交叉熵适合分类，而 MSE 适合回归?
    * 交叉熵
        * 概率解释性：交叉熵基于概率分布之间的差异进行度量，更适合分类问题，因为分类问题通常涉及对不同类别的概率分布进行建模和预测。
        * 梯度更强烈：相对于MSE，交叉熵的梯度更加陡峭，这可以加快模型的收敛速度。对于分类问题，更快的收敛速度可能是一个优势。
    * MES
        * 数学上的合理性：MSE 是对预测值与真实值的差异的平方进行度量，可以提供对预测误差的较为精确的度量。
        * 对异常值不敏感：平方差的计算使得 MSE 对异常值不敏感，因为平方操作会放大异常值的影响。这在某些回归问题中可能是有益的。

Reference
* [王木头学科学](https://www.youtube.com/@wkaing)
* https://zhuanlan.zhihu.com/p/104130889
* [回归与分类问题区别](https://cloud.tencent.com/developer/article/1604194)
# Transformer 

## positional encoding
位置编码的要求：选择正弦跟余弦组合编码
* 每个位置都有唯一的编码。
* 在不同长度的句子中，两个时间步之间的距离应该一致。
* 模型不受句子长短的影响，并且编码范围是有界的。（不会随着句子加长数字就无限增大）
* 必须是确定性的。

总结
* 问题及其解答：
    * 为什么没有直接使用 1,2,3...这种线性编码？
        * 原因：周期性模式在位置编码中的不同维度上呈现出不同的变化速度和周期（下面例子会说明）
            * 捕捉长距离依赖关系（线性模式也能做到，但是不够精细）
            * 提供更丰富的表示能力：较低频率的维度具有较长的周期，可以捕捉到大范围的序列结构，而较高频率的维度可以更细致地表示局部模式和短距离的依赖关系。
            * 避免过拟合：随着句子变长，这些值可能会变得特别大，并且我们的模型可能会遇到比训练时更长的句子
* 思考例子：
    * 第一个词编码为 [1,2,3]， 则位置可用向量 [秒，分，时]来表示；第二个词编码为 [4,5,6]， 则位置可用向量 [秒 + 1，分 + 1/60，时 + 1/360] 来表示
    * 周期：在一个词向量上会出现不同的周期变化，能同时追踪近距离跟远距离的词关系：秒针走一个周期 60 秒，分针走一步；分走一个周期 60 分， 时针+1；
    * 周期设定：通过设定 秒，分，时之间的周期关系（比如可以设定600秒，分针才走一步，则会拉上周期变化，追踪更远的词关系）

Reference
* [positional encoding blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
* [positional encoding stackexchange + youtube](https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model)

## self-attention

思考
* 多头注意力机制与卷积的多通道（channel）进行类比。多头注意力机制和卷积的多通道都涉及并行地学习不同的特征表示。它们都致力于提取输入数据的多样化特征，并捕捉输入中的不同模式和关联性。

Reference
* [self-attention](https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-self-attention-f5fb363c736d)
* [multi-head attention in transformer](https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-multi-head-attention-3bc3409c4312))


# one hot 编码
One-hot 编码是一种将离散的分类标签转换为二进制向量的方法，它的优点是可以消除不同类别之间的偏序关系，使得特征之间的距离计算更加合理。（方便在机器学习分类任务计算 LOSS）
## 例子
比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。

两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。

那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。
## 实现
```python
# 假设 text 中的字符集是由大小写字母和数字组成的，共有 62 个字符（例如，char_set = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"），则 self.char_set_len 的值为 62。对于每个字符 ch，它在字符集中的索引位置是唯一的，因此 i * self.char_set_len + self.char_set.index(ch) 的结果也是唯一的。
vector = np.zeros(self.max_captcha * self.char_set_len) # shape = [max_captcha*36]
for i, ch in enumerate(text):
    idx = i * self.char_set_len + self.char_set.index(ch) # idx = (0-(max_captcha-1))*36+(0-36)
    vector[idx] = 1
```
思考：如果是不定长编码，该如何改进?

* [机器学习：数据预处理之独热编码（One-Hot）](https://zhuanlan.zhihu.com/p/39012149)
## CRNN + CTC 解决变长文本识别

### CTC 如何计算预测序列跟目标序列的相识度？
1. 生成 time step 的预测概率（有 CRNN 模型输出），每个 time step 会有对类别的预测概率向量
2. 通过算法将所有可能路径的概率相加，并取对数（通常用于数值稳定性）得到最终的相似度分数。
[Reference](https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI)
# 深度学习编程范式
Tensorflow vs Pytorch（符号式与命令式程序）
命令式
* 更加灵活：原生语言的灵活性跟运行时断点
```python
    import numpy as np
    a = np.ones(10)
    b = np.ones(10) * 2
    c = b * a
    d = c + 1
```
对应符号式（DSL）：
* 节省内存：掌控全局的内存分析并优化
```python
    A = Variable('A')
    B = Variable('B')

    # 当执行 C = B * A 时，不会发生任何计算。相反，此操作会生成表示计算的计算图（也称为符号图）
    C = B * A 
    D = C + Constant(1)

    # compiles the function 并真正的执行计算结果
    f = compile(D) 
    d = f(A=np.ones(10), B=np.ones(10)*2)
```
符号图：![符号图](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/prog_model/comp_graph.png)

类比：类似 react jsx命令式（直接难优化）模板跟 vue 的声明式（真正执行前能做各种运行时优化）模板？

[Reference blog](https://mxnet.apache.org/versions/1.9.1/api/architecture/program_model#:~:text=Symbolic%20Programs%20Tend%20to%20be,flow%20of%20a%20host%20language.)

# 机器学习分类的一点技巧
* 对数据进行分类
    * 有特征：直接通过已经有的分类进行绘制图
        * 先通过特征维度绘制图；例如：数据集仅包含两个分离相当明显的聚类。其中一个簇包含 Iris setosa，而另一个簇包含 Iris virginica 和 Iris versicolor；通过特征绘制出的图会分成明显2堆，其中一堆是交错2种类型 Iris
        * 如果没有明显的聚类，并且数据维度多，可以通过 PCA 等方式降维后再分
    * 无特征：则先通过 KMeans 能方式聚类，再通过有特征方式分析
# 尝试理解 ONNX （Open Neural Network Exchange）
* 是什么？ONNX = （模型本身 + 模型训练好的权重跟偏置）的一种更加抽象的表达
* 如何表示？使用预定义的 operator（描述输入与输出的关系，例如：add算子=  inputA + inputB = OutputC，可拓展）来描述模型，用向量描述训练好的参数
* 作用？实现不同深度学习框架和平台之间的模型互操作性
* 为什么 pytorch 在导出 ONNX 的时候需要传入一组输入？原因：
    * ONNX 并非像编译器一样彻底解析原模型的代码，记录所有控制流；而是不考虑控制流的静态图
    * 而是利用 pytorch trace 机制，将参数传入模型执行，并记录执行这组输入对应的计算图

# 深度学习中的 Epoch 和 Batch
1. Epoch 是什么？ 
一次 Epoch = 让所有数据通过模型正向+反向传播一次 = 一个完整的学习周期
2. Epoch 设置多少次合适？
无定论：
次数少会导致欠拟合；
次数多会导致过拟合；
3. 什么是 Batch？
Batch Size = 一次训练的样本数
每一次参数的更新所需要损失函数并不是由一个数据获得的，而是由一批数据加权得到的
4. Batch 的作用？
* 效率：利用矩阵计算加速（相对于单个去训练）
* 稳定性：平均每个数据样本的贡献，减少梯度的方差
* 多大合适：看情况，太小会导致训练太久；太大会导致内存受不了

Reference
* [epoch-vs-iterations-vs-batch-size](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)

# GPT 尝试
1. 代码尝试
    * 重构转换
        * 输入 js -> ts
    * 优化
        * 格式化代码
    * 创建
        * 创建插件（eslint）步骤：写测试用例 -> 输入 gpt -> 生成插件 -> 微调成型
            * 问题：自动生成的代码会比较繁琐或者隐藏逻辑问题，也不会去利用第三方的包的能力
            * 目前方案：需要 developer 找到更便捷的方式再去投喂给 gpt 生成更加合理简洁的代码

# CNN 的简单理解
    * 网络越深，学习的知识越抽象：比如第一层hidden layer负责编码诸如点、线、边缘等浅层信息；第二层hidden layer编码简单点的纹理、形状等信息；第三层hidden layer编码诸如眼睛、鼻子等目标的形状...，然后逐层学习，不断地提取抽象的特征，一气呵成，最终学会了辨识花草树木、飞禽走兽等等。 - [reference](https://zhuanlan.zhihu.com/p/112513743)
    * 网络越宽，每一层学习的知识越丰富：增加网络的宽度意味着同一个hidden layer有着更多的神经元，每一个神经元代表一种颜色，一个方向，一种纹理，组合起来便可以学习到更多不同的颜色信息，各个不同的方向以及不同频率的条纹信息。

# 一句话信息
    * 生成对抗网络（GAN） VS 变分自编码器（VAE）： GAN 倾向于生成逼真的合成样本，而 VAE 倾向于生成具有一定程度多样性的样本。如果期望生成特定目标样本，可以考虑 CGAN 跟 CVAE
        * [GAN 基本原理及其应用](https://easyai.tech/ai-definition/gan/)
    * VIT - Google推出了VIT（Vision Transformer）：一个和Bert几乎一致，同时不添加任何卷积结构的图像分类模型。VIT在Transformer上的成功，证明了可以用统一的模型，来处理不同领域（语言/图像/视频）的任务，进而开启了多模态模型研究的新篇章。
    * 文摘 - 高手解决问题的方式从来都不是纠结问题本身，而是升维；升维成功，问题也就解决了
    * 雷军2023演讲 - 如何快速学习：知识不全是线性的，大部分是网状的，知识点之间不一定有绝对的先后关系；前面内容看不懂，跳过去，并不影响学后面的；后面的学会了，有时候更容易看懂前面的。

# AGI 的一点理解

机器学习训练了很多模型，而 LLM 只是其中之一；
ChatGPT 之所以跟 AGI 最接近，是因为语言模型的通用性；
如果能够理解自然语言，那就可以实现所有文字能够描述的任务；
其他 AI 任务，比如图片识别，则只能做到图片相似度能人物处理（推荐），无法拓展到相对通用的任务，除非日常交流能够通过表情包完成

# gpt 可能的研究方向

* 建设高难度的综合任务评测数据集（LLM 的测试用例，越完备 -> 越强大）
* 高质量数据工程（密集+多样性）：LLM 进化 = 更多高质量数据
    * 数据例子
        * 密度极高的高质量数据：wiki
        * 高质量问答：quora，知乎
        * 高质量图片：
    * 思考
        * 高质量数据消耗完后 gpt 如何进化？
        * 能否自己创造知识自己消费（类似 alpha-go 自我对弈的进化）？
        * 如果 gpt 成长的资料来源于人类，那能否突破人类知识的边界？
* 探索 LLM 模型的规模天花板：大模型大数据，能参与的玩家不多
    * 思考：是否会出现共建超大模型
* 增强 LLM 的复杂推理能力
* LLM 纳入 NLP之外更多其它研究领域：多模态？
    * 如何突破符号领域？如果某个领域是非成文的，不能用符号记录表达，那么 GPT 是否就无能为力。比如，人类的很多心理活动、潜意识、灵感、顿悟等等，GPT 如何模拟生成。
* 更易用的人和LLM的交互接口：听觉？
* 超大LLM模型Transformer的稀疏化：相同算力下提高训练速度

参考
* [通向AGI之路：大型语言模型（LLM）技术精要](https://zhuanlan.zhihu.com/p/597586623)
# 关于 chatGPT 引发的人工智能思考 2023-3-1

* 人跟AI的关系：淘汰还是互补？
    * 人有自主目的性（AI暂无），AI是实现目的的工具；
* 提问跟回答能力，哪个更能生存下来？
    * 往后提出好问题能力的重要性将越来越超过回答问题能力
* 教育
    * 投喂答案的教育模式需要变革 -> 把提问能力列入考核标准，更能培养出人机协作人才
    * 让人利用机器，而不是把人培养成机器
    * 文理分科这种教育模式急迫需要改变：chatGPT 需要文理结合，提好问题，同时认清答案

# chatGPT 衍生的未来职业？（更新 2023-3-13，[参考](https://www.youtube.com/watch?v=UsaZhQ9bY2k)）

场景跟问题
* 更精准提出需求，才能利用好 chatGPT
* 辅助 chatGPT 修正回答错误，同时又不影响模型输出的其他答案
* 检测回答是否由机器生成
* 如何避免 chatGPT 泄密，如何做隐私保护（目前可以 chatGPT 被催眠然后突破本身不泄密的限制）
* AI 训练，避免伦理问题
* AI 本身安全：解决提示注入（类似 网页的 xss ，SQL 注入等），越狱等安全问题
* 知识产权重新定义：AI生成的东西到底算不算侵权？（例如之前的爬别人网站的数据作为自身的商业盈利依据，是否算侵权？）
* 趋势预测
    * 动作（运动，游戏等数据）文件化：可以对整场羽毛球做文字标记序列化，然后输入 chatGPT，最后可以预测落点跟个人行为

职业名？
* 标注师：标记信息，投喂并训练 AI
* 安全员：确保 AI 不被攻破

gpt自己的回答
* 提示工程师：提示是一种指导GPT-4生成内容的文本或图像，通常包含一些特殊的符号或指令。提示工程师就是专门设计和优化提示的人员，他们需要了解GPT-4的内部机制和逻辑，以及不同领域和场景下用户的需求和偏好。提示工程师可以为各种应用场景提供高质量、高效率、高安全性的提示服务。
* 内容审核员：虽然GPT-4具有强大的生成能力，但它也可能会产生一些不合适或有害的内容，如色情、暴力、歧视、谣言等。内容审核员就是负责检查和过滤GPT-4生成内容中是否存在这些问题，并及时删除或修改不良内容。内容审核员需要具备一定的专业知识和判断能力，以及良好的道德素养和责任心。
* 内容运营师：内容运营师是利用GPT-4为各种平台和渠道提供优质内容服务的人员，他们需要根据目标受众和市场需求，选择合适的提示和参数来调用GPT-4生成相应类型和风格的内容，并进行编辑、优化和发布。内容运营师需要具备一定的创意思维和文案能力，以及对各种媒体平台和行业动态有一定了解。
* 内容创作者：内容创作者是利用GPT-4辅助自己进行创作活动的人员，他们可以将自己想要表达或传达给用户
