# VGG 网络
视觉几何组（Visual Geometry Group）所开发，VGG网络使得网络设计的理念发生了重要转变，即通过重复使用简单的层结构（3x3卷积核和2x2池化层）并深化网络结构，来提高性能。VGG网络同时还证实了深度是实现优秀性能的关键因素之一。
# 感知损失（perceptual loss）
也称为内容损失（content loss），是一种在深度学习特别是在视觉相关任务中使用的损失函数。它不同于传统的像素级损失函数（例如L1损失和L2损失），感知损失更注重于图像内容的感知相似性而不只是像素值的相似性。

* 例子:假设我们正在进行一个图像风格迁移任务，其中目标是将一幅图像的风格（如梵高的画风）迁移到另一幅图像上，同时保留图像的内容。理想情况下，生成的图像应该在视觉上看起来要有梵高笔触风格的颜色和纹理，但同时能识别出原图的内容（如城市的轮廓、天空的位置等）。
在这个任务中，如果使用像素级损失，那么模型可能会非常注重确保生成图像在像素层面与原图尽可能接近，而忽视了风格上的转变。这可能导致风格迁移效果不明显。

* 实现方式：如果使用感知损失来训练模型，我们会首先通过一个预先训练好的深度CNN（如VGG网络）传递原图和生成图，然后计算这两幅图在某些内部层激活值的差异。这些层的激活值代表了图像的高级特征，所以这种差异反映了它们在内容和感知上的相似度。最小化这种差异可以鼓励生成的图像在视觉感知层面上更贴近原图的内容，同时也有目标风格的特质。

* 适用任务：风格迁移、超分辨率和图像合成等。

问题：哪一图层是提取的风格信息？

>>在CNN中，随着层级的加深：
* 初始层主要捕捉基础信息，如边缘和颜色。这些层对图片细节的响应很敏感，但并不捕捉具体的风格信息。
* 中间层捕捉更复杂的特征，如纹理和图案，这些正是构成图像风格的要素。
* 深层则表示更高级的内容，例如图像中的对象和整体布局。

# [stable diffusion](https://zhuanlan.zhihu.com/p/632809634)
* stable diffusion 是一个生成模型
    * 目标：用降噪网络生成清晰的图像（加噪是辅助训练降噪）
    * 类比：类似 GAN 模型，只不过 GAN 是个步骤的对抗训练，而 SD 是一个多步骤
    * 方法
        * 加噪过程（前向过程）是一个马尔可夫链，它逐步将随机噪声添加到数据中，直到数据变成纯噪声。这个过程是可控的，因为我们知道每一步加入的噪声量。通过这种方式，模型可以学习在任何给定的时间步骤预测噪声的分布。
        * 降噪过程（逆向过程）中，模型使用在加噪过程中学到的知识来预测噪声，并从噪声数据中去除这些噪声，逐步恢复出清晰的数据。如果没有加噪过程，模型就没有机会学习这些噪声分布的信息，也就无法有效地进行降噪和数据重建

疑问
    * SD 中将加噪图片输入U-Net中预测噪声如何理解？
        * "预测噪声"是扩散模型的一部分，它涉及到将图像从含有噪声的状态逐步恢复到清晰的状态。SD模型首先将一张完全随机的噪声图像（或者是经过一系列噪声添加步骤后的图像）输入到U-Net中。U-Net的目标是预测这张噪声图像中的原始噪声成分。一旦预测出这些噪声，模型就可以从噪声图像中去除它们，从而使图像逐渐变得更清晰。
    * 降噪过程中的预测噪声跟实际噪声的对比，这里的实际噪声是从加噪中得到的么？
    * 什么是噪声分布信息？
        * 噪声类型（高斯噪声等），噪声参数（均值方差），时间依赖项（噪声的量和性质会随着时间步骤的推进而改变）

# [神经网络可解释性](https://zhuanlan.zhihu.com/p/479485138)
# 增量学习
理论：增量学习的核心在于模型能够通过不断学习新数据来提升自身的性能，即使这些数据是由模型自身已经准确识别过的。源于人类的终身学习能力，即不断获取、调整和转移知识的能力，同时避免灾难性遗忘——即新知识的学习对旧知识造成的干扰。

思考：意味着模型识别准确的新数据再来投喂给模型本身训练也能提升模型准确率？
# data-centric AI 
Data-centric AI is the discipline of systematically engineering the data used to build an AI system. — Andrew Ng
* [SAM  data-centric AI](https://www.zhihu.com/question/521096166)
# 核函数
kernel function or kernel trick
* 概念：将原始空间中的向量作为输入向量，并返回特征空间（转换后的数据空间,可能是高维）中向量的点积的函数称为核函数。
    * 简单理解：一种便捷的计算在高维空间里的内积的方法。高维空间的数据计算存在困难。所以替代方案是在特征空间中计算相似度度量，而不是计算向量的坐标，然后应用只需要该度量值的算法。用点积(dot product)表示相似性度量。
* 歧义：把数据从低维映射到高维的是映射函数而不是核函数

* [带例子的核函数解释](https://www.zhihu.com/question/24627666/answer/28440943)
* [核函数概念](https://blog.csdn.net/mengjizhiyou/article/details/103437423)
# 嵌入层
嵌入层是一种将离散值转换为连续向量的技术；

例如：torch.nn.Embedding(10, 5) 将创建一个嵌入层，该层可以将离散值（例如：[1,'a','你好']）映射到连续向量空间中的 5 维向量。
* 如何理解这里的离散跟连续呢？
    * 向量里的每一项是否连续，决定了向量本身是连续的还是离散的。例如：[0.2,0.3,0.5] 是一个连续向量，因为它的每一项都是实数，而实数是连续的。 这样的向量可以进行连续的运算和比较，比如求和，求差，求点积，求模长等。
    * 而离散是指每一项都不是连续的。例如，[1,‘a’,‘你好’]是一个离散向量，因为它的每一项都是离散的，而且不能进行连续的运算和比较。中的1不可能是’a’，因为它们属于不同的集合，也没有定义它们之间的转换规则。
# 机器学习与英语学习
* 机器学习的内容输出可以类比为通过沉浸式学习英语后能够说出英语，其中另一种方式是通过先学习词法和语法规则。在机器学习中，模型通过大量的数据输入（数据投喂）来理解其中的潜在规律和特征。类似地，通过沉浸式学习英语，我们可以在大量的语言环境中感知和理解英语的潜在规律和特征，从而能够流利地说出英语。
* 早期的人工智能（通过条件语句进行判断然后做输出）与先学习词法和语法规则的方法与相似。早期的人工智能系统通常使用预定义的规则和条件语句来处理输入并生成输出。这些规则和条件语句基于词法和语法规则，用于处理特定的输入情况。类似地，通过先学习词法和语法规则，我们可以在语言学习中掌握词汇和语法规则，并使用它们来理解和生成语言。

两种机器学习区别：通过先预定义的规则和条件语句的方法在一些特定场景下具有**精确性和可解释性**的优势，而通过机器学习从数据中学习的方法则**更加灵活、适应性强，并能够处理复杂情况**。

两种英语学习区别：通过先学习词法和语法规则来学习英语可以提供**结构化学习和准确性**，但**缺乏实际应用，语感欠缺，学习繁琐**，而沉浸式学习英语则更加贴近**实际应用、注重流利性和文化融合**，但**需要环境支持，初始困难**。

思考：意味着初始的时候先了解基本语法，后续不断去读各种精选文章（而不是研究更深的语法规则）才是英文学习的最佳路线？而机器学习没有初始化烦恼，所以直接去学海量数据就好？

# AI 视频生成发展
RNN -> GAN -> 自回归(transformer) -> diffusion -> sora/V-JEPA
* RNN：循环神经网络，是一种能够处理序列数据的神经网络结构，可以捕捉视频帧之间的时间依赖关系，生成连续的视频序列。RNN的代表模型有LSTM、GRU等。
* GAN：生成对抗网络，是一种无监督的生成模型框架，通过让两个神经网络相互博弈来进行机器学习。GAN可以生成视觉逼真度高的视频，但控制难度大、时序建模较弱。GAN的代表模型有DCGAN、WGAN、Pix2Pix、PatchGAN等。
* 自回归（transformer）：自回归模型是一种基于概率的生成模型，可以根据已有的数据预测下一个数据的概率分布。自回归模型可以实现细粒度语义控制，时序建模能力强，但计算量大。自回归模型的代表模型有Transformer、BART、GPT等。
* [diffusion](https://learnopencv.com/denoising-diffusion-probabilistic-models/) ：扩散模型是一种基于能量最小化的生成模型，可以将数据从高维空间扩散到低维空间，再从低维空间还原到高维空间。扩散模型可以生成高质量、高分辨率的视频，但生成速度慢。扩散模型的代表模型有DDPM、DVAE、Score-based等。 
    * dVAE 的基本思想是，首先使用 VAE 将原始数据编码成离散的潜在变量，然后使用扩散模型将潜在变量逐步加入噪声，最后使用逆向过程从噪声中重建潜在变量和原始数据
* sora/V-JEPA：sora是一种基于扩散模型的视频生成方法，可以从文本生成视频，具有高效、稳定、可控的特点。V-JEPA是一种基于自回归模型的视频生成方法，可以从图片生成视频，具有高质量、高分辨率、高连贯性的特点。

V-JEPA 是一种视频自监督学习的方法，它可以通过观看视频来学习视觉表示。它的特征预测方法是，让模型能够预测视频中一个区域（称为目标区域y）的特征表示，这个预测基于另一个区域（称为源区域x）的特征表示。这样，模型可以学习视频中不同时间步骤之间的语义关系，以及视频中的高级概念信息。

# GAN，VAE，Diffusion 生成模型理解
## 潜空间
* GAN 是先随机一个符合高斯分布潜在空间作为Generator输入生成图片，然后投喂给Discriminate 作为输入判定然后做 反向传播；
* VAE 是通过训练 Encoder 将输入映射到复合高斯分布（实际通过训练得到均值和方差）的潜在空间，然后解码器通过对潜在空间解码得到输出；数据先降维再升维
    * 在 MNist 中潜在空间可以是20维长度的向量，来表示 20 个不同的均值和方差分布，来代表 20 个可能的特征？
* Diffusion模型和其他生成模型一样，实现从噪声（采样自简单的分布）生成目标数据样本。
    * 核心原理是通过一个随机的前向过程（Forward Process）和一个去噪的逆向过程（Reverse Process）来实现从噪声（Noise）到目标数据样本（Data Sample）的转换。

## Diffusion vs GAN
* 速度：Diffusion 需要多步骤到图片，而 GAN 是一步到位；所以Diffusion 会慢，但是训练过程也更稳定
* 应用面：扩散模型可以利用多种条件来控制生成的图像，比如文本描述、图像掩码、深度图等，而 GAN 通常只能利用类别标签或噪声作为条件。这使得扩散模型可以更灵活地应用于不同的任务，比如图像编辑、图像修复、图像翻译等


# Diffusion 模型过程

* 初始噪声：从某个先验分布中生成初始噪声信号。
* 扩散过程：通过一系列步骤，将当前噪声信号逐渐扩散，以生成下一个时间步的噪声信号。这个过程中使用了逆扩散方程，可以将当前步骤的噪声信号映射到上一步骤的噪声信号。
* 逆扩散采样：通过逆扩散过程中的采样操作，将当前噪声信号转化为以下一步的噪声信号。
* 生成器网络：使用生成器网络将当前噪声信号映射回高维空间，生成一帧图像。
* 损失函数与优化：根据生成图像与目标真实图像之间的差异，定义适当的损失函数，并通过反向传播和优化算法来更新生成器网络的参数。

# 理解 VAE
* 变分推理
* KL 散度
    * [自信息、熵、交叉熵与KL散度 的推导](https://zhuanlan.zhihu.com/p/345025351)
    * [KL-Divergence 与交叉熵](https://blog.csdn.net/Dby_freedom/article/details/83374650)
    * [KL 散度形象说明（翻译）](https://www.jianshu.com/p/43318a3dc715)
* [EM——期望最大 算法](https://zhuanlan.zhihu.com/p/78311644)
* 交叉熵
* 贝叶斯定理
* 自由能

# 深度学习优化器

优化方向

    * 基于动量（NAG）
    * 基于自动学习率 （例如 RMSprop，配置训练简单）
    * 结合两者的（Adam 一般最优）

为什么 WGAN 选择 RMSprop 作为优化器，而不是 Adam?
>> WGAN的目标是通过最小化生成器和判别器之间的Wasserstein距离来提高生成样本的质量。传统的生成对抗网络（GANs）在训练过程中容易出现梯度伪影的问题（告诉你错了，但并没有指出错在哪里，导致更新方向错误，把正确改掉，错误留下），即判别器的梯度无法提供有关生成器当前状态的准确信息，导致训练不稳定。RMSProp优化算法通过自适应地调整学习率来减轻梯度伪影问题，有助于更稳定地训练WGAN。

* [深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)

# 反向传播
有一个简单的神经元函数 y = w * x，模拟计算梯度和进行反向传播的过程。

一次权重更新过程（给定初始数据输入 x = 1，w = 10，实际输出是 10，期望输出 y1 = 2 则 目标 w 为 2）：
```
输入 x = 1
初始权重 w = 10
计算输出 y = w * x = 10 * 1 = 10
```
计算损失：
```
给定期望输出 y1 = 2
计算损失函数 loss = (y1 - y)^2 = (2 - 10)^2 = 64
```
计算梯度：
```
损失函数对权重 w 的梯度：dL/dw = 2 * (y1 - y) * (-x)
将具体数值代入：

dL/dw = 2 * (2 - 10) * (-1) = 2 * (-8) * (-1) = 16
因此，梯度为 dL/dw = 16。
```
反向传播：
```
使用梯度下降法更新权重 w：
w = w - learning_rate * dL/dw
假设学习率（learning rate）为 0.1，将梯度代入：

w = 10 - 0.1 * 16 = 10 - 1.6 = 8.4
更新后的权重为 w = 8.4。
```
# CGAN MNIST 训练步骤
1. 固定 generator （ real_label = [batch_size, 10] 的对真实 label的 one-hot 编码 ）
    * 用真数据训练 output_label = Discriminator(real_image)，d_real_loss = BCELoss(out_label,real_label)
    * 用虚假数据（噪音 + 真实标签 [batch_size, noise_dim（满足0~1正态分布）] + real_label = [batch_size, noise_dim+10] = z_tensor ）
    * 训练 fake_image = Generator(z_tensor) 得出 fake_image( Tensor[batch_size, 1, 28, 28])
    * 再次 out_label = Discriminator(fake_image) ，d_fake_loss = BCELoss(out_label,fake_label(全0))
    * 计算 D_loss = d_real_loss + d_fake_loss 反向传播，更新 Discriminator
2. 固定 discriminator
    * 由 fake_image = Generator(z_tensor)
    * 由 Discriminator(fake_image) 得出 out_label ，g_loss = BCELoss(out_label,real_label)
    * 计算 G_loss = g_loss 反向传播，更新 Generator

# 预处理

* 归一化：一种常见的图像预处理操作，它用于将图像的像素值归一化为均值为0、标准差为1的分布，或者只将数据收窄到 -1 ~ 1 之间。常用于 CNN 网络数据预处理
    * 加速训练：常用的激活函数如 Sigmoid 和 Tanh 在输入值较大或较小的区域会饱和，导致梯度接近或完全为零，从而使梯度下降变得非常缓慢或停滞。通过将像素值缩放到 -1 到 1 的范围，可以使输入值位于激活函数的线性区域，避免梯度饱和问题，提高网络的训练效果。
    * 模型稳定性：在优化算法中，例如梯度下降法，较大的梯度值可能导致参数更新过大，从而使优化过程不稳定甚至发散。通过将像素值缩放到 -1 到 1 的范围，可以将梯度控制在较小的范围内，提高优化算法的数值稳定性，使模型更容易收敛。
    * 数据分布一致性：将像素值缩放到 -1 到 1 的范围可以使不同图像之间的像素分布更加一致。这样做的目的是确保输入数据的统计特性在整个训练集上是相似的，从而提高模型的泛化能力。
    * 推广：Batch Normalization (BN) 层作用类似，但是应用在**训练阶段**，对每个小批量数据进行标准化

# 损失函数
## 交叉熵

* 熵：阿根廷 1/4概率打进决赛 ，1/2 概率获得冠军，1/8 获得冠军，则有 f(1/8) = f(1/2) + f(1/4)，f(x) := 信息量，推出可能的 f(x) := -log(x) （log 2为底单调上升，加负号才则单调向下）
* 交叉熵：KL 散度是一种用于衡量两个概率分布之间差异的度量，KL(P || Q) = Σ(P(i) * log(P(i) / Q(i)))，固定分布 P 的时候 KL 散度可以化简为交叉熵 KL(P || Q) = Σ(P(i) * log(P(i) / Q(i))) = -Σ(P(i) * log(Q(i))) = -H(P, Q)；可以很好的用于机器学习损失计算

### 问题
* 回归跟分类区别？
    * 分类例子：识别图片是猫还是狗
    * 回归例子：通过特征1-n预测房价
    * 思考：分类跟回归的区别是目标的 离散跟连续 区别？还是说输出的label之间是否有“距离度量”？
* 为什么交叉熵适合分类，而 MSE 适合回归?
    * 交叉熵
        * 概率解释性：交叉熵基于概率分布之间的差异进行度量，更适合分类问题，因为分类问题通常涉及对不同类别的概率分布进行建模和预测。
        * 梯度更强烈：相对于MSE，交叉熵的梯度更加陡峭，这可以加快模型的收敛速度。对于分类问题，更快的收敛速度可能是一个优势。
    * MES
        * 数学上的合理性：MSE 是对预测值与真实值的差异的平方进行度量，可以提供对预测误差的较为精确的度量。
        * 对异常值不敏感：平方差的计算使得 MSE 对异常值不敏感，因为平方操作会放大异常值的影响。这在某些回归问题中可能是有益的。

Reference
* [王木头学科学](https://www.youtube.com/@wkaing)
* https://zhuanlan.zhihu.com/p/104130889
* [回归与分类问题区别](https://cloud.tencent.com/developer/article/1604194)
# Transformer 

## positional encoding
位置编码的要求：选择正弦跟余弦组合编码
* 每个位置都有唯一的编码。
* 在不同长度的句子中，两个时间步之间的距离应该一致。
* 模型不受句子长短的影响，并且编码范围是有界的。（不会随着句子加长数字就无限增大）
* 必须是确定性的。

总结
* 问题及其解答：
    * 为什么没有直接使用 1,2,3...这种线性编码？
        * 原因：周期性模式在位置编码中的不同维度上呈现出不同的变化速度和周期（下面例子会说明）
            * 捕捉长距离依赖关系（线性模式也能做到，但是不够精细）
            * 提供更丰富的表示能力：较低频率的维度具有较长的周期，可以捕捉到大范围的序列结构，而较高频率的维度可以更细致地表示局部模式和短距离的依赖关系。
            * 避免过拟合：随着句子变长，这些值可能会变得特别大，并且我们的模型可能会遇到比训练时更长的句子
* 思考例子：
    * 第一个词编码为 [1,2,3]， 则位置可用向量 [秒，分，时]来表示；第二个词编码为 [4,5,6]， 则位置可用向量 [秒 + 1，分 + 1/60，时 + 1/360] 来表示
    * 周期：在一个词向量上会出现不同的周期变化，能同时追踪近距离跟远距离的词关系：秒针走一个周期 60 秒，分针走一步；分走一个周期 60 分， 时针+1；
    * 周期设定：通过设定 秒，分，时之间的周期关系（比如可以设定600秒，分针才走一步，则会拉上周期变化，追踪更远的词关系）

Reference
* [positional encoding blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
* [positional encoding stackexchange + youtube](https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model)

## self-attention

思考
* 多头注意力机制与卷积的多通道（channel）进行类比。多头注意力机制和卷积的多通道都涉及并行地学习不同的特征表示。它们都致力于提取输入数据的多样化特征，并捕捉输入中的不同模式和关联性。

Reference
* [self-attention](https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-self-attention-f5fb363c736d)
* [multi-head attention in transformer](https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-multi-head-attention-3bc3409c4312))


# one hot 编码
One-hot 编码是一种将离散的分类标签转换为二进制向量的方法，它的优点是可以消除不同类别之间的偏序关系，使得特征之间的距离计算更加合理。（方便在机器学习分类任务计算 LOSS）
## 例子
比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。

两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。

那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。
## 实现
```python
# 假设 text 中的字符集是由大小写字母和数字组成的，共有 62 个字符（例如，char_set = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"），则 self.char_set_len 的值为 62。对于每个字符 ch，它在字符集中的索引位置是唯一的，因此 i * self.char_set_len + self.char_set.index(ch) 的结果也是唯一的。
vector = np.zeros(self.max_captcha * self.char_set_len) # shape = [max_captcha*36]
for i, ch in enumerate(text):
    idx = i * self.char_set_len + self.char_set.index(ch) # idx = (0-(max_captcha-1))*36+(0-36)
    vector[idx] = 1
```
思考：如果是不定长编码，该如何改进?

* [机器学习：数据预处理之独热编码（One-Hot）](https://zhuanlan.zhihu.com/p/39012149)
## CRNN + CTC 解决变长文本识别

### CTC 如何计算预测序列跟目标序列的相识度？
1. 生成 time step 的预测概率（有 CRNN 模型输出），每个 time step 会有对类别的预测概率向量
2. 通过算法将所有可能路径的概率相加，并取对数（通常用于数值稳定性）得到最终的相似度分数。
[Reference](https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI)
# 深度学习编程范式
Tensorflow vs Pytorch（符号式与命令式程序）
命令式
* 更加灵活：原生语言的灵活性跟运行时断点
```python
    import numpy as np
    a = np.ones(10)
    b = np.ones(10) * 2
    c = b * a
    d = c + 1
```
对应符号式（DSL）：
* 节省内存：掌控全局的内存分析并优化
```python
    A = Variable('A')
    B = Variable('B')

    # 当执行 C = B * A 时，不会发生任何计算。相反，此操作会生成表示计算的计算图（也称为符号图）
    C = B * A 
    D = C + Constant(1)

    # compiles the function 并真正的执行计算结果
    f = compile(D) 
    d = f(A=np.ones(10), B=np.ones(10)*2)
```
符号图：![符号图](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/prog_model/comp_graph.png)

类比：类似 react jsx命令式（直接难优化）模板跟 vue 的声明式（真正执行前能做各种运行时优化）模板？

[Reference blog](https://mxnet.apache.org/versions/1.9.1/api/architecture/program_model#:~:text=Symbolic%20Programs%20Tend%20to%20be,flow%20of%20a%20host%20language.)

# 机器学习分类的一点技巧
* 对数据进行分类
    * 有特征：直接通过已经有的分类进行绘制图
        * 先通过特征维度绘制图；例如：数据集仅包含两个分离相当明显的聚类。其中一个簇包含 Iris setosa，而另一个簇包含 Iris virginica 和 Iris versicolor；通过特征绘制出的图会分成明显2堆，其中一堆是交错2种类型 Iris
        * 如果没有明显的聚类，并且数据维度多，可以通过 PCA 等方式降维后再分
    * 无特征：则先通过 KMeans 能方式聚类，再通过有特征方式分析
# 尝试理解 ONNX （Open Neural Network Exchange）
* 是什么？ONNX = （模型本身 + 模型训练好的权重跟偏置）的一种更加抽象的表达
* 如何表示？使用预定义的 operator（描述输入与输出的关系，例如：add算子=  inputA + inputB = OutputC，可拓展）来描述模型，用向量描述训练好的参数
* 作用？实现不同深度学习框架和平台之间的模型互操作性
* 为什么 pytorch 在导出 ONNX 的时候需要传入一组输入？原因：
    * ONNX 并非像编译器一样彻底解析原模型的代码，记录所有控制流；而是不考虑控制流的静态图
    * 而是利用 pytorch trace 机制，将参数传入模型执行，并记录执行这组输入对应的计算图

# 深度学习中的 Epoch 和 Batch
1. Epoch 是什么？ 
一次 Epoch = 让所有数据通过模型正向+反向传播一次 = 一个完整的学习周期
2. Epoch 设置多少次合适？
无定论：
次数少会导致欠拟合；
次数多会导致过拟合；
3. 什么是 Batch？
Batch Size = 一次训练的样本数
每一次参数的更新所需要损失函数并不是由一个数据获得的，而是由一批数据加权得到的
4. Batch 的作用？
* 效率：利用矩阵计算加速（相对于单个去训练）
* 稳定性：平均每个数据样本的贡献，减少梯度的方差
* 多大合适：看情况，太小会导致训练太久；太大会导致内存受不了

Reference
* [epoch-vs-iterations-vs-batch-size](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)

# GPT 尝试
1. 代码尝试
    * 重构转换
        * 输入 js -> ts
    * 优化
        * 格式化代码
    * 创建
        * 创建插件（eslint）步骤：写测试用例 -> 输入 gpt -> 生成插件 -> 微调成型
            * 问题：自动生成的代码会比较繁琐或者隐藏逻辑问题，也不会去利用第三方的包的能力
            * 目前方案：需要 developer 找到更便捷的方式再去投喂给 gpt 生成更加合理简洁的代码

# CNN 的简单理解
* 网络越深，学习的知识越抽象：比如第一层hidden layer负责编码诸如点、线、边缘等浅层信息；第二层hidden layer编码简单点的纹理、形状等信息；第三层hidden layer编码诸如眼睛、鼻子等目标的形状...，然后逐层学习，不断地提取抽象的特征，一气呵成，最终学会了辨识花草树木、飞禽走兽等等。 - [reference](https://zhuanlan.zhihu.com/p/112513743)
    * 网络越宽，每一层学习的知识越丰富：增加网络的宽度意味着同一个hidden layer有着更多的神经元，每一个神经元代表一种颜色，一个方向，一种纹理，组合起来便可以学习到更多不同的颜色信息，各个不同的方向以及不同频率的条纹信息。

# 一句话信息
* 生成对抗网络（GAN） VS 变分自编码器（VAE）： GAN 倾向于生成逼真的合成样本，而 VAE 倾向于生成具有一定程度多样性的样本。如果期望生成特定目标样本，可以考虑 CGAN 跟 CVAE* [GAN 基本原理及其应用](https://easyai.tech/ai-definition/gan/)
* DALL-E uses Discrete Variational Autoencoder (dVAE) for this step. dVAE is a variant of traditional Variational Autoencoder (VAE) that operates in a discrete latent space. It is similar to VQ-VAE but uses distribution instead of nearest neighbor.
* [VIT](https://blog.csdn.net/lsb2002/article/details/135320751) - Google推出了VIT（Vision Transformer）：一个和Bert几乎一致，同时不添加任何卷积结构的图像分类模型。VIT在Transformer上的成功，证明了可以用统一的模型，来处理不同领域（语言/图像/视频）的任务，进而开启了多模态模型研究的新篇章。
    * [vit彻底赢了 CNN 么](https://www.zhihu.com/question/531529633)：transformer全局感受野，在大图片或者说找东西时效果好（类似近视眼，能够感受图像大轮廓）。cnn局部感受野，对细节处理较好（理解像素级别的问题，例如 医疗影像）。
* 文摘 - 高手解决问题的方式从来都不是纠结问题本身，而是升维；升维成功，问题也就解决了
* 雷军2023演讲 - 如何快速学习：知识不全是线性的，大部分是网状的，知识点之间不一定有绝对的先后关系；前面内容看不懂，跳过去，并不影响学后面的；后面的学会了，有时候更容易看懂前面的。

# AGI 的一点理解

机器学习训练了很多模型，而 LLM 只是其中之一；
ChatGPT 之所以跟 AGI 最接近，是因为语言模型的通用性；
如果能够理解自然语言，那就可以实现所有文字能够描述的任务；
其他 AI 任务，比如图片识别，则只能做到图片相似度能人物处理（推荐），无法拓展到相对通用的任务，除非日常交流能够通过表情包完成

# gpt 可能的研究方向

* 建设高难度的综合任务评测数据集（LLM 的测试用例，越完备 -> 越强大）
* 高质量数据工程（密集+多样性）：LLM 进化 = 更多高质量数据
    * 数据例子
        * 密度极高的高质量数据：wiki
        * 高质量问答：quora，知乎
        * 高质量图片：
    * 思考
        * 高质量数据消耗完后 gpt 如何进化？
        * 能否自己创造知识自己消费（类似 alpha-go 自我对弈的进化）？
        * 如果 gpt 成长的资料来源于人类，那能否突破人类知识的边界？
* 探索 LLM 模型的规模天花板：大模型大数据，能参与的玩家不多
    * 思考：是否会出现共建超大模型
* 增强 LLM 的复杂推理能力
* LLM 纳入 NLP之外更多其它研究领域：多模态？
    * 如何突破符号领域？如果某个领域是非成文的，不能用符号记录表达，那么 GPT 是否就无能为力。比如，人类的很多心理活动、潜意识、灵感、顿悟等等，GPT 如何模拟生成。
* 更易用的人和LLM的交互接口：听觉？
* 超大LLM模型Transformer的稀疏化：相同算力下提高训练速度

参考
* [通向AGI之路：大型语言模型（LLM）技术精要](https://zhuanlan.zhihu.com/p/597586623)
# 关于 chatGPT 引发的人工智能思考 2023-3-1

* 人跟AI的关系：淘汰还是互补？
    * 人有自主目的性（AI暂无），AI是实现目的的工具；
* 提问跟回答能力，哪个更能生存下来？
    * 往后提出好问题能力的重要性将越来越超过回答问题能力
* 教育
    * 投喂答案的教育模式需要变革 -> 把提问能力列入考核标准，更能培养出人机协作人才
    * 让人利用机器，而不是把人培养成机器
    * 文理分科这种教育模式急迫需要改变：chatGPT 需要文理结合，提好问题，同时认清答案

# chatGPT 衍生的未来职业？（更新 2023-3-13，[参考](https://www.youtube.com/watch?v=UsaZhQ9bY2k)）

场景跟问题
* 更精准提出需求，才能利用好 chatGPT
* 辅助 chatGPT 修正回答错误，同时又不影响模型输出的其他答案
* 检测回答是否由机器生成
* 如何避免 chatGPT 泄密，如何做隐私保护（目前可以 chatGPT 被催眠然后突破本身不泄密的限制）
* AI 训练，避免伦理问题
* AI 本身安全：解决提示注入（类似 网页的 xss ，SQL 注入等），越狱等安全问题
* 知识产权重新定义：AI生成的东西到底算不算侵权？（例如之前的爬别人网站的数据作为自身的商业盈利依据，是否算侵权？）
* 趋势预测
    * 动作（运动，游戏等数据）文件化：可以对整场羽毛球做文字标记序列化，然后输入 chatGPT，最后可以预测落点跟个人行为

职业名？
* 标注师：标记信息，投喂并训练 AI
* 安全员：确保 AI 不被攻破

gpt自己的回答
* 提示工程师：提示是一种指导GPT-4生成内容的文本或图像，通常包含一些特殊的符号或指令。提示工程师就是专门设计和优化提示的人员，他们需要了解GPT-4的内部机制和逻辑，以及不同领域和场景下用户的需求和偏好。提示工程师可以为各种应用场景提供高质量、高效率、高安全性的提示服务。
* 内容审核员：虽然GPT-4具有强大的生成能力，但它也可能会产生一些不合适或有害的内容，如色情、暴力、歧视、谣言等。内容审核员就是负责检查和过滤GPT-4生成内容中是否存在这些问题，并及时删除或修改不良内容。内容审核员需要具备一定的专业知识和判断能力，以及良好的道德素养和责任心。
* 内容运营师：内容运营师是利用GPT-4为各种平台和渠道提供优质内容服务的人员，他们需要根据目标受众和市场需求，选择合适的提示和参数来调用GPT-4生成相应类型和风格的内容，并进行编辑、优化和发布。内容运营师需要具备一定的创意思维和文案能力，以及对各种媒体平台和行业动态有一定了解。
* 内容创作者：内容创作者是利用GPT-4辅助自己进行创作活动的人员，他们可以将自己想要表达或传达给用户
