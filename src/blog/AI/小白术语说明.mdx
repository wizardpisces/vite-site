# 小白问术语

## 学习

### 语言模型是如何学习语法知识
当预训练语言模型学习语法知识时，通常是通过预测句子中的语法结构或者句法信息。以下是一个简单的例子：

假设有一个句子："The dog jumps over the lazy cat."，我们想要让预训练语言模型学习到这个句子中的语法知识，例如主语、谓语、宾语等。

为了实现这一点，我们可以对句子进行一些操作，例如：

遮蔽（Mask）主语：将句子中的主语"The dog"遮蔽（Mask）掉，然后让预训练语言模型去预测这个位置的单词或短语。通过这种方式，模型可以学习到主语通常是哪些类型的单词或短语，并且可以学习到它们与其他单词或短语之间的语义关系。
遮蔽（Mask）谓语：将句子中的谓语"jumps"遮蔽（Mask）掉，然后让预训练语言模型去预测这个位置的单词或短语。通过这种方式，模型可以学习到谓语通常是哪些类型的单词或短语，并且可以学习到它们与其他单词或短语之间的语义关系。
遮蔽（Mask）宾语：将句子中的宾语"the lazy cat"遮蔽（Mask）掉，然后让预训练语言模型去预测这个位置的单词或短语。通过这种方式，模型可以学习到宾语通常是哪些类型的单词或短语，并且可以学习到它们与其他单词或短语之间的语义关系。
通过这些操作，预训练语言模型可以通过自监督学习任务来学习到语法知识，例如主语、谓语、宾语等。这些知识可以帮助模型更好地理解自然语言，从而在实际的下游任务中表现更加优秀。

### 对比学习

对比学习就是让模型通过看一些相似的和不相似的东西，来学习它们的特点。比如，你可以通过看一些猫和狗的图片，来学习它们的样子，而不需要别人告诉你哪个是猫，哪个是狗。你只需要知道哪些图片是同一种动物，哪些图片是不同种动物，然后让模型尽量把同一种动物的图片放得近一点，把不同种动物的图片放得远一点。这样，模型就可以自己学习到猫和狗的特点。

优点：

- 可以利用大量的无标签数据来学习数据的特征，提高模型的泛化能力和性能。
- 可以用于预训练模型，为下游任务提供一个良好的初始化，减少训练时间和数据需求。
- 可以用于多种领域和任务，比如图像，文本，音频等，以及分类，聚类，检索等。

缺点：

- 需要设计合适的数据增强方法，以生成有效的正样本对和负样本对。
- 需要选择合适的批次大小，以保证有足够的负样本对进行对比。
- 需要避免一些假负样本对的影响，比如不同类别但相似的数据点。
- 需要平衡对比损失函数和下游任务损失函数之间的权重，以避免过拟合或欠拟合。

## Loss Function

不同的 losses 的作用：

- ContrastiveLoss: 假设我们想训练一个模型，让它能够判断两个句子是否有相同的意思。我们可以给模型一些句子对，比如：

```python
('A dog is an animal', 'A cat is an animal') # 相同的意思
('A dog is an animal', 'A plane is a vehicle') # 不同的意思
```

我们可以使用 ContrastiveLoss 来让模型学习，使得相同意思的句子对之间的距离尽可能小，不同意思的句子对之间的距离尽可能大。

- MultipleNegativesRankingLoss: 假设我们想训练一个模型，让它能够从多个句子中选择最合适的一个。我们可以给模型一些句子列表，比如：

```python
['This framework generates embeddings for each input sentence', # 锚点
 'Sentences are passed as a list of string.', # 正确的候选
 'The quick brown fox jumps over the lazy dog.', # 错误的候选
 'A man is eating pasta.', # 错误的候选
 'Can you help me with something?' # 错误的候选
]
```

我们可以使用 MultipleNegativesRankingLoss 来让模型学习，使得锚点和正确候选句子之间的距离尽可能小，锚点和错误候选句子之间的距离尽可能大。

- CosineSimilarityLoss: 假设我们想训练一个模型，让它能够计算两个句子之间的相似度分数。我们可以给模型一些句子对和分数，比如：

```python
('A dog is an animal', 'A cat is an animal', 0.9) # 高度相似
('A dog is an animal', 'A plane is a vehicle', 0.1) # 完全不相似
('A plane is a vehicle', 'A car is a vehicle', 0.8) # 相似
('A plane is a vehicle', 'A bike is a vehicle', 0.7) # 相似
```

我们可以使用 CosineSimilarityLoss 来让模型学习，使得模型预测的相似度分数与人工标注的相似度分数尽可能接近。

### 交叉熵损失函数

最小二乘法比较直观，很容易解释，但不具有普遍意义，对于更多其他机器学习问题，比如二分类和多分类问题，最小二乘法就难以派上用场

交叉熵损失函数是一种常用于分类问题中的损失函数，它的大小表示两个概率分布之间的差异。交叉熵损失函数可以通过最小化交叉熵来得到目标概率分布的近似分布。

举例说明交叉熵损失函数，假设我们需要对数字1，2，3进行分类，它们的label依次为： [1,0,0]， [0,1,0]， [0,0,1]。当输入的图像为数字1时，它的输出和label为： [0.3,0.4,0.3] , [1,0,0]。接下来我们就可以利用交叉熵计算网络的 loss = - (1log (0.3) + 0 + 0) = 1.20。随着训练次数的增加，模型的参数得到优化，这时的输出变为： [0.8,0.1,0.1]。则 loss = - (1log (0.8) + 0 + 0) = 0.22。可以发现loss由1.20减小为0.22，而判断输入图像为数字1的概率由原本的0.3增加为0.8，说明训练得到的概率分布越来越接近真实的分布，这样就大大的提高了预测的准确性

例子
```python
import numpy as np

# 模型的输出概率分布
predictions = np.array([0.2, 0.3, 0.5])

# 真实标签的概率分布（假设为独热编码）
labels = np.array([0, 0, 1])

# 计算交叉熵损失函数
loss = -np.sum(labels * np.log(predictions))

print("交叉熵损失：", loss)
```
### CTC 损失函数

假设我们有一个模型用于识别手写数字的文本序列，输入是一系列图像，输出是对应的文本序列。我们使用CTC损失函数来训练这个模型。

```python
import torch
import torch.nn as nn
from torch.nn.functional import ctc_loss

# 假设有一个模型的输出序列，形状为（batch_size, seq_length, num_classes + 1），
# num_classes表示类别数量，最后一个类别为CTC的空白标记
logits = torch.tensor([[[0.1, 0.6, 0.1, 0.1, 0.1, 0.1, 0.1],   # 第一个时间步的预测概率分布
                        [0.1, 0.1, 0.6, 0.1, 0.1, 0.1, 0.1],   # 第二个时间步的预测概率分布
                        [0.1, 0.1, 0.1, 0.6, 0.1, 0.1, 0.1]],  # 第三个时间步的预测概率分布
                       [[0.1, 0.1, 0.1, 0.1, 0.6, 0.1, 0.1],
                        [0.1, 0.1, 0.1, 0.1, 0.1, 0.6, 0.1],
                        [0.1, 0.6, 0.1, 0.1, 0.1, 0.1, 0.1]]])

# 假设有对应的标签序列，形状为（batch_size, label_length）
labels = torch.tensor([[1, 2, 3],   # 第一个样本的标签序列
                       [4, 5]])     # 第二个样本的标签序列

# 计算CTC损失函数
loss = ctc_loss(logits, labels, [logits.size(1)] * logits.size(0), [labels.size(1)] * labels.size(0))

print("CTC损失：", loss.item())
```

在这个例子中，模型的输出`logits`是一个三维张量，形状为`(batch_size, seq_length, num_classes + 1)`。我们假设有两个样本，`batch_size`为2，序列长度为3，类别数量为6（0-5表示数字0-5，最后一个类别为CTC的空白标记）。

对应的标签序列`labels`是一个二维张量，形状为`(batch_size, label_length)`。第一个样本的标签序列为[1, 2, 3]，第二个样本的标签序列为[4, 5]。

```
##### CTC Beam Search 例子

CTC（Connectionist Temporal Classification）是一种用于序列标注的算法，它可以在不需要对齐输入和输出的情况下，直接从输入序列（如声音或图像）预测输出序列（如文字或标签）¹。CTC的核心是定义一个概率分布，用于计算输入序列和输出序列之间的对应关系²。

CTC中的解码（decoding）是指根据模型预测的概率分布，找出最可能的输出序列的过程³。CTC中有三种常用的解码方法：greedy decode，prefix beam decode和beam search⁴。其中，beam search是一种基于全局搜索的解码方法，它维护一个固定大小的候选集合（beam），每个候选项都是一个完整的输出序列。在每个时间步上，它会根据当前的候选集合和预测概率，生成新的候选集合，并保留概率最高的若干个输出序列。这种方法的优点是可以找到全局最优或者近似最优的输出序列，缺点是计算量非常大，需要遍历所有可能的输出序列和概率。

举一个简单的例子，假设我们有一个四分类问题，即$n=4$，要判断一段语音是A、B、C还是D。我们用0表示空白符（blank），1表示A，2表示B，3表示C，4表示D。假设一段语音的真实标签是ABCD，即$y_0=1,y_1=2,y_2=3,y_3=4$，而模型预测的概率分布如下：

| 时间步 | 0    | 1    | 2    | 3    | 4    |
| ------ | ---- | ---- | ---- | ---- | ---- |
| 1      | 0.6  | 0.2  | 0.1  | 0.05 | 0.05 |
| 2      | 0.5  | 0.3  | 0.1  | 0.05 | 0.05 |
| 3      | 0.4  | 0.35 | 0.15 | 0.05 | 0.05 |
| 4      | 0.3  | 0.25 | 0.25 | 0.1  | 0.1  |
| 5      | 0.2  | 0.15 | 0.25 | 0.2  | 0.2  |
| ...    |

假设我们设置beam size为2，即每个时间步只保留两个最优的候选序列。那么beam search的过程如下：

- 时间步1：我们从第一行中选择概率最大的两个类别作为初始候选序列，即(0)和(1)，它们对应的概率分别为$P(0)=\log(0.6)=-0.51$和$P(1)=\log(0.2)=-1.61$。
- 时间步2：我们从第二行中选择每个类别，并将其添加到前一步的候选序列中，形成新的候选序列。例如，对于类别1，我们可以得到两个新的候选序列：(01)和(11)，它们对应的概率分别为$P(01)=P(0)+\log(0.3)=-1.41$和$P(11)=P(1)+\log(0.3)=-2.51$。同理，我们可以得到其他八个新的候选序列：(02)，(12)，(03)，(13)，(04)，(14)，(00)，(10)。然后我们从这十个候选序列中选择概率最大的两个作为当前时间步的最优候选序列，即(01)和(02)，它们对应的概率分别为$P(01)=-1.41$和$P(02)=-1.71$。
- 时间步3：我们重复上一步的操作，从第三行中选择每个类别，并将其添加到前一步的候选序列中，形成新的候选序列。例如，对于类别1，我们可以得到两个新的候选序列：(011)和(021)，它们对应的概率分别为$P(011)=P(01)+\log(0.35)=-2.31$和$P(021)=P(02)+\log(0.35)=-2.61$。同理，我们可以得到其他八个新的候选序列：(012)，(022)，(013)，(023)，(014)，(024)，(010)，(020)。然后我们从这十个候选序列中选择概率最大的两个作为当前时间步的最优候选序列，即(011)和(012)，它们对应的概率分别为$P(011)=-2.31$和$P(012)=-2.51$。
- 时间步4：我们继续重复上一步的操作，从第四行中选择每个类别，并将其添加到前一步的候选序列中，形成新的候选序列。例如，对于类别1，我们可以得到两个新的候选序列：(0111)和(0121)，它们对应的概率分别为$P(0111)=P(011)+\log(0.25)=-3.21$和$P(0121)=P(012)+\log(0.25)=-3.41$。同理，我们可以得到其他八个新的候选序列：(0112)，(0122)，(0113)，(0123)，(0114)，(0124)，(0110)，(0120)。然后我们从这十个候选序列中选择概率最大的两个作为当前时间步的最优候选序列，即(0112)和(0113)，它们对应的概率分别为$P(0112)=-3.11$和$P(0113)=-3.31$。
- 时间步5：我们继续重复上一步的操作，从第五行中选择每个类别，并将其添加到前一步的候选序列中，形成新的候选序列。例如，对于类别1，我们可以得到两个新的候选序列：(01121)和(01131)，它们对应的概率分别为$P(01121)=P(0112)+\log
#### Reference
* [如何通俗地理解概率论中的「极大似然估计法」?](https://www.zhihu.com/question/24124998/answer/242682386)
* [线性回归的最大似然估计](https://lulaoshi.info/deep-learning/linear-model/maximum-likelihood-estimation.html#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)

## Pretrained model

预训练模型是如何存储的？不同的预训练模型可能有不同的存储方式。但是，一般来说，预训练模型都是通过一些参数来存储的，这些参数是在大量的数据上训练得到的，可以反映出模型学习到的知识。

这些参数通常是一些数值，可以用矩阵或张量的形式来表示。比如，一个预训练模型可能有几百万或几十亿个参数，每个参数都有一个数值，这些数值就可以组成一个很大的矩阵或张量。这样，预训练模型就可以用一个文件来存储这个矩阵或张量，文件的大小取决于参数的数量和精度。

当我们使用预训练模型时，我们就可以加载这个文件，把参数读取到内存中，然后用这些参数来初始化我们的模型。这样，我们就可以利用预训练模型已经学习到的知识，而不需要从头开始训练。

### 模型 all-MiniLM-L6-v2
模型 all-MiniLM-L6-v2 是一个用于句子相似度的预训练模型，它是基于MiniLM的一个变种，使用了6层的自注意力机制（Attention）和384维的隐藏层。它是在一个包含10亿个句子对的数据集上使用对比学习（Contrastive Learning）的方法进行训练的。它可以将句子或段落映射到一个384维的稠密向量空间，可以用于聚类或语义搜索等任务。

举个例子，假设我们有以下三个句子：

A: 我喜欢吃苹果
B: 苹果是我的最爱
C: 我讨厌吃香蕉
我们可以使用 all-MiniLM-L6-v2 模型来计算这三个句子的向量表示，然后计算它们之间的余弦相似度（Cosine Similarity），得到以下结果：

sim(A, B) = 0.87
sim(A, C) = 0.12
sim(B, C) = 0.11
从这个结果可以看出，句子A和B非常相似，因为它们都表达了喜欢苹果的意思，而句子A和C，以及句子B和C都很不相似，因为它们表达了不同的情感和食物偏好。这样，我们就可以利用 all-MiniLM-L6-v2 模型来判断句子之间的语义关系。

## Transformer 训练模型

Transformer 模型是一种基于自注意力机制的深度学习模型，主要用于自然语言处理和计算机视觉领域。Transformer 模型由编码器（Encoder）和解码器（Decoder）两个部分组成，可以处理变长的输入序列。Transformer 模型训练出来的模型是指模型在训练过程中学习到的参数和权重，以及模型在测试数据上的表现和评估。

Transformer 模型的训练过程大致如下：

* 数据准备：收集、标注、清洗、划分、转换等操作，使得数据符合模型的输入要求。使用子词分词器（subwords tokenizer）对文本进行分词，并添加开始和结束标记。
* 模型定义：选择合适的网络结构、损失函数、优化器等组件，构建模型的计算图。使用位置编码（Positional encoding）表示单词在序列中的位置。使用多头注意力（Multi-head attention）和前馈网络（Feed forward network）构成编码器和解码器的层。
* 模型训练：使用训练数据不断地更新模型的参数和权重，使得模型的误差变小。使用遮挡（Masking）操作防止解码器看到未来的信息。使用学习率调度（Learning rate scheduling）策略控制优化速度。
* 模型测试：使用测试数据评估模型的泛化能力，如准确率、召回率、F1值等指标。使用贪心搜索（Greedy search）或集束搜索（Beam search）生成翻译结果。
* 模型保存：将训练好的模型保存为文件，以便后续使用或部署。

### 注意力机制
注意力机制（英语：attention）是人工神经网络中一种**模仿认知注意力**的技术。这种机制可以增强神经网络输入数据中某些部分的权重，同时减弱其他部分的权重，以此将网络的关注点聚焦于数据中最重要的一小部分。数据中哪些部分比其他部分更重要取决于上下文。可以通过梯度下降法对注意力机制进行训练。

优缺点：

优点：

可以提高模型的性能和准确度，尤其是对于长或复杂的数据，可以减少信息的丢失和混淆。
可以减少模型的计算量和内存消耗，因为模型不需要处理所有的数据，而只需要关注最重要或最相关的部分。
可以增强模型的可解释性和可视化性，因为模型可以显示出它关注的部分和权重，方便人们理解模型的行为和决策。

缺点：

可能会导致模型过于依赖注意力机制，忽略其他重要的因素或特征。
可能会增加模型的训练难度和时间，因为注意力机制需要额外的计算和参数。
可能会存在一些局限性或问题，如注意力机制如何适应不同类型和领域的数据，如何处理多头或多层的注意力机制，如何避免注意力机制的冗余或冲突等。

### Transformer 模型如何解决神经网络的梯度下降问题

Transformer 模型使用了残差连接（residual connection）和层归一化（layer normalization）的技术，可以有效地防止梯度消失或爆炸的问题。残差连接可以让信息在深层网络中更容易地传递，层归一化可以稳定梯度的分布和大小。
>> 残差连接就是让模型的输入和输出相加，这样可以让模型保留输入的信息，不会丢失或改变太多。你可以想象模型是一个大盒子，里面有很多小盒子，每个小盒子都要对输入的信息做一些处理，然后传给下一个小盒子。但是有些小盒子可能处理得不好，或者处理得太多，导致信息变得不清楚或不完整。为了避免这种情况，我们可以让每个小盒子在处理完信息后，再把原来的信息加回去，这样就相当于给了信息一个保险，让它不会丢失或改变太多。
层归一化就是让模型的输出在每一层都保持一个合适的大小，不会太大或太小。你可以想象模型是一个大水管，里面有很多小水管，每个小水管都要对水流做一些调节，然后传给下一个小水管。但是有些小水管可能调节得不好，或者调节得太多，导致水流变得太大或太小。为了避免这种情况，我们可以让每个小水管在调节完水流后，再把水流调整到一个合适的范围内，这样就相当于给了水流一个标准，让它不会超出或低于正常的水平。

Transformer 模型使用了多头注意力机制（multi-head attention），可以增强模型的表达能力和泛化能力，同时也可以并行计算，提高训练效率。多头注意力机制可以让模型关注到不同子空间的信息，捕捉到更加丰富的特征信息。
>>多头注意力机制就是让模型可以同时注意到不同的信息，提高模型的能力和效率。你可以想象模型是一个大眼睛，里面有很多小眼睛，每个小眼睛都要对输入的信息做一些观察，然后传给下一个层。但是有些小眼睛可能观察得不好，或者观察得太少，导致信息变得不清楚或不完整。为了避免这种情况，我们可以让每个小眼睛在观察完信息后，再把不同小眼睛的观察结果合并起来，这样就相当于给了信息一个全面的视角，让它不会遗漏或重复。

Transformer 模型使用了位置编码（positional encoding），可以将词序信息加到词向量上，帮助模型学习这些信息。位置编码可以让模型知道每个词在句子中的相对和绝对的位置信息，而不需要依赖于循环神经网络结构。
>>Transformer 模型是一种可以处理文字和声音的模型，它有一个特别的功能，就是可以让模型知道每个词在句子中的位置，帮助模型理解句子的意思。你可以想象模型是一个大耳朵，里面有很多小耳朵，每个小耳朵都要对输入的信息做一些听取，然后传给下一个层。但是这些小耳朵只能听到每个词的声音，不能听到每个词在句子中的顺序。为了让模型知道每个词在句子中的顺序，我们可以给每个词加上一个标签，比如第一个词是1，第二个词是2，以此类推。这样就相当于给了模型一个位置编码，让它能够听到每个词在句子中的相对和绝对的位置信息。而且这种位置编码不需要像其他模型那样一个一个地处理词，所以模型可以更快地学习和处理信息。

### 位置编码的示意图

| 词 | 词向量 | 位置编码 | 词向量+位置编码 |
|:--:|:------:|:--------:|:--------------:|
| 我 | [0.1, 0.2, 0.3] | [1, 0, 0] | [1.1, 0.2, 0.3] |
| 爱 | [0.4, 0.5, 0.6] | [0, 1, 0] | [0.4, 1.5, 0.6] |
| 你 | [0.7, 0.8, 0.9] | [0, 0, 1] | [0.7, 0.8, 1.9] |

这里我们假设每个词的词向量和位置编码都是三维的，位置编码用一个单位矩阵表示，每个词在句子中的位置对应矩阵的一行。然后我们把词向量和位置编码相加，得到一个新的词向量，这个新的词向量就包含了词序信息。这样模型就可以知道每个词在句子中的相对和绝对的位置信息了。

## 梯度
### 梯度下降

假设你要找一座山上的最低点，你可以用梯度下降算法来帮你。梯度下降算法的基本思想是，你先随机选择一个位置，然后看看你周围的坡度，沿着最陡的下坡方向走一步，然后再看看你周围的坡度，再沿着最陡的下坡方向走一步，重复这个过程，直到你找到最低点。这里，你走的步长就是学习率，你周围的坡度就是梯度。

但是，这个过程中可能会遇到一些问题或挑战，比如：

* 学习率问题：如果你走的步长太大，可能会导致你在最低点附近来回跳动，或者跳过最低点，无法停下来；如果你走的步长太小，可能会导致你走得太慢，或者卡在一个不太低的地方，无法继续下降。
* 局部最小值问题：如果这座山不是光滑的，而是有很多凹凸不平的地方，那么可能存在很多低洼的地方，但不一定是最低的地方。如果你碰巧卡在一个低洼的地方，而周围都是上坡，那么你就无法继续下降，而错过了真正的最低点。
* 鞍点问题：如果这座山不是单峰的，而是有很多平缓的地方，那么可能存在很多平坦的地方，但既不是最高也不是最低。如果你碰巧卡在一个平坦的地方，而周围都是平坦或微弱的上下坡，那么你就无法确定该往哪个方向走，而停滞不前。
* 梯度消失或爆炸问题：如果这座山非常高或非常低，那么可能存在很多极端的坡度。如果你碰到一个非常陡或非常缓的坡度，那么你可能会走得非常快或非常慢，导致你无法控制好自己的速度和方向。
### 梯度跟损失函数的关系

假设您想要爬上一座山的最高峰，但是您不知道具体的路线，只能根据眼前的情况来决定下一步怎么走。您的目标就是让自己的高度尽可能地增加，也就是最大化高度函数。

梯度就是您在某个位置能够看到的最陡峭的方向，它表示了高度函数在这个位置上的变化率。如果您沿着梯度的方向走，就能够让自己的高度增加得最快。如果您沿着梯度的反方向走，就能够让自己的高度减少得最快。

损失函数就是您和最高峰之间的距离，它表示了您离目标有多远。您的目标就是让损失函数尽可能地减小，也就是最小化损失函数。

梯度和损失函数的关系就是：如果您沿着梯度的反方向走，就能够让损失函数减小得最快。如果您沿着梯度的方向走，就能够让损失函数增加得最快。

所以，在机器学习中，我们通常使用梯度下降法来求解损失函数的最小值，也就是找到最佳的参数。我们从一个随机的参数开始，计算损失函数和梯度，然后沿着梯度的反方向更新参数，使得损失函数逐渐减小，直到达到一个局部最小值或者收敛。

**例子：**

假设您想要找到一个一元函数的最小值，也就是 y = f(x) ，其中 x 是输入变量， y 是输出变量。您不知道函数的具体形式，只能通过观察函数在不同位置的值来判断。

损失函数就是您和目标之间的距离，它表示了您当前位置和最低点之间的高度差。在这个例子中，我们可以使用 y 作为损失函数，它的定义是：

$$
L(x) = y = f(x)
$$

梯度就是损失函数在某个位置上的变化率，它表示了损失函数在这个位置上沿着不同方向（也就是不同 x ）的变化程度。在这个例子中，梯度是一个一维数值，它是损失函数对 x 的导数，它的定义是：

$$
\\frac{dL}{dx} = \\frac{dy}{dx} = f'(x)
$$

梯度和损失函数的关系就是：如果您沿着梯度的反方向更新 x ，就能够让损失函数减小得最快。如果您沿着梯度的方向更新 x ，就能够让损失函数增加得最快。

所以，在数学中，我们通常使用梯度下降法来求解损失函数的最小值，也就是找到最佳的 x 。我们从一个随机的 x 开始，计算损失函数和梯度，然后沿着梯度的反方向更新 x ，使得损失函数逐渐减小，直到达到一个局部最小值或者收敛。

具体来说，梯度下降法的步骤如下：

- 初始化 x 为任意值
- 选择一个学习率 \\alpha ，它表示每次更新 x 的步长
- 重复以下步骤直到收敛或者达到最大迭代次数：
    - 计算损失函数 L(x) 
    - 计算梯度 \\frac{dL}{dx}
    - 更新 x = x - \\alpha \\frac{dL}{dx}

下图展示了一个简单的例子，其中函数是 y = x^2 + 2x + 1 ，我们初始化 x = -3 ，并设置学习率为 0.1 。我们可以看到随着迭代次数的增加， x 不断接近真实值（也就是 -1 ），而损失函数不断减小，最终收敛到一个很小的值。

#### 如何解决？
## 算法
### 近似最近邻搜索

近似最近邻搜索（Approximate Nearest Neighbor Search）是一种在高维空间中快速找到跟一个给定点最相似的点的方法。它不一定能找到最准确的最近邻，但是可以节省很多时间和空间。

近似最近邻搜索有很多种算法，其中一种比较常用的是基于局部敏感哈希（Locality Sensitive Hashing，LSH）的方法。它的基本思想是，**把高维空间中的点映射到低维空间中的哈希值，使得相似的点有更大的概率映射到相同或者相近的哈希值，而不相似的点有更大的概率映射到不同或者相远的哈希值。**

具体来说，LSH方法有以下几个步骤：

- 定义一个哈希函数族，每个哈希函数可以把一个高维向量映射到一个标量值。
- 从哈希函数族中随机选取k个哈希函数，组成一个复合哈希函数，它可以把一个高维向量映射到一个k维向量。
- 对于每个数据点，用复合哈希函数计算它的哈希值，并把它存储在一个哈希表中，哈希表的键是哈希值，值是对应的数据点。
- 对于一个查询点，用同样的复合哈希函数计算它的哈希值，并在哈希表中查找是否有相同或者相近的哈希值，如果有，就返回对应的数据点作为候选的最近邻。
- 对于候选的最近邻，用真实的距离度量（如欧氏距离）来计算它们与查询点的相似度，并返回最相似的一个或者几个作为近似的最近邻。

这种方法可以大大减少搜索空间和计算量，因为它只需要比较少数的候选点，而不是所有的数据点。当然，这种方法也有一些缺点，比如它需要调整合适的参数（如k和哈希函数族），它可能会丢失一些真正的最近邻（如映射到不同哈希值的点），它可能会返回一些不太相似的点（如映射到相同或者相近哈希值但实际距离较远的点）。

简单例子：
LSH的方法是把这些数字变成一些字母，比如A，B，C，D，E，F，G，H，I，J。它用了一种特别的规则来变换，使得接近的数字变成相同或者相近的字母，而不接近的数字变成不同或者相远的字母。比如它可以用这样的规则：

- 如果数字是奇数，就变成A
- 如果数字是偶数，并且小于等于6，就变成B
- 如果数字是偶数，并且大于6，就变成C

按照这个规则，我们可以得到这样的结果：

- 1 -> A
- 2 -> B
- 3 -> A
- 4 -> B
- 5 -> A
- 6 -> B
- 7 -> C
- 8 -> C
- 9 -> A
- 10 -> C

然后我们把这些结果放在一个表格里面，表格的第一列是字母，第二列是对应的数字。比如这样：

| 字母 | 数字 |
| --- | --- |
| A | 1, 3, 5, 9 |
| B | 2, 4, 6 |
| C | 7, 8, 10 |

对于一个给定的数字，我们先用同样的规则把它变成一个字母。然后我们在表格里面找有没有相同或者相近的字母。如果有，就返回对应的数字作为候选的最接近的数字。然后我们再用真正的距离来比较候选的数字和给定的数字有多接近，并返回最接近的一个或者几个作为近似的最接近的数字。

举个例子吧，假设我们要找跟6最接近的数字。我们先用规则把6变成B。然后我们在表格里面找有没有B或者跟B相近的字母。我们发现有B和C两个字母符合条件。所以我们返回B和C对应的数字作为候选的最接近的数字。也就是2, 4, 6, 7, 8, 10这几个数字。然后我们再用真正的距离来比较它们和6有多接近，并返回最接近的一个或者几个作为近似的最接近的数字。显然最接近6的是6本身和4。

这种方法可以大大减少搜索和计算的次数，因为它只需要比较少数的候选数字，而不是所有的数字。当然，这种方法也有一些缺点，比如它需要想好合适的规则（如怎么变换和怎么判断相近），它可能会丢失一些真正最接近的数字（如5和7都跟6很接近但是被变成了A和C），它可能会返回一些不太接近的数字（如10跟6不太接近但是被变成了C）。


### RandomForestClassifier

>问：请用小白能懂的方式解释如下一段话：RandomForestClassifier 的 fit 方法是用于训练随机森林分类器的方法。在训练过程中，随机森林会从数据集的不同子样本中拟合多个决策树分类器，并使用平均值来提高预测准确性和控制过度拟合。每个决策树都是在不同的随机样本和随机特征集上训练的，这有助于减少过度拟合并提高模型的泛化能力？

假设你有一堆动物的照片，你想让电脑能够识别出每张照片上是什么动物，这就是一个分类问题。你可以用随机森林分类器来解决这个问题。

随机森林分类器就像是一个团队，里面有很多个小伙伴，每个小伙伴都是一个决策树分类器。决策树分类器就像是一个问答游戏，它会根据照片上的一些特征，比如颜色、形状、大小等，来问一些是非问题，比如“这个动物有没有毛发？”，“这个动物有没有尾巴？”，“这个动物的耳朵是圆的还是尖的？”等，最后根据回答来判断这个动物是什么。

但是如果只有一个决策树分类器，它可能会问得太细或者太死板，导致只能识别出一些特定的照片，而对于其他的照片就不太准确了，这就叫做过度拟合。为了避免这种情况，我们可以让多个决策树分类器一起工作，每个决策树分类器都从所有的照片中随机挑选一些来学习，而且每次问问题的时候也只从所有的特征中随机挑选一些来问，这样每个决策树分类器都会有不同的视角和侧重点，这就叫做随机森林。

当我们要识别一张新的照片时，我们就把它给每个决策树分类器看一遍，让他们都给出自己的判断，然后我们看看他们中间哪种判断最多，就认为那种判断是正确的。比如说有10个决策树分类器，其中有6个说这张照片上是猫，3个说是狗，1个说是兔子，那么我们就认为这张照片上是猫。这样我们就可以利用多数投票的方式来提高预测的准确性和可靠性。

所以，RandomForestClassifier 的 fit 方法就是用来训练随机森林分类器的方法，它会让随机森林里面的每个决策树分类器都从数据集中学习一些知识，并且保持一定的多样性和随机性。

### 网格搜索与随机搜索

假设你要做一个蛋糕，你有一些原料，比如面粉、鸡蛋、牛奶、糖等，你也有一个烤箱，但是你不知道怎么配比这些原料，也不知道怎么调节烤箱的温度和时间，你只知道最后的蛋糕要好吃。这时候，你可以用网格搜索或者随机搜索来帮你找到最佳的配方和烘焙方法。

网格搜索的思路是这样的：你先列出每种原料和烤箱的可能取值，比如面粉可以是100克、200克、300克等，鸡蛋可以是1个、2个、3个等，牛奶可以是50毫升、100毫升、150毫升等，糖可以是10克、20克、30克等，烤箱的温度可以是150度、180度、210度等，烘焙的时间可以是10分钟、15分钟、20分钟等。然后你把所有可能的组合都试一遍，比如100克面粉+1个鸡蛋+50毫升牛奶+10克糖+150度+10分钟，200克面粉+2个鸡蛋+100毫升牛奶+20克糖+180度+15分钟，等等。每做一个蛋糕，你就尝一下味道，给它打一个分数。最后你找到分数最高的那个组合，就是最佳的配方和烘焙方法。

随机搜索的思路是这样的：你不用列出每种原料和烤箱的可能取值，而是给它们一个范围或者一个分布，比如面粉可以是0到500克之间的任意值，鸡蛋可以是0到5个之间的任意整数值，牛奶可以是0到200毫升之间的任意值，糖可以是0到50克之间的任意值，烤箱的温度可以是100到250度之间的任意值，烘焙的时间可以是5到30分钟之间的任意值。然后你随机地从这些范围或者分布中抽取一些组合，比如123克面粉+2个鸡蛋+87毫升牛奶+18克糖+167度+12分钟，456克面粉+4个鸡蛋+32毫升牛奶+45克糖+223度+27分钟，等等。每做一个蛋糕，你也尝一下味道，给它打一个分数。最后你找到分数最高的那个组合，就是最佳的配方和烘焙方法。

网格搜索和随机搜索的区别在于，网格搜索是把所有可能的组合都试一遍，而随机搜索是只试一部分随机的组合。网格搜索的优点是可以保证找到最佳的组合，但是缺点是需要花费很多时间和资源。随机搜索的优点是可以节省时间和资源，而且有时候也能找到很好的组合，但是缺点是不能保证找到最佳的组合。

### 划分数据集 - KMeans 原理

假设你有以下六个数据点：

```python
X = [[1, 2], [1, 4], [1, 0],
     [10, 2], [10, 4], [10, 0]]
```

你想要将它们分成两个组，也就是说，n_clusters = 2。KMeans 的工作流程如下：

1. 随机选择两个数据点作为初始的中心点，比如说 [1, 2] 和 [10, 4]。
2. 计算每个数据点到两个中心点的距离，然后将每个数据点分配给距离最近的中心点所在的组。这样，我们就得到了两个组，分别是 [[1, 2], [1, 4], [1, 0]] 和 [[10, 2], [10, 4], [10, 0]]。
3. 计算每个组的数据点的平均值，然后用这个平均值作为新的中心点。这样，我们就得到了两个新的中心点，分别是 [1, 2] 和 [10, 2]。
4. 重复步骤 2 和步骤 3，直到中心点不再变化或者达到最大迭代次数。在这个例子中，由于中心点已经不变了，所以我们就停止迭代，得到最终的聚类结果。

一个例子：

Sure, I can show you an example of using sklearn.cluster Kmeans. Here is a code snippet that imports the class, creates an instance with 3 clusters, and fits it on some sample data:

```python
#import the class
from sklearn.cluster import KMeans

#create some sample data
X = [[1, 2], [1, 4], [1, 0],
     [10, 2], [10, 4], [10, 0]]

#create an instance of KMeans with 3 clusters
kmeans = KMeans(n_clusters=3)

#fit the model on the data
kmeans.fit(X)

#print the cluster labels for each observation
print(kmeans.labels_)
```

Output:

```python
[1 1 1 0 0 0]
```

You can see that the model has assigned each observation to one of the three clusters based on their similarity. You can also use the `predict` method to assign new observations to existing clusters:

```python
#predict the cluster labels for new observations
new_X = [[0, 0], [12, 3]]
print(kmeans.predict(new_X))
```

Output:

```python
[1 0]
```


给定初始化质心的例子 k-means++

```python
# 导入所需的库
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 创建一个二维数据集
X = [[1, 2], [2, 3], [3, 4], [5, 6], [6, 7], [7, 8], [9, 10], [10, 11], [11, 12]]

# 指定初始点作为参数，形状为(3, 2)，其中3是聚类数，2是数据的维度
init = [[2, 3], [6, 7], [10, 11]]

# 使用K-means++算法，指定聚类数为3，初始点为init
kmeans = KMeans(n_clusters=3, init=init)
y_pred = kmeans.fit_predict(X) # [0 0 0 1 1 1 2 2 2]

# 绘制聚类结果
plt.figure(figsize=(8, 6))
plt.scatter([x[0] for x in X], [x[1] for x in X], c=y_pred)
plt.title('K-Means++ Clustering')
plt.show()

```
### k-means++
K-means++算法的过程是：

假设你有一堆小球，有红色的，绿色的和蓝色的。你想把它们分成三堆，每堆颜色都差不多。你可以用K-means++算法来做这件事，步骤如下：

1. 你随便拿一个小球，假设是红色的，放在一边，作为第一堆的开始。
2. 你再拿一个小球，但是这次你不是完全随便拿，而是根据它跟已经拿出来的小球的颜色不同来决定。比如说，如果你拿到了一个绿色的小球，它跟红色的小球颜色不同，所以你更有可能拿它；如果你拿到了一个粉色的小球，它跟红色的小球颜色差不多，所以你不太可能拿它。假设你最后拿了一个绿色的小球，放在另一边，作为第二堆的开始。
3. 你再拿一个小球，同样根据它跟已经拿出来的小球的颜色不同来决定。这次你要考虑它跟两个小球的颜色不同，选择跟它们都不太像的小球。比如说，如果你拿到了一个黄色的小球，它跟红色和绿色的小球都有一点颜色不同，所以你有一点可能拿它；如果你拿到了一个蓝色的小球，它跟红色和绿色的小球都有很大颜色不同，所以你更有可能拿它。假设你最后拿了一个蓝色的小球，放在第三边，作为第三堆的开始。
4. 现在你已经有了三堆小球的开始，分别是红色、绿色和蓝色的小球。接下来你要把剩下的小球分配给这三堆。你可以用一个简单的方法：对于每个剩下的小球，看看它跟哪堆开始的小球颜色最像，就把它放到那堆里面。比如说，如果你拿到了一个紫色的小球，它跟蓝色的小球颜色最像，就把它放到蓝色堆里面；如果你拿到了一个橙色的小球，它跟红色的小球颜色最像，就把它放到红色堆里面。
5. 当你把所有剩下的小球都分配完后，你会发现每堆里面有一些不太合适的小球，比如说红色堆里面可能有一些粉色或者棕色的小球。这时候你可以用一个简单的方法来调整一下：对于每堆，找出所有属于这堆的小球中颜色最中间（或者最普通）的那个作为新的开始；然后再重复步骤4，把所有的小球重新分配给新的开始。这样你就可以让每堆里面的小球颜色更加一致。
6. 你可以重复步骤5，直到你觉得每堆里面的小球颜色都很合适，或者你觉得再调整也没有太大的改变。这样你就完成了用K-means++算法把小球分成三堆的任务。

### 高维度数据的可视化 - TSNE

(t-distributed Stochastic Neighbor Embedding) 

> 什么是 t-分布随机近邻嵌入。

TSNE 是一种让我们看到高维数据的方法，它可以把高维数据变成二维或三维的图像，让我们更容易理解数据的特点。TSNE 的方法是这样的：

首先，它会看高维数据中每两个数据点有多相似，相似的数据点会有更大的概率成为邻居，不相似的数据点会有更小的概率成为邻居。这个概率也受到一个参数的影响，这个参数表示每个数据点想要有多少个邻居。
然后，它会在二维或三维空间中随机放一些点，这些点就是高维数据的映射。它也会计算这些点之间的相似性，用同样的方法，只是换了一个公式。
接着，它会比较高维空间和低维空间中的相似性，如果不一样，就会调整低维空间中的点的位置，让它们更接近高维空间中的相似性。它会重复这个过程，直到找到一个比较好的位置。
最后，它会把低维空间中的点画出来，不同类别的数据点用不同颜色表示。这样我们就可以看到高维数据在低维空间中的分布和聚类情况了。

## 存储
### 对象存储与文件存储
对象存储不是存储在云服务器的本地，而是存储在分布式的存储集群中，云服务器只是通过网络协议访问对象存储。
分布式的存储集群是对象存储的底层实现，它可能使用文件存储或者块存储来存储对象的数据和元数据，但是对外提供的接口仍然是对象存储的接口，不是文件存储的接口。

## ONNX Runtime

### ONNX.js 是如何运用 WebGL 运行模型的呢？
简单来说，ONNX.js 会将 ONNX 模型中的每个算子（operator）转换成一个 WebGL 着色器（shader），然后将这些着色器编译成 GPU 可执行的代码。着色器是一种用于控制图形渲染过程的程序，它可以对顶点、像素、纹理等进行计算和处理。ONNX.js 会将模型的输入和输出数据存储在 WebGL 纹理（texture）中，然后通过调用着色器来执行模型的计算。纹理是一种用于存储和映射图像数据的对象，它可以被绑定到着色器中的变量上，从而实现数据的传递和访问。

ONNX.js 还使用了一些优化技术，来提高 WebGL 后端的性能和稳定性。例如，ONNX.js 会根据浏览器和设备的支持情况，自动选择合适的精度和格式来存储纹理数据。ONNX.js 还会缓存已经编译好的着色器和已经分配好的纹理，以避免重复的开销。此外，ONNX.js 还支持一种叫做 pack/unpack 的模式，可以将多个低维度的张量（tensor）打包成一个高维度的张量，从而减少纹理切换和着色器调用的次数。

### ONNX.js 是如何运用 WebAssembly 来运行模型的呢？
简单来说，ONNX.js 会将 ONNX 模型中的每个算子（operator）转换成一个 WebAssembly 模块（module），然后将这些模块编译成二进制格式的字节码（bytecode）。字节码是一种可以被浏览器中的 WebAssembly 虚拟机（virtual machine）快速加载和执行的代码格式。ONNX.js 会将模型的输入和输出数据存储在 WebAssembly 的内存（memory）中，然后通过调用 WebAssembly 模块中的函数（function）来执行模型的计算。WebAssembly 的内存是一种可以被 JavaScript 和 WebAssembly 代码共享访问的线性数组（linear array）。ONNX.js 会使用 JavaScript 的类型化数组（typed array）来读写 WebAssembly 的内存。

ONNX.js 还使用了一些优化技术，来提高 WebAssembly 后端的性能和稳定性。例如，ONNX.js 会根据浏览器和设备的支持情况，自动选择合适的精度和格式来存储数据。ONNX.js 还会缓存已经编译好的 WebAssembly 模块，以避免重复的开销。此外，ONNX.js 还支持一种叫做 SIMD 的特性，可以让 WebAssembly 代码同时对多个数据进行相同的操作，从而提高并行计算的效率5。

### WASM 支持所有 ONNX 运算符，但 WebGL 和 WebGPU 目前仅支持一部分。

为了理解这句话，你需要知道以下几个概念：

- WASM是WebAssembly的缩写，它是一种在浏览器中运行二进制代码的技术，它可以提高代码的执行效率和安全性。
- WebGL是一种在浏览器中使用GPU进行图形渲染的技术，它可以提高图形处理的性能和质量。
- WebGPU是一种在浏览器中使用GPU进行通用计算的技术，它可以提高计算密集型任务的性能和兼容性。
- ONNX是一种表示机器学习模型的开放标准，它可以让不同的框架和平台之间共享和转换模型。
- ONNX运算符是一种定义模型中各种数学操作的标准，它可以让不同的后端实现和执行模型。

这句话的原因是：

- WASM后端是通过将原生的ONNX Runtime引擎编译成WASM格式来实现的，因此它可以保留所有ONNX Runtime支持的功能和特性。
- WebGL和WebGPU后端是通过将ONNX模型转换成相应的图形或计算着色器来实现的，因此它们需要根据不同的硬件和驱动来适配和优化。由于这些后端还处于开发阶段，它们还没有完全覆盖所有ONNX运算符。

# Reference 

* New Bing