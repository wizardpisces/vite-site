## 什么是 NLP理解类任务？

答：NLP理解类任务是指对自然语言进行分析和理解的任务，通常可以分为以下几种12：

序列标注（Sequence labeling）：对一个序列的每个元素（如词、字、句子）标注一个标签，如词性、命名实体、语义角色等。
分类任务（Classification task）：对一个文本或句子进行分类，如文本分类、情感分析、语种识别等。
句子关系判断（Sentence relation judgement）：判断两个句子之间的关系，如句法分析、蕴含关系判断等。
生成式任务（Generation task）：根据输入的文本或语音生成新的文本或语音，如机器翻译、文本摘要、对话系统等。
这些任务都需要对自然语言的结构和含义有一定的理解能力，而不仅仅是基于表面的形式。

## 为什么 gpt 选择自回归语言模型而不是双向语言模型？

GPT选择自回归语言模型的原因可能有以下几点：
* 自回归语言模型是传统的语言模型，可以从左到右建模序列的联合概率，适合文本生成等任务。
* 自回归语言模型可以天然地匹配生成类任务的过程，而不需要额外的处理。
* 自回归语言模型可以利用Transformer的decoder结构，实现高效的并行计算。

相比之下，双向语言模型的优势是：
* 双向语言模型可以同时利用上下文的信息，获取更丰富和灵活的语言表征。
* 双向语言模型可以适用于多种自然语言处理任务，例如阅读理解、问答等。
* 双向语言模型可以通过自编码或去噪自编码的方式，实现对输入序列的重构或预测。

问：自回归语言模型跟双向语言模型有什么区别？

答：自回归语言模型和双向语言模型是两种不同的预训练语言模型，它们的区别主要有以下几点：
* 自回归语言模型是根据上文或下文来预测当前词的概率分布，例如ELMO、GPT等。双向语言模型是根据整个句子的上下文来预测被遮盖的词，例如BERT等。
* 自回归语言模型只能利用单向的上下文信息，不能同时利用双向的上下文信息。双向语言模型能够自然地融合双向的上下文信息，更符合人类的语言理解过程。
* 自回归语言模型适合于生成类的NLP任务，例如文本摘要、机器翻译等，因为它们的生成过程是从左到右的。双向语言模型适合于理解类的NLP任务，例如阅读理解、命名实体识别等，因为它们需要同时看到问题和答案的上下文。
* 自回归语言模型在预训练和微调时是一致的，因为它们都是基于语言模型的目标函数。双向语言模型在预训练和微调时是不一致的，因为它们在预训练时需要引入[MASK]标记来遮盖部分词，而在微调时没有这样的标记

问：为什么 chatGPT 选择自回归语言模型，而不是双向语言模型？

答：chatGPT选择自回归语言模型，而不是双向语言模型，可能有以下几个原因：

* 自回归语言模型更适合于生成类的NLP任务，例如对话生成，因为它们的生成过程是从左到右的，自回归语言模型天然匹配这个过程。
* 自回归语言模型在预训练和微调时是一致的，因为它们都是基于语言模型的目标函数。双向语言模型在预训练和微调时是不一致的，因为它们在预训练时需要引入[MASK]标记来遮盖部分词，而在微调时没有这样的标记。
* chatGPT可以利用大量的公开的自然语言文本进行预训练，也可以通过抢先开始公测，收集了大量的用户的使用数据，这是更加宝贵的数据。双向语言模型可能需要更多的标注数据或者更复杂的预训练策略来达到同样的效果。


## 什么是泛化能力？

答：泛化能力是指机器学习算法对新鲜样本的适应能力，也就是在原有的数据集上添加新的数据集，通过训练输出一个合理的结果。泛化能力反映了模型有没有对客观世界做真实的刻画，还是发生了过拟合。泛化能力好的模型可以对从未见过的数据做出良好的预测，泛化能力差的模型则会在新数据上表现不佳。泛化能力通常通过测试误差来评价

问：你能给我一个泛化能力的例子吗？

答：假设我们有一个机器学习模型，它的任务是根据一张照片判断是猫还是狗。我们用一些猫和狗的照片来训练这个模型，然后用一些没有见过的猫和狗的照片来测试它。如果这个模型在测试集上的准确率很高，那么我们可以说它的泛化能力很好，因为它可以对新的数据做出正确的判断。如果这个模型在测试集上的准确率很低，那么我们可以说它的泛化能力很差，因为它不能适应新的数据。

## 什么是拟合？

答：拟合是一种数学方法，用来寻找一条曲线或者一个函数，使它能够尽可能地接近一组给定的数据点。拟合的目的是找到数据点之间的内在规律或者关系，或者对未知数据进行预测。拟合的过程通常需要选择一个合适的模型，比如线性函数、多项式函数、指数函数等，然后根据一定的准则，比如最小二乘法、最大似然法等，求解出模型的参数。拟合的结果可以用一些指标来衡量，比如相关系数、拟合优度、均方误差等。拟合在数学、物理、化学、生物、经济等领域都有广泛的应用。

问：什么是 gpt 模型的过拟合？

过拟合是指模型在训练数据上表现良好，但在新的数据上表现不佳的现象。过拟合的原因可能有以下几种

* 训练数据量不足或质量不高，导致模型无法学习到数据的真实分布和规律；
* 模型复杂度过高或参数过多，导致模型具有过强的拟合能力，而忽略了数据的噪声和变化；
* 训练过程中缺乏有效的正则化或停止准则，导致模型在训练过程中过度拟合训练数据。

GPT 模型的过拟合可能会导致以下几种问题：

* 语义不连贯或语句无意义。模型可能会生成一些与给定文本无关或毫无逻辑的语言输出，导致输出质量低下；
* 偏见或歧视性言论。模型可能会从训练数据中学习到一些偏见或歧视性的言论，并在生成过程中反复出现，导致输出不符合道德和社会标准；
* 错误的信息或伪造的信息。模型可能会误解或创造一些与真实世界不符或不准确的信息，并在生成过程中出现，导致输出不可信和误导性；
* 上下文缺失或理解错误。模型可能会忽略或错误地理解给定文本中的某些上下文信息或语言现象，并在生成过程中出现，导致输出不符合用户意图和期望。

问：过拟合的简单解释？

答：GPT 是一种可以根据给定的文字生成新的文字的模型，就像一个会说话的机器人。但是这个机器人不是很聪明，它只是从很多很多的文字中学习了一些规律，然后用这些规律来生成新的文字。所以有时候它会说一些不对的话，或者说一些不合适的话，或者说一些不明白的话，或者说一些不相关的话。这就是过拟合的意思，就是机器人只会按照它学过的规律来说话，而不会考虑真实世界的情况和变化。

## 什么是 sparse 化？

答：sparse化是指将原本的数据或矩阵中的大部分值变为0，从而减少存储空间和计算量，同时保留数据或矩阵的重要特征。

sparse化可以应用在不同的场景中，例如：

* 在数据科学和深度学习中，sparse化可以用来压缩稀疏矩阵，提高存储和运算效率2。稀疏矩阵是指矩阵中非零元素的数量远小于零元素的数量，例如文本数据的词频矩阵或词向量矩阵。稀疏矩阵有不同的存储格式，如COO、CSR、CSC、BSR、DOK、LIL、DIA等。
* 在神经网络中，sparse化可以用来剪枝网络参数，减少模型大小和推理时间，同时提高模型泛化能力。神经网络的参数是指网络中的权重和偏置，它们决定了网络的功能和性能。神经网络的参数通常是高维度的张量，其中很多值可能是冗余或无关紧要的。通过sparse化，可以将这些值设为0，从而减少参数的数量和复杂度。

问：sparse 化的例子？

一个sparse化的例子是将一个稀疏矩阵用一种更紧凑的数据结构来表示，从而节省存储空间和计算时间。

例如，假设我们有一个3×6的稀疏矩阵，如下所示：

1 0 0 1 0 0
0 0 2 0 0 1
0 0 0 2 0 0
这个矩阵有13个零值和5个非零值，其稀疏度为72%。如果我们用一个二维数组来存储这个矩阵，我们需要18个单元格，其中大部分是浪费的。

我们可以用一种叫做压缩稀疏行（CSR）的数据结构来sparse化这个矩阵2。CSR使用三个一维数组来存储非零值、行的范围和列索引。对于上面的矩阵，CSR的表示如下：

data:   [1, 1, 2, 1, 2]
indptr: [0, 2, 4, 5]
indices:[0, 3, 2, 5, 3]
这样，我们只需要12个单元格来存储这个矩阵，节省了33%的空间。而且，我们可以通过简单的算法来访问和操作这个矩阵，例如：

要获取第i行第j列的元素，我们先检查indptr[i]和indptr[i+1]之间是否有j值出现在indices数组中，如果有，则返回对应的data值，如果没有，则返回。
要获取第i行的所有元素，我们先找到indptr[i]和indptr[i+1]之间的区间，在这个区间内，indices数组中的每个值表示一个非零列索引，data数组中的每个值表示一个非零元素值，其余的元素都是。
要对两个CSR矩阵进行加法或乘法运算，我们可以利用indptr和indices数组来快速定位非零元素，并进行相应的操作。

问：sparse 在 LLM 模型中的作用？

答：一个 sparse 在 LLM 多语言模型中的例子是 ASR pathways，它是一种用于语音识别的稀疏多语言模型，它可以激活语言特定的子网络（"路径"），使得每种语言的参数可以显式地学习。12 通过重叠的子网络，共享的参数也可以实现低资源语言的知识转移。

问：举个小白都能懂的例子

答：假设您有一个多功能的电视遥控器，它可以控制不同品牌和型号的电视机。您可以把这个遥控器看作是一个 LLM 多语言模型，它可以处理不同语言的文本。但是，如果您只想控制一台电视机，您不需要使用遥控器上的所有按钮，只需要使用一部分按钮就可以了。这些按钮就相当于 sparse 在 LLM 多语言模型中的参数，它们只在特定的语言下被激活。这样，您就可以节省遥控器的电池和内存，也可以提高遥控器的效率和准确性。

## NLP乃至AI领域的研究范式是什么？

一种常见的划分方式是按照研究者对人工智能的定义和目标进行划分，可以分为以下四种范式：

* 逻辑主义范式：这种范式认为人工智能是一种基于逻辑推理和符号操作的计算机科学，其目标是让计算机能够模拟人类的思维过程和智能行为。这种范式强调知识表示、推理规则、搜索算法等，以及形式化逻辑、数学证明等作为验证方法。
* 连接主义范式：这种范式认为人工智能是一种基于神经网络和并行分布式处理的认知科学，其目标是让计算机能够模拟人类的大脑结构和功能。这种范式强调神经元模型、学习算法、优化方法等，以及实验数据、统计分析等作为验证方法。
* 行为主义范式：这种范式认为人工智能是一种基于感知-动作循环和自适应反馈的工程科学，其目标是让计算机能够实现与环境的有效交互和适应。这种范式强调感知器、执行器、控制器等，以及仿真实验、性能评估等作为验证方法。
* 综合主义范式：这种范式认为人工智能是一种基于多学科融合和多层次建模的综合科学，其目标是让计算机能够整合不同类型和层次的智能功能。这种范式强调多模态输入输出、多任务协同、多代理系统等，以及综合评价、案例分析等作为验证方法。

问：gpt 属于哪种研究范式
答：GPT是一种基于大规模预训练语言模型的自然语言处理技术，它可以根据给定的文本生成相关的文本，或者根据给定的问题生成相关的答案。GPT属于连接主义范式，因为它使用了神经网络和并行分布式处理的方法，模拟了人类大脑的结构和功能。GPT也可以看作是综合主义范式的一种体现，因为它结合了多学科的知识和方法，实现了不同类型和层次的智能功能。

问：ChatGPT是否带来了NLP乃至AI领域的研究范式转换？
答：ChatGPT是一种具有创新性和影响力的技术，它展示了大型语言模型在自然语言处理方面的潜力和挑战，也引发了关于人工智能伦理、安全、社会责任等方面的讨论和反思。但是，ChatGPT也有其局限性和缺陷，比如事实上的不准确性、对输入语句的敏感性、过度冗长或重复性等。因此，ChatGPT可能不足以构成一个范式转换，而更像是一个范式演进或拓展。

## LLM从海量数据中学到了什么知识？LLM又是如何存取这些知识的？
LLM是Large Language Model的缩写，指的是基于大规模语料训练的深度神经网络语言模型，如GPT、BERT、XLNet等。LLM从海量数据中学到了很多语言类和世界知识，包括词汇、语法、语义、逻辑、常识、事实等。LLM是如何存取这些知识的呢？一般来说，LLM把知识存储在Transformer的模型参数里，通过自注意力机制和位置编码来提取和利用这些知识。也就是说，LLM并没有显式地表示和存储知识，而是隐式地融合在模型的权重和激活值中。这种方式有利于提高模型的泛化能力和生成能力，但也带来了一些挑战，比如知识的可解释性、可控性和可更新性等。

问：能用小白能懂的方式解释么？

答：LLM就是一种让电脑能够理解和生成人类语言的技术，它需要从很多很多的文字资料中学习，比如新闻、小说、百科等等。LLM学习到的知识不是像我们一样用字或图来记住的，而是用一些数字和公式来记住的。这样做的好处是可以让电脑更快更准确地处理语言，但是也有一些不好的地方，比如我们不太容易知道电脑到底学到了什么，或者怎么让电脑忘掉一些错误的知识。

问：为什么用这种方式存取数据？

答：原因有以下几点：
* Transformer是基于自注意力机制的，自注意力机制可以捕捉到更丰富的特征和信息，比如比CNN看得更宽更远，比LSTM训练更快1。
* Transformer可以统一自然语言理解和自然语言生成任务的外在表现形式，使得同一个LLM生成模型可以解决几乎所有NLP问题1。
* Transformer可以通过多头注意力和位置编码来增强表达能力和泛化能力，提高模型的性能。
* Transformer可以通过权重共享和层归一化来减少参数的数量，加快收敛。

问：transformer 的自注意力机制是什么？用简单的语言描述

答：Transformer的自注意力机制是一种让电脑能够根据输入的文字的不同位置，分配不同的重要程度，从而提取出文字的含义的技术。用简单的语言来描述，就像是电脑在阅读一段文字时，会用不同颜色的笔划出不同的关键词，然后根据这些关键词来理解文字的意思。比如，电脑在阅读“我今天去了北京故宫，看到了很多珍贵的文物”这句话时，它会用红色的笔划出“北京故宫”和“文物”，因为这些词对于理解这句话的主题很重要，然后它会用蓝色的笔划出“我”和“今天”，因为这些词对于理解这句话的时间和主语很重要，最后它会用绿色的笔划出“去了”和“看到了”，因为这些词对于理解这句话的动作很重要。通过这样的方式，电脑就能够把注意力集中在文字中最有价值的信息上，从而提高理解和生成文字的能力。

## 随着LLM规模逐步增大，会带来什么影响？

答：随着 LLM 规模的增大，它们会表现出一些令人惊讶的能力，比如推理，常识，符号操作等。这些能力被称为 LLM 的涌现能力。

LLM 规模增大的影响有以下几个方面：

* 提高了 LLM 在各种自然语言处理任务上的效果和样本效率。
* 使得 LLM 能够通过特殊的提示方式（如 CoT）来执行一些复杂的推理任务。
* 使得 LLM 能够从数据中学习到一些隐含的知识和规则，从而实现一些自我训练和自我标注的功能。
也带来了一些挑战和风险，比如 LLM 的可解释性，可靠性，安全性，伦理性等。

问：为啥会出现涌现能力？

答：LLM 的涌现能力是指在较小的模型中不出现，而在较大的模型中出现的能力，比如推理，常识，符号操作等1。LLM 的涌现能力的原因可能有以下几个方面：

* 模型规模的增大使得 LLM 能够学习到更多的知识和规则，从而实现一些隐含的任务。
* 模型结构和训练方法的改进使得 LLM 能够更好地利用数据和参数，从而提高模型的质量和效率。
* 提示方式的创新使得 LLM 能够更好地理解任务的目标和要求，从而执行一些复杂的操作。
* 任务评价指标的不平滑性使得 LLM 在达到一定规模后才能观察到明显的效果提升。
* LLM 的内部机制和逻辑可能存在一些难以解释的现象，比如 Grokking，即 LLM 能够从数据中提取出一些抽象和通用的概念和关系。

问：Grokking 是什么？

答：Grokking 是一个由美国作家罗伯特·海因莱因在他的 1961 年的科幻小说《异乡人》中创造的新词。它的含义是“通过直觉或移情来理解，与……建立共鸣”或者“同情地沟通（与……）；也就是，体验快乐”。海因莱因的概念是更深刻和全面的，他认为 grokking 是一种将自己和被观察者融为一体的方式，从而完全理解和同化它。

在计算机科学和网络文化中，grokking 也有类似的含义，指的是对某个主题或系统有深刻的理解和掌握。例如，一个程序员可以说他 grokking 了某个编程语言或框架，意思是他能够熟练地使用它并了解它的内部原理。

问：涌现的局限性？

答：LLM 的涌现能力虽然令人惊叹，但也有一些局限性，比如：

* LLM 的涌现能力并不是普遍存在的，只有在一些特定的任务和提示方式下才能观察到 。
* LLM 的涌现能力并不是稳定可靠的，可能会受到数据，参数，结构，训练方法等多种因素的影响 。
* LLM 的涌现能力并不是完美无缺的，可能会出现一些错误，偏差，不一致等问题 。
* LLM 的涌现能力并不是完全可解释的，可能会存在一些难以理解和验证的机制和逻辑 。

问：为什么 llm 可能从数据中学习到隐含的知识跟规则？
答：因为数据本身就包含了一些信息和结构，而 LLM 的目标就是尽可能地拟合数据的分布和模式。例如，如果数据中有一些诗歌，那么 LLM 就可能学习到诗歌的韵律，格律，意象等特征，从而能够生成类似的诗歌。如果数据中有一些编程语言的代码，那么 LLM 就可能学习到代码的语法，逻辑，功能等特征，从而能够生成类似的代码。这些知识和规则并不是显式地给 LLM 教授的，而是通过大量的数据训练和优化得到的。这也是为什么 LLM 需要大量的数据和计算资源的原因之一。

## 什么是In Context Learning?为什么它是一项很神秘的技术？

答：In Context Learning是一种最近的自然语言理解的范式，它利用一个大型的预训练语言模型(LM)，在不更新模型参数的情况下，通过观察一个测试样例和一些训练样例作为输入，直接解码输出。这种方式可以让LM在不同的下游任务上表现出惊人的能力，只需给出少量的示例。

In Context Learning为什么是一项很神秘的技术呢？因为它并没有像传统的监督学习那样，通过优化参数来建模输入和输出之间的关系。而是通过展示样例中的输入、输出、以及输入+输出的语言表达风格来激活LM中已经存在的潜在概念。这种激活过程可以看作是一种隐式的贝叶斯推理，即LM根据展示样例来推断最有可能对应的任务概念，并根据该概念来生成预测。

In Context Learning还有很多未知的方面，比如它的可学习性、泛化性、鲁棒性等。目前有很多研究者在探索这些问题，并尝试提出更有效和可解释的In Context Learning方法

问：你能给我一个In Context Learning的例子吗？

答：当然可以。假设你有一个预训练的语言模型，你想让它做情感分析的任务，即给定一句话，判断它是正面的还是负面的。你可以用In Context Learning的方式，给模型一个包含几个输入输出对的前缀，如下：

情感分析： 这部电影很棒！ -> 正面 这个餐厅的服务太差了。 -> 负面 我今天很开心。 -> 正面 他对我很无礼。 -> 负面

然后，在前缀后面加上一个测试样例，比如：

这本书很无聊。

模型就会根据前缀中的示例，来推断出你想要它做情感分析的任务，并根据测试样例来生成一个输出，比如：

这本书很无聊。 -> 负面

问：In Context Learning有什么优点和缺点？

答：In Context Learning有以下一些优点：

* 它不需要更新模型参数，因此可以节省训练时间和计算资源。
* 它可以利用大型预训练语言模型的强大的泛化能力，从少量的示例中学习不同的任务。
* 它可以灵活地适应不同的输入格式和输出格式，只需调整前缀中的展示样例。
* 它可以避免一些传统监督学习中的问题，比如过拟合、灾难性遗忘、标签错误等。

In Context Learning也有以下一些缺点：

* 它并没有真正学习输入和输出之间的关系，而是依赖于模型在预训练阶段就已经获取的潜在概念。因此，它可能无法处理一些预训练数据中没有覆盖到的任务或领域。
* 它对前缀中的展示样例的质量和数量很敏感，如果展示样例不足或不准确，可能会导致模型无法正确推断出任务概念，或者产生错误的预测。
* 它缺乏可解释性和可控性，我们很难知道模型是如何根据展示样例来激活潜在概念的，也很难控制模型的输出风格或置信度。

## LLM具备推理能力吗？

答：LLM是Large Language Model的缩写，指的是那些有数十亿或数百亿参数的预训练语言模型，比如GPT-3、PaLM、InstructGPT等。LLM具备推理能力吗？这是一个很有趣也很有挑战的问题。根据不同的搜索结果123，我可以给你以下一些回答：

* LLM具备一定的推理能力，但是这种能力并不是通过优化参数来学习的，而是通过观察展示样例来激活的。这种方式被称为In Context Learning或Chain of Thought Prompting，它可以让LLM在少量或零样例的情况下，完成一些复杂的推理任务，比如算术、符号推理、逻辑推理等。
* LLM具备一定的推理能力，但是这种能力并不是固定的，而是可以通过自我思考来提升的。这种方式被称为Self-Improving，它可以让LLM在没有外部输入的情况下，通过生成和评估自己的推理过程，来提高自己的推理水平和准确率。
* LLM具备一定的推理能力，但是这种能力并不是完美的，而是有很多局限和缺陷的。比如LLM可能无法处理一些预训练数据中没有覆盖到的任务或领域，或者受到输入格式和展示样例的影响而产生错误或不一致的输出，或者缺乏可解释性和可控性等。

总之，LLM具备推理能力是一个既令人兴奋又令人困惑的现象，目前还有很多未知和未解决的问题，需要更多的研究和探索。

问：你觉得LLM如何获得推理能力的？

答：这是一个很难回答的问题，因为LLM的内部工作机制还不是很清楚。不过，我可以根据一些猜测和假设，给你一些可能的解释：

* 一种可能是，LLM在预训练阶段就已经获取了一些潜在的推理概念，比如数学运算、逻辑关系、因果推断等。这些概念可能是由于预训练数据中包含了大量的推理相关的文本，比如教科书、百科全书、新闻报道等。当LLM在下游任务中遇到展示样例时，它就可以根据展示样例中的输入、输出、格式等信息，来激活最有可能对应的推理概念，并根据该概念来生成输出。
* 另一种可能是，LLM在预训练阶段就已经学会了一些通用的语言技能，比如阅读理解、文本生成、文本摘要等。这些技能可能是由于预训练数据中包含了大量的语言多样性和复杂性，比如小说、诗歌、对话、评论等。当LLM在下游任务中遇到展示样例时，它就可以利用这些语言技能，来理解展示样例中的任务要求，并根据任务要求来生成输出。
* 还有一种可能是，LLM在预训练阶段就已经形成了一些隐式的知识图谱，比如实体、关系、属性、类别等。这些知识图谱可能是由于预训练数据中包含了大量的知识相关的文本，比如事实、定义、描述、分类等。当LLM在下游任务中遇到展示样例时，它就可以利用这些知识图谱，来检索展示样例中的相关知识，并根据相关知识来生成输出。

以上只是一些粗略的想法，并没有充分的证据和理论支持。我觉得LLM如何获得推理能力还需要更多的研究和分析。

问：思维链CoT又是怎么做的？

答：思维链CoT是一种利用展示样例中的中间推理步骤来提高大语言模型LLM的复杂推理能力的方法。它的基本思想是，通过给LLM一个包含一些输入输出对以及每个输出对应的推理链的前缀，让LLM在生成输出的同时，也生成相应的推理链。这样可以让LLM更好地理解任务要求，避免一些错误或不一致的输出，提高推理的准确性和可解释性。

思维链CoT是怎么做的呢？具体来说，有以下几个步骤：

1. 首先，选择一个合适的大语言模型，比如GPT-3、PaLM、InstructGPT等。
2. 然后，根据任务类型和数据格式，设计一个合适的前缀模板，比如“算术：”、“符号推理：”等。
3. 接着，在前缀模板后面添加一些展示样例，每个展示样例包括一个输入、一个输出、以及一个推理链。

推理链是一系列中间步骤，用来说明如何从输入得到输出。比如：
算术： 如果你有12个苹果，你吃了3个，你还剩多少个？ -> 9个 推理链：12 - 3 = 9

最后，在展示样例后面加上一个测试样例，比如：
如果你有8个香蕉，你给了朋友4个，你还剩多少个？

模型就会根据前缀中的展示样例和测试样例，来生成一个输出和一个推理链，比如：

如果你有8个香蕉，你给了朋友4个，你还剩多少个？ -> 4个 推理链：8 - 4 = 4

这就是思维链CoT的基本做法。当然，在实际应用中，还有很多细节和技巧可以改进和优化这种方法，比如使用多数投票、自我思考、自我提升等。

问：Cot 有什么优缺点？

答：思维链CoT有以下一些优点：

* 它可以显著地提高大语言模型的复杂推理能力，让它们能够完成一些之前难以完成的任务，比如算术、符号推理、逻辑推理等。
* 它可以提高大语言模型的可解释性，让它们能够生成中间推理步骤，说明它们是如何从输入得到输出的。
* 它可以提高大语言模型的鲁棒性，让它们能够避免一些错误或不一致的输出，比如直接预测输出空间过大或过小的问题。

思维链CoT也有以下一些缺点：

* 它需要人工设计合适的前缀模板和展示样例，这可能需要一定的专业知识和经验，而且可能不适用于所有的任务或数据格式。
* 它依赖于大语言模型在预训练阶段就已经获取的潜在推理概念，如果这些概念不足或不准确，可能会导致模型无法正确生成推理链或输出。
* 它可能会受到展示样例的质量和数量的影响，如果展示样例不足或不代表性，可能会导致模型无法正确推断出任务要求或推理规则。


## TODO 问题
### GPT4 论文精度后的疑问
* GPT4 如何做到更加 尊重事实，安全可控?
* 消融实验？
* 为什么小规模成本下跑的结果无法在大规模下复现，因为大规模下的涌现不可控？那为啥 gpt4 声称已经克服这个？
* 什么是 language modeling loss
* 为什么 gpt4 被称为 close-ai
* 什么是 zero shot / few shot

***本文通过给 GPT 提问生成 [2023-4-1]***