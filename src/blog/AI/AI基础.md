## AI 发展历史

* 2013 卷积神经网络（图像识别领域）：GPU 与大规模数据训练数据集搭档的起点
    * 卷积？
* 2013 VAE 变分自动编码器，在对位空间对输入数据压缩：为生成式建模和数据生成开辟了新的路径
    * 生成式建模？
* 2014  GAN，生成数据跟数据识别器做对抗训练 ：不依赖于显示的数据标注就生成高质量的数据样本，为无监督学习的发展做出了贡献
* 2015 ResNet：解决了梯度消失的问题，让训练更深层次神经网络成为可能
    * 梯度消失？梯度问题？如何解决？
    * 门控机制？为什么能更好的理解文本及其上下文含义：翻译，文本生成，情感分析
* 2016 AlphaGo
* 2017 Transformer 架构，Attention is all you need：能够有效处理距离较远的依赖关系（长期以来是传统 RNN 架构的挑战）
    * RNN 的挑战为什么是这个？
    * Transformer？
* 2018 GPT-1, BERT，GNN（拓展了社交网络分析，推荐系统，药物发现等领域）
* 2019 GPT-2
* 2020 GPT-3
* 2021 EvoFormer
* 2022 chatGPT , Stable Diffusion
* 2023 GPT-4

## google 与 AI
* 基础领域的突破性研究：Transformers、Word2Vec、序列到序列学习、联邦学习、模型蒸馏、扩散模型、深度强化学习、具有树搜索的神经网络、自学习-学习系统、神经架构搜索、自回归模型、具有外部存储器的网络、大规模分布式深度网络、张量处理单元
* 科学跟工程领域：绘制几乎所有已知蛋白质的图谱、预测蛋白质的功能、在神经科学研究中绘制大脑的一部分图谱、发现更快的算法、量子计算的进步以及物理学，包括核聚变创新
* 人工智能基础设施：包括计算（例如张量处理单元、Google Tensor 和 Colab）和广泛使用的软件框架（例如 TensorFlow、Jax、Android ML 和私有计算）。

[Reference](https://ai.google/why-ai/)
## 梯度下降问题

梯度下降问题是指在机器学习中，使用梯度下降算法来优化模型参数的过程中可能遇到的一些问题或挑战。梯度下降算法的基本思想是，根据目标函数（如损失函数）的梯度（即导数或斜率）的方向和大小，来不断地更新模型参数，使目标函数达到最小值。梯度下降问题可以分为以下几类：

* 学习率问题：学习率是指每次更新模型参数时，沿着梯度方向移动的步长。学习率过大可能导致模型参数在最小值附近震荡或者跳过最小值，无法收敛；学习率过小可能导致模型参数收敛速度过慢或者陷入局部最小值，无法达到最优解。
* 局部最小值问题：局部最小值是指目标函数在某个区域内的最小值，但不一定是全局的最小值。如果目标函数是非凸的，那么可能存在多个局部最小值，梯度下降算法可能会停留在其中一个局部最小值，而无法找到全局最小值。
* 鞍点问题：鞍点是指目标函数在某个点处的梯度为零，但该点既不是最大值也不是最小值，而是一个平坦的区域。如果目标函数是高维的，那么可能存在多个鞍点，梯度下降算法可能会停留在其中一个鞍点，而无法继续下降。
* 梯度消失或爆炸问题：梯度消失或爆炸是指在深层神经网络中，由于链式求导的原理，反向传播时计算出的梯度可能会变得非常小或非常大，导致模型参数更新缓慢或不稳定。

## 解决问题场景
1. 监督学习
    * 回归问题：预测多少
    * 分类问题（需要考虑损失函数）：例如二项分类，是猫还是狗子
    * 标记问题：标签
    * 搜索：rank
    * 推荐系统
    * 序列学习：memory and learn
2. 无监督学习
3. 与环境互动（数据非离线）

[reference](https://zh.d2l.ai/chapter_introduction/index.html) 
## 入门 深度学习

* 基础知识，包括 Python 编程、数学（线性代数、微积分、概率论等）、机器学习理论（感知机、朴素贝叶斯、决策树、逻辑回归等）。
* 深度学习理论，包括神经网络、反向传播、损失函数、优化器、正则化、卷积神经网络、循环神经网络、Transformer 等。
* 深度学习实践，包括使用深度学习框架（如 PyTorch 或 TensorFlow）构建和训练模型，参与深度学习竞赛或项目，阅读和复现深度学习论文等。
如果有一定的编程和数学基础，且能够持续投入时间和精力，那么你可能需要三个月左右的时间来入门深度学习。如果是零基础，那么你可能需要半年到一年的时间来理解深度学习的基本概念和方法。

## miniGPT vs nanoGPT

答：
* miniGPT 是由 Andrej Karpathy 创建的，它的目标是提供一个小而清晰、可解释和教育性的 GPT 实现，只有大约 300 行代码。它可以用来训练和推理 GPT 模型，也可以加载 OpenAI 的 GPT-2 预训练权重。它提供了一些示例项目，如数字加法、字符级语言模型、排序等。
* nanoGPT 是由 Andrej Karpathy 重新编写的 miniGPT，它的目标是提供一个更快、更强大的 GPT 实现，同时保持代码的简洁和可读性。它可以在单个 GPU 节点上复现 GPT-2 (124M) 在 OpenWebText 上的预训练，并且可以加载 OpenAI 的 GPT-2 1.3B 预训练权重。它还提供了一些示例项目，如莎士比亚文本生成、图像描述、图像生成等。

如果你是 AI 零基础，想要了解 Transformer 模型，我建议你从 miniGPT 开始，因为它更注重教育性和可解释性，而且有更多的注释和文档。如果你已经有一些基础知识，想要尝试更高级的功能和性能，你可以尝试 nanoGPT，因为它更注重效率和实用性，而且有更多的应用场景和数据集。

## 机器学习跟深度学习的区别

机器学习是一种人工智能的分支，它使用算法来让计算机从数据中学习。
深度学习是机器学习的一种特殊形式，它使用深度神经网络来学习数据的表示。
深度神经网络是一种由多个层组成的神经网络，每个层都可以学习数据的不同特征。与传统机器学习算法相比，深度学习算法可以处理更复杂的数据，并且通常需要更多的计算资源和数据

神经网络是机器学习中的一种算法，它可以用于分类、回归、聚类等任务。LLM 使用了神经网络，但它不是机器学习中的神经网络学习。这是因为 LLM 的目标是学习文本数据的表示，而不是完成某个特定的任务。因此，LLM 使用了一种叫做 transformer 的神经网络结构，它可以处理变长的序列数据，并且在自然语言处理领域取得了很好的效果。


## 数学

1. 为什么调和平均数更能反应算法的好坏？（参照[这里](https://github.com/openai/openai-cookbook/blob/main/examples/Classification_using_embeddings.ipynb)，对准确率，召回率后又出现了调和平均数）
2. 如何理解 PR 曲线？应用场景？(参照)[https://www.ylkz.life/machinelearning/mlwm/p10975749/]
    1. 为什么 precision-recall 曲线越凸向右上角越好？这个曲线是如何画出来的
        * precision-recall 曲线是一种用于评估二分类模型性能的图形工具，它以精确率（Precision）为纵轴，召回率（Recall）为横轴，描述了模型在不同分类阈值下的精确率和召回率之间的关系。精确率表示在所有被模型判定为正类的样本中，有多少是真正的正类；召回率表示在所有真正的正类样本中，有多少被模型判定为正类。precision-recall 曲线越凸向右上角越好，表示模型在提高召回率的同时也能保持较高的精确率。
        * precision-recall 曲线是通过在不同的阈值下计算模型的精确率和召回率，并将它们在图中绘制出来，并依次连接起来而得到的。阈值是指模型将样本判定为正类的概率或分数的界限，例如0.5。当阈值较高时，模型只有对正类非常有信心的时候才会将样本判定为正类，这样可以提高精确率，但会降低召回率；当阈值较低时，模型对正类的要求较低，会将更多的样本判定为正类，这样可以提高召回率，但会降低精确率。因此，precision-recall 曲线反映了模型在不同阈值下的性能变化。

### 协方差

协方差是一种衡量两个随机变量之间的相关性和变化趋势的统计量。它的计算公式是：

$$
\mathrm{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})
$$

其中，$n$是样本数量，$\bar{X}$和$\bar{Y}$是两个随机变量的样本均值。

通俗地理解，协方差反映了两个随机变量的变化方向和程度。如果协方差为正，说明两个随机变量同向变化，即一个变量增大时另一个变量也增大，或者一个变量减小时另一个变量也减小。如果协方差为负，说明两个随机变量反向变化，即一个变量增大时另一个变量减小，或者一个变量减小时另一个变量增大。如果协方差为零，说明两个随机变量没有线性相关性，即一个变量的变化不影响另一个变量的变化。

协方差的绝对值大小表示两个随机变量的相关程度。协方差的绝对值越大，说明两个随机变量的相关程度越高，即它们的变化趋势越一致或越相反。协方差的绝对值越小，说明两个随机变量的相关程度越低，即它们的变化趋势越无关或越随机。

协方差接近于零，说明$X$和$Y$没有线性相关性。

* [从协方差到协方差矩阵，再到特征向量和特征值](https://blog.csdn.net/weixin_46021869/article/details/117334362)
* [如何通俗地理解协方差和相关系数？](https://www.zhihu.com/tardis/zm/art/70644127?source_id=1003#:~:text=%E5%8D%8F%E6%96%B9%E5%B7%AE(Covariance)%E5%AE%9A%E4%B9%89%E4%B8%BA,%E8%BE%83%E5%A4%A7%E5%80%BC%E6%97%B6Y%E2%80%A6)

## 模型
1. 二分类模型（Precision-Recall 曲线适合评估此类模型）
机器学习中除了二分类模型，还有以下几种模型：

* 多分类模型：多分类模型是指可以将数据分为多个类别的模型，例如决策树、支持向量机、逻辑回归等。
* 回归模型：回归模型是指可以预测一个连续值的模型，例如线性回归、非线性回归、高斯过程回归等。
* 聚类模型：聚类模型是指可以将数据分为若干个簇的模型，例如 K 均值、层次聚类、密度聚类等。
* 降维模型：降维模型是指可以将高维数据降到低维空间的模型，例如主成分分析、线性判别分析、TSNE 等。
* 深度学习模型：深度学习模型是指可以利用多层神经网络来学习数据特征和表达的模型，例如卷积神经网络、循环神经网络、变分自编码器等。

## 多维数据降维

* PCA(Principal Component Analysis 主成分分析)[PCA的数学原理](http://blog.codinglabs.org/articles/pca-tutorial.html)，[图解](https://www.showmeai.tech/article-detail/198)
    * 简介：PCA是一种线性的降维方法（降维线性可分数据），它通过找到数据的最大方差方向，也就是主成分，来保留数据的最大信息量。PCA的优点是速度快，结果唯一，缺点是不能处理非线性的数据结构，可能会丢失一些重要的特征
    * 概念
        * 两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。
        * 如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。也就是期望协方差为 0（表示数据维度之间的相关性，为0表示没有相关性）
    * 优势
        * 快
        * 可复用：得出的协方差矩阵可复用到新的一组数据
    * 劣势
        * 无法处理非线性数据结构
    
* [T-SNE](https://blog.csdn.net/xieshangxin/article/details/89682607)
    * 简介：t-SNE是一种非线性的降维方法（降维线性不可分数据），它通过计算高维空间和低维空间中数据点之间的条件概率分布，并使它们尽可能相似，来保留数据的局部结构。t-SNE的优点是能够处理复杂的非线性数据，展示数据的聚类情况，缺点是速度慢，结果不唯一，参数敏感
    * 优势
        * 
    * 劣势
        * 慢
        * 迭代的，不可复用到一组新数据
    * TODO：如何跟 CNN 结合进行隐藏层可视化？

一般操作：KMeans 先对未分类的数据聚类 -> T-SNE 进行数据降维

###

## Reference

* New Bing