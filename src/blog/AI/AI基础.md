## AI 发展历史

* 2013 卷积神经网络（图像识别领域）：GPU 与大规模数据训练数据集搭档的起点
    * 卷积？
* 2013 VAE 变分自动编码器，在对位空间对输入数据压缩：为生成式建模和数据生成开辟了新的路径
    * 生成式建模？
* 2014  GAN，生成数据跟数据识别器做对抗训练 ：不依赖于显示的数据标注就生成高质量的数据样本，为无监督学习的发展做出了贡献
* 2015 ResNet：解决了梯度消失的问题，让训练更深层次神经网络成为可能
    * 梯度消失？梯度问题？如何解决？
    * 门控机制？为什么能更好的理解文本及其上下文含义：翻译，文本生成，情感分析
* 2016 AlphaGo
* 2017 Transformer 架构，Attention is all you need：能够有效处理距离较远的依赖关系（长期以来是传统 RNN 架构的挑战）
    * RNN 的挑战为什么是这个？
    * Transformer？
* 2018 GPT-1, BERT，GNN（拓展了社交网络分析，推荐系统，药物发现等领域）
* 2019 GPT-2
* 2020 GPT-3
* 2021 EvoFormer
* 2022 chatGPT , Stable Diffusion
* 2023 GPT-4

## google 与 AI
* 基础领域的突破性研究：Transformers、Word2Vec、序列到序列学习、联邦学习、模型蒸馏、扩散模型、深度强化学习、具有树搜索的神经网络、自学习-学习系统、神经架构搜索、自回归模型、具有外部存储器的网络、大规模分布式深度网络、张量处理单元
* 科学跟工程领域：绘制几乎所有已知蛋白质的图谱、预测蛋白质的功能、在神经科学研究中绘制大脑的一部分图谱、发现更快的算法、量子计算的进步以及物理学，包括核聚变创新
* 人工智能基础设施：包括计算（例如张量处理单元、Google Tensor 和 Colab）和广泛使用的软件框架（例如 TensorFlow、Jax、Android ML 和私有计算）。

[Reference](https://ai.google/why-ai/)
## 梯度下降问题

梯度下降问题是指在机器学习中，使用梯度下降算法来优化模型参数的过程中可能遇到的一些问题或挑战。梯度下降算法的基本思想是，根据目标函数（如损失函数）的梯度（即导数或斜率）的方向和大小，来不断地更新模型参数，使目标函数达到最小值。梯度下降问题可以分为以下几类：

* 学习率问题：学习率是指每次更新模型参数时，沿着梯度方向移动的步长。学习率过大可能导致模型参数在最小值附近震荡或者跳过最小值，无法收敛；学习率过小可能导致模型参数收敛速度过慢或者陷入局部最小值，无法达到最优解。
* 局部最小值问题：局部最小值是指目标函数在某个区域内的最小值，但不一定是全局的最小值。如果目标函数是非凸的，那么可能存在多个局部最小值，梯度下降算法可能会停留在其中一个局部最小值，而无法找到全局最小值。
* 鞍点问题：鞍点是指目标函数在某个点处的梯度为零，但该点既不是最大值也不是最小值，而是一个平坦的区域。如果目标函数是高维的，那么可能存在多个鞍点，梯度下降算法可能会停留在其中一个鞍点，而无法继续下降。
* 梯度消失或爆炸问题：梯度消失或爆炸是指在深层神经网络中，由于链式求导的原理，反向传播时计算出的梯度可能会变得非常小或非常大，导致模型参数更新缓慢或不稳定。

# 几种训练网络

* ResNet（残差网络）：多个子网叠加得到最终结果，规避梯度消失跟爆炸问题
* RNN（循环神经网络）：适合 文本，音频，视频等连续输入，且前后有相关性的场景
    * 缺陷（可以尝试用 Transformer 解决部分问题）
        * 难并行，训练效率低：RNN是一种序列模型，需要按顺序处理输入序列的每个元素，这样会导致计算效率低下，难以实现并行处理
        * 在处理长序列时，容易出现梯度消失或梯度爆炸的问题，导致难以捕捉长距离的依赖关系。这是因为RNN在反向传播时，梯度需要通过多个时间步骤进行链式乘法，如果梯度值过大或过小，就会导致梯度指数级增长或衰减
        * 编码长度固定，影响信息质量：无论输入序列长度如何，RNN都会将其压缩成一个固定长度的向量，这就限制了RNN对于长序列信息的编码能力。

* Transformer（自注意力机制）：
    * 优势
        * 并行提升训练效率：注意力机制可以一次性地“看见”所有输入的元素，不需要按顺序处理，可以实现高效的并行计算。
        * 能处理长的上下文：注意力机制可以根据查询（Query）和键值（Key-Value）之间的相似度，为每个元素分配不同的权重，从而实现对不同范围和距离的依赖关系的捕捉。
        * 编码长度可变：注意力机制可以将输入序列的所有元素进行加权组合，得到一个动态长度的上下文向量（Context Vector），从而实现对序列信息的有效编码

* CNN（卷积神经网络）：主要训练图片，寻找模式，识别分类，eg： MNIST；应用场景
    * 医学成像：CNN 可以检查数千份病理报告，以直观地检测图像中是否存在癌细胞。
    * 音频处理：关键字检测可用于任何带有麦克风的设备，以检测何时说出某个单词或短语（“嘿 Siri！”）。无论环境如何，CNN 都可以准确地学习和检测关键字，同时忽略所有其他短语。
    * 物体检测：自动驾驶依靠 CNN 来准确检测标志或其他物体的存在，并根据输出做出决策。
    * 合成数据生成：使用生成对抗网络（GAN），可以生成新图像以用于深度学习应用，包括人脸识别和自动驾驶。
* GAN（生成对抗网络 = 一个生成器 + 一个鉴别器）：生成器和鉴别器一起接受训练，相互对抗，直到生成器能够创建鉴别器无法再确定是假的真实合成数据。成功训练后，生成器产生的数据可用于创建新的合成数据，可用作其他深度神经网络的输入。
    * 场景：学习生成任何数据类型的新实例，例如面部合成图像、某种风格的新歌曲或特定流派的文本

[Reference](https://www.mathworks.com/discovery/convolutional-neural-network-matlab.html#:~:text=A%20convolutional%20neural%20network%20(CNN,%2Dseries%2C%20and%20signal%20data.)

## 解决问题场景
1. 监督学习
    * 回归问题：预测多少
    * 分类问题（需要考虑损失函数）：例如二项分类，是猫还是狗子
    * 标记问题：标签
    * 搜索：rank
    * 推荐系统
    * 序列学习：memory and learn
2. 无监督学习
3. 与环境互动（数据非离线）

[reference](https://zh.d2l.ai/chapter_introduction/index.html) 
## 入门 深度学习

* 基础知识，包括 Python 编程、数学（线性代数、微积分、概率论等）、机器学习理论（感知机、朴素贝叶斯、决策树、逻辑回归等）。
* 深度学习理论，包括神经网络、反向传播、损失函数、优化器、正则化、卷积神经网络、循环神经网络、Transformer 等。
* 深度学习实践，包括使用深度学习框架（如 PyTorch 或 TensorFlow）构建和训练模型，参与深度学习竞赛或项目，阅读和复现深度学习论文等。
如果有一定的编程和数学基础，且能够持续投入时间和精力，那么你可能需要三个月左右的时间来入门深度学习。如果是零基础，那么你可能需要半年到一年的时间来理解深度学习的基本概念和方法。

## miniGPT vs nanoGPT

答：
* miniGPT 是由 Andrej Karpathy 创建的，它的目标是提供一个小而清晰、可解释和教育性的 GPT 实现，只有大约 300 行代码。它可以用来训练和推理 GPT 模型，也可以加载 OpenAI 的 GPT-2 预训练权重。它提供了一些示例项目，如数字加法、字符级语言模型、排序等。
* nanoGPT 是由 Andrej Karpathy 重新编写的 miniGPT，它的目标是提供一个更快、更强大的 GPT 实现，同时保持代码的简洁和可读性。它可以在单个 GPU 节点上复现 GPT-2 (124M) 在 OpenWebText 上的预训练，并且可以加载 OpenAI 的 GPT-2 1.3B 预训练权重。它还提供了一些示例项目，如莎士比亚文本生成、图像描述、图像生成等。

如果你是 AI 零基础，想要了解 Transformer 模型，我建议你从 miniGPT 开始，因为它更注重教育性和可解释性，而且有更多的注释和文档。如果你已经有一些基础知识，想要尝试更高级的功能和性能，你可以尝试 nanoGPT，因为它更注重效率和实用性，而且有更多的应用场景和数据集。

## 机器学习跟深度学习的区别

机器学习是一种人工智能的分支，它使用算法来让计算机从数据中学习。
深度学习是机器学习的一种特殊形式，它使用深度神经网络来学习数据的表示。
深度神经网络是一种由多个层组成的神经网络，每个层都可以学习数据的不同特征。与传统机器学习算法相比，深度学习算法可以处理更复杂的数据，并且通常需要更多的计算资源和数据

神经网络是机器学习中的一种算法，它可以用于分类、回归、聚类等任务。LLM 使用了神经网络，但它不是机器学习中的神经网络学习。这是因为 LLM 的目标是学习文本数据的表示，而不是完成某个特定的任务。因此，LLM 使用了一种叫做 transformer 的神经网络结构，它可以处理变长的序列数据，并且在自然语言处理领域取得了很好的效果。


## 数学

1. 为什么调和平均数更能反应算法的好坏？（参照[这里](https://github.com/openai/openai-cookbook/blob/main/examples/Classification_using_embeddings.ipynb)，对准确率，召回率后又出现了调和平均数）
2. 如何理解 PR 曲线？应用场景？(参照)[https://www.ylkz.life/machinelearning/mlwm/p10975749/]
    1. 为什么 precision-recall 曲线越凸向右上角越好？这个曲线是如何画出来的
        * precision-recall 曲线是一种用于评估二分类模型性能的图形工具，它以精确率（Precision）为纵轴，召回率（Recall）为横轴，描述了模型在不同分类阈值下的精确率和召回率之间的关系。精确率表示在所有被模型判定为正类的样本中，有多少是真正的正类；召回率表示在所有真正的正类样本中，有多少被模型判定为正类。precision-recall 曲线越凸向右上角越好，表示模型在提高召回率的同时也能保持较高的精确率。
        * precision-recall 曲线是通过在不同的阈值下计算模型的精确率和召回率，并将它们在图中绘制出来，并依次连接起来而得到的。阈值是指模型将样本判定为正类的概率或分数的界限，例如0.5。当阈值较高时，模型只有对正类非常有信心的时候才会将样本判定为正类，这样可以提高精确率，但会降低召回率；当阈值较低时，模型对正类的要求较低，会将更多的样本判定为正类，这样可以提高召回率，但会降低精确率。因此，precision-recall 曲线反映了模型在不同阈值下的性能变化。

### 协方差

协方差是一种衡量两个随机变量之间的相关性和变化趋势的统计量。它的计算公式是：

$$
\mathrm{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})
$$

其中，$n$是样本数量，$\bar{X}$和$\bar{Y}$是两个随机变量的样本均值。

通俗地理解，协方差反映了两个随机变量的变化方向和程度。如果协方差为正，说明两个随机变量同向变化，即一个变量增大时另一个变量也增大，或者一个变量减小时另一个变量也减小。如果协方差为负，说明两个随机变量反向变化，即一个变量增大时另一个变量减小，或者一个变量减小时另一个变量增大。如果协方差为零，说明两个随机变量没有线性相关性，即一个变量的变化不影响另一个变量的变化。

协方差的绝对值大小表示两个随机变量的相关程度。协方差的绝对值越大，说明两个随机变量的相关程度越高，即它们的变化趋势越一致或越相反。协方差的绝对值越小，说明两个随机变量的相关程度越低，即它们的变化趋势越无关或越随机。

协方差接近于零，说明$X$和$Y$没有线性相关性。

* [从协方差到协方差矩阵，再到特征向量和特征值](https://blog.csdn.net/weixin_46021869/article/details/117334362)
* [如何通俗地理解协方差和相关系数？](https://www.zhihu.com/tardis/zm/art/70644127?source_id=1003#:~:text=%E5%8D%8F%E6%96%B9%E5%B7%AE(Covariance)%E5%AE%9A%E4%B9%89%E4%B8%BA,%E8%BE%83%E5%A4%A7%E5%80%BC%E6%97%B6Y%E2%80%A6)

## 模型
1. 二分类模型（Precision-Recall 曲线适合评估此类模型）
机器学习中除了二分类模型，还有以下几种模型：

* 多分类模型：多分类模型是指可以将数据分为多个类别的模型，例如决策树、支持向量机、逻辑回归等。
* 回归模型：回归模型是指可以预测一个连续值的模型，例如线性回归、非线性回归、高斯过程回归等。
* 聚类模型：聚类模型是指可以将数据分为若干个簇的模型，例如 K 均值、层次聚类、密度聚类等。
* 降维模型：降维模型是指可以将高维数据降到低维空间的模型，例如主成分分析、线性判别分析、TSNE 等。
* 深度学习模型：深度学习模型是指可以利用多层神经网络来学习数据特征和表达的模型，例如卷积神经网络、循环神经网络、变分自编码器等。

## 多维数据降维

* PCA(Principal Component Analysis 主成分分析)[PCA的数学原理](http://blog.codinglabs.org/articles/pca-tutorial.html)，[图解](https://www.showmeai.tech/article-detail/198)
    * 简介：PCA是一种线性的降维方法（降维线性可分数据），它通过找到数据的最大方差方向，也就是主成分，来保留数据的最大信息量。PCA的优点是速度快，结果唯一，缺点是不能处理非线性的数据结构，可能会丢失一些重要的特征
    * 概念
        * 两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。
        * 如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。也就是期望协方差为 0（表示数据维度之间的相关性，为0表示没有相关性）
    * 优势
        * 快
        * 可复用：得出的协方差矩阵可复用到新的一组数据
    * 劣势
        * 无法处理非线性数据结构
    
* [T-SNE](https://blog.csdn.net/xieshangxin/article/details/89682607)
    * 简介：t-SNE是一种非线性的降维方法（降维线性不可分数据），它通过计算高维空间和低维空间中数据点之间的条件概率分布，并使它们尽可能相似，来保留数据的局部结构。t-SNE的优点是能够处理复杂的非线性数据，展示数据的聚类情况，缺点是速度慢，结果不唯一，参数敏感
    * 优势
        * 
    * 劣势
        * 慢
        * 迭代的，不可复用到一组新数据
    * TODO：如何跟 CNN 结合进行隐藏层可视化？

一般操作：KMeans 先对未分类的数据聚类 -> T-SNE 进行数据降维

## 模型蒸馏
是一种让小模型学习大模型的知识的方法，可以提高小模型的性能，减少计算资源的消耗。模型蒸馏的基本思想是，大模型（称为教师模型）在训练数据上有很好的泛化能力，可以预测出每个类别的概率，而不仅仅是最可能的类别。这些概率包含了大模型学习到的暗知识（dark knowledge），例如哪些类别之间比较相似，哪些类别之间比较容易混淆等。小模型（称为学生模型）可以通过拟合大模型的概率输出，而不是真实的标签，来学习这些暗知识，从而提高自己的泛化能力。

一个简单的例子是，假设有一个手写数字识别的任务，有10个类别（0-9）。大模型在一个数字8的图片上的输出可能是：

| 类别 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 概率 | 0.01 | 0.02 | 0.03 | 0.05 | 0.04 | 0.06 | 0.07 | 0.08 | 0.55 | 0.09 |

从这个输出中，我们可以看出大模型认为8最有可能是正确的类别，但也有一定的可能性是3、5、6、7或9，因为这些数字和8有一些相似之处。而真实的标签只能告诉我们8是正确的类别，其他都是错误的。如果小模型只学习真实的标签，那么它可能会忽略掉这些相似性和区别性的信息，导致在新数据上表现不佳。如果小模型学习大模型的概率输出，那么它就可以捕捉到这些暗知识，从而更好地泛化到新数据上。

## 知识蒸馏和神经网络剪枝的区别

- 假设你有一个很大的蛋糕，上面有很多奶油和水果，这个蛋糕就像一个大模型，很美味，但是也很占地方，不方便携带和分享。你想把这个蛋糕变小一点，有两种方法：
    - 一种方法是用一个小模具把蛋糕切成一个小块，然后把剩下的部分扔掉，这就像神经网络剪枝，你只保留了一部分的蛋糕，减少了大小，但是也可能损失了一些美味。
    - 另一种方法是用一个小模具做一个新的蛋糕，然后用大蛋糕的奶油和水果来装饰它，让它尽量和大蛋糕一样好看好吃，这就像知识蒸馏，你用一个小模型来学习一个大模型的输出分布，保留了大模型的知识，但是也需要额外的工作。
- 假设你有一个很复杂的拼图，上面有很多细节和颜色，这个拼图就像一个大模型，很精确，但是也很耗时，不容易完成。你想把这个拼图变简单一点，有两种方法：
    - 一种方法是用剪刀把拼图的一些碎片剪掉，然后用剩下的碎片来拼成一个完整的图案，这就像神经网络剪枝，你只保留了一部分的参数或结构，减少了计算量，但是也可能损失了一些精度。
    - 另一种方法是用一个简单的画板画出一个新的图案，然后用拼图的颜色和细节来填充它，让它尽量和拼图一样清晰漂亮，这就像知识蒸馏，你用一个小模型来学习一个大模型的注意力机制或中间层特征，提高了小模型的性能，但是也需要额外的训练。

## Reference

* New Bing