## AI 发展历史

* 2013 卷积神经网络（图像识别领域）：GPU 与大规模数据训练数据集搭档的起点
    * 卷积？
* 2013 VAE 变分自动编码器，在对位空间对输入数据压缩：为生成式建模和数据生成开辟了新的路径
    * 生成式建模？
* 2014  GAN，生成数据跟数据识别器做对抗训练 ：不依赖于显示的数据标注就生成高质量的数据样本，为无监督学习的发展做出了贡献
* 2015 ResNet：解决了梯度消失的问题，让训练更深层次神经网络成为可能
    * 梯度消失？梯度问题？如何解决？
    * 门控机制？为什么能更好的理解文本及其上下文含义：翻译，文本生成，情感分析
* 2016 AlphaGo
* 2017 Transformer 架构，Attention is all you need：能够有效处理距离较远的依赖关系（长期以来是传统 RNN 架构的挑战）
    * RNN 的挑战为什么是这个？
    * Transformer？
* 2018 GPT-1, BERT，GNN（拓展了社交网络分析，推荐系统，药物发现等领域）
* 2019 GPT-2
* 2020 GPT-3
* 2021 EvoFormer
* 2022 chatGPT , Stable Diffusion
* 2023 GPT-4

## 梯度下降问题

梯度下降问题是指在机器学习中，使用梯度下降算法来优化模型参数的过程中可能遇到的一些问题或挑战。梯度下降算法的基本思想是，根据目标函数（如损失函数）的梯度（即导数或斜率）的方向和大小，来不断地更新模型参数，使目标函数达到最小值。梯度下降问题可以分为以下几类：

* 学习率问题：学习率是指每次更新模型参数时，沿着梯度方向移动的步长。学习率过大可能导致模型参数在最小值附近震荡或者跳过最小值，无法收敛；学习率过小可能导致模型参数收敛速度过慢或者陷入局部最小值，无法达到最优解。
* 局部最小值问题：局部最小值是指目标函数在某个区域内的最小值，但不一定是全局的最小值。如果目标函数是非凸的，那么可能存在多个局部最小值，梯度下降算法可能会停留在其中一个局部最小值，而无法找到全局最小值。
* 鞍点问题：鞍点是指目标函数在某个点处的梯度为零，但该点既不是最大值也不是最小值，而是一个平坦的区域。如果目标函数是高维的，那么可能存在多个鞍点，梯度下降算法可能会停留在其中一个鞍点，而无法继续下降。
* 梯度消失或爆炸问题：梯度消失或爆炸是指在深层神经网络中，由于链式求导的原理，反向传播时计算出的梯度可能会变得非常小或非常大，导致模型参数更新缓慢或不稳定。

## 解决问题场景
1. 监督学习
    * 回归问题：预测多少
    * 分类问题（需要考虑损失函数）：例如二项分类，是猫还是狗子
    * 标记问题：标签
    * 搜索：rank
    * 推荐系统
    * 序列学习：memory and learn
2. 无监督学习
3. 与环境互动（数据非离线）

[reference](https://zh.d2l.ai/chapter_introduction/index.html) 
## 入门 深度学习

* 基础知识，包括 Python 编程、数学（线性代数、微积分、概率论等）、机器学习理论（感知机、朴素贝叶斯、决策树、逻辑回归等）。
* 深度学习理论，包括神经网络、反向传播、损失函数、优化器、正则化、卷积神经网络、循环神经网络、Transformer 等。
* 深度学习实践，包括使用深度学习框架（如 PyTorch 或 TensorFlow）构建和训练模型，参与深度学习竞赛或项目，阅读和复现深度学习论文等。
如果有一定的编程和数学基础，且能够持续投入时间和精力，那么你可能需要三个月左右的时间来入门深度学习。如果是零基础，那么你可能需要半年到一年的时间来理解深度学习的基本概念和方法。

## miniGPT vs nanoGPT

答：
* miniGPT 是由 Andrej Karpathy 创建的，它的目标是提供一个小而清晰、可解释和教育性的 GPT 实现，只有大约 300 行代码。它可以用来训练和推理 GPT 模型，也可以加载 OpenAI 的 GPT-2 预训练权重。它提供了一些示例项目，如数字加法、字符级语言模型、排序等。
* nanoGPT 是由 Andrej Karpathy 重新编写的 miniGPT，它的目标是提供一个更快、更强大的 GPT 实现，同时保持代码的简洁和可读性。它可以在单个 GPU 节点上复现 GPT-2 (124M) 在 OpenWebText 上的预训练，并且可以加载 OpenAI 的 GPT-2 1.3B 预训练权重。它还提供了一些示例项目，如莎士比亚文本生成、图像描述、图像生成等。

如果你是 AI 零基础，想要了解 Transformer 模型，我建议你从 miniGPT 开始，因为它更注重教育性和可解释性，而且有更多的注释和文档。如果你已经有一些基础知识，想要尝试更高级的功能和性能，你可以尝试 nanoGPT，因为它更注重效率和实用性，而且有更多的应用场景和数据集。

## 机器学习跟深度学习的区别

机器学习是一种人工智能的分支，它使用算法来让计算机从数据中学习。
深度学习是机器学习的一种特殊形式，它使用深度神经网络来学习数据的表示。
深度神经网络是一种由多个层组成的神经网络，每个层都可以学习数据的不同特征。与传统机器学习算法相比，深度学习算法可以处理更复杂的数据，并且通常需要更多的计算资源和数据

神经网络是机器学习中的一种算法，它可以用于分类、回归、聚类等任务。LLM 使用了神经网络，但它不是机器学习中的神经网络学习。这是因为 LLM 的目标是学习文本数据的表示，而不是完成某个特定的任务。因此，LLM 使用了一种叫做 transformer 的神经网络结构，它可以处理变长的序列数据，并且在自然语言处理领域取得了很好的效果。


## 数学

1. 为什么调和平均数更能反应算法的好坏？（参照[这里](https://github.com/openai/openai-cookbook/blob/main/examples/Classification_using_embeddings.ipynb)，对准确率，召回率后又出现了调和平均数）
2. 如何理解 PR 曲线？应用场景？(参照)[https://www.ylkz.life/machinelearning/mlwm/p10975749/]
    1. 为什么 precision-recall 曲线越凸向右上角越好？这个曲线是如何画出来的
        * precision-recall 曲线是一种用于评估二分类模型性能的图形工具，它以精确率（Precision）为纵轴，召回率（Recall）为横轴，描述了模型在不同分类阈值下的精确率和召回率之间的关系。精确率表示在所有被模型判定为正类的样本中，有多少是真正的正类；召回率表示在所有真正的正类样本中，有多少被模型判定为正类。precision-recall 曲线越凸向右上角越好，表示模型在提高召回率的同时也能保持较高的精确率。
        * precision-recall 曲线是通过在不同的阈值下计算模型的精确率和召回率，并将它们在图中绘制出来，并依次连接起来而得到的。阈值是指模型将样本判定为正类的概率或分数的界限，例如0.5。当阈值较高时，模型只有对正类非常有信心的时候才会将样本判定为正类，这样可以提高精确率，但会降低召回率；当阈值较低时，模型对正类的要求较低，会将更多的样本判定为正类，这样可以提高召回率，但会降低精确率。因此，precision-recall 曲线反映了模型在不同阈值下的性能变化。

## 模型
1. 二分类模型（Precision-Recall 曲线适合评估此类模型）
机器学习中除了二分类模型，还有以下几种模型：

* 多分类模型：多分类模型是指可以将数据分为多个类别的模型，例如决策树、支持向量机、逻辑回归等。
* 回归模型：回归模型是指可以预测一个连续值的模型，例如线性回归、非线性回归、高斯过程回归等。
* 聚类模型：聚类模型是指可以将数据分为若干个簇的模型，例如 K 均值、层次聚类、密度聚类等。
* 降维模型：降维模型是指可以将高维数据降到低维空间的模型，例如主成分分析、线性判别分析、TSNE 等。
* 深度学习模型：深度学习模型是指可以利用多层神经网络来学习数据特征和表达的模型，例如卷积神经网络、循环神经网络、变分自编码器等。

## 工具

### Pytorch
[] 符号执行是什么？跟 torch.fx 什么关系


## Reference

* New Bing