# UNet
UNet是像素级分类，输出的则是每个像素点的类别，常常用在生物医学图像上，而该任务中图片数据往往较少。(卷积神经网络一般输出的结果是整个图像的类标签。)

* UNet采用全卷积神经网络
* 左边网络为特征提取网络：使用conv和pooling
* 右边网络为特征融合网络：使用上采样产生的特征图与左侧特征图进行concatenate操作。原因

  1. **特征融合**：U-Net的设计采用了跳过连接（skip connection），将编码阶段（收缩路径）的特征图与解码阶段（扩展路径）对应层级的特征图进行拼接。这种操作的目的是为了融合不同分辨率层级的特征信息，既包括高级语义特征（在解码阶段通过上采样获得的低分辨率、高级别的特征），也包括低级细节特征（在编码阶段直接从输入图像中提取的高分辨率、低级别特征）。

  2. **细节恢复**：在图像分割中，细节是非常重要的。U-Net的编码阶段会不可避免地丢失一些局部细节信息，因为卷积和池化操作会降低空间分辨率。通过将编码阶段的特征与解码阶段的特征合并，模型能够重新利用这些细节信息，帮助更精确地定位边界和恢复图像的详细结构。

  3. **上下文信息**：解码阶段的上采样逐步恢复图像的空间分辨率，但这些上采样的特征可能缺乏必要的上下文信息来正确地分割图像。通过concatenate操作，模型在每次上采样后将之前阶段的特征图引入到当前层级，通过这种方式整合了高层次的上下文信息。

  4. **改善梯度流**：跳过连接不仅有助于在网络中融合不同层次的特征，还有助于改善梯度在网络中的流动，这对于训练深度网络至关重要。这样的网络设计减少了训练中的梯度消失问题，并有助于网络的更深层次训练。

  5. **训练稳定性**：通过重新引入编码阶段的特征，可以稳定网络的训练过程，加速收敛，并提高模型对噪声和其他训练不稳定因素的鲁棒性。

## 名词解释

### 图像高低级别特征

- **低级别的特征**：在网络的较早阶段（也就是靠近输入层）提取的特征被称为"低级别"的特征。这些特征通常与原始图像密切相关，捕捉到的是图像中的基础信息，比如边缘、角点、颜色和纹理等。在高分辨率的图像中，这些特征能够提供详细的空间和结构信息。

- **高级别的特征**：在网络的较深阶段（也就是更靠近输出层）提取的特征被称为"高级别"的特征。这些特征表达的内容更为抽象，有更高的语义含义，比如识别出整个物体、情感、场景等结构性和语义性强的信息。因为通过多次池化或卷积操作，这些特征的空间维度(分辨率)通常比原始输入要小，但它们能够表示的信息更为全局和高级。

举个例子，假设我们使用一个深度学习模型处理图像中的猫。低级别的特征可能会捕捉到猫的边缘、毛发的纹理、眼睛的颜色等信息；而高级别的特征可能已经能够捕捉到整个猫的形状，或者猫的情绪（悠闲、警觉等）。


### DiceLoss

**Dice Loss** 是一种用于图像分割任务的损失函数，旨在应对正负样本强烈不平衡的场景。让我来详细解释一下。

- **Dice Coefficient**（Dice系数）是Dice Loss的基础。它是一种用于评估两个样本相似性的度量函数，取值范围在0到1之间，值越大表示越相似。Dice Coefficient定义如下：

  $$ \text{Dice} = \frac{2|X \cap Y|}{|X| + |Y|} $$

  其中，$|X \cap Y|$表示样本X和Y之间的交集，$|X|$和$|Y|$分别表示X和Y的元素个数。分子乘以2是为了保证分母重复计算后取值范围在[0, 1]之间。

  dice_loss = 1 - dice_coefficient，所以优化目标是缩小 dice_loss 的值。

### 召回率

**召回率**（Recall）是一种用于评估分类模型性能的指标，特别是在二元或多分类中，常常用于检测模型找到相关实例的能力。在目标检测或信息检索中，召回率也很重要。

具体来说，召回率定义如下：

$$ \text{Recall} = \frac{TP}{TP + FN} $$

其中：
- **TP**（True Positive，真正例（真阳））：被正确预测的正例，即该数据的真实值为正例，预测值也为正例的情况。
- **FN**（False Negative，假阴性）：被错误预测的反例，即该数据的真实值为正例，但被错误预测成了反例的情况。

召回率衡量了模型正确识别为正类的实例（真正类）占所有实际正类实例的比例。在某些场景下，召回率比准确率更重要，例如医学诊断中，我们更关心是否漏诊（即FN 阳的被诊断为阴性）。

## 问题

* 为什么最后 U-Net 的损失是交叉熵跟 dice_loss 组成？

  在U-Net这样的图像分割模型中，通常将交叉熵损失（Cross-Entropy Loss）和Dice损失（Dice Loss）结合使用，原因主要有以下几点：

  1. **类不平衡（Class Imbalance）**：在医学图像分割等任务中，目标区域（感兴趣的区域，如肿瘤）往往比背景（其余部分）要小得多，这导致了类别之间的不平衡，简单的交叉熵损失函数可能会导致模型偏向于多数类。因此，需要一个比单一的交叉熵损失更能够处理类不平衡的损失函数。

  2. **Dice系数的特性**：Dice系数是一个衡量两个样本间相似度的统计量，它的值介于0到1之间，当值为1时表明预测和真实标签之间完全一致。它在图像分割任务中极其有用，因为它直接量化了预测和真实标签之间的重叠区域。这种度量对于捕获边缘区域尤其重要，正如分割任务中常常遇到的。

  3. **交叉熵和Dice损失结合的优势**：交叉熵损失关注于像素级别上的分类准确性，每个像素点分类的正确与否都会影响到损失。而Dice损失关注预测区域与真实区域的重叠度。这两种损失在目标函数中的结合使得模型可以在整体（全局分割准确性）和局部（特别是小区域或边缘的分割）方面进行优化。

  4. **梯度行为**：交叉熵损失和Dice损失对梯度的贡献也有所不同。交叉熵对于接近决策边界的样本会产生较大的梯度，这有助于确保学习进程。而Dice损失，则可以在模型对某些类别的预测很自信时，仍然提供一定的梯度，因而有助于学习过程中对不平衡数据的校正。

  5. **性能提升**：实验表明，仅使用交叉熵或Dice损失的模型性能通常不如两者结合使用。通过融合这两种损失函数，模型可以同时从各自的优点中受益。


* UNet 跟 SAM 的区别？

# Reference

* [UNet详解](https://blog.csdn.net/weixin_45074568/article/details/114901600)
* [UNet image](https://pic2.zhimg.com/80/v2-68ffbaff593f95cc96fc4b6811356e39_1440w.webp)
* [model source code](https://github.dev/milesial/Pytorch-UNet/blob/master/unet/unet_model.py)
* gpt