早在 2023 年 10 月 3 日，Ilya 曾在伯克利大学做过一次演讲，题为《一个无监督学习的理论》（A Theory of Unsupervised Learning）。由于内容艰涩，知晓者寥寥，而它却是人工智能史上最重要的时刻之一，注定将载入史册。

观点
* 监督学习：低训练误差+大训练集，就能确保模型的泛化能力
  * **Hoeffding 不等式**，其主要含义是：当训练误差足够低，且训练样本数远大于「模型自由度」（可以理解为模型的规模）时，测试误差也能保证足够低；
  * 宏观上和理论上，**万能近似定理**（Universal Approaximation Theorem）早已论证了深层神经网络可以逼近任意函数。
  * 模型规模一定要小于数据规模，否则，它根本就不用做真正的「压缩」或抽象，不去找规律，它就全部死记硬背了。我们知道死记硬背的模型，没有泛化能力。
* 无监督学习的本质是分布匹配，是一种规律性的模式匹配
  * 只要两种语言原生数据足够丰富，一种语言的输入作为条件就能几乎唯一地确定另一种语言的翻译等价物，就是所谓「压缩」理论
  * 无监督学习其实就是在寻找最优的数据压缩方法
  * 一个好的无监督学习算法，应该能找到数据的最简洁表示（即 **K 氏复杂度**），同时又能最大限度地利用这种表示来完成下游任务
* 从条件建模到联合建模
  * 与其像监督学习那样将 X 和 Y 视为条件与结果，不如将它们视为一个整体，在一个巨大的模型里面一起进行压缩。也就是说，我们要寻找一个联合的 **K 氏复杂度 K(X,Y)**，即同时压缩 X 和 Y 的最短程序长度，这就是我们的无监督学习出来的预训练大模型（LLM）。
  * 无监督学习的新范式，它将传统的独立建模（如英语模型、汉语模型；再如，语言模型、视觉模型，等等）提升到了大一统的关联建模的高度。在这个范式下，无监督学习的目标不再是单纯地压缩单一群体的数据，而是寻找数据之间的联系。
  * 压缩的对象是数据集，而不是数据点，这一点非常重要，这其实是形式压缩与内容压缩的分水岭。*形式压缩只是一个机械过程，产生不了智能。只有内容压缩才能成就人工智能。*
  * 个体对象转变为群体对象的时候，形式的压缩就自然转化为内容的压缩。这是因为*群体虽然是个体组成的，但为群体压缩，如同是为群体「画像」，勾勒的是群体的统计性形象，它看上去可能是个个体，但它不是原数据中的任何一个特定的个体复制，否则就不是模型，而是记忆库了。*
    * 大模型压缩的本意就是要找出数据集的特征和规律性。大模型 GPT4 生成的文字，我们可能似曾相读；大模型 Suno 生成的音乐，我们可能似曾相闻；大模型 Sora 生成的视频，我们可能似曾相见；大模型 MJ 生成的图片，我们可能似曾相识。

这种跨模式、跨模态的学习，才是通用人工智能的高级形态。

# Reference
* [「安全智能」的背后，Ilya 究竟看到了什么？](https://hub.baai.ac.cn/view/38778)
* [SITUATIONAL AWARENESS: The Decade Ahead](https://situational-awareness.ai/)