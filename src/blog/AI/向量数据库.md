## Vector Embedding 是什么？

基本解释：Vector Embeddings是通过一些机器学习算法来生成的，这些算法可以将复杂的非结构化数据（例如文本或图像）转换为固定长度的数值向量，从而方便机器学习算法处理。生成Vector Embeddings的一种方法是使用**特征工程**，即利用领域知识来设计向量的数值，以捕捉数据的语义。另一种方法是使用**神经网络**，例如自编码器或预测器，来自动学习向量的数值，以最大化数据之间的相似性或预测目标。不同的数据类型和任务可能需要不同的算法来生成Vector Embeddings，例如在自然语言处理中常用的Word2Vec，GloVe或FastText。

存在原因：Vector Embedding解决了如何用数字表示一些抽象的数据，比如文字，图片，声音等，让计算机能够理解它们的意义和关系的问题。如果不用Vector Embedding，计算机就很难处理这些数据，因为它们没有固定的格式和大小，也没有明确的规则和逻辑。Vector Embedding把这些数据变成了一串数字，就像给它们编了一个密码，让计算机能够识别它们，并且比较它们的相似度和差异。Vector Embedding还有一个好处，就是可以减少数据的大小，节省存储空间和计算时间。

Vector Embedding不是唯一的方案，还有其他的方法可以表示抽象的数据，比如符号表示法，图表示法等。但是Vector Embedding有一些优势，比如可以用数学运算来处理向量，可以用神经网络来自动学习向量，可以用向量空间来可视化数据等。所以Vector Embedding是目前最流行和最有效的方案之一。

### 生成 Vector Embedding 的方法？
* Random Projection（随机投影）是一种基于线性变换和降维的方法，它可以在高维空间中减少数据之间的距离差异，并且保留数据之间的相对距离。
* Product Quantization（乘积量化）是一种基于向量分割和编码的方法，它可以在高维空间中将数据压缩成短码，并且保留数据之间的近似距离。
* Locality-sensitive hashing（局部敏感哈希）是一种基于哈希函数和桶划分的方法，它可以在高维空间中将数据映射成数字，并且保留数据之间的相似概率。
* Hierarchical Navigable Small World (HSNW)（分层可导航小世界）是一种基于图结构和贪心搜索的方法，它可以在高维空间中构建一个近似最近邻图，并且保留数据之间的局部连通性。
其他
* KD树：KD树是一种基于空间划分和二分搜索的方法，它可以在低维空间中快速地找到最近邻点，但是在高维空间中效果会变差，并且对动态数据更新不友好。
* PCA：PCA是一种基于线性变换和降维的方法，它可以在高维空间中减少冗余信息和噪声，并且保留主要特征和距离信息，但是它需要计算所有数据点之间的协方差矩阵，并且对非线性结构不敏感2。
* ANN：ANN是一种基于神经网络和深度学习的方法，它可以在高维空间中学习复杂和非线性的特征表示，并且适应各种类型和结构

优缺点
向量量化（Vector Quantization）的优点是压缩率高，但是计算量大，而且需要大量的存储空间。
Locality-Sensitive Hashing（LSH）的优点是计算速度快，但是精度不高。
Product Quantization with Residuals（PQR）的优点是可以提高压缩率和精度。

## Vector search 中的 distance metrics 是什么？

答：Vector search distance metrics是指用于衡量两个向量之间相似度或距离的度量标准。不同的距离度量可能反映不同的向量特征或关系，因此在进行向量检索时，需要根据具体的应用场景和数据特点选择合适的距离度量。

举例来说，假设我们有以下四个向量：

- A = [1, 2]
- B = [3, 4]
- C = [5, 6]
- D = [-1, -2]

我们可以用不同的距离度量来计算它们之间的相似度或距离，例如：

- 内积：内积是两个向量的点乘，它反映了两个向量在同一方向上的投影长度的乘积。内积越大，表示两个向量越相似。例如，A和B的内积是11，A和D的内积是-5，所以A和B更相似。
- 余弦相似度：余弦相似度是两个向量的夹角的余弦值，它反映了两个向量之间的方向相似度。余弦相似度越接近1，表示两个向量越相似。例如，A和B的余弦相似度是0.98，A和D的余弦相似度是-1，所以A和B更相似。
- 欧氏距离：欧氏距离是两个向量之间的直线距离，它反映了两个向量之间的绝对差异。欧氏距离越小，表示两个向量越相似。例如，A和B的欧氏距离是2.83，A和D的欧氏距离是4.24，所以A和B更相似。
- 曼哈顿距离：曼哈顿距离是两个向量之间的坐标差的绝对值之和，它反映了两个向量在各个维度上的差异。曼哈顿距离越小，表示两个向量越相似。例如，A和B的曼哈顿距离是4，A和D的曼哈顿距离是6，所以A和B更相似。
- 汉明距离：汉明距离是两个二进制向量之间不同位的个数，它反映了两个向量在二进制表示上的差异。汉明距离越小，表示两个向量越相似。例如，如果我们将A、B、C、D转换为二进制形式：

  - A = [01, 10]
  - B = [11, 00]
  - C = [01, 10]
  - D = [11, 00]

  那么A和B的汉明距离是4，A和C的汉明距离是0，所以A和C更相似。

问：向量数据是如何存储的？

答：向量数据的存储方式取决于向量数据库的具体实现，不同的方案可能有不同的优化技术。但是一般来说，向量数据的存储可以分为两个层次：

- 数据层：这一层负责存储向量数据的原始值，以及与之相关的结构化数据，如主键、标签等。数据层通常采用分布式文件系统或对象存储系统来实现，以支持海量数据的存储和备份。例如，Milvus²使用MinIO作为其数据层的存储系统。
- 索引层：这一层负责为向量数据构建索引结构，以加速相似度搜索的效率。索引层通常采用各种近似最近邻（ANN）算法来实现，如LSH、k-d tree、PQ等。索引层通常需要占用较少的存储空间，但是需要较高的计算能力。例如，Milvus支持多种索引类型，如IVF_FLAT、IVF_SQ8、HNSW等。

向量数据库在存储向量数据时，通常需要在数据层和索引层之间进行同步和更新，以保证数据的一致性和可用性。同时，向量数据库还需要提供查询接口和分析接口，以方便用户对向量数据进行检索和分析。

例子：

假设我们有一些向量数据，它们是由一些数字组成的，比如[1,2,3]，[4,5,6]，[7,8,9]等。我们想把这些向量数据存储起来，方便以后查找和分析。

- 一种方法是用数据层，就是把向量数据的原始值直接存储在一个大文件里，每个向量占一行，每个数字用逗号隔开。比如：

```
1,2,3
4,5,6
7,8,9
...
```

这种方法可以保证数据的完整性，但是如果我们想要查找和某个向量最相似的向量，就需要遍历整个文件，比较每个向量与给定向量之间的距离或相似度，这样效率很低。

- 另一种方法是用索引层，就是为向量数据构建一个索引结构，把相似的向量放在一起，不相似的向量分开。比如：

```
[1,2,3] -> [4,5,6] -> [7,8,9]
[10,11,12] -> [13,14,15] -> [16,17,18]
...
```

这种方法可以提高查询效率，因为我们只需要在相似的向量中查找，而不需要遍历整个文件。但是这种方法也有缺点，就是需要占用额外的存储空间，并且需要较高的计算能力来构建和维护索引结构。

向量数据库在存储向量数据时，通常会同时使用数据层和索引层，以平衡存储空间和查询效率。同时，向量数据库还会提供一些接口，让我们可以方便地对向量数据进行检索和分析。

## 向量索引的维度跟索引类型有哪些？

答：向量索引的维度是指向量的长度，也就是向量中包含的浮点数的个数。向量的维度决定了向量所能表示的信息的复杂度和精度。一般来说，向量的维度越高，向量所能表示的信息越丰富，但也越难进行检索和计算。不同的应用场景可能需要不同的向量维度，例如文本嵌入通常在几百到几千维之间，而图像嵌入通常在几千到几万维之间。

向量索引的类型是指用于存储和检索向量的数据结构和算法。向量索引的类型决定了向量检索的效率和准确度。一般来说，向量索引可以分为两大类：精确索引和近似索引。精确索引可以保证找到与查询向量最相似的向量，但是计算代价很高，只适合小规模数据集。近似索引可以在牺牲一定准确度的情况下，大大提高检索速度和扩展性，适合大规模数据集。

不同的向量索引类型有不同的优缺点和适用场景，例如：

- 线性扫描：最简单的精确索引方法，就是遍历所有向量，计算与查询向量的相似度，然后排序返回。这种方法没有任何预处理或空间优化，因此非常慢，只适合极小规模数据集。
- 倒排文件：一种常用的精确索引方法，就是将每个向量分成若干段，并为每个段建立一个倒排列表，记录包含该段的所有向量的ID。这样，在检索时，只需要查找与查询向量相同段的倒排列表，然后对候选向量进行精确计算。这种方法可以大大减少检索范围，提高检索速度，但是需要较大的存储空间，并且对于高维或稀疏向量效果不佳。
- 局部敏感哈希：一种常用的近似索引方法，就是将每个向量映射到一个或多个哈希桶中，使得相似的向量有较高概率落入同一个桶。这样，在检索时，只需要查找与查询向量相同桶的候选向量，然后对其进行排序返回。这种方法可以有效降低检索复杂度，并且支持多种相似度度量，但是需要调整合适的哈希函数和参数，并且可能存在哈希冲突或丢失最近邻。
- 乘积量化：一种常用的近似索引方法，就是将每个向量分成若干子向量，并为每个子向量建立一个有限大小的码本，记录子向量可能取值的离散集合。这样，在存储时，只需要记录每个子向量对应码本中的编码，在检索时，只需要对编码进行快速比较或计算近似距离。这种方法可以大大压缩存储空间，并且提高检索速度，但是需要预先训练码本，并且牺牲一定准确度。
- 图搜索：一种常用的近似索引方法，就是将所有向量构建成一个图结构，并为每个节点（即每个向量）维护一个邻居列表，记录与其最相似的若干节点。这样，在检索时，只需要从一个随机节点开始，在图上进行贪心遍历或随机游走，直到找到局部最优或全局最优节点作为结果返回。这种方法可以有效利用数据之间的结构信息，并且支持动态更新数据集，但是需要预先构建图结构，并且可能存在局部最优或陷入死循环。

Pinecone是一个云端的向量数据库服务，它支持多种维度和类型的向量索引。根据官方文档：

- Pinecone支持任意正整数作为向量维度
- Pinecone支持三种类型的向量索引：flat、ivf和hnsw
- flat类型是线性扫描方法，适合小规模数据集或对准确度要求很高的场景
- ivf类型是乘积量化方法，适合中等规模数据集或对速度和空间要求较高的场景
- hnsw类型是图搜索方法，适合大规模数据集或对动态更新要求较高的场景

## 基础

### 举例说明数据库分片
数据库分片是一种将大型数据库拆分为多个较小的数据库的技术，以提高性能和可扩展性。可以用一个简单的例子来说明数据库分片的原理：

假设你有一个图书馆，里面有很多书籍，你需要将它们按照类别进行分类和存放。如果你只有一个大书架，那么你可能会把所有的书籍都放在一起，按照类别的字母顺序排列。这样做的好处是你可以很容易地找到任何一本书，只要你知道它的类别和名称。但是这样做也有一些缺点：

- 如果你的图书馆很受欢迎，那么你可能会有很多读者同时来借阅书籍，这样就会造成大书架上的拥挤和混乱。
- 如果你的图书馆不断增加新的书籍，那么你可能会发现大书架已经放不下了，你需要购买更多的书架或者扩建图书馆，这样就会增加成本和维护难度。
- 如果你的图书馆发生了火灾或者其他灾难，那么你可能会损失所有的书籍，因为它们都存放在同一个地方。

为了解决这些问题，你可以考虑将你的图书馆进行分片，也就是将你的大书架拆分为多个小书架，每个小书架只存放一类或者几类书籍。这样做的好处是：

- 你可以将不同类别的书籍分布在不同的位置，这样就可以减少读者之间的冲突和等待时间，提高借阅效率。
- 你可以根据每个小书架的容量和需求来灵活地调整和扩展你的图书馆，只要有空余的位置，你就可以添加新的小书架或者移动旧的小书架，而不需要改变整个图书馆的结构。
- 你可以提高你的图书馆的安全性和可靠性，因为即使某个小书架发生了损坏或者丢失，也不会影响其他小书架上的书籍，你可以很快地恢复或者替换损坏的小书架。

当然，将图书馆进行分片也有一些缺点：

- 你需要设计一个合理的分片规则，来确定每本书应该放在哪个小书架上。如果分片规则太复杂或者太简单，都可能导致数据分布不均匀或者查询效率低下。
- 你需要维护一个目录表或者索引表，来记录每个小书架上存放了哪些类别和名称的书籍。这样当读者想要借阅某本书时，你可以根据目录表或者索引表快速地找到对应的小书架。
- 你需要处理跨小书架的查询和操作，比如如果读者想要借阅不同类别或者不同名称的多本书籍，那么你可能需要访问多个小书架，并将结果合并返回给读者。

### 算法

* K-Means 算法 

k-means 算法是一种基于距离的聚类算法，它的目的是将给定的数据集划分为 k 个簇，使得同一个簇内的数据点相似度高，不同簇之间的数据点相似度低¹²³。k-means 算法的基本步骤是：

- 随机选择 k 个数据点作为初始的聚类中心；
- 对于每个数据点，计算它到 k 个聚类中心的距离，并将其分配到距离最近的聚类中心所对应的簇中；
- 对于每个簇，重新计算它的聚类中心，即该簇内所有数据点的均值；
- 重复上述步骤，直到聚类中心不再变化或达到最大迭代次数。

k-means 算法有很多应用场景，比如：

- 数据挖掘和分析，比如对客户、商品、文档等进行分类和聚合；
- 图像处理和压缩，比如对图像进行分割、去噪、降维等；
- 机器学习和模式识别，比如对特征进行提取、降维、初始化等。


## Demo（目标）

1. 使用 Pinecone + OpenAI Embedding API实现WTF开源课程的语义化搜索  的代码示例 ，reference [pinecone_and_openai_stack](https://docs.pinecone.io/docs/openai#creating-embeddings)
  * 目标：熟悉基本的向量转换，以及语义模糊搜索
2. chroma (pinecone免费替代品，需要安装 chromaDB) + Word2Vec(OpenAI Embedding 免费替代品) 再实现示例 1
  * 目标：熟悉 LangChain 调用 chroma，熟悉 pytorch 使用 Word2Vec；
3. 使用卷积神经网络（CNN 转化图片为 vector）+ chromaDB 进行图片相识度搜索
  * 目标：熟悉卷积神经网络

### Demo 涉及的基础概念
## 文字转Vector
Word2vec 和 openai 的 embeddings 接口是两种不同的方法，用于将词语转换为数值向量，以便进行后续的分析和处理。它们的**区别**主要有以下几点：

- Word2vec 是一种无监督的学习方法，它利用词语的周边词来学习词语的嵌入表示。它可以学习到相关词，但是只能捕捉到局部分布信息。Word2vec 的输出是一个静态的嵌入表，对于每个词语，都有一个固定的向量表示，与词语所在的句子无关。
- openai 的 embeddings 接口是一种基于 transformer 模型的预训练方法，它利用双向上下文信息来学习词语的嵌入表示。它可以学习到更深层次的语义信息，包括词序、句子层面和跨句子的关系。openai 的 embeddings 接口的输出是一个动态的嵌入表，对于每个词语，都有一个根据句子生成的向量表示，与词语所在的句子相关。

Word2vec 和 openai 的 embeddings 接口各有**优劣势**，具体取决于使用场景和需求。一般来说，可以从以下几个方面进行比较：

- 数据量和训练时间。Word2vec 是一种无监督的方法，它不需要标注数据，只需要大量的文本语料就可以训练出词向量。这样可以节省数据标注的成本和时间，也可以利用海量的文本数据来提高词向量的质量。openai 的 embeddings 接口是一种基于预训练模型的方法，它需要使用标注数据来微调模型，以适应特定的任务和领域。这样可以提高词向量的精确度和泛化能力，但也需要更多的数据标注和训练时间。
- 语义信息和上下文信息。Word2vec 的词向量是静态的，它只能捕捉到词语本身的语义信息，而不能考虑词语所在的上下文信息。这样可能导致同一个词语在不同的句子中有相同的向量表示，而忽略了词语的多义性和歧义性。openai 的 embeddings 接口的词向量是动态的，它能够捕捉到词语的双向上下文信息，以及句子层面和跨句子的关系。这样可以提高词向量的丰富度和灵活度，能够更好地处理词语的多义性和歧义性。
- 通用性和可定制性。Word2vec 的词向量是通用的，它可以适用于不同的任务和领域，而不需要进行特定的调整或优化。这样可以提高词向量的可复用性和可移植性，也可以减少模型开发和部署的复杂度。openai 的 embeddings 接口的词向量是可定制的，它可以根据不同的任务和领域进行微调或优化，以提高词向量的适应性和效果。这样可以提高词向量的针对性和精确性，也可以增加模型开发和部署的灵活性。
## Reference

* New Bing
* https://www.pinecone.io/learn/vector-embeddings-for-developers/
* https://www.pinecone.io/learn/vector-database/
* https://weaviate.io/blog/distance-metrics-in-vector-search
* https://platform.openai.com/docs/guides/embeddings/use-cases
* https://docs.pinecone.io/docs/openai#creating-embeddings