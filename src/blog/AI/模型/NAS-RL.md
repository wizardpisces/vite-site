# NAS-RL
全称 Neural Architecture Search（神经架构搜索），是一种自动化搜索最佳神经网络架构的方法。

传统上，设计一个好的神经网络架构需要专家反复试验和直觉，而 NAS 通过自动化这一过程，NAS 通常使用一种称为「控制器」的模型来进行这些序列决策。这个控制器可能是一个 RNN 或者其他序列模型，它负责在每个步骤做出选择，根据先前的选择历史来决定下一层的类型。控制器通过不断地在搜索空间中探索并优化决策序列，最终找到一个在性能上最优的网络架构。

* 核心思想：将网络架构设计问题转换为一个序列决策问题

## NAS 可以理解成深度学习模型的深度学习模型？

可以；它用一个深度学习模型（控制器）不断地生成和优化另一个用于实际任务的深度学习模型（目标模型）的架构。这种方法不仅减少了人工干预，还能发现潜在的、更优的神经网络架构。

类似于元学习（Meta-Learning）的一种思路，在元学习中，我们学习如何更好地进行学习。在 NAS 中，我们学习如何设计更好的深度学习模型。两者都体现了自我改进和自动化优化的理念。

## 控制器的原理？为什么通常是 RNN？

通常，RNN 被用作控制器是因为其在处理序列数据方面的优势。下面我们详细解释一下控制器的原理以及为什么 RNN 通常被选择用作控制器。

### 控制器的原理

1. **生成架构**：
   - 控制器模型通过生成一系列离散的决策来定义目标模型的架构。每个决策对应一个具体的架构组件（如选择层的类型、大小、连接方式等）。
   - 控制器的输出是一个序列，这个序列描述了目标模型的结构。例如，控制器可能输出一个这样的序列：Conv(64, 3x3) -> MaxPool(2x2) -> Dense(128)。

2. **序列决策**：
   - 控制器在每一步生成一个决策，这个决策依赖于之前所生成的决策。这个过程自然而然地适合用处理序列数据的模型来实现，因为序列中的每个元素（即每个决策）都与前面的决策有关联。

3. **优化过程**：
   - 生成架构后，对对应的目标模型进行训练，并在验证集上评估其性能。
   - 将评估结果作为反馈给控制器，调整控制器的权重，使其倾向于生成性能更好的架构。这通常通过强化学习（Reinforcement Learning）或进化算法（Evolutionary Algorithms）来实现。


### 为什么通常是 RNN？

使用 RNN 是因为其在处理和生成序列数据方面的特点和优势。以下是详细原因：

1. **序列数据处理能力**：
   - RNN 的设计使得其擅长处理序列数据，即可以根据前一步的输出影响下一步的输入，捕捉序列中的依赖关系。
   - 在 NAS 中，网络结构是一个序列化的决策过程。例如，选择层的类型（卷积层、全连接层等）、层的参数（如滤波器数量、核大小等）、层与层之间的连接等，这些决策需要逐步生成，每一步都依赖于前面的决策。

2. **记忆和状态保持**：
   - RNN 能够保留之前步骤的状态信息，这是通过其隐藏状态实现的。在架构生成过程中，每个决策都是基于前面所有决策的综合结果。
   - 这种记忆能力在生成复杂的神经网络架构时特别有用，因为每一层的配置可能依赖于之前层的配置。

3. **灵活性**：
   - RNN 可以很自然地扩展到生成变长序列，这对于生成不定长的神经网络架构非常重要。
   - 控制器可以学习到在何时停止生成新的层，从而生成不同长度的架构。

### 控制器 - RNN 的工作流程示例

1. **初始化**：
   - 控制器 RNN 初始化隐藏状态和开始令牌，准备生成第一个决策。

2. **生成第一个决策**：
   - 根据当前隐藏状态和输入令牌（开始令牌），RNN 生成第一个决策，例如选择一个卷积层 Conv(64, 3x3)。

3. **更新状态并生成后续决策**：
   - RNN 将当前决策作为输入，并更新隐藏状态，生成下一个决策，例如选择一个最大池化层 MaxPool(2x2)。

4. **重复**：
   - 重复以上步骤，直到生成完整的神经网络架构。

5. **输出架构**：
   - 控制器输出整个序列，表示生成的神经网络架构。

6. **训练并评估目标模型**：
   - 用该架构训练目标模型，并在验证集上评估其性能。

7. **反馈**：
   - 将评估结果作为奖励（或惩罚），反馈给控制器 RNN，调整其权重，以优化架构生成策略。

### 其他替代方法

虽然 RNN 是常用的控制器模型，但也有其他方法可以用来实现控制器的功能：

1. **注意力机制（Attention Mechanism）**：
   - 使用 Transformer 等基于注意力机制的模型提升生成速度和性能。

2. **强化学习（Reinforcement Learning）**：
   - 利用强化学习中的 Q-learning 或者 Actor-Critic 方法来替代 RNN 生成架构决策。

3. **进化算法（Evolutionary Algorithms）**：
   - 使用基因算法（Genetic Algorithm）等进化算法通过变异、交叉等操作生成和优化架构。

### 总结

使用 RNN 作为控制器模型的方式使得 NAS 能够处理复杂的结构生成任务并捕捉到生成过程中序列决策的依赖关系。它的序列处理能力和状态保留特性，使其成为生成神经网络架构的理想选择。当然，随着技术的进步，也出现了其他替代方法，可以根据实际需求选择合适的控制器模型。

## 解决评估时间过长？

在神经网络架构搜索（NAS）中，由于控制器生成的模型数量可能非常庞大，这会导致训练和评估整体耗时漫长。以下几种方法可以有效缓解这个问题：

### 1. 使用代理任务

**代理任务**是一种缩短每次训练和评估时间的方法，通过对一个简化但相关性强的子任务进行优化，以推测完整任务上的性能。

- **缩小数据集**：对一个更小的数据集进行训练和验证，尽快获得性能的反馈。
- **缩小模型规模**：使用简化的模型（如较少的参数和层）以减少每次训练时间。
- **早停法**：监测模型的验证集性能，在检测到过拟合趋势时立即中止训练。

### 2. 并行搜索

**并行搜索**利用分布式计算资源同时评估多个候选模型的大规模并行能力。例如：

- **分布式计算**：用多个计算节点同时进行训练和评估，充分利用计算资源。
- **多GPU训练**：在多GPU平台上进行并行训练，通过同步或者异步评估减少总体时间。

### 3. 基于权重分享的方法
（迁移学习）
**基于权重分享的方法**（例如 One-shot NAS 和NASNet）通过共享部分权重，不需要每个架构分别训练。

- **One-shot模型**：所有候选架构共享一个超大模型中的参数，通过不同路径选择进行训练和评估，只需训练一次。
- **权重继承**：在不同候选架构之间传递和共享权重，无需完全从头开始训练每个模型。

### 4. 强化学习和进化算法优化

利用强化学习和进化算法进一步优化架构生成和选择的效率。

- **强化学习**：优化控制器生成决策的策略，使其更快找到高质量的架构。这可以减小搜索空间，提高搜索效率。
- **进化算法**：通过选择、交叉、变异等操作，确保每代评估的架构具有更高的潜力。

### 5. 代理模型（Performance Predictors）

**代理模型**（例如元学习）可以用来预测未训练架构的性能，从而减少实际训练的次数。

- **性能预测器**：训练代理模型，根据当前架构的一些特征快速估计其性能，以替代或减少实际训练。
- **元学习**：基于以前的训练结果，学习更好的模型搜索策略，提高样本效率。

## 更先进的“NAS”

### DARTS
