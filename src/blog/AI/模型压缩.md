# 模型压缩

* 知识蒸馏
  * 通过训练一个较小的“学生”模型来模仿一个大的“教师”模型的行为，从而达到压缩模型的目的。

* 低秩分解
    * 效果
      * 低秩分解不仅减少了模型的计算和存储需求，还有助于提高模型的泛化能力（参数少，有助于降低过拟合，则泛化能力越强？）。
      * 压缩：图像矩阵的压缩
    * How-to：权重矩阵 W 可以表示为两个低秩矩阵A和B的乘积：W = A * B^T。这里A和B的秩远小于W，从而降低了GPU显存占用。
    * reference
      * [模型压缩之模型分解篇：SVD分解，CP分解和Tucker分解](https://zhuanlan.zhihu.com/p/490455377)


* 量化
    * 将权重和激活函数的精度从单精度浮点数（32位）降低到更低位的表示，如16位、8位甚至更低。
    * 在极端的情况下，可以进行二值化或三值化，此时权重和激活只取{-1, 0, 1}这几个值。

* 权重剪枝
  * 稀疏剪枝：移除模型中权重的一部分，通常是那些接近于零的权重。
  * 结构化剪枝：按照某种模式移除权值，比如剪枝整个卷积核或卷积滤波器。

* 共享权重：
  * 在网络的不同部分共享同一组参数，这在循环神经网络（RNNs）中很常见。

* 紧凑型卷积核：
  * 使用更小的卷积核或组合小卷积核来替代大卷积核，如深度可分离卷积。

  小波变换：
  * 使用小波变换替代一部分卷积层来实现特征提取与降维。

* 使用更高效的架构：
  * 设计或使用特别为效率优化的网络架构，如MobileNet、SqueezeNet、EfficientNet等。

重参数化不仅仅指低秩分解，它指的是用一种不同的方式重新表达模型中的参数，以达到某种目的，比如减少参数总数、增加模型的稳定性或是改善性能，或者让模型能够进行求导梯度下降（例如 VAE 的训练）等。
**低秩分解**是重参数化的一种方法。

# Reference
* gpt