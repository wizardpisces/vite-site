## 简介
业务目标是解决自动化场景遇到的验证码识别

## 问题具体化

识别变长不规则的图片验证码（包含字母或数字）
## 第三方工具调研

* 遇到的问题
    * 付费且不准
* 原因：结果很不准，需要付费
    * 成熟开源的 OCR 只能识别比较规整的字母跟数字
* 结果：不选择，决定找开源模型，自己训练
    * 网上模型很多，需要甄选并改造成合适的模型

## CNN 调研
寻找到并调研了的[CNN 模型](https://github.com/nickliqian/cnn_captcha)，赞数比较多，应该靠谱

阅读源码后判断只能做定长识别
### 技术栈
模型实现：Tensorflow 框架的 CNN

* CNN（Convolutional Neural Network 卷积神经网络） 做图片特征提取
* 定长编码：one hot编码，将离散的分类数据转换为神经网络等模型可以处理的向量表示。如果编码是 4 位，则是一个识别 4 位字符的分类任务
* 交叉熵损失函数推动反向传播


### 遇到的问题

问题1
* 我们的目标验证码是变长的
    * 处理变长可能方案：裁剪，切割，并一个个识别文本
        * 结果：放弃直接的 CNN 模型，寻找更合适的模型
        * 原因：图片背景有噪声，导致对图片进行切割困难，会损失一些文本细节导致识别不准。

## CRNN 调研
寻找到并调研的[CRNN 模型](https://github.com/GitYCC/crnn-pytorch)，赞数相对 CNN 少了一个量级，看起来很顺眼，有论文支持

阅读源码后判断可以做变长序列识别
### 技术栈
Pytorch 框架的 CRNN 流程 
![CRNN 流程](https://raw.githubusercontent.com/GitYCC/crnn-pytorch/master/misc/crnn_structure.png)
* CNN 做图片特征提取
*  LSTM(RNN)(Long-short term memory)  对 CNN 提取的特征序列建模，利用上下文信息**提高识别的准确性**；
        * 对于 RNN 的作用
            * 作用1（字符内部像素序列）：对于常见的随机顺序验证码，RNN 通过处理构成字符的像素序列并捕捉序列中的特征信息，学习到字符的局部和全局特征，包括形状、纹理、笔画等信息，从而提升判定字符的准确率。
            * 作用2（字符序列）：对于非随机顺序验证码，对于一些相似或者易混淆的字符，RNN可以根据前后的字符来判断最可能的结果。例如，如果验证码中有一个字符“l”，它可能是字母“l”或者数字“1”，但是如果前面的字符是“o”，并且训练数据中经常出现 ol，那么RNN就可以推断出它更可能是字母“l”；也即 RNN 能够在大样本中寻找到生成验证码序列的一些可能得规律，从而增加准确性
        * PS：定长任务中可以理解成是对 CNN 识别能力的增强？
            
* CTC（Connectionist temporal classification 连接时序分类） 对 RNN 的输出序列对齐 + 计算损失驱动梯度下降反向传播
    * CTC 算法不需要训练数据对齐（降低人工对齐工作量），它会把所有相同输出的对齐合并。帮助模型学习字符级别的对齐和映射关系，尤其在没有明确字符分隔符的情况下
    * 其他应用：适用于音频到文字的转码任务（音频的时长 t0->t1 可能对应一个字符）

### 遇到的问题

问题1
* 样本量评估（经验问题）：到底多少数据才能训练出可观效果？知道量级后才能给同事时发出定量的帮忙请求，做到有的放失
* 实践步骤
    1. 参考开源库
        * 基本都在 >=五位数
    2. 测试量级；找到最小量级同时又能得出不错效果的大概样本数
        * 找到合适的自动样本生成库，用自动生成的样本测试 5 位数能达到 90%；降低量级测试到 6k 数据能达到 80% 左右准确性；于是暂定为 6k 的目标标注数据
     
问题2
* 真实样本数不足导致学习慢（损失率下不去）：1k 多的标注数据不足以直接训练出效果（发动人民群众标注前提是看到效果（死锁），后面知道，标注本身让人没啥好感）
    * 当时标注一个样本大致需要 10s，刷新 -> 下载 -> 打开 -> 标注；还是比较费时间
    * 标注加速（解决标注慢问题）：脚本实现样本批量下载，批量合并
    * 数据清洗：对大家帮忙标注的数据进行简单核对，例如：对包含 0，1，o，l 等的可能出错的标注进行简单核对（ls sample/train | grep 'l'）

实践步骤
* 微调
    * 猜想（微调）：迁移学习跟特征共享，用少量样本训练出效果
        * 对比：九年义务教育中老师口中的举一反三，先学到如何使用公司（底层特征），然后套公式（知识迁移）；机器学习也可以通过其他样本（与真实样本共享一些特征）的学习积累底层特征，然后应用到少量样本加速学习过程
    * 猜想实践：
        1. 用第三方库使用同样的字符集生成6万张验证码样本，4,5,6长度分别是2万张；划分为训练 57k + 测试集 30k，花费17个小时（M1 CPU，在更高级版本的 pytorch 中能够支持 M1 的 GPU）训练出参数模型 A.pt
            * 效果：识别自身测试集准确率 90% 左右
        2. 基于 A.pt 的参数，使用 1k 多的标注数据进行训练模型 B.pt（体积是 31M）
            * 效果：损失率下降很快，3个小时，达到准确率 44% 左右
        3. 改造数据加载器, 调参
        4. 大家帮忙持续标注更多真实数据并优化训练模型
        5. 数据修正
        6. 最后将模型识别部署成 python 服务（目前达到准确率 75% 左右（训练集3k，测试集 250））

感悟
* 给定模型下，能搞到合适的训练数据集很重要

是否可以通过已有样本进行变换生成新的样本？
## 思考
### 哪些场景可能会出现过拟合？
* 数据集较小
    * 本次应对：增加真实数据量标注
    * 后续尝试（捞出识别错误的真实样本进行增强，然后针对性”刻意练习“）：
        1. 使用数据增强技术，如随机裁剪、旋转、翻转等，扩充训练数据的多样性。
        2. 使用 VAE（变分自编码器） 模型根据现有数据生成类似样本（因为训练 VAE 本身就需要比较多的样本数据，并且 VAE 也需要知道生成样本的Label）
        3. GAN?[开发基于 mnist 数据集的 GAN 网络](https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/)
            1. 尝试了一般 cgan ，3k 张图训练不出效果
            2. 后续尝试 wgan（训练更平滑）
        4. stable diffusion 图生图？
* 模型复杂度过高：过多的参数和复杂的模型结构会使模型在训练数据上表现很好，但在新数据上泛化能力较差。
    * 本次应对：使用比较靠谱的论文模型；PS：模型使用的 LSTM 在每个 Step 共享参数能避免过拟合（RNN 的升级版，RNN 会有梯度消失跟爆炸问题（而正是由于共享参数导致）） 
    * 方案汇总：模型参数缩减（剪枝（去除重要要的参数），量化（eg:缩减精度），知识蒸馏等），但是这个度不太好把控；例子：使用 LLaMa 模型初始权重（通过无监督学习获得）微调出更小的但是效果还不错的新模型 Alpaca 跟 Vicuna
* 噪声和异常样本：当训练数据中存在噪声、异常样本或标注错误时，CRNN可能会过拟合这些不代表真实数据分布的异常情况。模型会试图适应这些异常数据，导致在新数据上的表现下降。
    * 本次应对：手动清理：模型去预测测试集，对测试集预测错误的样本进行归纳，再反向应用到训练集
    * 其他方案：基于统计，聚类，邻近度，机器学习，时间序等；异常检测方法的选择和调整需要根据具体问题和数据的特点进行
* 不平衡的类别分布：如果训练数据中的类别分布不平衡，即某些类别的样本数量远远多于其他类别，模型可能会倾向于过拟合训练数据中较多的类别。这会导致模型对于少数类别的泛化能力较差。
    * 本次应对：真实的数据抓取，符合真实数据的生成规律分布，规避问题
* 过度训练：CRNN进行过多的训练轮次或使用过小的学习率，模型可能会过度拟合训练数据。过度训练会导致模型过度适应训练数据中的细节和噪声，而无法泛化到新数据。
    * 应对：早停法：验证集上监控模型性能，当性能不再提升时停止训练。

## References

* [CRNN Paper](https://arxiv.org/abs/1507.05717)
* [architecture - program mode](https://mxnet.apache.org/versions/1.9.1/api/architecture/program_model#:~:text=Symbolic%20Programs%20Tend%20to%20be,flow%20of%20a%20host%20language.)
* [Text-Recognition-With-CRNN-CTC-Network](https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI)
* [epoch-vs-iterations-vs-batch-size](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)
* [understanding lstm](https://alanlee.fun/2017/12/29/understanding-lstms/)
* [convolutional-neural-network](https://www.mathworks.com/discovery/convolutional-neural-network-matlab.html)
* [CNN captcha](https://github.com/nickliqian/cnn_captcha)
* [CRNN pytorch](https://github.com/GitYCC/crnn-pytorch)
* [RNN基础](https://zhuanlan.zhihu.com/p/30844905)
* [交叉熵损失函数](https://blog.csdn.net/SongGu1996/article/details/99056721)


