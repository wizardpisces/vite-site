# 神经网络
神经网络可以分为不同的类型，如前馈网络，卷积网络，循环网络等，用于解决不同的问题，如图像识别，自然语言处理，时间序列预测等。

## 常见神经网络

* 前馈神经网络（Feedforward Neural Network）：这类神经网络中，信息只从输入层向输出层传递，没有反馈或循环的连接。每一层的神经元只与下一层的神经元相连，不与同层或上一层的神经元相连。前馈神经网络的代表有全连接神经网络（FCN）、卷积神经网络（CNN）、生成对抗网络（GAN）等。
* 反馈神经网络（Feedback Neural Network）：这类神经网络中，信息可以在不同层之间反馈或循环，形成有向或无向的环路。这样可以使神经网络具有记忆功能，在不同时刻具有不同的状态。反馈神经网络的代表有循环神经网络（RNN）、长短期记忆网络（LSTM）、Hopfield网络、玻尔兹曼机等。
* 自组织神经网络（Self-organizing Neural Network）：这类神经网络中，信息的传递和处理是根据输入数据的特征自动调整的，而不是由预先设定的规则或参数决定的。这样可以使神经网络具有自适应和自学习能力，能够发现输入数据中的模式和规律。自组织神经网络的代表有自组织映射（SOM）、学习向量量化（LVQ）、自适应共振理论（ART）等。
* 混合神经网络（Hybrid Neural Network）：这类神经网络中，信息的传递和处理是由不同类型的子网络组合而成的，每个子网络可以有不同的结构和功能，以实现更复杂和高级的任务。混合神经网络的代表有变分自编码器（VAE）、Transformer、BERT等。

### LSTM 跟 RNN 的区别

LSTM和RNN的区别主要体现在以下几个方面：

- LSTM引入了细胞状态（cell state），它是一条贯穿整个链式结构的水平线，可以在不同时间步之间传递信息，而不受梯度消失或爆炸的影响。
- LSTM通过三个门结构（遗忘门、输入门、输出门）来控制细胞状态中的信息流动，可以选择性地添加或删除信息，从而实现对长期和短期信息的记忆和忘记。
- LSTM的激活函数不仅使用了tanh函数，还使用了sigmoid函数，并结合求和操作，使得梯度更容易保持稳定。
- LSTM可以更好地处理不同长度的序列数据，而RNN通常需要固定长度的输入和输出。

### 为什么 LSTM 设计的 每个 step 的参数一样，而不是不同？

参数共享的好处之一是减少了LSTM模型的参数量。如果每个时间步都有不同的参数，模型的参数量将随着时间步数的增加而成倍增加，导致模型变得非常庞大且难以训练。通过共享参数，LSTM模型可以更高效地学习和表示序列数据的长期依赖关系。

* 学习共享模式和规律：参数共享使得模型能够在整个序列中共享相同的门控参数，从而更好地学习序列中的共享模式和规律。这意味着模型能够捕捉到序列数据中的共性特征，而不仅仅是对单个时间步的输入进行建模。通过共享参数，模型可以更好地理解序列数据中的长期依赖关系，并且在新数据中也能够适应这些共性特征。

* 减少过拟合的风险：参数共享有助于降低模型的复杂性和参数量，从而减少过拟合的风险。当模型的参数量过多时，容易导致过拟合，即在训练数据上表现良好，但在新数据上表现较差。通过参数共享，模型的参数量得到控制，使得模型更加简洁，减少了过拟合的可能性，并提高了模型的泛化能力。

* 提取通用特征表示：参数共享有助于模型学习到更通用的特征表示。通过在不同时间步共享参数，模型能够将重要的信息从一个时间步传递到下一个时间步，形成连续的特征表示。这样，模型能够更好地捕捉序列中的关键特征，并将其应用于未知数据的推断和泛化。

因为参数少，所以避免了过拟合，所以提取的才是更加通用的特征？
#### CNN + RNN
- CNN 是卷积神经网络，它可以有效地提取图像的局部特征，但是它对图像的整体结构和顺序不敏感，因此不适合处理变长的数据。
- RNN 是循环神经网络，它可以处理序列数据，比如文本、语音、视频等，它有记忆功能，可以保存前面的信息，并影响后面的输出。但是它也有一些问题，比如梯度消失或爆炸、计算复杂度高、难以并行等。
- CNN 和 RNN 的结合可以克服各自的缺点，发挥各自的优势。一种常见的结合方式是先用 CNN 对图像进行特征提取，然后用 RNN 对提取出来的特征进行序列生成或分类。这样可以利用 CNN 的速度和轻量，以及 RNN 的顺序敏感性。
- 识别变长的字母是一个典型的场景，它需要同时考虑图像和文本的信息。一个可能的解决方案是先用 CNN 对图像进行卷积和池化，得到一个二维的特征图，然后将特征图切分成多个一维的向量，每个向量对应一个字母的位置。接着用 RNN 对这些向量进行编码或解码，得到最终的字母序列。这样可以实现对变长字母的识别。

#### CRNN + CTC 中的 CTC 是什么

CTC，即 Connectionist Temporal Classification，是一种用于解决序列标注问题的方法。它的设计目标是允许输入序列和输出序列的长度不完全匹配，并且不需要对齐信息。CTC 在深度学习中的应用较为广泛，特别是在语音识别、文本识别（OCR）和语言翻译等领域。

CTC 的工作原理如下：

* 输入序列经过 CNN 进行特征提取和降维处理，得到特征序列。
* 特征序列经过 RNN 进行时序建模，得到隐藏状态序列。
* 隐藏状态序列经过全连接层映射到输出空间，如字符或标签的概率分布。
* CTC 损失函数根据输出序列和标签序列之间的对应关系计算损失，用于训练模型。

CTC 适用于序列标注问题，允许处理不完全对齐的输入和输出序列，而交叉熵损失函数适用于分类问题，用于衡量模型输出的概率分布与真实标签之间的差异。

#### 其他方案

- LSTM+CTC：这种方法使用了循环神经网络（LSTM）和联结时间分类器（CTC）来处理序列数据，可以处理不定长的文字，不需要对齐输入和输出。LSTM可以提取文字的时序特征，CTC可以解决重复字符和空白字符的问题。这种方法适用于印刷文字、验证码等场景。
- CRNN：这种方法是卷积神经网络（CNN）和循环神经网络（RNN）的结合，使用CNN来提取图像的局部特征，然后使用RNN来提取文字的序列特征，并通过全连接层生成最终的标签预测。这种方法也可以配合CTC来处理不定长的文字。这种方法适用于场景文字、手写文字等场景。
- CnOCR：这是一个开源的Python OCR工具包，支持简体中文、繁体中文（部分模型）、英文和数字的常见字符识别，支持竖排文字的识别。它自带了20多个训练好的识别模型，适用于不同应用场景，安装后即可直接使用。它还集成了场景文字检测功能，可以处理各种图片。它追求的目标是使用简单，而不是实现最新的模型算法。

## 激活函数

### SoftMax 函数
输入向量 [ 1 , 2 , 3 , 4 , 1 , 2 , 3 ] 对应的Softmax函数的值为 [ 0.024 , 0.064 , 0.175 , 0.475 , 0.024 , 0.064 , 0.175 ] 。
输出向量中拥有最大权重的项对应着输入向量中的最大值“4”。这也显示了这个函数通常的意义：对向量进行归一化，凸显其中最大的值并抑制远低于最大值的其他分量。

## 反向传播与调参

反向传播是一种用来训练神经网络的常见方法，它的基本思想是**通过导数链式法则，计算损失函数对网络参数的梯度，并根据梯度更新参数，以达到最小化损失函数的目的**。

反向传播的过程可以分为以下几个步骤：

1. **前向传播**：将训练数据输入到神经网络的输入层，经过隐藏层和激活函数，最后到达输出层并输出预测结果。
2. **计算误差**：将预测结果与真实标签进行比较，得到损失函数的值，例如均方误差或交叉熵等。
3. **反向传播误差**：从输出层开始，逐层计算损失函数对每个神经元的输出和输入的偏导数，即误差信号。这一步利用了导数链式法则，即如果Z = g(Y)而Y = f(X)，那么∂Z/∂X = (∂Z/∂Y)·(∂Y/∂X)。
4. **更新参数**：根据损失函数对每个神经元的输入的偏导数，即误差信号和激活函数的导数，计算损失函数对每个权重和偏置的偏导数，即梯度。然后根据梯度和学习率，更新每个权重和偏置的值，使损失函数减小。

## Reference

* [神经网络基础](https://www.cnblogs.com/maybe2030/p/5597716.html)
* [反向传播算法及其调参 - 详细图解，看完就能懂](https://blog.csdn.net/ft_sunshine/article/details/90221691)
* [激活函数的形象解释](https://www.zhihu.com/question/22334626)
* [SoftMax函数](https://en.wikipedia.org/wiki/Softmax_function)
* [如何理解导数](https://www.zhihu.com/question/28684811)
* [一组图诠释CNN及RNN的区别](https://blog.csdn.net/buptgshengod/article/details/78362575)
* [深度学习中的注意力机制](https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216)
* [极大似然估计](https://www.matongxue.com/madocs/447/)
* [交叉熵](https://zhuanlan.zhihu.com/p/149186719)

LSTM
* [LSTM 是什么](https://alanlee.fun/2017/12/29/understanding-lstms/)
* [BI-LSTM 是什么](https://blog.csdn.net/SunJW_2017/article/details/82837072)
