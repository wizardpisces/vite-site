# 神经网络
神经网络可以分为不同的类型，如前馈网络，卷积网络，循环网络等，用于解决不同的问题，如图像识别，自然语言处理，时间序列预测等。

## 常见神经网络

* 前馈神经网络（Feedforward Neural Network）：这类神经网络中，信息只从输入层向输出层传递，没有反馈或循环的连接。每一层的神经元只与下一层的神经元相连，不与同层或上一层的神经元相连。前馈神经网络的代表有全连接神经网络（FCN）、卷积神经网络（CNN）、生成对抗网络（GAN）等。
* 反馈神经网络（Feedback Neural Network）：这类神经网络中，信息可以在不同层之间反馈或循环，形成有向或无向的环路。这样可以使神经网络具有记忆功能，在不同时刻具有不同的状态。反馈神经网络的代表有循环神经网络（RNN）、长短期记忆网络（LSTM）、Hopfield网络、玻尔兹曼机等。
* 自组织神经网络（Self-organizing Neural Network）：这类神经网络中，信息的传递和处理是根据输入数据的特征自动调整的，而不是由预先设定的规则或参数决定的。这样可以使神经网络具有自适应和自学习能力，能够发现输入数据中的模式和规律。自组织神经网络的代表有自组织映射（SOM）、学习向量量化（LVQ）、自适应共振理论（ART）等。
* 混合神经网络（Hybrid Neural Network）：这类神经网络中，信息的传递和处理是由不同类型的子网络组合而成的，每个子网络可以有不同的结构和功能，以实现更复杂和高级的任务。混合神经网络的代表有变分自编码器（VAE）、Transformer、BERT等。

## 激活函数

### SoftMax 函数
输入向量 [ 1 , 2 , 3 , 4 , 1 , 2 , 3 ] 对应的Softmax函数的值为 [ 0.024 , 0.064 , 0.175 , 0.475 , 0.024 , 0.064 , 0.175 ] 。
输出向量中拥有最大权重的项对应着输入向量中的最大值“4”。这也显示了这个函数通常的意义：对向量进行归一化，凸显其中最大的值并抑制远低于最大值的其他分量。

## 反向传播与调参

反向传播是一种用来训练神经网络的常见方法，它的基本思想是**通过导数链式法则，计算损失函数对网络参数的梯度，并根据梯度更新参数，以达到最小化损失函数的目的**。

反向传播的过程可以分为以下几个步骤：

1. **前向传播**：将训练数据输入到神经网络的输入层，经过隐藏层和激活函数，最后到达输出层并输出预测结果。
2. **计算误差**：将预测结果与真实标签进行比较，得到损失函数的值，例如均方误差或交叉熵等。
3. **反向传播误差**：从输出层开始，逐层计算损失函数对每个神经元的输出和输入的偏导数，即误差信号。这一步利用了导数链式法则，即如果Z = g(Y)而Y = f(X)，那么∂Z/∂X = (∂Z/∂Y)·(∂Y/∂X)。
4. **更新参数**：根据损失函数对每个神经元的输入的偏导数，即误差信号和激活函数的导数，计算损失函数对每个权重和偏置的偏导数，即梯度。然后根据梯度和学习率，更新每个权重和偏置的值，使损失函数减小。

## TODO 概念

* [x] 梯度下降和链式法则？前向传播跟反向传播？RNN 会涉及到梯度消失跟梯度爆炸？激活函数 sigmoid tanH，LSTM 模型能解决梯度消失跟爆炸问题
* [x] 感知机学习算法：感知机学习、最小二乘法和梯度下降法
* [x] 为什么说感知机主要的本质缺陷是它不能处理线性不可分问题
* [] 无梯度架构？
* [] 多项线性判别分析，朴素贝叶斯分类器和人工神经网络
* [] 信息熵表达式
* [] 朴素贝叶斯分类器和人工神经网络
* [] NLP 算法
* [] NLP四大任务类型：分类、序列标注、文本匹配、文本生成，都需要完整实现一遍。
* [] 梯度下降法的优化
* [] GAN
* [] ResNet
* [] 拉格朗日函数 跟 乘法

损失函数
* [x] 最小二乘法
* [x] 极大似然估计
* [x] 交叉熵：熵是服从某一特定概率分布事件的理论最小平均编码长度
## Reference

* [神经网络基础](https://www.cnblogs.com/maybe2030/p/5597716.html)
* [反向传播算法及其调参 - 详细图解，看完就能懂](https://blog.csdn.net/ft_sunshine/article/details/90221691)
* [激活函数的形象解释](https://www.zhihu.com/question/22334626)
* [SoftMax函数](https://en.wikipedia.org/wiki/Softmax_function)
* [如何理解导数](https://www.zhihu.com/question/28684811)
* [一组图诠释CNN及RNN的区别](https://blog.csdn.net/buptgshengod/article/details/78362575)
* [深度学习中的注意力机制](https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216)
* [极大似然估计](https://www.matongxue.com/madocs/447/)
* [交叉熵](https://zhuanlan.zhihu.com/p/149186719)
