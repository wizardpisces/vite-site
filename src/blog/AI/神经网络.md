# 神经网络
神经网络可以分为不同的类型，如前馈网络，卷积网络，循环网络等，用于解决不同的问题，如图像识别，自然语言处理，时间序列预测等。

## 常见神经网络

* 前馈神经网络（Feedforward Neural Network）：这类神经网络中，信息只从输入层向输出层传递，没有反馈或循环的连接。每一层的神经元只与下一层的神经元相连，不与同层或上一层的神经元相连。前馈神经网络的代表有全连接神经网络（FCN）、卷积神经网络（CNN）、生成对抗网络（GAN）等。
* 反馈神经网络（Feedback Neural Network）：这类神经网络中，信息可以在不同层之间反馈或循环，形成有向或无向的环路。这样可以使神经网络具有记忆功能，在不同时刻具有不同的状态。反馈神经网络的代表有循环神经网络（RNN）、长短期记忆网络（LSTM）、Hopfield网络、玻尔兹曼机等。
* 自组织神经网络（Self-organizing Neural Network）：这类神经网络中，信息的传递和处理是根据输入数据的特征自动调整的，而不是由预先设定的规则或参数决定的。这样可以使神经网络具有自适应和自学习能力，能够发现输入数据中的模式和规律。自组织神经网络的代表有自组织映射（SOM）、学习向量量化（LVQ）、自适应共振理论（ART）等。
* 混合神经网络（Hybrid Neural Network）：这类神经网络中，信息的传递和处理是由不同类型的子网络组合而成的，每个子网络可以有不同的结构和功能，以实现更复杂和高级的任务。混合神经网络的代表有变分自编码器（VAE）、Transformer、BERT等。

### LSTM 跟 RNN 的区别

LSTM和RNN的区别主要体现在以下几个方面：

- LSTM引入了细胞状态（cell state），它是一条贯穿整个链式结构的水平线，可以在不同时间步之间传递信息，而不受梯度消失或爆炸的影响。
- LSTM通过三个门结构（遗忘门、输入门、输出门）来控制细胞状态中的信息流动，可以选择性地添加或删除信息，从而实现对长期和短期信息的记忆和忘记。
- LSTM的激活函数不仅使用了tanh函数，还使用了sigmoid函数，并结合求和操作，使得梯度更容易保持稳定。
- LSTM可以更好地处理不同长度的序列数据，而RNN通常需要固定长度的输入和输出。
## 应用

### 变长文本识别

#### CNN + RNN
- CNN 是卷积神经网络，它可以有效地提取图像的局部特征，但是它对图像的整体结构和顺序不敏感，因此不适合处理变长的数据。
- RNN 是循环神经网络，它可以处理序列数据，比如文本、语音、视频等，它有记忆功能，可以保存前面的信息，并影响后面的输出。但是它也有一些问题，比如梯度消失或爆炸、计算复杂度高、难以并行等。
- CNN 和 RNN 的结合可以克服各自的缺点，发挥各自的优势。一种常见的结合方式是先用 CNN 对图像进行特征提取，然后用 RNN 对提取出来的特征进行序列生成或分类。这样可以利用 CNN 的速度和轻量，以及 RNN 的顺序敏感性。
- 识别变长的字母是一个典型的场景，它需要同时考虑图像和文本的信息。一个可能的解决方案是先用 CNN 对图像进行卷积和池化，得到一个二维的特征图，然后将特征图切分成多个一维的向量，每个向量对应一个字母的位置。接着用 RNN 对这些向量进行编码或解码，得到最终的字母序列。这样可以实现对变长字母的识别。

#### CRNN + CTC 中的 CTC 是什么

CTC，即 Connectionist Temporal Classification，是一种用于解决序列标注问题的方法。它的设计目标是允许输入序列和输出序列的长度不完全匹配，并且不需要对齐信息。CTC 在深度学习中的应用较为广泛，特别是在语音识别、文本识别（OCR）和语言翻译等领域。

要理解 CTC，可以从以下几个方面入手：

* 序列标注问题：CTC 主要用于解决序列标注问题，即将一个输入序列映射到一个输出序列，其中输入序列和输出序列之间的对齐关系不是一对一的。例如，在文本识别任务中，输入是一张包含文字的图像，而输出是对应的文字序列。

* 带有空白标记的输出空间：CTC 引入了一个特殊的空白标记，用于表示输入序列中没有对应输出的区域。通过在输出序列中插入空白标记，可以构建一个更大的输出空间，使得输出序列长度可以超过输入序列长度。

* 损失函数计算：CTC 使用一种基于对齐路径的损失函数来训练模型。该损失函数考虑了输出序列中每个时间步骤的概率分布和对应的标签，通过动态规划算法计算出最可能的对齐路径，并计算出整个序列的损失。

* 解码方法：在推理阶段，可以使用解码方法（如束搜索）来根据输出序列的概率分布找到最有可能的标签序列。解码过程会考虑到空白标记和重复标记的去除，从而得到更准确的输出序列。

总体而言，CTC 提供了一种处理序列标注问题的方法，允许输入序列和输出序列长度不完全匹配，并且无需对齐信息。通过引入空白标记和使用动态规划算法计算损失，CTC 在训练阶段可以有效地进行模型优化，并在推理阶段提供准确的输出序列解码。

CTC 的工作原理如下：

* 输入序列经过 CNN 进行特征提取和降维处理，得到特征序列。
* 特征序列经过 RNN 进行时序建模，得到隐藏状态序列。
* 隐藏状态序列经过全连接层映射到输出空间，如字符或标签的概率分布。
* CTC 损失函数根据输出序列和标签序列之间的对应关系计算损失，用于训练模型。

#### CTC 跟 交叉熵损失函数区别

CTC（Connectionist Temporal Classification）和交叉熵损失函数是两种不同的损失函数，用于解决不同类型的问题。

1. 应用领域：
   - CTC：主要用于处理序列标注问题，例如语音识别、文本识别（OCR）等。CTC 能够处理输入序列和输出序列长度不完全匹配的情况。
   - 交叉熵损失函数：广泛应用于分类问题，例如图像分类、对象识别等。它将模型输出的概率分布与真实标签之间的差异作为损失进行优化。

2. 对齐机制：
   - CTC：CTC 引入了空白标记，允许模型输出序列长度超过输入序列长度，并且不需要对齐信息。CTC 使用动态规划算法计算最可能的对齐路径，并据此计算损失。
   - 交叉熵损失函数：在分类任务中，模型的输出和真实标签是一一对应的。模型输出的概率分布与真实标签之间的差异通过交叉熵损失函数来衡量。

3. 输出形式：
   - CTC：CTC 的输出是对输入序列进行标注的结果序列，其中可能包含空白标记和重复标记，需要使用解码方法来获得最终的输出序列。
   - 交叉熵损失函数：交叉熵损失函数的输出是模型输出的概率分布和真实标签之间的差异，用于衡量模型输出的准确性。

4. 损失计算方式：
   - CTC：CTC 使用动态规划算法计算最可能的对齐路径，并将路径上的概率乘积作为损失。这样可以考虑到输出序列中的重复标记和空白标记。
   - 交叉熵损失函数：交叉熵损失函数通过计算模型输出的概率分布与真实标签之间的差异来得到损失，通常使用对数函数来处理概率。

结论：
* CTC 和交叉熵损失函数是针对不同问题的不同损失函数。
* CTC 适用于序列标注问题，允许处理不完全对齐的输入和输出序列，而交叉熵损失函数适用于分类问题，用于衡量模型输出的概率分布与真实标签之间的差异。
CTC 本身并不依赖于 RNN。CTC 损失函数只需要模型输出的概率分布和标签之间的对应关系，而不关心模型内部的结构。

##### 一个 CTC 例子
假设我们有一个变长模型，用于将图像中的文本转录为字符序列，其中序列的长度在4到6之间，字符集合包括0到9的数字和小写字母a到z。

在这种情况下，假设我们有一个输入图像，并且模型在某个时间步长上产生的概率分布如下：

时间步长1：[0.05, 0.1, 0.1, ..., 0.05, 0.05, ..., 0.05]（长度为36，表示0到9和a到z每个字符的概率）

时间步长2：[0.05, 0.05, ..., 0.1, 0.1, ..., 0.05, 0.05]（长度为36）

时间步长3：[0.05, 0.05, ..., 0.05, 0.1, ..., 0.1, 0.05]（长度为36）

时间步长4：[0.1, 0.1, ..., 0.05, 0.05, ..., 0.05, 0.05]（长度为36）

时间步长5：[0.05, 0.1, 0.1, ..., 0.05, 0.05, ..., 0.05]（长度为36）

时间步长6：[0.05, 0.05, 0.05, ..., 0.1, 0.1, ..., 0.05]（长度为36）

在每个时间步长上，概率分布都是一个长度为36的向量，表示了模型对于每个字符的预测概率。这里的概率分布只是一个示例，具体的概率值会根据模型训练和输入图像的特征而有所不同。

时间步长（Time steps）通常对应于输入数据中的序列长度或时间维度。在图像文本识别中，时间步长可以对应于图像的水平方向上的像素位置或者文本序列的字符位置。

具体来说，时间步长的决定因素如下：

1. 图像宽度：对于基于图像的文本识别任务，时间步长通常与图像宽度有关。每个时间步可对应图像中的一个像素位置或一个字符位置。

2. 序列长度：输入序列的长度也可以决定时间步长。例如，如果输入的文本序列平均长度为10个字符，可以选择一个适当的时间步长来处理每个字符的预测。

3. 模型架构：模型架构的选择也会影响时间步长。例如，一些模型可能会在卷积层中使用池化操作或步幅，从而导致时间步长的变化。

需要根据具体任务和数据特点来确定时间步长的设置。确保时间步长足够覆盖输入数据的所有位置，并且能够满足模型的需求。在一些情况下，可以根据数据统计信息或经验进行合理的选择，或者使用自适应的方法来确定时间步长。

#### 其他方案

- LSTM+CTC：这种方法使用了循环神经网络（LSTM）和联结时间分类器（CTC）来处理序列数据，可以处理不定长的文字，不需要对齐输入和输出。LSTM可以提取文字的时序特征，CTC可以解决重复字符和空白字符的问题。这种方法适用于印刷文字、验证码等场景。
- CRNN：这种方法是卷积神经网络（CNN）和循环神经网络（RNN）的结合，使用CNN来提取图像的局部特征，然后使用RNN来提取文字的序列特征，并通过全连接层生成最终的标签预测。这种方法也可以配合CTC来处理不定长的文字。这种方法适用于场景文字、手写文字等场景。
- CnOCR：这是一个开源的Python OCR工具包，支持简体中文、繁体中文（部分模型）、英文和数字的常见字符识别，支持竖排文字的识别。它自带了20多个训练好的识别模型，适用于不同应用场景，安装后即可直接使用。它还集成了场景文字检测功能，可以处理各种图片。它追求的目标是使用简单，而不是实现最新的模型算法。

## 激活函数

### SoftMax 函数
输入向量 [ 1 , 2 , 3 , 4 , 1 , 2 , 3 ] 对应的Softmax函数的值为 [ 0.024 , 0.064 , 0.175 , 0.475 , 0.024 , 0.064 , 0.175 ] 。
输出向量中拥有最大权重的项对应着输入向量中的最大值“4”。这也显示了这个函数通常的意义：对向量进行归一化，凸显其中最大的值并抑制远低于最大值的其他分量。

## 反向传播与调参

反向传播是一种用来训练神经网络的常见方法，它的基本思想是**通过导数链式法则，计算损失函数对网络参数的梯度，并根据梯度更新参数，以达到最小化损失函数的目的**。

反向传播的过程可以分为以下几个步骤：

1. **前向传播**：将训练数据输入到神经网络的输入层，经过隐藏层和激活函数，最后到达输出层并输出预测结果。
2. **计算误差**：将预测结果与真实标签进行比较，得到损失函数的值，例如均方误差或交叉熵等。
3. **反向传播误差**：从输出层开始，逐层计算损失函数对每个神经元的输出和输入的偏导数，即误差信号。这一步利用了导数链式法则，即如果Z = g(Y)而Y = f(X)，那么∂Z/∂X = (∂Z/∂Y)·(∂Y/∂X)。
4. **更新参数**：根据损失函数对每个神经元的输入的偏导数，即误差信号和激活函数的导数，计算损失函数对每个权重和偏置的偏导数，即梯度。然后根据梯度和学习率，更新每个权重和偏置的值，使损失函数减小。

## Reference

* [神经网络基础](https://www.cnblogs.com/maybe2030/p/5597716.html)
* [反向传播算法及其调参 - 详细图解，看完就能懂](https://blog.csdn.net/ft_sunshine/article/details/90221691)
* [激活函数的形象解释](https://www.zhihu.com/question/22334626)
* [SoftMax函数](https://en.wikipedia.org/wiki/Softmax_function)
* [如何理解导数](https://www.zhihu.com/question/28684811)
* [一组图诠释CNN及RNN的区别](https://blog.csdn.net/buptgshengod/article/details/78362575)
* [深度学习中的注意力机制](https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216)
* [极大似然估计](https://www.matongxue.com/madocs/447/)
* [交叉熵](https://zhuanlan.zhihu.com/p/149186719)

LSTM
* [LSTM 是什么](https://alanlee.fun/2017/12/29/understanding-lstms/)
* [BI-LSTM 是什么](https://blog.csdn.net/SunJW_2017/article/details/82837072)
