# AiNotes
AI 的摘要与思考


---

# [Defeating Nondeterminism in LLM Inference](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/)

* 现象：LLM 推理结果不一致，根本原因是浮点数加法顺序受 kernel 并行实现影响，尤其是 attention kernel 的分段加法。
* 解决方案：只要让加法顺序与 batch 大小无关（batch-invariant），就能实现完全确定性的推理。（采用“固定分段大小”的策略（fixed split-size），而不是“固定分段数”。对于同一个序列长度，无论这个序列在什么样的batch中，拆分方式都完全一样。）

* 性能 vs 确定性的权衡：
    * 非确定性策略：根据 GPU 利用率动态调整拆分，性能更好
    * 确定性策略：固定拆分规则，牺牲一些性能换取完全可复现

# [《Why Language Models Hallucinate》](https://www.arxiv.org/pdf/2509.04664)

* LLM 的“幻觉”（生成自信但错误的信息）并非单纯实现缺陷，而是训练与评估流程在统计上会鼓励“瞎蒙/猜测”而不是承认不确定性——换言之，现有的 benchmarking/leaderboard 激励把模型当“考试机器”训练，猜答案能拿高分

* 从评估层面改动：不要再用简单的二元正确/错误来主导 leaderboard；改成允许并奖励合理的置信表达（explicit confidence targets）、部分信用或使用 proper scoring rules

AI 应用启发
* 认识“幻觉”的本质：不可避免的下界，是统计学习的必然产物（尤其在低频/未见过的知识点上）。
* 改变评估与训练目标：奖励“诚实”而非“乱猜”
* 结合检索与外部验证：RAG + 校验链，让模型返回 {answer, confidence, evidence} 三元组，而不是裸文本。
* 产品层面的用户体验设计：预期管理，高风险业务辅助决策，设计 “检索失败时拒答” 的路径。

# 可解释性技术

* 稀疏自编码器（Sparse Autoencoders, SAEs）
* 激活可视化
* 注意力追踪

# LLM 的可解释性研究 > 其他多模态

* 可解释性在 LLM 领域盛行，是因为文本输出与人类逻辑、推理、知识系统高度对齐，错误成本高，需要理解决策因果；
* 而视觉、视频生成输出高维、主观性强，现有可解释性更多偏向感知可视化，而非严格的逻辑可追溯。

# [不适感让你变强大](https://desunit.com/blog/in-the-long-run-llms-make-us-dumber/)

* 明智地使用 AI，不要让它帮你解数学方程式，而要让它查看你的答案，来解释你可能错在哪里。你的原则是坚持独立思考，在这个基础上再加入 AI。

个人思考：使用 AI 做启发性的事情，自己得出结果，再让 AI Review；

经常思考困难的问题，让思想经受考验，你才能学会思考。

# 模拟实验

* 模拟 = 想象：模拟实验体现了 AI 的“想象力”，它不只是重复人类发现的知识，而是通过“再现过程”来理解规律。
* 过程优于结果：实验让 AI 不再是黑箱地直接吐答案，而是通过一个透明的“试验过程”得出结论。
* 走向具身智能：这也是“具身智能（embodied intelligence）”的重要思想——AI 不只是语言符号操作，而是与世界互动。

* [Mind’s Eye: Grounded Language Model Reasoning through Simulation](https://arxiv.org/abs/2210.05359?utm_source=chatgpt.com)

# 推理的本质

**本质**
1. 大语言模型（LLM）并不是天然的推理器，而是强大的模式匹配机器。
2. 但通过适当的提示（prompting）和方法设计，可以让 LLM 具备一定的“推理”能力。
3. 推理本质上是一种将问题拆解为中间步骤、再逐步构建答案的过程。

**方法论**
| 方法                              | 核心思想                   | 演讲示例                                           | 优势                | 可能局限                     |
| ------------------------------- | ---------------------- | ---------------------------------------------- | ----------------- | ------------------------ |
| **Chain of Thought (CoT)**      | 让模型逐步写出中间推理步骤，而不是直接给答案 | 算术题：12 个苹果卖 3 个，再买 5 个 → 先算 12-3=9，再 +5=14     | 显著提高数学/逻辑题正确率     | 如果一开始步骤错，后续全错；无法处理复杂多跳问题 |
| **Self-Consistency**            | 生成多条思维链，取多数结果，降低单次输出错误 | 复杂数学题或奥数题，生成 10 条推理链，多数投票得到正确答案                | 对抗随机性，提高正确率       | 增加计算量；仍受 CoT 输出质量限制      |
| **Least-to-Most Prompting**     | 将复杂问题拆成一系列简单子问题，逐步解决   | 3×3 网格放 2 个不同颜色的球 → 先问第一个球位置，再问第二个球位置，最后考虑颜色排列 | 在多步复杂推理问题上稳定性强    | 拆分问题依赖人为设计或提示策略          |
| **Step Back Prompting (退一步思考)** | 先让模型抽象化总结问题类型/背景知识，再回答 | 电梯楼层问题：先识别为“动态规划问题”，再写递推公式 F(n)=F(n-2)+F(n-3)  | 激活潜在知识，降低模型陷入局部错误 | 如果模型抽象错误，仍会导致结果错误；增加推理时间 |


* [The Nature of Reasoning](https://dennyzhou.github.io/LLM-Reasoning-Stanford-CS-25.pdf)

# Genie 3.0

* Genie 1：证明从图像/艺术生成“可玩场景”是可行的。
* Genie 2：增强物理与多视角，朝着“模拟器”方向发展。
* Genie 3：跨入 3D、文本驱动、长时一致，真正能作为 AI 的虚拟“世界”。

世界模型（Genie）+ 多模态大模型（Gemini）= AGI 的必要条件

# MoE
MoE 的设计目标不是为了提高同规模 dense 模型的性能，而是为了在**推理计算量不增加的**前提下增加总参数量。

* 如果发现 大部分神经元都频繁激活 → dense 模型的容量还没到冗余阶段，用 MoE 会白白增加路由噪声
* 如果发现 很多神经元只在特定输入模式激活 → 说明模型有明显“子任务分工” → MoE 能让这些子任务变成显式专家，减少无用计算

# GPT-5 发布

GPT-5 在性能和能力上有显著提升，但目前来看，它更像是对 GPT-4 系列的持续追赶和优化，尚未带来范式级的根本变革。是否能称为“范式转变”，还需要时间和更多实际应用的检验。

（但价格是真卷，比 gpt-4.1 还低）

# OpenAI 开放权重模型 gpt‑oss

* gpt‑oss‑120b 模型：有 117 B 参数，其中每个 token 激活约 5.1 B 参数；全模型共有 128 个 experts，每 token 激活 4 个专家；上下文长度达 128 k tokens。
* gpt‑oss‑20b 有 21 B 参数，每 token 激活约 3.6 B 参数；共有 32 个 experts，每 token 同样激活 4 个专家；上下文长度也是 128 k tokens。

技术优化简要介绍
1. YaRN 对 RoPE 的增强补丁，使模型能在超长文本（如 128K token）下依然保持高效和稳定。
2. 训练阶段量化代替训练后量化
3. 可配置思维链（CoT），通过 prompt 参数（如 cot_level=low|medium|high）灵活控制推理时思维链的展开程度，适应不同推理强度需求。

# LLM 工具调用与异步推理猜想

1. 当前现状
* 现在 LLM 每次调用工具都要重新发起一次 API 请求，思考被中断，token 成本高。
* 像 LangGraph 这样的框架只是用状态机封装了这种多轮过程，本质上仍是“断点式”推理。

2. 设想
> 能否让 LLM 在**一次推理中暂停 → 等待工具 → 再继续思考**？做到真正的「思维不中断」。

3. 技术难点
* Transformer 架构不支持推理中“暂停+续写”。
* 训练数据没教会模型怎么等待工具再继续思考。
* 推理引擎不支持异步插入外部结果。

4. 战略意义
* 实现这种异步能力可以**大幅提升智能体性能与效率**。解锁长链条任务的真正自动化，极大节省 token 成本 + 推理延迟，极大提升智能体协作能力


# LLM 可解释性研究促进大脑神经科学进步？
LLM 的可解释性可以借鉴传统大脑神经科学的研究方法论，反过来有没可能促进大脑神经科学的研究？

交叉案例：
* 稀疏编码理论：最早由神经科学家提出，后来被 AI 领域广泛应用，反过来又用于解释大脑视觉皮层的编码方式。
* 可解释性工具：如“特征可视化”“神经元激活分析”等，已经被用于分析真实大脑的神经元活动模式。
* AI 模型辅助脑科学：AI 被用来分析大规模脑成像数据、预测神经元活动、模拟认知过程等。

# [LLM 可解释性](https://www.anthropic.com/research/tracing-thoughts-language-model)
* 核心机制：追踪模型在推理/生成过程中，内部信息（特征、概念、激活模式）是如何流动、转化和组合的。
* 本质：不仅仅是找一个方向，而是还原出一条条“思维链路”或“电路”，揭示模型内部的“因果路径”和“信息流”。
* 技术上：tracing 结合了多种可解释性技术，包括但不限于：
* 特征提取（如字典学习、PCA、ICA等无监督方法）
* 有监督的特征方向（如人格向量）
* “电路追踪”/“路径分析”/“因果干预”等

## [人格向量](https://www.anthropic.com/research/persona-vectors)
* 核心机制：通过对比有/无某种性格特质时的神经元激活，提取出一个“人格向量”。
* 本质：这是“有监督的特征方向提取”，关注单一特质的激活模式。
* 技术上：更像是“差分分析”或“投影”，而不是字典学习。

不良人格约束
* 推理时抑制（Inference-time Steering）
    * 在模型推理（生成答案）时，如果发现某种不良性格特质（如“阿谀奉承”、“邪恶”等）的人格向量被激活，可以人为地减去（反向注入）这个人格向量，让模型在这个方向上的表现变弱。
* 训练时预防（Preventative Steering / “疫苗”原理）
    * 在训练阶段，通过“注入”不良人格向量，让模型把“邪恶”这种特质变成了一个明确、集中的特征方向，而不是分散在网络各处、难以追踪和控制。这样，“邪恶”特质就像被“收拢”到一个专门的“开关”上，模型参数学会了“即使有这个开关，也能输出安全内容”。


## [字典学习](https://www.anthropic.com/research/mapping-mind-language-model)
无监督的特征方向提取

字典学习的核心：
* 自动找出一组“基础部件”（字典）
* 用这些部件的稀疏组合来还原所有数据
* 通过不断优化，让字典越来越能代表数据的本质

在大模型应用中：
* 收集大量神经元激活数据
* 用字典学习算法分解为“特征激活模式”（训练）
* 通过分析和操控（放大或者抑制）这些特征，实现对模型内部机制的解释和理解

# LLM 的智能来源
* Compression（压缩）
* Composition（组合）
* Retrospection（反思）

其他智能：Emergence (涌现能力)，Reasoning (推理)
# Transformer 理解

1. Embedding + 位置编码（绝对/相对/旋转/可学习，RoPE 旋转编码能够表示相对关系）
2. 稀疏/多头/线性自注意力（QK 关系）
3. 注意力权重加权 V（全局信息融合）
4. 前馈网络（FC/MoE/门控/分离）
5. 归一化与残差（LayerNorm/Residual）
6. 正则化与高效训练（Dropout/蒸馏/混合精度）
7. 只在推理使用：输出与解码（高效采样/KV Cache）

# LoRA(low-rank-adapter) and ControlNet

* LoRA
假设你有一个庞大的神经网络，其中一个线性层的参数是一个很大的矩阵 W（比如 4096 × 4096），直接 fine-tune 需要更新这么大的矩阵，成本很高。
LoRA 是通过“低秩分解”的方式，在不动大模型的基础上，用极少参数（A×B）学习新任务，就像在不换设备的前提下加个外挂模块。

```
W_new = W + ΔW
ΔW = B @ A    # A 是低秩（r×d），B 是（d×r），r 远小于 d
```
* ControlNet

ControlNet 是在旁边复制了“一个小副脑”，专门接收“视觉提示”，然后每层通过管道把控制信号注入主脑。

```
Encoder (Down) → Bottleneck → Decoder (Up)
```

```
ControlNet:
  [control image] → 复制的 UNet (副脑)
                           ↓
          +----------------+----------------+
          |                 hint injection   |
          ↓                                  ↓
Original UNet (主脑) ←←←←←←←←←←←←←←←←←←←←←←←←
          ↑
   [latent vector from text + noise]
```

* 其他 fine-tuning

例子：resnet 最后一层 fc（全连接层）换成新的，用于识别更多类别（比如从 1000 类 → 102 类花）

学名：Full Fine-tuning with a modified classification head

# AI 安全 - 思维链监控

举例：一个写 Python 的 AI 被要求实现某功能，在中间思维链中它曾思考“如果直接用 eval() 可以作弊通过测试”，但最终它没用。只看输出是无法发现这类意图的。

* 思维链掩饰问题（Steganographic CoT）：模型可能为了“过审”而主动隐藏有害意图（比如写一个非常无害的 CoT，但最后行为仍执行恶意操作）。
* CoT 与行为不一致问题（CoT-behavior mismatch）：研究发现部分模型“思维链写得像个圣人”，但行动像个骗子。
* CoT 混淆：模仿式 vs 真实 reasoning：模型可能只是模仿 CoT 格式写出“看起来有逻辑的输出”，而不是在真实使用它作为推理。

| 建议方向                        | 描述                                  |
| --------------------------- | ----------------------------------- |
| 🧪 微调模型强化显性思维链输出            | 强制模型在所有复杂任务中提供 reasoning trace      |
| 🤖 弱模型审查强模型                 | 类似“GPT-4 审问 GPT-5”思维链的结构化审查机制       |
| 📊 设定 CoT 质量与行为一致性的指标       | 类似“计划一致率”、“奖励误导率”等安全评估指标            |
| 🔐 结合工具链、API 调用路径等多模态数据进行监督 | 不光看语言，还看行为路径、调用链等多维线索               |
| 🚨 将 CoT 监控作为部署前安全审核必要项     | 和 RLHF 或 red teaming 一样，成为标准安全流程一部分 |

* Reference
* [白盒监控的提出与实验结果](https://openai.com/index/chain-of-thought-monitoring/)
* [tracing-thoughts-language-model](https://www.anthropic.com/news/tracing-thoughts-language-model)
* [大型联合呼吁：“思维链的可监控性”是当前窗口期](https://arxiv.org/abs/2507.11473)

# AI IDE 的演进猜想

需求文档本质上就是“新形态的代码”，而传统代码只是对需求的一种有损投影。因此，需求分析与编码这两个工种很可能会逐步融合，以减少信息转化的损耗。

如果将 LLM 视为新一代操作系统，它或许可以跳过“将 spec 转换为 code 并编译执行”的流程，直接理解并运行需求本身。在这种演化路径下，AI 编辑器的核心功能也将从编写代码转向管理和演化 spec，实现真正的“以意图驱动开发”。

阶段：code → code with LLM assist → spec-driven code（当下 AI IDE） → spec-driven execution

# 何时需要 Agent，而不是 Call LLM

* 本质：许多“Agent”系统其实只是复杂版 prompt 拼接器的本质。
* 一句话区别：LLM 是智能核心；Agent 是任务流程管理器。 —— 多数时候用好 prompt 和工具组合就够了，只有自动化高复杂任务时才值得用 Agent 架构。
* 如何选择：当任务涉及非线性的多步骤流程、步骤之间存在复杂的条件依赖、需要动态规划和决策、以及可能进行试错和自我修正时，Agent 模型是更优的选择。对于指令明确、工具调用路径相对固定的场景，直接使用大语言模型（LLM）结合工具就足够了。
* 具体场景：steps 是非线性，steps 之间依赖条件工具调用

# LlamaIndex 是多层索引体系（Index over Index）
LlamaIndex 是数据知识库的“索引协调器”，它不替代数据库，而是组织向量索引 + 文本内容 + Metadata + LLM 调用的多层索引系统，构建一个 Agent 可检索、可记忆、可回答的知识结构。
```
原始文档
   │
   ▼
  切分器（chunking / NodeParser）
   │
   ▼
Chunk（Node） ←→ Metadata
   │             ↘
   ▼              ▼
Embedding       文本数据库（DocStore）
   │
   ▼
向量数据库（VectorStore，如 FAISS / Qdrant）
   │
   ▼
LlamaIndex 的顶层索引（VectorStoreIndex） ←→ Retriever / QueryEngine
```
例子：“我给一段 query，查找与之最相似的旧 issue 说明文”
```
用户 query 文本
   ↓
Tokenizer + Embedding（BGE）
   ↓
VectorStore（如 FAISS）→ 找出相似向量（返回 ID）
   ↓
DocumentStore → 根据 ID 找回原始 chunk 文本 + metadata
   ↓
LlamaIndex Index → 合并上下文、拼接 Prompt
   ↓
LLM 回答 or 做分类 / 推理
```


# Cursor LSP 增强架构

传统LSP：
```
编辑器 ←→ Language Server ←→ 代码分析引擎
```

Cursor 增强架构：
```
编辑器 ←→ AI智能层 ←→ LSP ←→ 代码分析引擎
                ↓
            语义索引库
                ↓
            上下文管理器
```

# LlamaIndex + LangChain
```
 原始数据             用户提问
   ↓                    ↓
 LlamaIndex         LangChain
(切分 + 向量召回)     (Prompt 构造 + 多步调用 + Tool 调度)
   ↓                    ↓
【高质量知识片段】 → 拼接 → 模型输入 → 模型输出
```

# LLM对教育的颠覆到底有多大?

LLM对教育的颠覆，不是“工具层”的提升，而是“范式级”的改变。它会彻底改变“学什么、怎么学、谁来教、学到哪为止”这四个教育根基。

| 维度  | 传统范式  | 被LLM颠覆后    |
| --- | ----- | ---------- |
| 学什么 | 知识为主  | 能力与提问为主    |
| 谁来教 | 老师中心  | 多AI协同+人类引导 |
| 怎么学 | 教材+考试 | 对话+共创+定制反馈 |
| 学到哪 | 学历为终点 | 终身学习+AI伴学  |

# “暗知识”（Dark Knowledge）是什么？

“暗知识”是深度学习中一个非常重要但不直观的概念，最早由 Geoffrey Hinton 提出。它指的是：模型中“没有明确标签”的那些知识。

具体而言：

一个分类模型学到的不只是“答案对不对”，还学到了其他类之间的“相似性结构”。

这些知识不会显式体现在训练标签中，但却保留在模型的内部权重中。

例如：

一个猫 vs 狗的模型，虽然只输出0/1，但它在内部可能知道“狐狸长得也像狗”。

它没有学过“狐狸”，但“感知到了类似性”——这就是暗知识。

# AIGC产生的内容“反哺”模型训练会发生什么?

≈ AI的“近亲繁殖”
就像基因多样性丧失会导致家族退化，语言模型只从自己身上学东西也会变得“封闭”、“退化”、“失真”。

一个更哲学的问题：是否可以“自我进化”？
* 如果AI足够强大，并具备自我校正、事实验证、知识迁移能力，理论上是可以实现某种“自我成长闭环”的
* 但当前阶段，没有外部人类校正和监督的 AI，无法长期稳定进步（这和人类文明演化中“对照现实、试错迭代”的方式类似）

# 扣子

* 扣子本质上就是一个定位在“业务友好型”的低代码平台，其核心能力更偏向传统前端 low-code，而非真正意义上的 AI-native 架构工具。
    * 对开发者来说，扣子容易“鸡肋”：简单场景嫌它多余，复杂场景又容易踩坑。
    * 它更适合那些“需求明确 + 快速上线 + 不考虑扩展性”的轻量内部场景，而不是构建真正需要持续演进的系统。

# Thinking Mode

* 按步骤思考（step-by-step reasoning）
* 思维链条（Chain-of-Thought）
* 自主规划与内在对话（e.g. scratchpad, inner monologue）
* 过程显式化、假设验证、推理链延伸

这类 “思考方式”不是被模型“硬编码”的，而是通过数据和训练方式诱导出一种生成偏好，让模型更倾向于展开推理。

# LLM AS a judge
* 如果是它自己写的答案，怎么可能它在判断对错时更准确呢？因为这不是同一种任务，判断任务（**明确标准或事实依据的情况下**）会激活模型更擅长的事情，毕竟判断比生成任务更加容易，任务越容易，准确率越高
* “LLM-as-a-Judge”：一种受控的、评估文本质量的技术，它本质上是一种**基于指令和模式的模仿**

[LLM as a judge](https://www.evidentlyai.com/llm-guide/llm-as-a-judge)
# 单/双编码器
* 单编码器：
    * 一个上下文： 在单编码器模型中，查询（query）和文档（document）会被拼接起来，形成一个统一的输入序列。它们在同一个上下文窗口内被编码器处理。
    * 深度交互： 模型可以利用其内部的交叉注意力（cross-attention）机制，让查询中的每一个词与文档中的每一个词进行交互，从而捕捉它们之间所有复杂的、细粒度的关系。这种深度交互使得它在判断两个文本的精确关系时非常准确。
    * 例子： BERT 的下一个句子预测、问答系统（判断一个段落是否包含某个问题的答案）。
* 双编码器：
    * 两个独立上下文： 在双编码器模型中，查询和文档是分别输入给各自的编码器（即使编码器权重共享），各自独立生成 embedding。它们不在同一个上下文内进行交叉注意力。
    * 无直接交互： 模型无法在编码阶段进行查询和文档之间的词级别深度交互。它依赖于每个文本的独立语义表示。
    * 目标： 通过对比学习训练，模型被教导将语义相关的查询和文档映射到同一个向量空间中相近的位置。这样，即使没有直接交互，它们的向量距离也能反映语义相似度。
    * 例子： 语义搜索、推荐系统、向量检索。eg: CodeRankEmbed
# AI 客户端端的发展趋势
* 链接更大的外部世界
    * 例如 cursor 对 外部 MCP 工具的调用，以及对 外部文档的索引
* 压缩上下文（更少的 token 使用
    * 例如 cursor 对 database 的索引构建
* 边缘 AI 与本地处理
* 个性化与自适应学习
    * 例如 cursor 根据用户操作自动生成 rules
* 增强的人机协作与迭代优化（从“一次生成”到“共同创造”）

# AIGC 发展的一些难点

1. 能一次性能够从 0 -> 1 更高效的完成任务（类似 lowCode，或者 no code；
    * 难点：如何维护和二次编辑这些一次性生成的结果。
        * 图像生成：不再有图层、蒙版和可单独操作的对象（传统 PS
            * 解决方案：微调，局部重绘/扩图（in-painting/out-painting）（编辑图像的特定区域）或使用控制网络（如ControlNet）
        * 代码生成：可能缺乏结构化的输出
            * 解决方案：约束成结构化输出（代码可能通过文件结构约束

# LLM 评估
1. SWE-bench 评估
* GitHub Issue修复   | 2294个真实项目问题
* 评分只能代表这个 LLM 在提供的 Agent 下的表现能力，只能做纵向比较，而不能做不同品类横向比较（比如 chatgpt 跟 gemini 的 agent 设计上就可能不同）

2. **Aider Polyglot-Diff**  
   - 核心：**最小化代码变更**（只改必要行）
   - 多语言一致性：Python/JS/Java/Go/Rust

* reference
    * [gpt-4.1 intro](https://openai.com/index/gpt-4-1/)

# AI Agent

LLM 在逐步吸收和内化“规划”、“ReAct”和“工具使用”的核心能力。这种趋势使得LLM能够成为更加自主和多功能的AI代理。那么 Agent = LLM + Planning + reAct + tools 还是否成立？

未来 Agent 是何种形态？Agent ≈ (高度集成且内化了规划、ReAct和工具使用的) LLM + 强大的记忆系统 + 多模态感知与交互能力 + (可选的) 多智能体协作 + 物理世界具身能力 + 内置的安全与对齐机制。

# LangGraph 是为了自主可控
LangGraph 提供了显式且可控的编排能力。它不是让 LLM 完全自由地去规划一切（LangChain），而是让 LLM 在你设计的“导航图”中做决策。这使得你在处理复杂、需要严格控制流程、或有确定性循环和分支的任务时，能够有更高的可预测性和健壮性。

# chatGPT vs Gemini
* chatGPT 4o 会在回答结尾附加一个问题，引导用户继续提问
* Gemini 2.5 Pro 会给出简洁的回答，很少去引导交互

chatGPT 的商业模式是基于用户提问次数收费，这是主业务，所以针对性做引导继续提问的训练），而 Gemini 当前阶段首要目标可能是服务 google 内部的各种产品，所以需要简洁的回答？

2025-04-27
# AI CodeReview 感悟

* RAG，retrieval augmented generation，实际是对 diff 代码的与提示词组装成**非结构化** context 投喂给模型得出想要的**结构化数据**
* 越是弱的模型，越需要好更加具体的提示词来引导，比如 runtime error 意味着哪些场景？
* 不同时期对于提示词（可调节）的要求不同，比如早期模型可能需要关注 runtime error 的场景，而后面可能关注 performance 的场景

# ChatGPT 图像-安全性-出处查询
通过 C2PA 和内部可逆搜索查找出处
* C2PA 是“身份证”，明确写在文件里，别人可以直接查。方法：
    * 图像的 meta 信息部分
* 内部可逆搜索 是“DNA 检测”，即使你撕掉身份证，也可以通过生物特征确认你的身份。（使得像素分布符合特定模式）可能的方法：
    * 模型指纹（Model Fingerprinting）
        每个 AI 生成的图片或文本都可能带有独特的统计特征（如噪声模式、像素分布等）。
        即使去除了元数据（如 C2PA），OpenAI 仍然可以通过这些特征识别内容是否来自 GPT-4o。
    * 隐式水印（Invisible Watermarking）（可能的技术之一）
        某些 AI 生成内容可能使用不可见的水印，例如：
        轻微调整像素值，使其符合特定数学模式，但不影响肉眼可见的内容。
        类似于 Google DeepMind 开发的 SynthID 技术，嵌入“难以去除但可识别”的水印。
    * 神经网络特征分析（AI-Generated Content Detection）
        AI 生成的图片、文本、代码往往有特定的模式（如 GPT 生成的代码可能更符合语法规范但缺乏创意错误）。
        通过训练检测模型，OpenAI 可以用机器学习方法来判断某个内容是否由 GPT-4o 生成，即使没有元数据。

# [知识图谱原理](https://chatgpt.com/share/67e3db1c-3314-800f-a04d-8e232d2bce0e)
# [大模型反编译代码](https://ghuntley.com/tradecraft/)
# AI 对技术传播的双重影响
是促进，还是限制？如果大家都使用大模型解决技术问题，而大模型又是基于已有数据（旧技术）训练，新技术的应用会落后，没人使用，会导致网上有关新技术的信息很少，导致 AI 缺乏新技术的训练数据，那新技术就很难被普及？
* [AI 是阻碍技术传播的元凶](https://vale.rocks/posts/ai-is-stifling-tech-adoption)
# world labs 图片生成 3D 视频
[demo](https://www.worldlabs.ai/blog)
# [supervision](https://github.com/roboflow/supervision)
# [AI 图片识别中的视觉注入(visual-prompt-injections)](https://www.lakera.ai/blog/visual-prompt-injections)
1. The Invisibility Cloak(隐身斗篷)
2. I, Robot(我，机器人)
3. One advert to rule them all(一个广告统治他们所有)

* [GPT-4 Vision 提示注入](https://blog.roboflow.com/gpt-4-vision-prompt-injection/)

# [奇异值分解](https://www.cvmart.net/community/detail/4092)
* 奇异值分解（SVD）是一种矩阵分解技术，它将一个矩阵分解为三个矩阵的乘积：一个正交矩阵、一个对角矩阵和一个正交矩阵的转置。
* 奇异值分解在数据降维、图像压缩、推荐系统等领域有广泛应用。

# AI DOOM
* 传统的电子游戏开发流程，通常涉及复杂的预设逻辑，遵循用户输入、更新游戏状态、渲染画面的固定循环。
* DeepMind提出了一种革命性的想法 - 通过生成式AI模型完全抛弃这些预设逻辑，依赖AI实时生成游戏的内容与状态更新。
* [AI技术颠覆游戏开发：谷歌DeepMind GameNGen实时生成《DOOM》探秘](https://blog.csdn.net/weixin_41496173/article/details/141937965)

# [人工智能的现状，任务，架构 与统一](http://www.stat.ucla.edu/~sczhu/Blog_articles/%E6%B5%85%E8%B0%88%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.pdf)

# 视频生成训练的演化?
关键帧（+描述） + 插值 -> 端到端的学习

分镜技术 + patches 技术

# 一种快速理解模型的方法
从损失函数开始
* U-Net 是像素级别的分类损失跟 dice_loss 组成
* Segment Anything Model (SAM) 使用了一种基于交叉熵的多任务损失函数，其中包括了像素级别的分类损失和边界框级别的回归损失
# 边界框回归损失

# 机器学习基础数据集
* 图像分类领域：MNist, Imagenet, CIFAR 10 等
* 自然语言处理领域：IMDb Large Movie Review Dataset 等
* 图像分割：COCO
[reference](https://juejin.cn/post/6844903826680446990)

# 神经网络逐步取代 SVM 原因

为什么我们大部分使用神经网络进行分类而非SVM？因为核函数是我们手动设计的固定算法进行特征提取，就如同图像处理早期的手动设计的卷积核一样，无法训练，这样的方法能力上限有限，所以就被神经网络淘汰了。

# 人脸识别与普通图像识别的训练区别
* 人脸类别太多（可能成千上万），每个类别数据量太少；而且检测同一人的人脸应该聚类，即使在面部表情、光照、头部姿态等方面有极大变化。
    * SphereFace（Angular Softmax Loss）提出了一种新的损失函数 -- A-Softmax 损失，这种损失函数专注于学习每个人脸类别在角度边界上的分布。具体来说，这种损失函数强调同类别的人脸相似度和不同类别的人脸的区别，有助于模型把相同人的面部图像映射到相近的位置，将不同人的面部图像映射到远离的位置。
# scaling law
* LLM中的Scaling Law（比例律）是指， Language Model（语言模型）的性能会随着模型规模的增加而指数增长
* 生物学：代谢率，心跳频率，生物体的结构支持系统，生物的寿命，运动能力等与体型关系
* 社会性：人群规模对社会行为和现象；例如，城市规模与城市发展、社会交流以及资源利用之间可能存在一些规模效应
# Towhee 框架
是一个用于处理非结构化数据的框架，它利用最新的机器学习模型来创建 ETL（提取、转换、加载）流水线。非结构化数据是指无法存储在表格或键值对格式中的数据，如图像、视频、文本等。
# VGG 网络
视觉几何组（Visual Geometry Group）所开发，VGG网络使得网络设计的理念发生了重要转变，即通过重复使用简单的层结构（3x3卷积核和2x2池化层）并深化网络结构，来提高性能。VGG网络同时还证实了深度是实现优秀性能的关键因素之一。
# 感知损失（perceptual loss）
也称为内容损失（content loss），是一种在深度学习特别是在视觉相关任务中使用的损失函数。它不同于传统的像素级损失函数（例如L1损失和L2损失），感知损失更注重于图像内容的感知相似性而不只是像素值的相似性。

* 例子:假设我们正在进行一个图像风格迁移任务，其中目标是将一幅图像的风格（如梵高的画风）迁移到另一幅图像上，同时保留图像的内容。理想情况下，生成的图像应该在视觉上看起来要有梵高笔触风格的颜色和纹理，但同时能识别出原图的内容（如城市的轮廓、天空的位置等）。
在这个任务中，如果使用像素级损失，那么模型可能会非常注重确保生成图像在像素层面与原图尽可能接近，而忽视了风格上的转变。这可能导致风格迁移效果不明显。

* 实现方式：如果使用感知损失来训练模型，我们会首先通过一个预先训练好的深度CNN（如VGG网络）传递原图和生成图，然后计算这两幅图在某些内部层激活值的差异。这些层的激活值代表了图像的高级特征，所以这种差异反映了它们在内容和感知上的相似度。最小化这种差异可以鼓励生成的图像在视觉感知层面上更贴近原图的内容，同时也有目标风格的特质。

* 适用任务：风格迁移、超分辨率和图像合成等。

问题：哪一图层是提取的风格信息？

>>在CNN中，随着层级的加深：
* 初始层主要捕捉基础信息，如边缘和颜色。这些层对图片细节的响应很敏感，但并不捕捉具体的风格信息。
* 中间层捕捉更复杂的特征，如纹理和图案，这些正是构成图像风格的要素。
* 深层则表示更高级的内容，例如图像中的对象和整体布局。

# [stable diffusion](https://zhuanlan.zhihu.com/p/632809634)
* stable diffusion 是一个生成模型
    * 目标：用降噪网络生成清晰的图像（加噪是辅助训练降噪）
    * 类比：类似 GAN 模型，只不过 GAN 是个步骤的对抗训练，而 SD 是一个多步骤
    * 方法
        * 加噪过程（前向过程）是一个马尔可夫链，它逐步将随机噪声添加到数据中，直到数据变成纯噪声。这个过程是可控的，因为我们知道每一步加入的噪声量。通过这种方式，模型可以学习在任何给定的时间步骤预测噪声的分布。
        * 降噪过程（逆向过程）中，模型使用在加噪过程中学到的知识来预测噪声，并从噪声数据中去除这些噪声，逐步恢复出清晰的数据。如果没有加噪过程，模型就没有机会学习这些噪声分布的信息，也就无法有效地进行降噪和数据重建

疑问
    * SD 中将加噪图片输入U-Net中预测噪声如何理解？
        * "预测噪声"是扩散模型的一部分，它涉及到将图像从含有噪声的状态逐步恢复到清晰的状态。SD模型首先将一张完全随机的噪声图像（或者是经过一系列噪声添加步骤后的图像）输入到U-Net中。U-Net的目标是预测这张噪声图像中的原始噪声成分。一旦预测出这些噪声，模型就可以从噪声图像中去除它们，从而使图像逐渐变得更清晰。
    * 降噪过程中的预测噪声跟实际噪声的对比，这里的实际噪声是从加噪中得到的么？
    * 什么是噪声分布信息？
        * 噪声类型（高斯噪声等），噪声参数（均值方差），时间依赖项（噪声的量和性质会随着时间步骤的推进而改变）

# [神经网络可解释性](https://zhuanlan.zhihu.com/p/479485138)
# 增量学习
理论：增量学习的核心在于模型能够通过不断学习新数据来提升自身的性能，即使这些数据是由模型自身已经准确识别过的。源于人类的终身学习能力，即不断获取、调整和转移知识的能力，同时避免灾难性遗忘——即新知识的学习对旧知识造成的干扰。

思考：意味着模型识别准确的新数据再来投喂给模型本身训练也能提升模型准确率？
# data-centric AI 
Data-centric AI is the discipline of systematically engineering the data used to build an AI system. — Andrew Ng
* [SAM  data-centric AI](https://www.zhihu.com/question/521096166)
# 核函数
kernel function or kernel trick
* 概念：将原始空间中的向量作为输入向量，并返回特征空间（转换后的数据空间,可能是高维）中向量的点积的函数称为核函数。
    * 简单理解：一种便捷的计算在高维空间里的内积的方法。高维空间的数据计算存在困难。所以替代方案是在特征空间中计算相似度度量，而不是计算向量的坐标，然后应用只需要该度量值的算法。用点积(dot product)表示相似性度量。
* 歧义：把数据从低维映射到高维的是映射函数而不是核函数

* [带例子的核函数解释](https://www.zhihu.com/question/24627666/answer/28440943)
* [核函数概念](https://blog.csdn.net/mengjizhiyou/article/details/103437423)
# 嵌入层
嵌入层是一种将离散值转换为连续向量的技术；

例如：torch.nn.Embedding(10, 5) 将创建一个嵌入层，该层可以将离散值（例如：[1,'a','你好']）映射到连续向量空间中的 5 维向量。
* 如何理解这里的离散跟连续呢？
    * 向量里的每一项是否连续，决定了向量本身是连续的还是离散的。例如：[0.2,0.3,0.5] 是一个连续向量，因为它的每一项都是实数，而实数是连续的。 这样的向量可以进行连续的运算和比较，比如求和，求差，求点积，求模长等。
    * 而离散是指每一项都不是连续的。例如，[1,‘a’,‘你好’]是一个离散向量，因为它的每一项都是离散的，而且不能进行连续的运算和比较。中的1不可能是’a’，因为它们属于不同的集合，也没有定义它们之间的转换规则。
# 机器学习与英语学习
* 机器学习的内容输出可以类比为通过沉浸式学习英语后能够说出英语，其中另一种方式是通过先学习词法和语法规则。在机器学习中，模型通过大量的数据输入（数据投喂）来理解其中的潜在规律和特征。类似地，通过沉浸式学习英语，我们可以在大量的语言环境中感知和理解英语的潜在规律和特征，从而能够流利地说出英语。
* 早期的人工智能（通过条件语句进行判断然后做输出）与先学习词法和语法规则的方法与相似。早期的人工智能系统通常使用预定义的规则和条件语句来处理输入并生成输出。这些规则和条件语句基于词法和语法规则，用于处理特定的输入情况。类似地，通过先学习词法和语法规则，我们可以在语言学习中掌握词汇和语法规则，并使用它们来理解和生成语言。

两种机器学习区别：通过先预定义的规则和条件语句的方法在一些特定场景下具有**精确性和可解释性**的优势，而通过机器学习从数据中学习的方法则**更加灵活、适应性强，并能够处理复杂情况**。

两种英语学习区别：通过先学习词法和语法规则来学习英语可以提供**结构化学习和准确性**，但**缺乏实际应用，语感欠缺，学习繁琐**，而沉浸式学习英语则更加贴近**实际应用、注重流利性和文化融合**，但**需要环境支持，初始困难**。

思考：意味着初始的时候先了解基本语法，后续不断去读各种精选文章（而不是研究更深的语法规则）才是英文学习的最佳路线？而机器学习没有初始化烦恼，所以直接去学海量数据就好？

# GAN，VAE，Diffusion 生成模型理解
## 潜空间
* GAN 是先随机一个符合高斯分布潜在空间作为Generator输入生成图片，然后投喂给Discriminate 作为输入判定然后做 反向传播；
* VAE 是通过训练 Encoder 将输入映射到复合高斯分布（实际通过训练得到均值和方差）的潜在空间，然后解码器通过对潜在空间解码得到输出；数据先降维再升维
    * 在 MNist 中潜在空间可以是20维长度的向量，来表示 20 个不同的均值和方差分布，来代表 20 个可能的特征？
* Diffusion模型和其他生成模型一样，实现从噪声（采样自简单的分布）生成目标数据样本。
    * 核心原理是通过一个随机的前向过程（Forward Process）和一个去噪的逆向过程（Reverse Process）来实现从噪声（Noise）到目标数据样本（Data Sample）的转换。

## Diffusion vs GAN
* 速度：Diffusion 需要多步骤到图片，而 GAN 是一步到位；所以Diffusion 会慢，但是训练过程也更稳定
* 应用面：扩散模型可以利用多种条件来控制生成的图像，比如文本描述、图像掩码、深度图等，而 GAN 通常只能利用类别标签或噪声作为条件。这使得扩散模型可以更灵活地应用于不同的任务，比如图像编辑、图像修复、图像翻译等


# Diffusion 模型过程

* 初始噪声：从某个先验分布中生成初始噪声信号。
* 扩散过程：通过一系列步骤，将当前噪声信号逐渐扩散，以生成下一个时间步的噪声信号。这个过程中使用了逆扩散方程，可以将当前步骤的噪声信号映射到上一步骤的噪声信号。
* 逆扩散采样：通过逆扩散过程中的采样操作，将当前噪声信号转化为以下一步的噪声信号。
* 生成器网络：使用生成器网络将当前噪声信号映射回高维空间，生成一帧图像。
* 损失函数与优化：根据生成图像与目标真实图像之间的差异，定义适当的损失函数，并通过反向传播和优化算法来更新生成器网络的参数。

# 理解 VAE

“局部是由于方差控制，而全局是由于均值控制”这句话强调了在VAE模型中，方差向量主要影响生成数据的局部细节和变异性，而均值向量则决定了数据在潜在空间中的全局分布和结构。

* [VAE理论跟数学](https://zhuanlan.zhihu.com/p/144649293)

* 变分推理
* KL 散度
    * 在训练辨别模型时，为了简化计算，人们往往直接对交叉熵进行优化。 而在 在训练生成模型时，为了使分布与相互接近，我们必须直接对KL散度进行优化。
    * [自信息、熵、交叉熵与KL散度 的推导](https://zhuanlan.zhihu.com/p/345025351)
    * [KL-Divergence 与交叉熵](https://blog.csdn.net/Dby_freedom/article/details/83374650)
    * [KL 散度形象说明（翻译）](https://www.jianshu.com/p/43318a3dc715)
* [EM——期望最大 算法](https://zhuanlan.zhihu.com/p/78311644)
* 交叉熵
* 贝叶斯定理
* 自由能

# 深度学习优化器

优化方向

    * 基于动量（NAG）
    * 基于自动学习率 （例如 RMSprop，配置训练简单）
    * 结合两者的（Adam 一般最优）

为什么 WGAN 选择 RMSprop 作为优化器，而不是 Adam?
>> WGAN的目标是通过最小化生成器和判别器之间的Wasserstein距离来提高生成样本的质量。传统的生成对抗网络（GANs）在训练过程中容易出现梯度伪影的问题（告诉你错了，但并没有指出错在哪里，导致更新方向错误，把正确改掉，错误留下），即判别器的梯度无法提供有关生成器当前状态的准确信息，导致训练不稳定。RMSProp优化算法通过自适应地调整学习率来减轻梯度伪影问题，有助于更稳定地训练WGAN。

* [深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)

# 反向传播
有一个简单的神经元函数 y = w * x，模拟计算梯度和进行反向传播的过程。

一次权重更新过程（给定初始数据输入 x = 1，w = 10，实际输出是 10，期望输出 y1 = 2 则 目标 w 为 2）：
```
输入 x = 1
初始权重 w = 10
计算输出 y = w * x = 10 * 1 = 10
```
计算损失：
```
给定期望输出 y1 = 2
计算损失函数 loss = (y1 - y)^2 = (2 - 10)^2 = 64
```
计算梯度：
```
损失函数对权重 w 的梯度：dL/dw = 2 * (y1 - y) * (-x)
将具体数值代入：

dL/dw = 2 * (2 - 10) * (-1) = 2 * (-8) * (-1) = 16
因此，梯度为 dL/dw = 16。
```
反向传播：
```
使用梯度下降法更新权重 w：
w = w - learning_rate * dL/dw
假设学习率（learning rate）为 0.1，将梯度代入：

w = 10 - 0.1 * 16 = 10 - 1.6 = 8.4
更新后的权重为 w = 8.4。
```
# CGAN MNIST 训练步骤
1. 固定 generator （ real_label = [batch_size, 10] 的对真实 label的 one-hot 编码 ）
    * 用真数据训练 output_label = Discriminator(real_image)，d_real_loss = BCELoss(out_label,real_label)
    * 用虚假数据（噪音 + 真实标签 [batch_size, noise_dim（满足0~1正态分布）] + real_label = [batch_size, noise_dim+10] = z_tensor ）
    * 训练 fake_image = Generator(z_tensor) 得出 fake_image( Tensor[batch_size, 1, 28, 28])
    * 再次 out_label = Discriminator(fake_image) ，d_fake_loss = BCELoss(out_label,fake_label(全0))
    * 计算 D_loss = d_real_loss + d_fake_loss 反向传播，更新 Discriminator
2. 固定 discriminator
    * 由 fake_image = Generator(z_tensor)
    * 由 Discriminator(fake_image) 得出 out_label ，g_loss = BCELoss(out_label,real_label)
    * 计算 G_loss = g_loss 反向传播，更新 Generator

# 预处理

* 归一化：一种常见的图像预处理操作，它用于将图像的像素值归一化为均值为0、标准差为1的分布，或者只将数据收窄到 -1 ~ 1 之间。常用于 CNN 网络数据预处理
    * 加速训练：常用的激活函数如 Sigmoid 和 Tanh 在输入值较大或较小的区域会饱和，导致梯度接近或完全为零，从而使梯度下降变得非常缓慢或停滞。通过将像素值缩放到 -1 到 1 的范围，可以使输入值位于激活函数的线性区域，避免梯度饱和问题，提高网络的训练效果。
    * 模型稳定性：在优化算法中，例如梯度下降法，较大的梯度值可能导致参数更新过大，从而使优化过程不稳定甚至发散。通过将像素值缩放到 -1 到 1 的范围，可以将梯度控制在较小的范围内，提高优化算法的数值稳定性，使模型更容易收敛。
    * 数据分布一致性：将像素值缩放到 -1 到 1 的范围可以使不同图像之间的像素分布更加一致。这样做的目的是确保输入数据的统计特性在整个训练集上是相似的，从而提高模型的泛化能力。
    * 推广：Batch Normalization (BN) 层作用类似，但是应用在**训练阶段**，对每个小批量数据进行标准化

# 损失函数
## 交叉熵

* 熵：阿根廷 1/4概率打进决赛 ，1/2 概率获得冠军，1/8 获得冠军，则有 f(1/8) = f(1/2) + f(1/4)，f(x) := 信息量，推出可能的 f(x) := -log(x) （log 2为底单调上升，加负号才则单调向下）
* 交叉熵：KL 散度是一种用于衡量两个概率分布之间差异的度量，KL(P || Q) = Σ(P(i) * log(P(i) / Q(i)))，固定分布 P 的时候 KL 散度可以化简为交叉熵 KL(P || Q) = Σ(P(i) * log(P(i) / Q(i))) = -Σ(P(i) * log(Q(i))) = -H(P, Q)；可以很好的用于机器学习损失计算

### 问题
* 回归跟分类区别？
    * 分类例子：识别图片是猫还是狗
    * 回归例子：通过特征1-n预测房价
    * 思考：分类跟回归的区别是目标的 离散跟连续 区别？还是说输出的label之间是否有“距离度量”？
* 为什么交叉熵适合分类，而 MSE 适合回归?
    * 交叉熵
        * 概率解释性：交叉熵基于概率分布之间的差异进行度量，更适合分类问题，因为分类问题通常涉及对不同类别的概率分布进行建模和预测。
        * 梯度更强烈：相对于MSE，交叉熵的梯度更加陡峭，这可以加快模型的收敛速度。对于分类问题，更快的收敛速度可能是一个优势。
    * MES
        * 数学上的合理性：MSE 是对预测值与真实值的差异的平方进行度量，可以提供对预测误差的较为精确的度量。
        * 对异常值不敏感：平方差的计算使得 MSE 对异常值不敏感，因为平方操作会放大异常值的影响。这在某些回归问题中可能是有益的。

Reference
* [王木头学科学](https://www.youtube.com/@wkaing)
* https://zhuanlan.zhihu.com/p/104130889
* [回归与分类问题区别](https://cloud.tencent.com/developer/article/1604194)
# Transformer 

## positional encoding
位置编码的要求：选择正弦跟余弦组合编码
* 每个位置都有唯一的编码。
* 在不同长度的句子中，两个时间步之间的距离应该一致。
* 模型不受句子长短的影响，并且编码范围是有界的。（不会随着句子加长数字就无限增大）
* 必须是确定性的。

总结
* 问题及其解答：
    * 为什么没有直接使用 1,2,3...这种线性编码？
        * 原因：周期性模式在位置编码中的不同维度上呈现出不同的变化速度和周期（下面例子会说明）
            * 捕捉长距离依赖关系（线性模式也能做到，但是不够精细）
            * 提供更丰富的表示能力：较低频率的维度具有较长的周期，可以捕捉到大范围的序列结构，而较高频率的维度可以更细致地表示局部模式和短距离的依赖关系。
            * 避免过拟合：随着句子变长，这些值可能会变得特别大，并且我们的模型可能会遇到比训练时更长的句子
* 思考例子：
    * 第一个词编码为 [1,2,3]， 则位置可用向量 [秒，分，时]来表示；第二个词编码为 [4,5,6]， 则位置可用向量 [秒 + 1，分 + 1/60，时 + 1/360] 来表示
    * 周期：在一个词向量上会出现不同的周期变化，能同时追踪近距离跟远距离的词关系：秒针走一个周期 60 秒，分针走一步；分走一个周期 60 分， 时针+1；
    * 周期设定：通过设定 秒，分，时之间的周期关系（比如可以设定600秒，分针才走一步，则会拉上周期变化，追踪更远的词关系）

Reference
* [positional encoding blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
* [positional encoding stackexchange + youtube](https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model)

## self-attention

思考
* 多头注意力机制与卷积的多通道（channel）进行类比。多头注意力机制和卷积的多通道都涉及并行地学习不同的特征表示。它们都致力于提取输入数据的多样化特征，并捕捉输入中的不同模式和关联性。

Reference
* [self-attention](https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-self-attention-f5fb363c736d)
* [multi-head attention in transformer](https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-multi-head-attention-3bc3409c4312))


# one hot 编码
One-hot 编码是一种将离散的分类标签转换为二进制向量的方法，它的优点是可以消除不同类别之间的偏序关系，使得特征之间的距离计算更加合理。（方便在机器学习分类任务计算 LOSS）
## 例子
比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。

两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。

那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。
## 实现
```python
# 假设 text 中的字符集是由大小写字母和数字组成的，共有 62 个字符（例如，char_set = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"），则 self.char_set_len 的值为 62。对于每个字符 ch，它在字符集中的索引位置是唯一的，因此 i * self.char_set_len + self.char_set.index(ch) 的结果也是唯一的。
vector = np.zeros(self.max_captcha * self.char_set_len) # shape = [max_captcha*36]
for i, ch in enumerate(text):
    idx = i * self.char_set_len + self.char_set.index(ch) # idx = (0-(max_captcha-1))*36+(0-36)
    vector[idx] = 1
```
思考：如果是不定长编码，该如何改进?

* [机器学习：数据预处理之独热编码（One-Hot）](https://zhuanlan.zhihu.com/p/39012149)
## CRNN + CTC 解决变长文本识别

### CTC 如何计算预测序列跟目标序列的相识度？
1. 生成 time step 的预测概率（有 CRNN 模型输出），每个 time step 会有对类别的预测概率向量
2. 通过算法将所有可能路径的概率相加，并取对数（通常用于数值稳定性）得到最终的相似度分数。
[Reference](https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI)
# 深度学习编程范式
Tensorflow vs Pytorch（符号式（也叫声明式）与命令式程序）
命令式
* 更加灵活：原生语言的灵活性跟运行时断点
```python
    import numpy as np
    a = np.ones(10)
    b = np.ones(10) * 2
    c = b * a
    d = c + 1
```
对应符号式（DSL）：
* 节省内存：掌控全局的内存分析并优化
```python
    A = Variable('A')
    B = Variable('B')

    # 当执行 C = B * A 时，不会发生任何计算。相反，此操作会生成表示计算的计算图（也称为符号图）
    C = B * A 
    D = C + Constant(1)

    # compiles the function 并真正的执行计算结果
    f = compile(D) 
    d = f(A=np.ones(10), B=np.ones(10)*2)
```
符号图：![符号图](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/prog_model/comp_graph.png)

类比：类似 react jsx命令式（直接难优化）模板跟 vue 的声明式（真正执行前能做各种运行时优化）模板？

[Reference blog](https://mxnet.apache.org/versions/1.9.1/api/architecture/program_model#:~:text=Symbolic%20Programs%20Tend%20to%20be,flow%20of%20a%20host%20language.)

# 机器学习分类的一点技巧
* 对数据进行分类
    * 有特征：直接通过已经有的分类进行绘制图
        * 先通过特征维度绘制图；例如：数据集仅包含两个分离相当明显的聚类。其中一个簇包含 Iris setosa，而另一个簇包含 Iris virginica 和 Iris versicolor；通过特征绘制出的图会分成明显2堆，其中一堆是交错2种类型 Iris
        * 如果没有明显的聚类，并且数据维度多，可以通过 PCA 等方式降维后再分
    * 无特征：则先通过 KMeans 能方式聚类，再通过有特征方式分析
# 尝试理解 ONNX （Open Neural Network Exchange）
* 是什么？ONNX = （模型本身 + 模型训练好的权重跟偏置）的一种更加抽象的表达
* 如何表示？使用预定义的 operator（描述输入与输出的关系，例如：add算子=  inputA + inputB = OutputC，可拓展）来描述模型，用向量描述训练好的参数
* 作用？实现不同深度学习框架和平台之间的模型互操作性
* 为什么 pytorch 在导出 ONNX 的时候需要传入一组输入？原因：
    * ONNX 并非像编译器一样彻底解析原模型的代码，记录所有控制流；而是不考虑控制流的静态图
    * 而是利用 pytorch trace 机制，将参数传入模型执行，并记录执行这组输入对应的计算图

# 深度学习中的 Epoch 和 Batch
1. Epoch 是什么？ 
一次 Epoch = 让所有数据通过模型正向+反向传播一次 = 一个完整的学习周期
2. Epoch 设置多少次合适？
无定论：
次数少会导致欠拟合；
次数多会导致过拟合；
3. 什么是 Batch？
Batch Size = 一次训练的样本数
每一次参数的更新所需要损失函数并不是由一个数据获得的，而是由一批数据加权得到的
4. Batch 的作用？
* 效率：利用矩阵计算加速（相对于单个去训练）
* 稳定性：平均每个数据样本的贡献，减少梯度的方差
* 多大合适：看情况，太小会导致训练太久；太大会导致内存受不了

Reference
* [epoch-vs-iterations-vs-batch-size](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)

# GPT 尝试
1. 代码尝试
    * 重构转换
        * 输入 js -> ts
    * 优化
        * 格式化代码
    * 创建
        * 创建插件（eslint）步骤：写测试用例 -> 输入 gpt -> 生成插件 -> 微调成型
            * 问题：自动生成的代码会比较繁琐或者隐藏逻辑问题，也不会去利用第三方的包的能力
            * 目前方案：需要 developer 找到更便捷的方式再去投喂给 gpt 生成更加合理简洁的代码

# CNN 的简单理解
* 网络越深，学习的知识越抽象：比如第一层hidden layer负责编码诸如点、线、边缘等浅层信息；第二层hidden layer编码简单点的纹理、形状等信息；第三层hidden layer编码诸如眼睛、鼻子等目标的形状...，然后逐层学习，不断地提取抽象的特征，一气呵成，最终学会了辨识花草树木、飞禽走兽等等。 - [reference](https://zhuanlan.zhihu.com/p/112513743)
    * 网络越宽，每一层学习的知识越丰富：增加网络的宽度意味着同一个hidden layer有着更多的神经元，每一个神经元代表一种颜色，一个方向，一种纹理，组合起来便可以学习到更多不同的颜色信息，各个不同的方向以及不同频率的条纹信息。

# 一句话信息
* 生成对抗网络（GAN） VS 变分自编码器（VAE）： GAN 倾向于生成逼真的合成样本，而 VAE 倾向于生成具有一定程度多样性的样本。如果期望生成特定目标样本，可以考虑 CGAN 跟 CVAE* [GAN 基本原理及其应用](https://easyai.tech/ai-definition/gan/)
* DALL-E uses Discrete Variational Autoencoder (dVAE) for this step. dVAE is a variant of traditional Variational Autoencoder (VAE) that operates in a discrete latent space. It is similar to VQ-VAE but uses distribution instead of nearest neighbor.
* [VIT](https://blog.csdn.net/lsb2002/article/details/135320751) - Google推出了VIT（Vision Transformer）：一个和Bert几乎一致，同时不添加任何卷积结构的图像分类模型。VIT在Transformer上的成功，证明了可以用统一的模型，来处理不同领域（语言/图像/视频）的任务，进而开启了多模态模型研究的新篇章。
    * [vit彻底赢了 CNN 么](https://www.zhihu.com/question/531529633)：transformer全局感受野，在大图片或者说找东西时效果好（类似近视眼，能够感受图像大轮廓）。cnn局部感受野，对细节处理较好（理解像素级别的问题，例如 医疗影像）。
* 文摘 - 高手解决问题的方式从来都不是纠结问题本身，而是升维；升维成功，问题也就解决了
* 雷军2023演讲 - 如何快速学习：知识不全是线性的，大部分是网状的，知识点之间不一定有绝对的先后关系；前面内容看不懂，跳过去，并不影响学后面的；后面的学会了，有时候更容易看懂前面的。

# AGI 的一点理解

机器学习训练了很多模型，而 LLM 只是其中之一；
ChatGPT 之所以跟 AGI 最接近，是因为语言模型的通用性；
如果能够理解自然语言，那就可以实现所有文字能够描述的任务；
其他 AI 任务，比如图片识别，则只能做到图片相似度能人物处理（推荐），无法拓展到相对通用的任务，除非日常交流能够通过表情包完成

# gpt 可能的研究方向

* 建设高难度的综合任务评测数据集（LLM 的测试用例，越完备 -> 越强大）
* 高质量数据工程（密集+多样性）：LLM 进化 = 更多高质量数据
    * 数据例子
        * 密度极高的高质量数据：wiki
        * 高质量问答：quora，知乎
        * 高质量图片：
    * 思考
        * 高质量数据消耗完后 gpt 如何进化？
        * 能否自己创造知识自己消费（类似 alpha-go 自我对弈的进化）？
        * 如果 gpt 成长的资料来源于人类，那能否突破人类知识的边界？
* 探索 LLM 模型的规模天花板：大模型大数据，能参与的玩家不多
    * 思考：是否会出现共建超大模型
* 增强 LLM 的复杂推理能力
* LLM 纳入 NLP之外更多其它研究领域：多模态？
    * 如何突破符号领域？如果某个领域是非成文的，不能用符号记录表达，那么 GPT 是否就无能为力。比如，人类的很多心理活动、潜意识、灵感、顿悟等等，GPT 如何模拟生成。
* 更易用的人和LLM的交互接口：听觉？
* 超大LLM模型Transformer的稀疏化：相同算力下提高训练速度

参考
* [通向AGI之路：大型语言模型（LLM）技术精要](https://zhuanlan.zhihu.com/p/597586623)
# 关于 chatGPT 引发的人工智能思考 2023-3-1

* 人跟AI的关系：淘汰还是互补？
    * 人有自主目的性（AI暂无），AI是实现目的的工具；
* 提问跟回答能力，哪个更能生存下来？
    * 往后提出好问题能力的重要性将越来越超过回答问题能力
* 教育
    * 投喂答案的教育模式需要变革 -> 把提问能力列入考核标准，更能培养出人机协作人才
    * 让人利用机器，而不是把人培养成机器
    * 文理分科这种教育模式急迫需要改变：chatGPT 需要文理结合，提好问题，同时认清答案

# chatGPT 衍生的未来职业？（更新 2023-3-13，[参考](https://www.youtube.com/watch?v=UsaZhQ9bY2k)）

场景跟问题
* 更精准提出需求，才能利用好 chatGPT
* 辅助 chatGPT 修正回答错误，同时又不影响模型输出的其他答案
* 检测回答是否由机器生成
* 如何避免 chatGPT 泄密，如何做隐私保护（目前可以 chatGPT 被催眠然后突破本身不泄密的限制）
* AI 训练，避免伦理问题
* AI 本身安全：解决提示注入（类似 网页的 xss ，SQL 注入等），越狱等安全问题
* 知识产权重新定义：AI生成的东西到底算不算侵权？（例如之前的爬别人网站的数据作为自身的商业盈利依据，是否算侵权？）
* 趋势预测
    * 动作（运动，游戏等数据）文件化：可以对整场羽毛球做文字标记序列化，然后输入 chatGPT，最后可以预测落点跟个人行为

职业名？
* 标注师：标记信息，投喂并训练 AI
* 安全员：确保 AI 不被攻破

gpt自己的回答
* 提示工程师：提示是一种指导GPT-4生成内容的文本或图像，通常包含一些特殊的符号或指令。提示工程师就是专门设计和优化提示的人员，他们需要了解GPT-4的内部机制和逻辑，以及不同领域和场景下用户的需求和偏好。提示工程师可以为各种应用场景提供高质量、高效率、高安全性的提示服务。
* 内容审核员：虽然GPT-4具有强大的生成能力，但它也可能会产生一些不合适或有害的内容，如色情、暴力、歧视、谣言等。内容审核员就是负责检查和过滤GPT-4生成内容中是否存在这些问题，并及时删除或修改不良内容。内容审核员需要具备一定的专业知识和判断能力，以及良好的道德素养和责任心。
* 内容运营师：内容运营师是利用GPT-4为各种平台和渠道提供优质内容服务的人员，他们需要根据目标受众和市场需求，选择合适的提示和参数来调用GPT-4生成相应类型和风格的内容，并进行编辑、优化和发布。内容运营师需要具备一定的创意思维和文案能力，以及对各种媒体平台和行业动态有一定了解。
* 内容创作者：内容创作者是利用GPT-4辅助自己进行创作活动的人员，他们可以将自己想要表达或传达给用户
