# ControlNet 与 LoRA 插件
ControlNet 和 LoRA 并不是完全相同的东西，尽管它们都与神经网络和图像生成有关。

ControlNet：
  * 作用：用于控制 AI 图像生成。
  * 特点：允许用户对生成的图像进行精细的控制。
  * 应用：在计算机视觉、艺术设计、虚拟现实等领域中非常有用。
  * 示例：用户可以上传线稿，让 AI 帮助填色渲染、控制人物姿态等。

LoRA：
  * 作用：用于大模型参数高效微调。
  * 特点：通过降维和升维来模拟参数的更新量，从而减少训练成本。
  * 应用：在参数量较大的模型微调中表现优异。

# ControlNet

ControlNet 是一个神经网络架构，用于控制 Stable Diffusion（SD）模型并扩展其输入条件。

作用：
* 可控性提升：ControlNet 允许创作者通过添加额外的控制条件来引导 SD 模型生成图像，从而提高 AI 图像生成的可控性。
* 多样性：它支持多种输入条件，如 Canny 边缘、语义分割图、关键点、涂鸦等，拓展了 SD 的能力边界。

重要性：
* 创作自由度：ControlNet 让创作者更精细地控制生成图像的元素，包括主体、背景、风格等，从而实现更好的创作自由度。
* 商业应用：可控的 AI 绘画对商业落地具有重要意义，例如广告、设计、艺术等领域。

* [ControlNet 论文解析](https://juejin.cn/post/7210369671656505399)

# LoRA（Low-Rank Adaptation）低秩适应
LoRA 是一种用于大模型参数高效微调的方法。

## 与低秩分解的区别

* 插入位置：
  * LoRA是以残差连接的形式“并联”在Transformer的Q、K、V、O矩阵上。
  * 低秩分解通常是在全连接层后面插入适配器。
* 推理延迟：
  * LoRA在训练完后，其参数可以与原有预训练模型直接合并，变回单分支结构，不会引入额外的延迟。
  * 适配器由于引入了额外的串联网络层，会带来推理时间的增加。

LoRA通过只更新模型权重的小部分来提高效率，而适配器则是插入在不同位置，具有一些不同的特点。

## 降维和升维
降维 (Dimensionality Reduction)：
  * 在原始预训练语言模型（PLM）旁边增加一个旁路。
  * 通过低秩分解来模拟参数的更新量。
  * 降维通常包括两个步骤：降维矩阵 A 和升维矩阵 B。
  * 在训练过程中，原模型的参数保持固定，只训练降维矩阵 A 和升维矩阵 B。
  * 这样可以显著减少训练成本，同时保持模型的性能。
  
升维 (Dimensionality Expansion)：
  * 模型的输入输出维度不变。
  * 输出时将 BA 与 PLM 的参数叠加。
  * 用随机高斯分布初始化 A，用 0 矩阵初始化 B，保证训练的开始此旁路矩阵依然是 0 矩阵。
  * LoRA 允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。这种方法在参数量较全参数微调（Fine-Tuning）显著降低的同时，性能优于其他参数高效微调方法。

# Reference

* GPT